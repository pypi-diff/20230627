# Comparing `tmp/primeqa-0.8.1-py3-none-any.whl.zip` & `tmp/primeqa-0.9.7-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,238 +1,290 @@
-Zip file size: 270508 bytes, number of entries: 236
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/boolqa/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/boolqa/boolqa.py
--rw-r--r--  2.0 unx    14204 b- defN 22-Aug-17 14:21 primeqa/boolqa/run_boolqa_classifier.py
--rw-r--r--  2.0 unx     1651 b- defN 22-Aug-17 14:21 primeqa/boolqa/run_score_normalizer.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/dataset/__init__.py
--rw-r--r--  2.0 unx     2725 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/dataset/mrc2dataset.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/postprocessors/__init__.py
--rw-r--r--  2.0 unx     4150 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/postprocessors/boolqa_classifier.py
--rw-r--r--  2.0 unx     2287 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/postprocessors/extractive.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/preprocessors/__init__.py
--rw-r--r--  2.0 unx     4559 b- defN 22-Jul-05 16:32 primeqa/boolqa/processors/preprocessors/boolqa_classifier.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/boolqa/score_normalizer/__init__.py
--rw-r--r--  2.0 unx     3586 b- defN 22-Jul-05 16:32 primeqa/boolqa/score_normalizer/score_normalizer.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/calibration/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/calibration/calibration.py
--rw-r--r--  2.0 unx    11179 b- defN 22-Jul-05 16:32 primeqa/calibration/confidence_scorer.py
--rw-r--r--  2.0 unx    24873 b- defN 22-Aug-17 14:21 primeqa/calibration/train_confidence_calibrator.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/distillation/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/distillation/distillation.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/ir.py
--rw-r--r--  2.0 unx     4340 b- defN 22-Aug-17 14:21 primeqa/ir/run_bm25_retrieval.py
--rw-r--r--  2.0 unx     6844 b- defN 22-Aug-17 14:21 primeqa/ir/run_ir.py
--rw-r--r--  2.0 unx        2 b- defN 22-Jul-05 16:32 primeqa/ir/dense/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/__init__.py
--rw-r--r--  2.0 unx      555 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/setup.py
--rw-r--r--  2.0 unx      137 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/__init__.py
--rw-r--r--  2.0 unx      303 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/index.py
--rw-r--r--  2.0 unx     3457 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/indexer.py
--rw-r--r--  2.0 unx      571 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/parameters.py
--rw-r--r--  2.0 unx     1407 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/run_indexer.py
--rw-r--r--  2.0 unx     1783 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/run_searcher.py
--rw-r--r--  2.0 unx     1899 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/run_trainer.py
--rw-r--r--  2.0 unx     4285 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/searcher.py
--rw-r--r--  2.0 unx     1409 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/trainer.py
--rw-r--r--  2.0 unx      102 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/data/__init__.py
--rw-r--r--  2.0 unx     3386 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/data/collection.py
--rw-r--r--  2.0 unx      435 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/data/dataset.py
--rw-r--r--  2.0 unx     2914 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/data/examples.py
--rw-r--r--  2.0 unx     4614 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/data/queries.py
--rw-r--r--  2.0 unx     2984 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/data/ranking.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/distillation/__init__.py
--rw-r--r--  2.0 unx     1843 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/distillation/ranking_scorer.py
--rw-r--r--  2.0 unx     2562 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/distillation/scorer.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/evaluation/__init__.py
--rw-r--r--  2.0 unx     1034 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/evaluation/load_model.py
--rw-r--r--  2.0 unx     7260 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/evaluation/loaders.py
--rw-r--r--  2.0 unx     4447 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/evaluation/metrics.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/indexing/__init__.py
--rw-r--r--  2.0 unx     1931 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/indexing/collection_encoder.py
--rw-r--r--  2.0 unx    18291 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/indexing/collection_indexer.py
--rw-r--r--  2.0 unx      916 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/indexing/index_manager.py
--rw-r--r--  2.0 unx     2015 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/indexing/index_saver.py
--rw-r--r--  2.0 unx     2061 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/indexing/loaders.py
--rw-r--r--  2.0 unx     1875 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/indexing/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/__init__.py
--rw-r--r--  2.0 unx    11631 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual.py
--rw-r--r--  2.0 unx     3200 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings.py
--rw-r--r--  2.0 unx     1407 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings_strided.py
--rw-r--r--  2.0 unx       41 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/__init__.py
--rw-r--r--  2.0 unx     4914 b- defN 22-Sep-08 14:23 primeqa/ir/dense/colbert_top/colbert/infra/launcher.py
--rw-r--r--  2.0 unx      974 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/provenance.py
--rw-r--r--  2.0 unx     2691 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/run.py
--rw-r--r--  2.0 unx       46 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/config/__init__.py
--rw-r--r--  2.0 unx     4456 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/config/base_config.py
--rw-r--r--  2.0 unx      360 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/config/config.py
--rw-r--r--  2.0 unx     2425 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/config/core_config.py
--rw-r--r--  2.0 unx     5453 b- defN 22-Sep-08 14:23 primeqa/ir/dense/colbert_top/colbert/infra/config/settings.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/utilities/__init__.py
--rw-r--r--  2.0 unx     2050 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/utilities/create_triples.py
--rw-r--r--  2.0 unx     2525 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/infra/utilities/minicorpus.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/__init__.py
--rw-r--r--  2.0 unx     2276 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/base_colbert.py
--rw-r--r--  2.0 unx     6495 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/checkpoint.py
--rw-r--r--  2.0 unx    10107 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/modeling/colbert.py
--rw-r--r--  2.0 unx     4865 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/factory.py
--rw-r--r--  2.0 unx     3195 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert.py
--rw-r--r--  2.0 unx     3529 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert_xlmr.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/reranker/__init__.py
--rw-r--r--  2.0 unx     1306 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/reranker/electra.py
--rw-r--r--  2.0 unx      623 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/reranker/tokenizer.py
--rw-r--r--  2.0 unx      280 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization.py
--rw-r--r--  2.0 unx     3203 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization_xlmr.py
--rw-r--r--  2.0 unx     4466 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization.py
--rw-r--r--  2.0 unx     5064 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization_xlmr.py
--rw-r--r--  2.0 unx     2117 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/ranking/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/search/__init__.py
--rw-r--r--  2.0 unx     2075 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/search/candidate_generation.py
--rw-r--r--  2.0 unx     2869 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/search/index_loader.py
--rw-r--r--  2.0 unx     8520 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/search/index_storage.py
--rw-r--r--  2.0 unx     7419 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/search/strided_tensor.py
--rw-r--r--  2.0 unx     4109 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/search/strided_tensor_core.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/training/__init__.py
--rw-r--r--  2.0 unx     4511 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/training/eager_batcher_v2.py
--rw-r--r--  2.0 unx     3386 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/training/lazy_batcher.py
--rw-r--r--  2.0 unx     2859 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/training/rerank_batcher.py
--rw-r--r--  2.0 unx    25533 b- defN 22-Sep-08 14:23 primeqa/ir/dense/colbert_top/colbert/training/training.py
--rw-r--r--  2.0 unx     5941 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/training/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utilities/__init__.py
--rw-r--r--  2.0 unx     2660 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utilities/minicorpus.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utils/__init__.py
--rw-r--r--  2.0 unx     1124 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utils/amp.py
--rw-r--r--  2.0 unx     1087 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utils/distributed.py
--rw-r--r--  2.0 unx     3444 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utils/logging.py
--rw-r--r--  2.0 unx     9405 b- defN 22-Sep-08 14:23 primeqa/ir/dense/colbert_top/colbert/utils/parser.py
--rw-r--r--  2.0 unx     3694 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utils/runs.py
--rw-r--r--  2.0 unx      929 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/colbert/utils/signals.py
--rw-r--r--  2.0 unx     9481 b- defN 22-Aug-17 14:21 primeqa/ir/dense/colbert_top/colbert/utils/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/evaluate/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/preprocess/__init__.py
--rw-r--r--  2.0 unx     5171 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/preprocess/docs2passages.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/rankings/__init__.py
--rw-r--r--  2.0 unx     1455 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/rankings/dev_subsample.py
--rw-r--r--  2.0 unx     1726 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/rankings/merge.py
--rw-r--r--  2.0 unx     1378 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/rankings/split_by_offset.py
--rw-r--r--  2.0 unx     1832 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/rankings/split_by_queries.py
--rw-r--r--  2.0 unx     1865 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/rankings/tune.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/supervision/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/utils/__init__.py
--rw-r--r--  2.0 unx      788 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/utils/qa_loaders.py
--rw-r--r--  2.0 unx     2196 b- defN 22-Jul-05 16:32 primeqa/ir/dense/colbert_top/utility/utils/save_metadata.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/sparse/__init__.py
--rw-r--r--  2.0 unx     1957 b- defN 22-Jul-05 16:32 primeqa/ir/sparse/bm25_engine.py
--rw-r--r--  2.0 unx     1649 b- defN 22-Jul-05 16:32 primeqa/ir/sparse/config.py
--rw-r--r--  2.0 unx     4052 b- defN 22-Jul-05 16:32 primeqa/ir/sparse/indexer.py
--rw-r--r--  2.0 unx     4659 b- defN 22-Jul-05 16:32 primeqa/ir/sparse/retriever.py
--rw-r--r--  2.0 unx     2504 b- defN 22-Jul-05 16:32 primeqa/ir/sparse/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/ir/util/__init__.py
--rw-r--r--  2.0 unx     2472 b- defN 22-Jul-05 16:32 primeqa/ir/util/corpus_reader.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/__init__.py
--rw-r--r--  2.0 unx    24418 b- defN 22-Sep-08 14:23 primeqa/mrc/run_mrc.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/data_models/__init__.py
--rw-r--r--  2.0 unx      578 b- defN 22-Jul-05 16:32 primeqa/mrc/data_models/eval_prediction_with_processing.py
--rw-r--r--  2.0 unx      302 b- defN 22-Jul-05 16:32 primeqa/mrc/data_models/subsample_type.py
--rw-r--r--  2.0 unx      677 b- defN 22-Jul-05 16:32 primeqa/mrc/data_models/target_type.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/data_models/model_outputs/__init__.py
--rw-r--r--  2.0 unx     1412 b- defN 22-Jul-05 16:32 primeqa/mrc/data_models/model_outputs/extractive.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Sep-08 14:23 primeqa/mrc/metrics/nq_f1/__init__.py
--rw-r--r--  2.0 unx    16436 b- defN 22-Sep-08 14:23 primeqa/mrc/metrics/nq_f1/eval_utils.py
--rw-r--r--  2.0 unx    18258 b- defN 22-Sep-08 14:23 primeqa/mrc/metrics/nq_f1/nq_eval.py
--rw-r--r--  2.0 unx     6470 b- defN 22-Sep-08 14:23 primeqa/mrc/metrics/nq_f1/nq_f1.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/squad/__init__.py
--rw-r--r--  2.0 unx     3399 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/squad/evaluate.py
--rw-r--r--  2.0 unx     4814 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/squad/squad.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/tydi_f1/__init__.py
--rw-r--r--  2.0 unx     8405 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/tydi_f1/eval_utils.py
--rw-r--r--  2.0 unx    23329 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/tydi_f1/tydi_eval.py
--rw-r--r--  2.0 unx     6501 b- defN 22-Jul-05 16:32 primeqa/mrc/metrics/tydi_f1/tydi_f1.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/models/__init__.py
--rw-r--r--  2.0 unx     5459 b- defN 22-Jul-05 16:32 primeqa/mrc/models/task_model.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/models/heads/__init__.py
--rw-r--r--  2.0 unx     1102 b- defN 22-Jul-05 16:32 primeqa/mrc/models/heads/abstract.py
--rw-r--r--  2.0 unx    14847 b- defN 22-Jul-05 16:32 primeqa/mrc/models/heads/extractive.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/postprocessors/__init__.py
--rw-r--r--  2.0 unx     1524 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/postprocessors/abstract.py
--rw-r--r--  2.0 unx    15348 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/postprocessors/extractive.py
--rw-r--r--  2.0 unx     4092 b- defN 22-Sep-08 14:23 primeqa/mrc/processors/postprocessors/natural_questions.py
--rw-r--r--  2.0 unx     3891 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/postprocessors/scorers.py
--rw-r--r--  2.0 unx     2321 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/postprocessors/squad.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/preprocessors/__init__.py
--rw-r--r--  2.0 unx     6760 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/preprocessors/abstract.py
--rw-r--r--  2.0 unx    20677 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/preprocessors/base.py
--rw-r--r--  2.0 unx    11476 b- defN 22-Sep-08 14:23 primeqa/mrc/processors/preprocessors/natural_questions.py
--rw-r--r--  2.0 unx     2287 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/preprocessors/squad.py
--rw-r--r--  2.0 unx     5041 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/preprocessors/tydiqa.py
--rw-r--r--  2.0 unx     5849 b- defN 22-Jul-05 16:32 primeqa/mrc/processors/preprocessors/tydiqa_google.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/mrc/trainers/__init__.py
--rw-r--r--  2.0 unx    12694 b- defN 22-Aug-17 14:21 primeqa/mrc/trainers/mrc.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/pipelines/__init__.py
--rw-r--r--  2.0 unx     2774 b- defN 22-Aug-17 14:21 primeqa/pipelines/extractive_mrc_pipeline.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/__init__.py
--rw-r--r--  2.0 unx     8722 b- defN 22-Aug-17 14:21 primeqa/qg/run_qg.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/models/__init__.py
--rw-r--r--  2.0 unx     4053 b- defN 22-Aug-17 14:21 primeqa/qg/models/qg_model.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/models/passage_qg/__init__.py
--rw-r--r--  2.0 unx     3970 b- defN 22-Aug-17 14:21 primeqa/qg/models/passage_qg/answer_sampler.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/models/table_qg/__init__.py
--rw-r--r--  2.0 unx    19227 b- defN 22-Aug-17 14:21 primeqa/qg/models/table_qg/sql_sampler.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/processors/__init__.py
--rw-r--r--  2.0 unx     1865 b- defN 22-Aug-17 14:21 primeqa/qg/processors/data_loader.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/processors/passage_qg/__init__.py
--rw-r--r--  2.0 unx      638 b- defN 22-Aug-17 14:21 primeqa/qg/processors/passage_qg/squad_processor.py
--rw-r--r--  2.0 unx      598 b- defN 22-Aug-17 14:21 primeqa/qg/processors/passage_qg/tydiqa_processor.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/processors/table_qg/__init__.py
--rw-r--r--  2.0 unx     4516 b- defN 22-Aug-17 14:21 primeqa/qg/processors/table_qg/wikisql_processor.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/trainers/__init__.py
--rw-r--r--  2.0 unx      706 b- defN 22-Aug-17 14:21 primeqa/qg/trainers/qg_trainer.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/qg/utils/__init__.py
--rw-r--r--  2.0 unx      406 b- defN 22-Aug-17 14:21 primeqa/qg/utils/constants.py
--rw-r--r--  2.0 unx      977 b- defN 22-Aug-17 14:21 primeqa/qg/utils/data_collator.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/re2g/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/re2g/re2g.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/tableqa/__init__.py
--rw-r--r--  2.0 unx     4536 b- defN 22-Aug-17 14:21 primeqa/tableqa/run_tableqa.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/tableqa/metrics/__init__.py
--rw-r--r--  2.0 unx      675 b- defN 22-Aug-17 14:21 primeqa/tableqa/metrics/answer_accuracy.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/tableqa/models/__init__.py
--rw-r--r--  2.0 unx     2704 b- defN 22-Aug-17 14:21 primeqa/tableqa/models/tableqa_model.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/tableqa/postprocessor/__init__.py
--rw-r--r--  2.0 unx     2405 b- defN 22-Aug-17 14:21 primeqa/tableqa/postprocessor/wikisql.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/tableqa/preprocessors/__init__.py
--rw-r--r--  2.0 unx    15289 b- defN 22-Aug-17 14:21 primeqa/tableqa/preprocessors/convert_to_sqa_format.py
--rw-r--r--  2.0 unx     3511 b- defN 22-Aug-17 14:21 primeqa/tableqa/preprocessors/dataset.py
--rw-r--r--  2.0 unx     5003 b- defN 22-Aug-17 14:21 primeqa/tableqa/preprocessors/wikisql_preprocessor.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/tableqa/trainers/__init__.py
--rw-r--r--  2.0 unx     2645 b- defN 22-Aug-17 14:21 primeqa/tableqa/trainers/tableqa_trainer.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-17 14:21 primeqa/tableqa/utils/__init__.py
--rw-r--r--  2.0 unx     1172 b- defN 22-Aug-17 14:21 primeqa/tableqa/utils/data_collator.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/tableqg/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/tableqg/tableqg.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/util/__init__.py
--rw-r--r--  2.0 unx     3934 b- defN 22-Jul-05 16:32 primeqa/util/args_helper.py
--rw-r--r--  2.0 unx     8906 b- defN 22-Jul-05 16:32 primeqa/util/file_utils.py
--rw-r--r--  2.0 unx      766 b- defN 22-Jul-05 16:32 primeqa/util/metrics.py
--rw-r--r--  2.0 unx     6389 b- defN 22-Jul-05 16:32 primeqa/util/reporting.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/util/dataloader/__init__.py
--rw-r--r--  2.0 unx    10324 b- defN 22-Jul-05 16:32 primeqa/util/dataloader/distloader_base.py
--rw-r--r--  2.0 unx    12778 b- defN 22-Jul-05 16:32 primeqa/util/dataloader/distloader_seq_pair.py
--rw-r--r--  2.0 unx     3310 b- defN 22-Jul-05 16:32 primeqa/util/dataloader/file_splitter.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jul-05 16:32 primeqa/util/transformers_utils/__init__.py
--rw-r--r--  2.0 unx     9503 b- defN 22-Jul-05 16:32 primeqa/util/transformers_utils/hypers_base.py
--rw-r--r--  2.0 unx     5633 b- defN 22-Jul-05 16:32 primeqa/util/transformers_utils/model_utils.py
--rw-r--r--  2.0 unx     7817 b- defN 22-Jul-05 16:32 primeqa/util/transformers_utils/optimizer_utils.py
--rw-r--r--  2.0 unx     1387 b- defN 22-Jul-05 16:32 primeqa/util/transformers_utils/torch_utils.py
--rw-r--r--  2.0 unx    11558 b- defN 22-Sep-08 14:24 primeqa-0.8.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    12285 b- defN 22-Sep-08 14:24 primeqa-0.8.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Sep-08 14:24 primeqa-0.8.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 22-Sep-08 14:24 primeqa-0.8.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    24148 b- defN 22-Sep-08 14:24 primeqa-0.8.1.dist-info/RECORD
-236 files, 814129 bytes uncompressed, 230592 bytes compressed:  71.7%
+Zip file size: 328944 bytes, number of entries: 288
+-rw-r--r--  2.0 unx      349 b- defN 22-Nov-10 13:32 primeqa/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/boolqa/__init__.py
+-rw-r--r--  2.0 unx    13846 b- defN 22-Nov-10 13:32 primeqa/boolqa/run_boolqa_classifier.py
+-rw-r--r--  2.0 unx     1614 b- defN 22-Nov-10 13:32 primeqa/boolqa/run_score_normalizer.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/dataset/__init__.py
+-rw-r--r--  2.0 unx     2654 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/dataset/mrc2dataset.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/postprocessors/__init__.py
+-rw-r--r--  2.0 unx     4045 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/postprocessors/boolqa_classifier.py
+-rw-r--r--  2.0 unx     2231 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/postprocessors/extractive.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/preprocessors/__init__.py
+-rw-r--r--  2.0 unx     4445 b- defN 22-Nov-10 13:32 primeqa/boolqa/processors/preprocessors/boolqa_classifier.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/boolqa/score_normalizer/__init__.py
+-rw-r--r--  2.0 unx     3497 b- defN 22-Nov-10 13:32 primeqa/boolqa/score_normalizer/score_normalizer.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/calibration/__init__.py
+-rw-r--r--  2.0 unx    10918 b- defN 22-Nov-10 13:32 primeqa/calibration/confidence_scorer.py
+-rw-r--r--  2.0 unx    24341 b- defN 22-Nov-10 13:32 primeqa/calibration/train_confidence_calibrator.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/distillation/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/distillation/distillation.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/__init__.py
+-rw-r--r--  2.0 unx     4234 b- defN 22-Nov-10 13:32 primeqa/ir/run_bm25_retrieval.py
+-rw-r--r--  2.0 unx     7763 b- defN 22-Nov-10 13:32 primeqa/ir/run_ir.py
+-rw-r--r--  2.0 unx        1 b- defN 22-Nov-10 13:32 primeqa/ir/dense/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/__init__.py
+-rw-r--r--  2.0 unx      538 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/setup.py
+-rw-r--r--  2.0 unx      133 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/__init__.py
+-rw-r--r--  2.0 unx      286 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/index.py
+-rw-r--r--  2.0 unx     3356 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexer.py
+-rw-r--r--  2.0 unx      554 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/parameters.py
+-rw-r--r--  2.0 unx     1374 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/run_indexer.py
+-rw-r--r--  2.0 unx     1518 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/run_searcher.py
+-rw-r--r--  2.0 unx     1860 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/run_trainer.py
+-rw-r--r--  2.0 unx     4504 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/searcher.py
+-rw-r--r--  2.0 unx     1373 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/trainer.py
+-rw-r--r--  2.0 unx       97 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/data/__init__.py
+-rw-r--r--  2.0 unx     3286 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/data/collection.py
+-rw-r--r--  2.0 unx      421 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/data/dataset.py
+-rw-r--r--  2.0 unx     2832 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/data/examples.py
+-rw-r--r--  2.0 unx     4451 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/data/queries.py
+-rw-r--r--  2.0 unx     2960 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/data/ranking.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/distillation/__init__.py
+-rw-r--r--  2.0 unx     1791 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/distillation/ranking_scorer.py
+-rw-r--r--  2.0 unx     2494 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/distillation/scorer.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/evaluation/__init__.py
+-rw-r--r--  2.0 unx     1006 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/evaluation/load_model.py
+-rw-r--r--  2.0 unx     7051 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/evaluation/loaders.py
+-rw-r--r--  2.0 unx     4333 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/evaluation/metrics.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/__init__.py
+-rw-r--r--  2.0 unx     1886 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/collection_encoder.py
+-rw-r--r--  2.0 unx    17860 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/collection_indexer.py
+-rw-r--r--  2.0 unx      878 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/index_manager.py
+-rw-r--r--  2.0 unx     1954 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/index_saver.py
+-rw-r--r--  2.0 unx     1995 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/loaders.py
+-rw-r--r--  2.0 unx     1822 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/__init__.py
+-rw-r--r--  2.0 unx      982 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/decompress_residuals.cpp
+-rw-r--r--  2.0 unx     2982 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/decompress_residuals.cu
+-rw-r--r--  2.0 unx      284 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/packbits.cpp
+-rw-r--r--  2.0 unx     1558 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/packbits.cu
+-rw-r--r--  2.0 unx    11332 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual.py
+-rw-r--r--  2.0 unx     3109 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings.py
+-rw-r--r--  2.0 unx     1377 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings_strided.py
+-rw-r--r--  2.0 unx       40 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/__init__.py
+-rw-r--r--  2.0 unx     4767 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/launcher.py
+-rw-r--r--  2.0 unx      932 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/provenance.py
+-rw-r--r--  2.0 unx     2600 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/run.py
+-rw-r--r--  2.0 unx       45 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/config/__init__.py
+-rw-r--r--  2.0 unx     4334 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/config/base_config.py
+-rw-r--r--  2.0 unx      345 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/config/config.py
+-rw-r--r--  2.0 unx     2339 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/config/core_config.py
+-rw-r--r--  2.0 unx     5430 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/config/settings.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/utilities/__init__.py
+-rw-r--r--  2.0 unx     1998 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/utilities/create_triples.py
+-rw-r--r--  2.0 unx     2461 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/infra/utilities/minicorpus.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/__init__.py
+-rw-r--r--  2.0 unx     2211 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/base_colbert.py
+-rw-r--r--  2.0 unx     6330 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/checkpoint.py
+-rw-r--r--  2.0 unx     9898 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/colbert.py
+-rw-r--r--  2.0 unx     4758 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/factory.py
+-rw-r--r--  2.0 unx     3260 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert.py
+-rw-r--r--  2.0 unx     3648 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert_xlmr.py
+-rw-r--r--  2.0 unx     2782 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/segmented_maxsim.cpp
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/reranker/__init__.py
+-rw-r--r--  2.0 unx     1272 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/reranker/electra.py
+-rw-r--r--  2.0 unx      608 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/reranker/tokenizer.py
+-rw-r--r--  2.0 unx      277 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/__init__.py
+-rw-r--r--  2.0 unx     3126 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization.py
+-rw-r--r--  2.0 unx     3124 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization_xlmr.py
+-rw-r--r--  2.0 unx     4365 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization.py
+-rw-r--r--  2.0 unx     4952 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization_xlmr.py
+-rw-r--r--  2.0 unx     2054 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/ranking/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/__init__.py
+-rw-r--r--  2.0 unx     2012 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/candidate_generation.py
+-rw-r--r--  2.0 unx     5844 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/decompress_residuals.cpp
+-rw-r--r--  2.0 unx     5678 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/filter_pids.cpp
+-rw-r--r--  2.0 unx     2791 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/index_loader.py
+-rw-r--r--  2.0 unx     8332 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/index_storage.py
+-rw-r--r--  2.0 unx     4455 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/segmented_lookup.cpp
+-rw-r--r--  2.0 unx     7197 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/strided_tensor.py
+-rw-r--r--  2.0 unx     3992 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/search/strided_tensor_core.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/training/__init__.py
+-rw-r--r--  2.0 unx     4411 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/training/eager_batcher_v2.py
+-rw-r--r--  2.0 unx     3297 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/training/lazy_batcher.py
+-rw-r--r--  2.0 unx     2784 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/training/rerank_batcher.py
+-rw-r--r--  2.0 unx    25040 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/training/training.py
+-rw-r--r--  2.0 unx     5811 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/training/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utilities/__init__.py
+-rw-r--r--  2.0 unx     2594 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utilities/minicorpus.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/__init__.py
+-rw-r--r--  2.0 unx     1087 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/amp.py
+-rw-r--r--  2.0 unx     1050 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/distributed.py
+-rw-r--r--  2.0 unx     3344 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/logging.py
+-rw-r--r--  2.0 unx     9514 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/parser.py
+-rw-r--r--  2.0 unx     3589 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/runs.py
+-rw-r--r--  2.0 unx      886 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/signals.py
+-rw-r--r--  2.0 unx     9138 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/colbert/utils/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/evaluate/__init__.py
+-rw-r--r--  2.0 unx     3122 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/evaluate/annotate_EM.py
+-rw-r--r--  2.0 unx     2635 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/evaluate/annotate_EM_helpers.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/preprocess/__init__.py
+-rw-r--r--  2.0 unx     5017 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/preprocess/docs2passages.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/rankings/__init__.py
+-rw-r--r--  2.0 unx     1408 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/rankings/dev_subsample.py
+-rw-r--r--  2.0 unx     1669 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/rankings/merge.py
+-rw-r--r--  2.0 unx     1334 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/rankings/split_by_offset.py
+-rw-r--r--  2.0 unx     1765 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/rankings/split_by_queries.py
+-rw-r--r--  2.0 unx     1799 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/rankings/tune.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/supervision/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/utils/__init__.py
+-rw-r--r--  2.0 unx      755 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/utils/qa_loaders.py
+-rw-r--r--  2.0 unx     2129 b- defN 22-Nov-10 13:32 primeqa/ir/dense/colbert_top/utility/utils/save_metadata.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/sparse/__init__.py
+-rw-r--r--  2.0 unx     1946 b- defN 22-Nov-10 13:32 primeqa/ir/sparse/bm25_engine.py
+-rw-r--r--  2.0 unx     1609 b- defN 22-Nov-10 13:32 primeqa/ir/sparse/config.py
+-rw-r--r--  2.0 unx     3958 b- defN 22-Nov-10 13:32 primeqa/ir/sparse/indexer.py
+-rw-r--r--  2.0 unx     4522 b- defN 22-Nov-10 13:32 primeqa/ir/sparse/retriever.py
+-rw-r--r--  2.0 unx     2444 b- defN 22-Nov-10 13:32 primeqa/ir/sparse/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/ir/util/__init__.py
+-rw-r--r--  2.0 unx     2472 b- defN 22-Nov-10 13:32 primeqa/ir/util/corpus_reader.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/logger/__init__.py
+-rw-r--r--  2.0 unx      487 b- defN 22-Nov-10 13:32 primeqa/logger/logging_config.ini
+-rw-r--r--  2.0 unx      489 b- defN 22-Nov-10 13:32 primeqa/logger/verbose_logging_config.ini
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/__init__.py
+-rw-r--r--  2.0 unx    25752 b- defN 22-Nov-10 13:32 primeqa/mrc/run_mrc.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/data_models/__init__.py
+-rw-r--r--  2.0 unx      561 b- defN 22-Nov-10 13:32 primeqa/mrc/data_models/eval_prediction_with_processing.py
+-rw-r--r--  2.0 unx      290 b- defN 22-Nov-10 13:32 primeqa/mrc/data_models/subsample_type.py
+-rw-r--r--  2.0 unx      648 b- defN 22-Nov-10 13:32 primeqa/mrc/data_models/target_type.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/data_models/model_outputs/__init__.py
+-rw-r--r--  2.0 unx     1375 b- defN 22-Nov-10 13:32 primeqa/mrc/data_models/model_outputs/extractive.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/mlqa/__init__.py
+-rw-r--r--  2.0 unx     4164 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/mlqa/mlqa.py
+-rw-r--r--  2.0 unx     5558 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/mlqa/mlqa_evaluation_v1.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/nq_f1/__init__.py
+-rw-r--r--  2.0 unx    16022 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/nq_f1/eval_utils.py
+-rw-r--r--  2.0 unx    17826 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/nq_f1/nq_eval.py
+-rw-r--r--  2.0 unx     6304 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/nq_f1/nq_f1.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/squad/__init__.py
+-rw-r--r--  2.0 unx     3307 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/squad/evaluate.py
+-rw-r--r--  2.0 unx     4699 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/squad/squad.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/tydi_f1/__init__.py
+-rw-r--r--  2.0 unx     8192 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/tydi_f1/eval_utils.py
+-rw-r--r--  2.0 unx    22775 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/tydi_f1/tydi_eval.py
+-rw-r--r--  2.0 unx     6341 b- defN 22-Nov-10 13:32 primeqa/mrc/metrics/tydi_f1/tydi_f1.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/models/__init__.py
+-rw-r--r--  2.0 unx     5328 b- defN 22-Nov-10 13:32 primeqa/mrc/models/task_model.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/models/heads/__init__.py
+-rw-r--r--  2.0 unx     1066 b- defN 22-Nov-10 13:32 primeqa/mrc/models/heads/abstract.py
+-rw-r--r--  2.0 unx    14555 b- defN 22-Nov-10 13:32 primeqa/mrc/models/heads/extractive.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/postprocessors/__init__.py
+-rw-r--r--  2.0 unx     1477 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/postprocessors/abstract.py
+-rw-r--r--  2.0 unx    15065 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/postprocessors/extractive.py
+-rw-r--r--  2.0 unx     4010 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/postprocessors/natural_questions.py
+-rw-r--r--  2.0 unx     3798 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/postprocessors/scorers.py
+-rw-r--r--  2.0 unx     2266 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/postprocessors/squad.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/__init__.py
+-rw-r--r--  2.0 unx     6596 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/abstract.py
+-rw-r--r--  2.0 unx    20245 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/base.py
+-rw-r--r--  2.0 unx     2406 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/mrqa.py
+-rw-r--r--  2.0 unx    11243 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/natural_questions.py
+-rw-r--r--  2.0 unx     2235 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/squad.py
+-rw-r--r--  2.0 unx     4943 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/tydiqa.py
+-rw-r--r--  2.0 unx     5737 b- defN 22-Nov-10 13:32 primeqa/mrc/processors/preprocessors/tydiqa_google.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/mrc/trainers/__init__.py
+-rw-r--r--  2.0 unx    12415 b- defN 22-Nov-10 13:32 primeqa/mrc/trainers/mrc.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/pipelines/__init__.py
+-rw-r--r--  2.0 unx      622 b- defN 22-Nov-10 13:32 primeqa/pipelines/base.py
+-rw-r--r--  2.0 unx      510 b- defN 22-Nov-10 13:32 primeqa/pipelines/qa_pipeline.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/__init__.py
+-rw-r--r--  2.0 unx     1371 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/base.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/indexer/__init__.py
+-rw-r--r--  2.0 unx     3910 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/indexer/dense.py
+-rw-r--r--  2.0 unx      317 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/indexer/sparse.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/reader/__init__.py
+-rw-r--r--  2.0 unx     7856 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/reader/extractive.py
+-rw-r--r--  2.0 unx      533 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/reader/generative.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/retriever/__init__.py
+-rw-r--r--  2.0 unx     3111 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/retriever/dense.py
+-rw-r--r--  2.0 unx     1040 b- defN 22-Nov-10 13:32 primeqa/pipelines/components/retriever/sparse.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/__init__.py
+-rw-r--r--  2.0 unx     9047 b- defN 22-Nov-10 13:32 primeqa/qg/run_qg.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/models/__init__.py
+-rw-r--r--  2.0 unx     4237 b- defN 22-Nov-10 13:32 primeqa/qg/models/qg_model.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/models/passage_qg/__init__.py
+-rw-r--r--  2.0 unx     3896 b- defN 22-Nov-10 13:32 primeqa/qg/models/passage_qg/answer_sampler.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/models/table_qg/__init__.py
+-rw-r--r--  2.0 unx    18761 b- defN 22-Nov-10 13:32 primeqa/qg/models/table_qg/sql_sampler.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/processors/__init__.py
+-rw-r--r--  2.0 unx     2111 b- defN 22-Nov-10 13:32 primeqa/qg/processors/data_loader.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/processors/passage_qg/__init__.py
+-rw-r--r--  2.0 unx     1825 b- defN 22-Nov-10 13:32 primeqa/qg/processors/passage_qg/qg_processor.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/processors/table_qg/__init__.py
+-rw-r--r--  2.0 unx     5472 b- defN 22-Nov-10 13:32 primeqa/qg/processors/table_qg/sql_processor.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/trainers/__init__.py
+-rw-r--r--  2.0 unx      323 b- defN 22-Nov-10 13:32 primeqa/qg/trainers/qg_trainer.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/qg/utils/__init__.py
+-rw-r--r--  2.0 unx      390 b- defN 22-Nov-10 13:32 primeqa/qg/utils/constants.py
+-rw-r--r--  2.0 unx      952 b- defN 22-Nov-10 13:32 primeqa/qg/utils/data_collator.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/services/__init__.py
+-rw-r--r--  2.0 unx      433 b- defN 22-Nov-10 13:32 primeqa/services/application.py
+-rw-r--r--  2.0 unx    10041 b- defN 22-Nov-10 13:32 primeqa/services/configurations.py
+-rw-r--r--  2.0 unx      215 b- defN 22-Nov-10 13:32 primeqa/services/constants.py
+-rw-r--r--  2.0 unx     2307 b- defN 22-Nov-10 13:32 primeqa/services/cred_helpers.py
+-rw-r--r--  2.0 unx     1193 b- defN 22-Nov-10 13:32 primeqa/services/exceptions.py
+-rw-r--r--  2.0 unx     9376 b- defN 22-Nov-10 13:32 primeqa/services/factories.py
+-rw-r--r--  2.0 unx      566 b- defN 22-Nov-10 13:32 primeqa/services/parameters.py
+-rw-r--r--  2.0 unx     9412 b- defN 22-Nov-10 13:32 primeqa/services/store.py
+-rw-r--r--  2.0 unx     1167 b- defN 22-Nov-10 13:32 primeqa/services/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/services/config/__init__.py
+-rw-r--r--  2.0 unx      605 b- defN 22-Nov-10 13:32 primeqa/services/config/config.ini
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/__init__.py
+-rw-r--r--  2.0 unx     9577 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/indexer_service.py
+-rw-r--r--  2.0 unx     7044 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/reader_service.py
+-rw-r--r--  2.0 unx     7967 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/retriever_service.py
+-rw-r--r--  2.0 unx     3334 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/server.py
+-rw-r--r--  2.0 unx     3824 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/__init__.py
+-rw-r--r--  2.0 unx     7845 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/indexer_pb2.py
+-rw-r--r--  2.0 unx     6996 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/indexer_pb2_grpc.py
+-rw-r--r--  2.0 unx     1580 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/parameter_pb2.py
+-rw-r--r--  2.0 unx      159 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/parameter_pb2_grpc.py
+-rw-r--r--  2.0 unx     5972 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/reader_pb2.py
+-rw-r--r--  2.0 unx     3992 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/reader_pb2_grpc.py
+-rw-r--r--  2.0 unx     4943 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/retriever_pb2.py
+-rw-r--r--  2.0 unx     3988 b- defN 22-Nov-10 13:32 primeqa/services/grpc_server/grpc_generated/retriever_pb2_grpc.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/services/rest_server/__init__.py
+-rw-r--r--  2.0 unx     4121 b- defN 22-Nov-10 13:32 primeqa/services/rest_server/data_models.py
+-rw-r--r--  2.0 unx    23908 b- defN 22-Nov-10 13:32 primeqa/services/rest_server/server.py
+-rw-r--r--  2.0 unx     1913 b- defN 22-Nov-10 13:32 primeqa/services/rest_server/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/tableqa/__init__.py
+-rw-r--r--  2.0 unx     4436 b- defN 22-Nov-10 13:32 primeqa/tableqa/run_tableqa.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/tableqa/metrics/__init__.py
+-rw-r--r--  2.0 unx      653 b- defN 22-Nov-10 13:32 primeqa/tableqa/metrics/answer_accuracy.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/tableqa/models/__init__.py
+-rw-r--r--  2.0 unx     2618 b- defN 22-Nov-10 13:32 primeqa/tableqa/models/tableqa_model.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/tableqa/postprocessor/__init__.py
+-rw-r--r--  2.0 unx     2340 b- defN 22-Nov-10 13:32 primeqa/tableqa/postprocessor/wikisql.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/tableqa/preprocessors/__init__.py
+-rw-r--r--  2.0 unx    14844 b- defN 22-Nov-10 13:32 primeqa/tableqa/preprocessors/convert_to_sqa_format.py
+-rw-r--r--  2.0 unx     3426 b- defN 22-Nov-10 13:32 primeqa/tableqa/preprocessors/dataset.py
+-rw-r--r--  2.0 unx     4877 b- defN 22-Nov-10 13:32 primeqa/tableqa/preprocessors/wikisql_preprocessor.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/tableqa/trainers/__init__.py
+-rw-r--r--  2.0 unx     2587 b- defN 22-Nov-10 13:32 primeqa/tableqa/trainers/tableqa_trainer.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/tableqa/utils/__init__.py
+-rw-r--r--  2.0 unx     1144 b- defN 22-Nov-10 13:32 primeqa/tableqa/utils/data_collator.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/util/__init__.py
+-rw-r--r--  2.0 unx     3833 b- defN 22-Nov-10 13:32 primeqa/util/args_helper.py
+-rw-r--r--  2.0 unx     8652 b- defN 22-Nov-10 13:32 primeqa/util/file_utils.py
+-rw-r--r--  2.0 unx      736 b- defN 22-Nov-10 13:32 primeqa/util/metrics.py
+-rw-r--r--  2.0 unx     6234 b- defN 22-Nov-10 13:32 primeqa/util/reporting.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/util/dataloader/__init__.py
+-rw-r--r--  2.0 unx    10083 b- defN 22-Nov-10 13:32 primeqa/util/dataloader/distloader_base.py
+-rw-r--r--  2.0 unx    12521 b- defN 22-Nov-10 13:32 primeqa/util/dataloader/distloader_seq_pair.py
+-rw-r--r--  2.0 unx     3212 b- defN 22-Nov-10 13:32 primeqa/util/dataloader/file_splitter.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Nov-10 13:32 primeqa/util/transformers_utils/__init__.py
+-rw-r--r--  2.0 unx     9276 b- defN 22-Nov-10 13:32 primeqa/util/transformers_utils/hypers_base.py
+-rw-r--r--  2.0 unx     5516 b- defN 22-Nov-10 13:32 primeqa/util/transformers_utils/model_utils.py
+-rw-r--r--  2.0 unx     7654 b- defN 22-Nov-10 13:32 primeqa/util/transformers_utils/optimizer_utils.py
+-rw-r--r--  2.0 unx     1346 b- defN 22-Nov-10 13:32 primeqa/util/transformers_utils/torch_utils.py
+-rw-r--r--  2.0 unx    11357 b- defN 22-Nov-10 14:34 primeqa-0.9.7.dist-info/LICENSE
+-rw-r--r--  2.0 unx    16706 b- defN 22-Nov-10 14:34 primeqa-0.9.7.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 22-Nov-10 14:34 primeqa-0.9.7.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 22-Nov-10 14:34 primeqa-0.9.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    29627 b- defN 22-Nov-10 14:34 primeqa-0.9.7.dist-info/RECORD
+288 files, 1004294 bytes uncompressed, 280006 bytes compressed:  72.1%
```

## zipnote {}

```diff
@@ -1,16 +1,13 @@
 Filename: primeqa/__init__.py
 Comment: 
 
 Filename: primeqa/boolqa/__init__.py
 Comment: 
 
-Filename: primeqa/boolqa/boolqa.py
-Comment: 
-
 Filename: primeqa/boolqa/run_boolqa_classifier.py
 Comment: 
 
 Filename: primeqa/boolqa/run_score_normalizer.py
 Comment: 
 
 Filename: primeqa/boolqa/processors/__init__.py
@@ -42,17 +39,14 @@
 
 Filename: primeqa/boolqa/score_normalizer/score_normalizer.py
 Comment: 
 
 Filename: primeqa/calibration/__init__.py
 Comment: 
 
-Filename: primeqa/calibration/calibration.py
-Comment: 
-
 Filename: primeqa/calibration/confidence_scorer.py
 Comment: 
 
 Filename: primeqa/calibration/train_confidence_calibrator.py
 Comment: 
 
 Filename: primeqa/distillation/__init__.py
@@ -60,17 +54,14 @@
 
 Filename: primeqa/distillation/distillation.py
 Comment: 
 
 Filename: primeqa/ir/__init__.py
 Comment: 
 
-Filename: primeqa/ir/ir.py
-Comment: 
-
 Filename: primeqa/ir/run_bm25_retrieval.py
 Comment: 
 
 Filename: primeqa/ir/run_ir.py
 Comment: 
 
 Filename: primeqa/ir/dense/__init__.py
@@ -168,14 +159,26 @@
 
 Filename: primeqa/ir/dense/colbert_top/colbert/indexing/utils.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/__init__.py
 Comment: 
 
+Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/decompress_residuals.cpp
+Comment: 
+
+Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/decompress_residuals.cu
+Comment: 
+
+Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/packbits.cpp
+Comment: 
+
+Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/packbits.cu
+Comment: 
+
 Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings_strided.py
@@ -234,14 +237,17 @@
 
 Filename: primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert_xlmr.py
 Comment: 
 
+Filename: primeqa/ir/dense/colbert_top/colbert/modeling/segmented_maxsim.cpp
+Comment: 
+
 Filename: primeqa/ir/dense/colbert_top/colbert/modeling/reranker/__init__.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/modeling/reranker/electra.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/modeling/reranker/tokenizer.py
@@ -270,20 +276,29 @@
 
 Filename: primeqa/ir/dense/colbert_top/colbert/search/__init__.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/search/candidate_generation.py
 Comment: 
 
+Filename: primeqa/ir/dense/colbert_top/colbert/search/decompress_residuals.cpp
+Comment: 
+
+Filename: primeqa/ir/dense/colbert_top/colbert/search/filter_pids.cpp
+Comment: 
+
 Filename: primeqa/ir/dense/colbert_top/colbert/search/index_loader.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/search/index_storage.py
 Comment: 
 
+Filename: primeqa/ir/dense/colbert_top/colbert/search/segmented_lookup.cpp
+Comment: 
+
 Filename: primeqa/ir/dense/colbert_top/colbert/search/strided_tensor.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/search/strided_tensor_core.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/colbert/training/__init__.py
@@ -336,14 +351,20 @@
 
 Filename: primeqa/ir/dense/colbert_top/utility/__init__.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/utility/evaluate/__init__.py
 Comment: 
 
+Filename: primeqa/ir/dense/colbert_top/utility/evaluate/annotate_EM.py
+Comment: 
+
+Filename: primeqa/ir/dense/colbert_top/utility/evaluate/annotate_EM_helpers.py
+Comment: 
+
 Filename: primeqa/ir/dense/colbert_top/utility/preprocess/__init__.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/utility/preprocess/docs2passages.py
 Comment: 
 
 Filename: primeqa/ir/dense/colbert_top/utility/rankings/__init__.py
@@ -396,14 +417,23 @@
 
 Filename: primeqa/ir/util/__init__.py
 Comment: 
 
 Filename: primeqa/ir/util/corpus_reader.py
 Comment: 
 
+Filename: primeqa/logger/__init__.py
+Comment: 
+
+Filename: primeqa/logger/logging_config.ini
+Comment: 
+
+Filename: primeqa/logger/verbose_logging_config.ini
+Comment: 
+
 Filename: primeqa/mrc/__init__.py
 Comment: 
 
 Filename: primeqa/mrc/run_mrc.py
 Comment: 
 
 Filename: primeqa/mrc/data_models/__init__.py
@@ -423,14 +453,23 @@
 
 Filename: primeqa/mrc/data_models/model_outputs/extractive.py
 Comment: 
 
 Filename: primeqa/mrc/metrics/__init__.py
 Comment: 
 
+Filename: primeqa/mrc/metrics/mlqa/__init__.py
+Comment: 
+
+Filename: primeqa/mrc/metrics/mlqa/mlqa.py
+Comment: 
+
+Filename: primeqa/mrc/metrics/mlqa/mlqa_evaluation_v1.py
+Comment: 
+
 Filename: primeqa/mrc/metrics/nq_f1/__init__.py
 Comment: 
 
 Filename: primeqa/mrc/metrics/nq_f1/eval_utils.py
 Comment: 
 
 Filename: primeqa/mrc/metrics/nq_f1/nq_eval.py
@@ -501,14 +540,17 @@
 
 Filename: primeqa/mrc/processors/preprocessors/abstract.py
 Comment: 
 
 Filename: primeqa/mrc/processors/preprocessors/base.py
 Comment: 
 
+Filename: primeqa/mrc/processors/preprocessors/mrqa.py
+Comment: 
+
 Filename: primeqa/mrc/processors/preprocessors/natural_questions.py
 Comment: 
 
 Filename: primeqa/mrc/processors/preprocessors/squad.py
 Comment: 
 
 Filename: primeqa/mrc/processors/preprocessors/tydiqa.py
@@ -522,15 +564,51 @@
 
 Filename: primeqa/mrc/trainers/mrc.py
 Comment: 
 
 Filename: primeqa/pipelines/__init__.py
 Comment: 
 
-Filename: primeqa/pipelines/extractive_mrc_pipeline.py
+Filename: primeqa/pipelines/base.py
+Comment: 
+
+Filename: primeqa/pipelines/qa_pipeline.py
+Comment: 
+
+Filename: primeqa/pipelines/components/__init__.py
+Comment: 
+
+Filename: primeqa/pipelines/components/base.py
+Comment: 
+
+Filename: primeqa/pipelines/components/indexer/__init__.py
+Comment: 
+
+Filename: primeqa/pipelines/components/indexer/dense.py
+Comment: 
+
+Filename: primeqa/pipelines/components/indexer/sparse.py
+Comment: 
+
+Filename: primeqa/pipelines/components/reader/__init__.py
+Comment: 
+
+Filename: primeqa/pipelines/components/reader/extractive.py
+Comment: 
+
+Filename: primeqa/pipelines/components/reader/generative.py
+Comment: 
+
+Filename: primeqa/pipelines/components/retriever/__init__.py
+Comment: 
+
+Filename: primeqa/pipelines/components/retriever/dense.py
+Comment: 
+
+Filename: primeqa/pipelines/components/retriever/sparse.py
 Comment: 
 
 Filename: primeqa/qg/__init__.py
 Comment: 
 
 Filename: primeqa/qg/run_qg.py
 Comment: 
@@ -558,24 +636,21 @@
 
 Filename: primeqa/qg/processors/data_loader.py
 Comment: 
 
 Filename: primeqa/qg/processors/passage_qg/__init__.py
 Comment: 
 
-Filename: primeqa/qg/processors/passage_qg/squad_processor.py
-Comment: 
-
-Filename: primeqa/qg/processors/passage_qg/tydiqa_processor.py
+Filename: primeqa/qg/processors/passage_qg/qg_processor.py
 Comment: 
 
 Filename: primeqa/qg/processors/table_qg/__init__.py
 Comment: 
 
-Filename: primeqa/qg/processors/table_qg/wikisql_processor.py
+Filename: primeqa/qg/processors/table_qg/sql_processor.py
 Comment: 
 
 Filename: primeqa/qg/trainers/__init__.py
 Comment: 
 
 Filename: primeqa/qg/trainers/qg_trainer.py
 Comment: 
@@ -585,18 +660,105 @@
 
 Filename: primeqa/qg/utils/constants.py
 Comment: 
 
 Filename: primeqa/qg/utils/data_collator.py
 Comment: 
 
-Filename: primeqa/re2g/__init__.py
+Filename: primeqa/services/__init__.py
+Comment: 
+
+Filename: primeqa/services/application.py
+Comment: 
+
+Filename: primeqa/services/configurations.py
+Comment: 
+
+Filename: primeqa/services/constants.py
+Comment: 
+
+Filename: primeqa/services/cred_helpers.py
+Comment: 
+
+Filename: primeqa/services/exceptions.py
+Comment: 
+
+Filename: primeqa/services/factories.py
+Comment: 
+
+Filename: primeqa/services/parameters.py
+Comment: 
+
+Filename: primeqa/services/store.py
+Comment: 
+
+Filename: primeqa/services/utils.py
+Comment: 
+
+Filename: primeqa/services/config/__init__.py
+Comment: 
+
+Filename: primeqa/services/config/config.ini
+Comment: 
+
+Filename: primeqa/services/grpc_server/__init__.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/indexer_service.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/reader_service.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/retriever_service.py
 Comment: 
 
-Filename: primeqa/re2g/re2g.py
+Filename: primeqa/services/grpc_server/server.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/utils.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/__init__.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/indexer_pb2.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/indexer_pb2_grpc.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/parameter_pb2.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/parameter_pb2_grpc.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/reader_pb2.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/reader_pb2_grpc.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/retriever_pb2.py
+Comment: 
+
+Filename: primeqa/services/grpc_server/grpc_generated/retriever_pb2_grpc.py
+Comment: 
+
+Filename: primeqa/services/rest_server/__init__.py
+Comment: 
+
+Filename: primeqa/services/rest_server/data_models.py
+Comment: 
+
+Filename: primeqa/services/rest_server/server.py
+Comment: 
+
+Filename: primeqa/services/rest_server/utils.py
 Comment: 
 
 Filename: primeqa/tableqa/__init__.py
 Comment: 
 
 Filename: primeqa/tableqa/run_tableqa.py
 Comment: 
@@ -639,20 +801,14 @@
 
 Filename: primeqa/tableqa/utils/__init__.py
 Comment: 
 
 Filename: primeqa/tableqa/utils/data_collator.py
 Comment: 
 
-Filename: primeqa/tableqg/__init__.py
-Comment: 
-
-Filename: primeqa/tableqg/tableqg.py
-Comment: 
-
 Filename: primeqa/util/__init__.py
 Comment: 
 
 Filename: primeqa/util/args_helper.py
 Comment: 
 
 Filename: primeqa/util/file_utils.py
@@ -687,23 +843,23 @@
 
 Filename: primeqa/util/transformers_utils/optimizer_utils.py
 Comment: 
 
 Filename: primeqa/util/transformers_utils/torch_utils.py
 Comment: 
 
-Filename: primeqa-0.8.1.dist-info/LICENSE
+Filename: primeqa-0.9.7.dist-info/LICENSE
 Comment: 
 
-Filename: primeqa-0.8.1.dist-info/METADATA
+Filename: primeqa-0.9.7.dist-info/METADATA
 Comment: 
 
-Filename: primeqa-0.8.1.dist-info/WHEEL
+Filename: primeqa-0.9.7.dist-info/WHEEL
 Comment: 
 
-Filename: primeqa-0.8.1.dist-info/top_level.txt
+Filename: primeqa-0.9.7.dist-info/top_level.txt
 Comment: 
 
-Filename: primeqa-0.8.1.dist-info/RECORD
+Filename: primeqa-0.9.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## primeqa/__init__.py

```diff
@@ -0,0 +1,22 @@
+00000000: 696d 706f 7274 206f 730a 696d 706f 7274  import os.import
+00000010: 206c 6f67 6769 6e67 2e63 6f6e 6669 670a   logging.config.
+00000020: 6672 6f6d 2064 6973 7475 7469 6c73 2e75  from distutils.u
+00000030: 7469 6c20 696d 706f 7274 2073 7472 746f  til import strto
+00000040: 626f 6f6c 0a66 726f 6d20 706b 675f 7265  bool.from pkg_re
+00000050: 736f 7572 6365 7320 696d 706f 7274 2072  sources import r
+00000060: 6573 6f75 7263 655f 6669 6c65 6e61 6d65  esource_filename
+00000070: 0a0a 2320 436f 6e66 6967 7572 6520 6c6f  ..# Configure lo
+00000080: 6767 6572 0a6c 6f67 5f63 6f6e 6669 675f  gger.log_config_
+00000090: 6669 6c65 203d 2072 6573 6f75 7263 655f  file = resource_
+000000a0: 6669 6c65 6e61 6d65 280a 2020 2020 2270  filename(.    "p
+000000b0: 7269 6d65 7161 2e6c 6f67 6765 7222 2c0a  rimeqa.logger",.
+000000c0: 2020 2020 2276 6572 626f 7365 5f6c 6f67      "verbose_log
+000000d0: 6769 6e67 5f63 6f6e 6669 672e 696e 6922  ging_config.ini"
+000000e0: 0a20 2020 2069 6620 7374 7274 6f62 6f6f  .    if strtoboo
+000000f0: 6c28 6f73 2e67 6574 656e 7628 2256 4552  l(os.getenv("VER
+00000100: 424f 5345 222c 2022 4661 6c73 6522 2929  BOSE", "False"))
+00000110: 0a20 2020 2065 6c73 6520 226c 6f67 6769  .    else "loggi
+00000120: 6e67 5f63 6f6e 6669 672e 696e 6922 2c0a  ng_config.ini",.
+00000130: 290a 6c6f 6767 696e 672e 636f 6e66 6967  ).logging.config
+00000140: 2e66 696c 6543 6f6e 6669 6728 6c6f 675f  .fileConfig(log_
+00000150: 636f 6e66 6967 5f66 696c 6529 0a         config_file).
```

## primeqa/boolqa/run_boolqa_classifier.py

 * *Ordering differences only*

```diff
@@ -1,358 +1,358 @@
-#!/usr/bin/env python
-# based loosely on https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
-#
-#
-# coding=utf-8
-# Copyright 2020 The HuggingFace Inc. team. All rights reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-""" Finetuning the library models for n-way classification."""
-# You can also adapt this script on your own text classification task. Pointers for this are left as comments.
-
-import logging
-import os
-import sys
-from dataclasses import dataclass, field
-from sysconfig import is_python_build
-from typing import Optional, List
-import json
-from shutil import move
-from os import path
-
-import numpy as np
-from datasets import Dataset, DatasetDict
-
-
-import transformers
-from transformers import (
-    AutoConfig,
-    AutoModelForSequenceClassification,
-    AutoTokenizer,
-    DataCollatorWithPadding,
-    HfArgumentParser,
-    Trainer,
-    TrainingArguments,
-    default_data_collator,
-)
-from transformers.trainer_utils import is_main_process
-
-from primeqa.boolqa.processors.postprocessors.boolqa_classifier import BoolQAClassifierPostProcessor
-from primeqa.boolqa.processors.preprocessors.boolqa_classifier import BoolQAClassifierPreProcessor
-from primeqa.boolqa.processors.dataset.mrc2dataset import create_dataset_from_run_mrc_output
-
-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
-#check_min_version("4.6.0")
-
-logger = logging.getLogger(__name__)
-
-
-@dataclass
-class DataTrainingArguments:
-    """
-    Arguments pertaining to what data we are going to input our model for training and eval.
-
-    Using `HfArgumentParser` we can turn this class
-    into argparse arguments to be able to specify them on
-    the command line.
-    """
-
-
-    max_seq_length: int = field(
-        default=128,
-        metadata={
-            "help": "The maximum total input sequence length after tokenization. Sequences longer "
-            "than this will be truncated, sequences shorter will be padded."
-        },
-    )
-    overwrite_cache: bool = field(
-        default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
-    )
-    pad_to_max_length: bool = field(
-        default=True,
-        metadata={
-            "help": "Whether to pad all samples to `max_seq_length`. "
-            "If False, will pad the samples dynamically when batching to the maximum length in the batch."
-        },
-    )
-    max_train_samples: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
-            "value if set."
-        },
-    )
-    max_eval_samples: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
-            "value if set."
-        },
-    )
-    max_predict_samples: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "For debugging purposes or quicker training, truncate the number of prediction examples to this "
-            "value if set."
-        },
-    )
-    train_file: Optional[str] = field(
-        default=None, metadata={"help": "A csv or a json file containing the training data."}
-    )
-    validation_file: Optional[str] = field(
-        default=None, metadata={"help": "A csv or a json file containing the validation data."}
-    )
-    test_file: Optional[str] = field(default=None, metadata={"help": "A csv or a json file containing the test data."})
-    balanced: Optional[bool] = field(
-        default=False,
-        metadata={
-            "help": "balance the data if true "
-        },
-    )
-
-
-
-
-@dataclass
-class ModelArguments:
-    """
-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
-    """
-
-    model_name_or_path: str = field(
-        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
-    )
-    sentence2_key: str = field(
-        default=None,
-        metadata={"help": "the field in the input dataset to use as sentence2" }
-    )        
-    drop_label: str = field(
-        default=None,
-        metadata={"help": "dropping label 'no_answer' converts ternary classifier into binary"}  
-    )
-    sentence1_key: str = field(
-        default="question",
-        metadata={"help": "the field in the input dataset to use as sentence1" }
-    )
-    id_key: str = field(
-        default="example_id",
-        metadata={"help": "a field that can be used as a unique identifier of input records"}
-    )
-    label_list: List[str] = field(
-        default=None,
-        metadata={"help": "the labels used by the classifier (order is significant)"}
-    )
-    output_label_prefix: str = field(
-        default=None,
-        metadata={"help": "a prefix used in the output file eval_predictions.json to distinguish fields created by this invocation of the classifier"}
-    )
-    config_name: Optional[str] = field(
-        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
-    )
-    tokenizer_name: Optional[str] = field(
-        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
-    )
-    cache_dir: Optional[str] = field(
-        default=None,
-        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
-    )
-    use_fast_tokenizer: bool = field(
-        default=True,
-        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
-    )
-    model_revision: str = field(
-        default="main",
-        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
-    )
-    use_auth_token: bool = field(
-        default=False,
-        metadata={
-            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
-            "with private models)."
-        },
-    )
-
-
-def create_a_backup_file_if_file_exists(original_file: str):
-    if path.isfile(original_file):
-        backup_file = '%s.bak' % original_file
-        logging.debug('Found a pre-existing file at location %s.  Backing it up to %s' %
-                      (original_file, backup_file))
-        move(original_file, backup_file)
-
-def save_to_json_file(obj_to_save, out_file_path, with_backup=True, indent=4, sort_keys=True):
-    if with_backup:
-        create_a_backup_file_if_file_exists(out_file_path)
-
-    logging.debug('Writing %s as json to file: %s' % (type(obj_to_save), out_file_path))
-    with open(out_file_path, 'w') as outfile:
-        json.dump(obj_to_save, outfile, indent=indent, sort_keys=sort_keys)
-
-
-def main(raw_args):
-    # See all possible arguments in src/transformers/training_args.py
-    # or by passing the --help flag to this script.
-    # We now keep distinct sets of args, for a cleaner separation of concerns.
-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
-    if len(raw_args) == 2 and raw_args[1].endswith(".json"):
-        # If we pass only one argument to the script and it's the path to a json file,
-        # let's parse it to get our arguments.
-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(raw_args[1]))
-    elif len(raw_args) == 1:
-        model_args, data_args, training_args = parser.parse_dict(raw_args[0])
-    else:
-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
-
-    # Setup logging
-    logging.basicConfig(
-        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
-        datefmt="%m/%d/%Y %H:%M:%S",
-        handlers=[logging.StreamHandler(sys.stdout)],
-    )
-    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)
-
-    # Log on each process the small summary:
-    logger.warning(
-        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
-        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
-    )
-    # Set the verbosity to info of the Transformers logger (on main process only):
-    if is_main_process(training_args.local_rank):
-        transformers.utils.logging.set_verbosity_info()
-        transformers.utils.logging.enable_default_handler()
-        transformers.utils.logging.enable_explicit_format()
-    logger.info(f"Training/evaluation parameters {training_args}")
-
-    num_labels = len(model_args.label_list)        
-    logger.info("The following labels are being used, all other instances will be discarded.")
-    logger.info(model_args.label_list)
-
-
-
-
-    # Load pretrained model and tokenizer
-    #
-    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
-    # download model & vocab.
-    config = AutoConfig.from_pretrained(
-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
-        num_labels=num_labels,
-        finetuning_task=model_args.output_label_prefix,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
-    tokenizer = AutoTokenizer.from_pretrained(
-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
-        cache_dir=model_args.cache_dir,
-        use_fast=model_args.use_fast_tokenizer,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
-
-
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
-    # Padding strategy
-    if data_args.pad_to_max_length:
-        padding = "max_length"
-    else:
-        # We will pad later, dynamically at batch creation, to the max sequence length in each batch
-        padding = False
-
-
-    raw_datasets={}
-    raw_datasets['validation']=create_dataset_from_run_mrc_output(data_args.test_file, unpack=False)
-
-    #max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)
-    # load preprocessor
-    preprocessor_class = BoolQAClassifierPreProcessor # TODO task_args.preprocessor
-    preprocessor = preprocessor_class(
-        sentence1_key=model_args.sentence1_key,
-        sentence2_key=model_args.sentence2_key,
-        tokenizer=tokenizer,
-        load_from_cache_file=not data_args.overwrite_cache,
-        max_seq_len=tokenizer.model_max_length,
-        padding=padding
-    )
-
-    # process eval data
-    eval_examples = raw_datasets["validation"]
-    max_eval_samples = data_args.max_eval_samples
-    if max_eval_samples is not None:  # data_args.max_eval_samples is not None:
-        # We will select sample from whole data
-        eval_examples = eval_examples.select(range(max_eval_samples))
-    # Validation Feature Creation
-    with training_args.main_process_first(desc="validation dataset map pre-processing"):
-        eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)
-
-
-    if data_args.max_seq_length > tokenizer.model_max_length:
-        logger.warning(
-            f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the"
-            f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
-        )
-
-
-    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.
-    if data_args.pad_to_max_length:
-        data_collator = default_data_collator
-    elif training_args.fp16:
-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
-    else:
-        data_collator = None
-
-
-    
-    postprocessor_class = BoolQAClassifierPostProcessor  # TODO # taskargs.
-    postprocessor = postprocessor_class(
-        k=10, 
-        drop_label=model_args.drop_label,
-        label_list = model_args.label_list,
-        id_key=model_args.id_key,
-        output_label_prefix=model_args.output_label_prefix
-    )
-
-
-    # Initialize our Trainer
-    #trainer = NWayTrainer( 
-    trainer = Trainer( 
-        model=model,
-        args=training_args,
-        train_dataset=None,
-        eval_dataset=eval_dataset,
-        compute_metrics=None, #compute_metrics,
-        tokenizer=tokenizer,
-        data_collator=data_collator,
-    
-    )
-    # Predict on unlabeled data
-
-    logger.info("*** Predict ***")
-    predictions = trainer.predict(eval_dataset, metric_key_prefix="predict").predictions
-
-    eval_preds = postprocessor.process_references_and_predictions(eval_examples, eval_dataset, predictions)
-
-    with open(os.path.join(training_args.output_dir, 'eval_predictions.json'), 'w') as f:
-        json.dump(eval_preds.predictions, f, indent=4)
-    with open(os.path.join(training_args.output_dir, 'eval_predictions_processed.json'), 'w') as f:
-        json.dump(eval_preds.processed_predictions, f, indent=4)
-
-
-if __name__ == "__main__":
-    main(sys.argv)
+#!/usr/bin/env python
+# based loosely on https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
+#
+#
+# coding=utf-8
+# Copyright 2020 The HuggingFace Inc. team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+""" Finetuning the library models for n-way classification."""
+# You can also adapt this script on your own text classification task. Pointers for this are left as comments.
+
+import logging
+import os
+import sys
+from dataclasses import dataclass, field
+from sysconfig import is_python_build
+from typing import Optional, List
+import json
+from shutil import move
+from os import path
+
+import numpy as np
+from datasets import Dataset, DatasetDict
+
+
+import transformers
+from transformers import (
+    AutoConfig,
+    AutoModelForSequenceClassification,
+    AutoTokenizer,
+    DataCollatorWithPadding,
+    HfArgumentParser,
+    Trainer,
+    TrainingArguments,
+    default_data_collator,
+)
+from transformers.trainer_utils import is_main_process
+
+from primeqa.boolqa.processors.postprocessors.boolqa_classifier import BoolQAClassifierPostProcessor
+from primeqa.boolqa.processors.preprocessors.boolqa_classifier import BoolQAClassifierPreProcessor
+from primeqa.boolqa.processors.dataset.mrc2dataset import create_dataset_from_run_mrc_output
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+#check_min_version("4.6.0")
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+
+    Using `HfArgumentParser` we can turn this class
+    into argparse arguments to be able to specify them on
+    the command line.
+    """
+
+
+    max_seq_length: int = field(
+        default=128,
+        metadata={
+            "help": "The maximum total input sequence length after tokenization. Sequences longer "
+            "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
+    )
+    pad_to_max_length: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to pad all samples to `max_seq_length`. "
+            "If False, will pad the samples dynamically when batching to the maximum length in the batch."
+        },
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    max_predict_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of prediction examples to this "
+            "value if set."
+        },
+    )
+    train_file: Optional[str] = field(
+        default=None, metadata={"help": "A csv or a json file containing the training data."}
+    )
+    validation_file: Optional[str] = field(
+        default=None, metadata={"help": "A csv or a json file containing the validation data."}
+    )
+    test_file: Optional[str] = field(default=None, metadata={"help": "A csv or a json file containing the test data."})
+    balanced: Optional[bool] = field(
+        default=False,
+        metadata={
+            "help": "balance the data if true "
+        },
+    )
+
+
+
+
+@dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    sentence2_key: str = field(
+        default=None,
+        metadata={"help": "the field in the input dataset to use as sentence2" }
+    )        
+    drop_label: str = field(
+        default=None,
+        metadata={"help": "dropping label 'no_answer' converts ternary classifier into binary"}  
+    )
+    sentence1_key: str = field(
+        default="question",
+        metadata={"help": "the field in the input dataset to use as sentence1" }
+    )
+    id_key: str = field(
+        default="example_id",
+        metadata={"help": "a field that can be used as a unique identifier of input records"}
+    )
+    label_list: List[str] = field(
+        default=None,
+        metadata={"help": "the labels used by the classifier (order is significant)"}
+    )
+    output_label_prefix: str = field(
+        default=None,
+        metadata={"help": "a prefix used in the output file eval_predictions.json to distinguish fields created by this invocation of the classifier"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+
+
+def create_a_backup_file_if_file_exists(original_file: str):
+    if path.isfile(original_file):
+        backup_file = '%s.bak' % original_file
+        logging.debug('Found a pre-existing file at location %s.  Backing it up to %s' %
+                      (original_file, backup_file))
+        move(original_file, backup_file)
+
+def save_to_json_file(obj_to_save, out_file_path, with_backup=True, indent=4, sort_keys=True):
+    if with_backup:
+        create_a_backup_file_if_file_exists(out_file_path)
+
+    logging.debug('Writing %s as json to file: %s' % (type(obj_to_save), out_file_path))
+    with open(out_file_path, 'w') as outfile:
+        json.dump(obj_to_save, outfile, indent=indent, sort_keys=sort_keys)
+
+
+def main(raw_args):
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
+    if len(raw_args) == 2 and raw_args[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(raw_args[1]))
+    elif len(raw_args) == 1:
+        model_args, data_args, training_args = parser.parse_dict(raw_args[0])
+    else:
+        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    # Setup logging
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)
+
+    # Log on each process the small summary:
+    logger.warning(
+        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
+        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
+    )
+    # Set the verbosity to info of the Transformers logger (on main process only):
+    if is_main_process(training_args.local_rank):
+        transformers.utils.logging.set_verbosity_info()
+        transformers.utils.logging.enable_default_handler()
+        transformers.utils.logging.enable_explicit_format()
+    logger.info(f"Training/evaluation parameters {training_args}")
+
+    num_labels = len(model_args.label_list)        
+    logger.info("The following labels are being used, all other instances will be discarded.")
+    logger.info(model_args.label_list)
+
+
+
+
+    # Load pretrained model and tokenizer
+    #
+    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
+    # download model & vocab.
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
+        num_labels=num_labels,
+        finetuning_task=model_args.output_label_prefix,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+
+
+    model = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    # Padding strategy
+    if data_args.pad_to_max_length:
+        padding = "max_length"
+    else:
+        # We will pad later, dynamically at batch creation, to the max sequence length in each batch
+        padding = False
+
+
+    raw_datasets={}
+    raw_datasets['validation']=create_dataset_from_run_mrc_output(data_args.test_file, unpack=False)
+
+    #max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)
+    # load preprocessor
+    preprocessor_class = BoolQAClassifierPreProcessor # TODO task_args.preprocessor
+    preprocessor = preprocessor_class(
+        sentence1_key=model_args.sentence1_key,
+        sentence2_key=model_args.sentence2_key,
+        tokenizer=tokenizer,
+        load_from_cache_file=not data_args.overwrite_cache,
+        max_seq_len=tokenizer.model_max_length,
+        padding=padding
+    )
+
+    # process eval data
+    eval_examples = raw_datasets["validation"]
+    max_eval_samples = data_args.max_eval_samples
+    if max_eval_samples is not None:  # data_args.max_eval_samples is not None:
+        # We will select sample from whole data
+        eval_examples = eval_examples.select(range(max_eval_samples))
+    # Validation Feature Creation
+    with training_args.main_process_first(desc="validation dataset map pre-processing"):
+        eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)
+
+
+    if data_args.max_seq_length > tokenizer.model_max_length:
+        logger.warning(
+            f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the"
+            f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
+        )
+
+
+    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.
+    if data_args.pad_to_max_length:
+        data_collator = default_data_collator
+    elif training_args.fp16:
+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
+    else:
+        data_collator = None
+
+
+    
+    postprocessor_class = BoolQAClassifierPostProcessor  # TODO # taskargs.
+    postprocessor = postprocessor_class(
+        k=10, 
+        drop_label=model_args.drop_label,
+        label_list = model_args.label_list,
+        id_key=model_args.id_key,
+        output_label_prefix=model_args.output_label_prefix
+    )
+
+
+    # Initialize our Trainer
+    #trainer = NWayTrainer( 
+    trainer = Trainer( 
+        model=model,
+        args=training_args,
+        train_dataset=None,
+        eval_dataset=eval_dataset,
+        compute_metrics=None, #compute_metrics,
+        tokenizer=tokenizer,
+        data_collator=data_collator,
+    
+    )
+    # Predict on unlabeled data
+
+    logger.info("*** Predict ***")
+    predictions = trainer.predict(eval_dataset, metric_key_prefix="predict").predictions
+
+    eval_preds = postprocessor.process_references_and_predictions(eval_examples, eval_dataset, predictions)
+
+    with open(os.path.join(training_args.output_dir, 'eval_predictions.json'), 'w') as f:
+        json.dump(eval_preds.predictions, f, indent=4)
+    with open(os.path.join(training_args.output_dir, 'eval_predictions_processed.json'), 'w') as f:
+        json.dump(eval_preds.processed_predictions, f, indent=4)
+
+
+if __name__ == "__main__":
+    main(sys.argv)
```

## primeqa/boolqa/run_score_normalizer.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-from primeqa.boolqa.score_normalizer.score_normalizer import ScoreNormalizer
-import argparse
-import sys
-
-def main(args):
-    if len(args) == 1:
-        args = argparse.Namespace(**(args[0]))
-    else:
-        args = parse_arguments()
-        #args = args[0]
-    sn = ScoreNormalizer(args.model_name_or_path)
-    sn.load_model()
-    sn.normalize_scores(args.test_file, 
-                        args.output_dir,
-                        args.qtc_is_boolean_label, 
-                        args.evc_no_answer_class)
-
-def parse_arguments():
-    parser = argparse.ArgumentParser(description='Assigns YES/NO answers to the QA prediction file based on the question boolean classifier')
-    parser.add_argument('--test_file',  
-                    help='the prediction file produced by the boolean answer classifier',
-                    type=str)
-    parser.add_argument('--model_name_or_path',  
-                    help='the model for the score normalizer',
-                    type=str)        
-    parser.add_argument('--output_dir',  
-                    help='the output prediction files with YES/NO normalized answers',
-                    type=str)  
-    parser.add_argument('--qtc_is_boolean_label', type=str, default='boolean',
-                    help='the value assigned to the question_type_pred field for boolean questions')
-    parser.add_argument('--evc_no_answer_class', type=str, default='no_answer',
-                    help='the class label in the boolean_answer_scores field for no_answer questions')
-    
-    args = parser.parse_args()
-    return args
-  
-if __name__ == '__main__':
+from primeqa.boolqa.score_normalizer.score_normalizer import ScoreNormalizer
+import argparse
+import sys
+
+def main(args):
+    if len(args) == 1:
+        args = argparse.Namespace(**(args[0]))
+    else:
+        args = parse_arguments()
+        #args = args[0]
+    sn = ScoreNormalizer(args.model_name_or_path)
+    sn.load_model()
+    sn.normalize_scores(args.test_file, 
+                        args.output_dir,
+                        args.qtc_is_boolean_label, 
+                        args.evc_no_answer_class)
+
+def parse_arguments():
+    parser = argparse.ArgumentParser(description='Assigns YES/NO answers to the QA prediction file based on the question boolean classifier')
+    parser.add_argument('--test_file',  
+                    help='the prediction file produced by the boolean answer classifier',
+                    type=str)
+    parser.add_argument('--model_name_or_path',  
+                    help='the model for the score normalizer',
+                    type=str)        
+    parser.add_argument('--output_dir',  
+                    help='the output prediction files with YES/NO normalized answers',
+                    type=str)  
+    parser.add_argument('--qtc_is_boolean_label', type=str, default='boolean',
+                    help='the value assigned to the question_type_pred field for boolean questions')
+    parser.add_argument('--evc_no_answer_class', type=str, default='no_answer',
+                    help='the class label in the boolean_answer_scores field for no_answer questions')
+    
+    args = parser.parse_args()
+    return args
+  
+if __name__ == '__main__':
     main(sys.argv)
```

## primeqa/boolqa/processors/dataset/mrc2dataset.py

 * *Ordering differences only*

```diff
@@ -1,71 +1,71 @@
-import json
-import argparse
-import pandas as pd 
-import sys
-from primeqa.mrc.data_models.target_type import TargetType
-from datasets import Dataset
-
-def unpack_target_type(row):
-    '''
-    target_type_logits is an array of 5 numbers, with meanings determined by the 
-    TargetType enum.  This routine unpacks them into separate keys for easier handling
-    with pandas/datasets
-    '''
-    ttkey='target_type_logits'
-    ttlogits=row[ttkey]
-    for t in TargetType:
-        key=f'{ttkey}_{str(t)}'
-        row[key]=ttlogits[int(t)]
-    #row.pop(ttkey)
-    return row
-
-def unpack_span_answer(row):
-    '''
-    span_answer is a dictionary with two items - split into separate items for easier
-    handling with pandas/datasets
-    '''
-    row['span_answer_start_position'] = row['span_answer']['start_position']
-    row['span_answer_end_position'] = row['span_answer']['end_position']
-    row.pop('span_answer')
-    return row
-
-#----------------------------------------------------------------
-def create_dataset_from_run_mrc_output(mrcfn: str, unpack: bool) -> Dataset:
-    """Converts the output of run_mrc.py (eval_predictions.json, not eval_predictions_processed.json)
-    into a Dataset for use with the classifiers
-
-    Args:
-        mrcfn (str): path to the eval_predictions.json file produced by run_mrc.py 
-
-    Returns:
-        Dataset : a dataset containing all of the fields in mrcfn as features, for the top-ranked answer
-    """    
-    with open(mrcfn) as mrcin:
-        mrc=json.load(mrcin)
-    return create_dataset_from_json_str(mrc, unpack)
-
-def create_dataset_from_json_str(json_str: str, unpack: bool) -> Dataset:
-    """Converts the output of run_mrc.py (eval_predictions.json, not eval_predictions_processed.json)
-    into a Dataset for use with the classifiers
-
-    Args:
-        json_str (str): json encoding of the current state of eval_predictions
-
-    Returns:
-        Dataset : a dataset containing all of the fields in mrcfn as features, for the top-ranked answer
-    """    
-
-    # python dict class preserves order >3.6 - assuming it still works inside json
-    def read_mrc_inner():
-        for order, (key,vals) in enumerate(json_str.items()):
-            for rank,val in enumerate(vals):
-                if unpack:
-                    val = unpack_target_type(val)
-                    val = unpack_span_answer(val)
-                val['order']=order
-                val['rank']=rank
-                yield val
-    df=pd.DataFrame.from_records(read_mrc_inner())
-    df0=df.query('rank==0') # TODO danger - assumes upstream results are correctly ordered
-    ds=Dataset.from_pandas(df0, preserve_index=False)
-    return ds
+import json
+import argparse
+import pandas as pd 
+import sys
+from primeqa.mrc.data_models.target_type import TargetType
+from datasets import Dataset
+
+def unpack_target_type(row):
+    '''
+    target_type_logits is an array of 5 numbers, with meanings determined by the 
+    TargetType enum.  This routine unpacks them into separate keys for easier handling
+    with pandas/datasets
+    '''
+    ttkey='target_type_logits'
+    ttlogits=row[ttkey]
+    for t in TargetType:
+        key=f'{ttkey}_{str(t)}'
+        row[key]=ttlogits[int(t)]
+    #row.pop(ttkey)
+    return row
+
+def unpack_span_answer(row):
+    '''
+    span_answer is a dictionary with two items - split into separate items for easier
+    handling with pandas/datasets
+    '''
+    row['span_answer_start_position'] = row['span_answer']['start_position']
+    row['span_answer_end_position'] = row['span_answer']['end_position']
+    row.pop('span_answer')
+    return row
+
+#----------------------------------------------------------------
+def create_dataset_from_run_mrc_output(mrcfn: str, unpack: bool) -> Dataset:
+    """Converts the output of run_mrc.py (eval_predictions.json, not eval_predictions_processed.json)
+    into a Dataset for use with the classifiers
+
+    Args:
+        mrcfn (str): path to the eval_predictions.json file produced by run_mrc.py 
+
+    Returns:
+        Dataset : a dataset containing all of the fields in mrcfn as features, for the top-ranked answer
+    """    
+    with open(mrcfn) as mrcin:
+        mrc=json.load(mrcin)
+    return create_dataset_from_json_str(mrc, unpack)
+
+def create_dataset_from_json_str(json_str: str, unpack: bool) -> Dataset:
+    """Converts the output of run_mrc.py (eval_predictions.json, not eval_predictions_processed.json)
+    into a Dataset for use with the classifiers
+
+    Args:
+        json_str (str): json encoding of the current state of eval_predictions
+
+    Returns:
+        Dataset : a dataset containing all of the fields in mrcfn as features, for the top-ranked answer
+    """    
+
+    # python dict class preserves order >3.6 - assuming it still works inside json
+    def read_mrc_inner():
+        for order, (key,vals) in enumerate(json_str.items()):
+            for rank,val in enumerate(vals):
+                if unpack:
+                    val = unpack_target_type(val)
+                    val = unpack_span_answer(val)
+                val['order']=order
+                val['rank']=rank
+                yield val
+    df=pd.DataFrame.from_records(read_mrc_inner())
+    df0=df.query('rank==0') # TODO danger - assumes upstream results are correctly ordered
+    ds=Dataset.from_pandas(df0, preserve_index=False)
+    return ds
```

## primeqa/boolqa/processors/postprocessors/boolqa_classifier.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-from typing import List, Dict, Any
-
-from datasets import Dataset
-import numpy as np
-import logging
-from transformers import EvalPrediction
-
-from primeqa.mrc.processors.postprocessors.abstract import AbstractPostProcessor
-from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
-
-
-logger = logging.getLogger(__name__)
-
-
-class BoolQAClassifierPostProcessor(AbstractPostProcessor):
-    """
-    Post processor for use with the classifier components of the Tydi-Boolqa pipeline
-    so that additional fields can be added to the eval_predictions.json files
-    """    
-    def __init__(self, 
-                drop_label: str,
-                id_key: str,
-                label_list: List[str],
-                output_label_prefix: str,
-                *args, **kwargs):
-        """
-        Args:
-            drop_label: if specified, ignore this category for classifier output,
-                     e.g. "no_answer" converts a ("yes","no_answer","no") classifier into a ("yes", "no") classifier
-            id_key: unique identifier field of examples to be classified
-            label_list: the (human-readable) labels produced by the classifier
-            output_label_prefix: prefix for new output fields in eval_predictions.json
-            *args: Arguments for super class constructor.
-            **kwargs: Keyword Arguments for super class constructor.
-        """                
-        super().__init__(1, 1)
-        self.id_key=id_key
-        self.drop_label = drop_label
-        self.label_list = label_list
-        self.output_label_prefix = output_label_prefix
-
-
-    def process(self, examples: Dataset, features: Dataset, predictions: tuple):
-        """
-        Convert data and model predictions into MRC answers.
-        """
-        pass
-
-    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
-        """
-        Convert examples into references for use with metrics.
-        """
-        pass
-
-    def _get_prediction_from_predict_scores(self, predict_scores):
-        if self.drop_label:
-            # dropping NONE to get binary predictions from 3-way classifier
-            # TODO maybe this should be model dependent rather than task dependent
-            label_list=np.array(self.label_list)
-            mask=label_list==self.drop_label
-            masked_predict_scores=predict_scores.copy()
-            masked_predict_scores[:,mask]=-9e19
-        else:
-            masked_predict_scores=predict_scores
-
-        predictions = np.argmax(masked_predict_scores, axis=1)
-        return predictions
-
-
-
-    # TODO we aren't handling reference, metrics yet
-    def process_references_and_predictions(self, examples, features, predict_scores) -> EvalPrediction:
-        print('in process_references_and_predictions')
-#        references = self.prepare_examples_as_references(examples)
-        ipredictions=self._get_prediction_from_predict_scores(predict_scores)
-
-        fields = zip(features[self.id_key],
-            features["question"],
-            ipredictions, 
-            predict_scores)
-
-        preds_for_metric=[]
-        examples_json={}
-        for (ex, (example_id, question, item, scores)) in zip(examples, fields):
-            item_label = self.label_list[item]
-            p = {
-                "pred":str(item_label),
-                "conf":str(scores[item]),
-                "question":question,
-                "language":ex["language"],
-                "scores": { label:float(score) for label,score in zip(self.label_list, scores)}
-            }
-            preds_for_metric.append(p)
-
-            ex[self.output_label_prefix+'_pred'] = p['pred']
-            ex[self.output_label_prefix+'_scores'] = p['scores']
-            ex[self.output_label_prefix+'_conf'] = p['conf']
-            examples_json[example_id] = [ ex ]
-
-        # noinspection PyTypeChecker
-        return EvalPredictionWithProcessing(
-            label_ids=None,
-            predictions=examples_json,
-            processed_predictions=preds_for_metric,
-        )    
+from typing import List, Dict, Any
+
+from datasets import Dataset
+import numpy as np
+import logging
+from transformers import EvalPrediction
+
+from primeqa.mrc.processors.postprocessors.abstract import AbstractPostProcessor
+from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
+
+
+logger = logging.getLogger(__name__)
+
+
+class BoolQAClassifierPostProcessor(AbstractPostProcessor):
+    """
+    Post processor for use with the classifier components of the Tydi-Boolqa pipeline
+    so that additional fields can be added to the eval_predictions.json files
+    """    
+    def __init__(self, 
+                drop_label: str,
+                id_key: str,
+                label_list: List[str],
+                output_label_prefix: str,
+                *args, **kwargs):
+        """
+        Args:
+            drop_label: if specified, ignore this category for classifier output,
+                     e.g. "no_answer" converts a ("yes","no_answer","no") classifier into a ("yes", "no") classifier
+            id_key: unique identifier field of examples to be classified
+            label_list: the (human-readable) labels produced by the classifier
+            output_label_prefix: prefix for new output fields in eval_predictions.json
+            *args: Arguments for super class constructor.
+            **kwargs: Keyword Arguments for super class constructor.
+        """                
+        super().__init__(1, 1)
+        self.id_key=id_key
+        self.drop_label = drop_label
+        self.label_list = label_list
+        self.output_label_prefix = output_label_prefix
+
+
+    def process(self, examples: Dataset, features: Dataset, predictions: tuple):
+        """
+        Convert data and model predictions into MRC answers.
+        """
+        pass
+
+    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
+        """
+        Convert examples into references for use with metrics.
+        """
+        pass
+
+    def _get_prediction_from_predict_scores(self, predict_scores):
+        if self.drop_label:
+            # dropping NONE to get binary predictions from 3-way classifier
+            # TODO maybe this should be model dependent rather than task dependent
+            label_list=np.array(self.label_list)
+            mask=label_list==self.drop_label
+            masked_predict_scores=predict_scores.copy()
+            masked_predict_scores[:,mask]=-9e19
+        else:
+            masked_predict_scores=predict_scores
+
+        predictions = np.argmax(masked_predict_scores, axis=1)
+        return predictions
+
+
+
+    # TODO we aren't handling reference, metrics yet
+    def process_references_and_predictions(self, examples, features, predict_scores) -> EvalPrediction:
+        print('in process_references_and_predictions')
+#        references = self.prepare_examples_as_references(examples)
+        ipredictions=self._get_prediction_from_predict_scores(predict_scores)
+
+        fields = zip(features[self.id_key],
+            features["question"],
+            ipredictions, 
+            predict_scores)
+
+        preds_for_metric=[]
+        examples_json={}
+        for (ex, (example_id, question, item, scores)) in zip(examples, fields):
+            item_label = self.label_list[item]
+            p = {
+                "pred":str(item_label),
+                "conf":str(scores[item]),
+                "question":question,
+                "language":ex["language"],
+                "scores": { label:float(score) for label,score in zip(self.label_list, scores)}
+            }
+            preds_for_metric.append(p)
+
+            ex[self.output_label_prefix+'_pred'] = p['pred']
+            ex[self.output_label_prefix+'_scores'] = p['scores']
+            ex[self.output_label_prefix+'_conf'] = p['conf']
+            examples_json[example_id] = [ ex ]
+
+        # noinspection PyTypeChecker
+        return EvalPredictionWithProcessing(
+            label_ids=None,
+            predictions=examples_json,
+            processed_predictions=preds_for_metric,
+        )
```

## primeqa/boolqa/processors/postprocessors/extractive.py

 * *Ordering differences only*

```diff
@@ -1,56 +1,56 @@
-from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
-from datasets import Dataset
-import numpy as np
-from typing import Tuple
-import logging
-
-logger = logging.getLogger(__name__)
-
-logger.setLevel(logging.DEBUG)
-
-
-    
-def _get_passage_answer_span( example: dict, pred: dict ) -> str:
-    """ safely extract the text of the predicted passage answer
-    falls back to short_answer text if the extraction fails
-
-    Args:
-        example (dict): a row of the examples dataset (from process)
-        pred (dict): the predictions for output
-
-    Returns:
-        str: text of the passage answer
-    """ 
-    try:
-        passage_index=pred["passage_index"]
-        # these are chars not bytes
-        # see TyDiQAPreprocessor._convert_start_and_end_positions_from_bytes_to_chars
-        # where we also see the example['context'] is list of length 1
-        passage_start=example['passage_candidates']['start_positions'][passage_index]
-        passage_end=example['passage_candidates']['end_positions'][passage_index]
-        span=example['context'][0][passage_start:passage_end]
-    except Exception as x:
-        logger.info('unable to extract passage text - default to span_answer_text')
-        logger.info(str(pred))
-        logger.info(str(x))
-        span=pred["span_answer_text"]
-    return span
-
-
-class ExtractivePipelinePostProcessor(ExtractivePostProcessor):
-    """
-        PostProcessor that is just like ExtractivePostProcssor, but provides additional fields
-        needed for downstream boolean pipeline classifiers:
-        new fields:   language, question, passage_answer_text
-    """ 
-    def process(self, examples: Dataset, features: Dataset, predictions: Tuple[np.ndarray, np.ndarray, np.ndarray]):
-        predictions=super().process(examples, features, predictions)
-        for example_idx in range(examples.num_rows):     
-            example = examples[example_idx]
-            preds = predictions[example['example_id']]
-            for pred in preds:
-                pred["question"] = example["question"]
-                pred["language"] = example["language"]  
-                pred["passage_answer_text"] = _get_passage_answer_span(example, pred)
-
-        return predictions
+from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
+from datasets import Dataset
+import numpy as np
+from typing import Tuple
+import logging
+
+logger = logging.getLogger(__name__)
+
+logger.setLevel(logging.DEBUG)
+
+
+    
+def _get_passage_answer_span( example: dict, pred: dict ) -> str:
+    """ safely extract the text of the predicted passage answer
+    falls back to short_answer text if the extraction fails
+
+    Args:
+        example (dict): a row of the examples dataset (from process)
+        pred (dict): the predictions for output
+
+    Returns:
+        str: text of the passage answer
+    """ 
+    try:
+        passage_index=pred["passage_index"]
+        # these are chars not bytes
+        # see TyDiQAPreprocessor._convert_start_and_end_positions_from_bytes_to_chars
+        # where we also see the example['context'] is list of length 1
+        passage_start=example['passage_candidates']['start_positions'][passage_index]
+        passage_end=example['passage_candidates']['end_positions'][passage_index]
+        span=example['context'][0][passage_start:passage_end]
+    except Exception as x:
+        logger.info('unable to extract passage text - default to span_answer_text')
+        logger.info(str(pred))
+        logger.info(str(x))
+        span=pred["span_answer_text"]
+    return span
+
+
+class ExtractivePipelinePostProcessor(ExtractivePostProcessor):
+    """
+        PostProcessor that is just like ExtractivePostProcssor, but provides additional fields
+        needed for downstream boolean pipeline classifiers:
+        new fields:   language, question, passage_answer_text
+    """ 
+    def process(self, examples: Dataset, features: Dataset, predictions: Tuple[np.ndarray, np.ndarray, np.ndarray]):
+        predictions=super().process(examples, features, predictions)
+        for example_idx in range(examples.num_rows):     
+            example = examples[example_idx]
+            preds = predictions[example['example_id']]
+            for pred in preds:
+                pred["question"] = example["question"]
+                pred["language"] = example["language"]  
+                pred["passage_answer_text"] = _get_passage_answer_span(example, pred)
+
+        return predictions
```

## primeqa/boolqa/processors/preprocessors/boolqa_classifier.py

 * *Ordering differences only*

```diff
@@ -1,114 +1,114 @@
-import itertools
-from lib2to3.pgen2.tokenize import tokenize
-import random
-import uuid
-from operator import sub
-from typing import List, Iterable, Tuple, Any, Dict, Union
-import logging
-
-from datasets.arrow_dataset import Batch
-from transformers import BatchEncoding
-from datasets import Dataset
-from datasets.features.features import Sequence, Value
-from transformers import PreTrainedTokenizerFast, BatchEncoding
-
-
-from primeqa.mrc.processors.preprocessors.abstract import AbstractPreProcessor
-from primeqa.mrc.data_models.subsample_type import SubsampleType
-from primeqa.mrc.data_models.target_type import TargetType
-
-logger = logging.getLogger(__name__)
-
-
-# AbstractPreProcessor is too specific to extractive, we can't inherit from it here
-class BoolQAClassifierPreProcessor:
-    """
-    Preprocessor for TyDi-BoolQA pipeline classifiers that receive upstream input from
-    an eval_predictions.json file
-    """    
-    def __init__(self,
-                sentence1_key: str,
-                sentence2_key: str,
-                tokenizer: PreTrainedTokenizerFast,
-                max_seq_len: int,
-                padding: bool,
-                load_from_cache_file: bool = True):
-        """
-        Args:
-            sentence1_key:
-                the key for the first input field, typically "question"
-            sentence2_key:
-                the key for the second input field which is used as a passage
-            tokenizer:
-                Tokenizer used to prepare model inputs.             
-                Step size to move sliding window across context.
-            max_seq_len:
-                Maximum length of question and context inputs to the model (in word pieces/bpes).
-                Uses tokenizer default if not given.
-            padding:
-                padding argument for tokenizer
-            load_from_cache_file:
-                load_from_cache argument of dataset mapper
-        """                
-        self._sentence1_key=sentence1_key
-        self._sentence2_key=sentence2_key
-        self._tokenizer=tokenizer
-        self._max_seq_len=max_seq_len
-        self._padding=padding
-        self._load_from_cache_file=load_from_cache_file
-
-
-
-
-
-    def process_train(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
-        return self._process(examples, is_train=True)
-
-    def process_eval(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
-        return self._process(examples, is_train=False)
-
-
-
-    # TODO maybe this should be _process_batch, but that has more args that I don't understand
-    def _preprocess_function(self, examples):
-        # Tokenize the texts
-        args = (
-            (examples[self._sentence1_key],) if self._sentence2_key not in examples
-             else (examples[self._sentence1_key], examples[self._sentence2_key])
-        )
-        result = self._tokenizer(*args, padding=self._padding, max_length=self._max_seq_len, truncation=True)
-
-        # Map labels to IDs
-        #if self.label_to_id is not None and "label" in examples:
-        #    result["label"] = [(self.label_to_id[l] if l != -1 else -1) for l in examples["label"]]
-        return result
-
-
-
-
-    def _process(self, examples: Dataset, is_train: bool) -> Tuple[Dataset, Dataset]:
-        if examples.num_rows == 0:
-            raise ValueError("No examples to process")
-        if not 'question' in examples.column_names:
-            msg="""
-The eval_predictions.json file must contain a field 'question'.  This can be created by running the machine
-reading comprehension step in "run_mrc.py" with the option
---postprocessor primeqa.boolqa.processors.postprocessors.extractive.ExtractivePipelinePostProcessor
-rather than the default tydi postprocessor.
-            """
-            logger.error(msg)
-            raise ValueError("incorrectly formatted eval_predictions.json file")
-
-
-        input_features = Dataset.from_dict(
-            {'example_id': examples['example_id'],
-             'language': ['english'] * examples.num_rows,  # TODO english->none?
-             self._sentence1_key: examples[self._sentence1_key],
-             'label': [0] * examples.num_rows
-            })
-        if self._sentence2_key is not None:
-            input_features = input_features.add_column(self._sentence2_key, examples[self._sentence2_key])
-
-        features = input_features.map(self._preprocess_function, batched=True, load_from_cache_file=self._load_from_cache_file)
-        return examples, features
-
+import itertools
+from lib2to3.pgen2.tokenize import tokenize
+import random
+import uuid
+from operator import sub
+from typing import List, Iterable, Tuple, Any, Dict, Union
+import logging
+
+from datasets.arrow_dataset import Batch
+from transformers import BatchEncoding
+from datasets import Dataset
+from datasets.features.features import Sequence, Value
+from transformers import PreTrainedTokenizerFast, BatchEncoding
+
+
+from primeqa.mrc.processors.preprocessors.abstract import AbstractPreProcessor
+from primeqa.mrc.data_models.subsample_type import SubsampleType
+from primeqa.mrc.data_models.target_type import TargetType
+
+logger = logging.getLogger(__name__)
+
+
+# AbstractPreProcessor is too specific to extractive, we can't inherit from it here
+class BoolQAClassifierPreProcessor:
+    """
+    Preprocessor for TyDi-BoolQA pipeline classifiers that receive upstream input from
+    an eval_predictions.json file
+    """    
+    def __init__(self,
+                sentence1_key: str,
+                sentence2_key: str,
+                tokenizer: PreTrainedTokenizerFast,
+                max_seq_len: int,
+                padding: bool,
+                load_from_cache_file: bool = True):
+        """
+        Args:
+            sentence1_key:
+                the key for the first input field, typically "question"
+            sentence2_key:
+                the key for the second input field which is used as a passage
+            tokenizer:
+                Tokenizer used to prepare model inputs.             
+                Step size to move sliding window across context.
+            max_seq_len:
+                Maximum length of question and context inputs to the model (in word pieces/bpes).
+                Uses tokenizer default if not given.
+            padding:
+                padding argument for tokenizer
+            load_from_cache_file:
+                load_from_cache argument of dataset mapper
+        """                
+        self._sentence1_key=sentence1_key
+        self._sentence2_key=sentence2_key
+        self._tokenizer=tokenizer
+        self._max_seq_len=max_seq_len
+        self._padding=padding
+        self._load_from_cache_file=load_from_cache_file
+
+
+
+
+
+    def process_train(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
+        return self._process(examples, is_train=True)
+
+    def process_eval(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
+        return self._process(examples, is_train=False)
+
+
+
+    # TODO maybe this should be _process_batch, but that has more args that I don't understand
+    def _preprocess_function(self, examples):
+        # Tokenize the texts
+        args = (
+            (examples[self._sentence1_key],) if self._sentence2_key not in examples
+             else (examples[self._sentence1_key], examples[self._sentence2_key])
+        )
+        result = self._tokenizer(*args, padding=self._padding, max_length=self._max_seq_len, truncation=True)
+
+        # Map labels to IDs
+        #if self.label_to_id is not None and "label" in examples:
+        #    result["label"] = [(self.label_to_id[l] if l != -1 else -1) for l in examples["label"]]
+        return result
+
+
+
+
+    def _process(self, examples: Dataset, is_train: bool) -> Tuple[Dataset, Dataset]:
+        if examples.num_rows == 0:
+            raise ValueError("No examples to process")
+        if not 'question' in examples.column_names:
+            msg="""
+The eval_predictions.json file must contain a field 'question'.  This can be created by running the machine
+reading comprehension step in "run_mrc.py" with the option
+--postprocessor primeqa.boolqa.processors.postprocessors.extractive.ExtractivePipelinePostProcessor
+rather than the default tydi postprocessor.
+            """
+            logger.error(msg)
+            raise ValueError("incorrectly formatted eval_predictions.json file")
+
+
+        input_features = Dataset.from_dict(
+            {'example_id': examples['example_id'],
+             'language': ['english'] * examples.num_rows,  # TODO english->none?
+             self._sentence1_key: examples[self._sentence1_key],
+             'label': [0] * examples.num_rows
+            })
+        if self._sentence2_key is not None:
+            input_features = input_features.add_column(self._sentence2_key, examples[self._sentence2_key])
+
+        features = input_features.map(self._preprocess_function, batched=True, load_from_cache_file=self._load_from_cache_file)
+        return examples, features
+
```

## primeqa/boolqa/score_normalizer/score_normalizer.py

 * *Ordering differences only*

```diff
@@ -1,89 +1,89 @@
-import pickle
-import numpy
-import os
-import json
-import importlib
-from primeqa.boolqa.processors.dataset.mrc2dataset import create_dataset_from_run_mrc_output
-import argparse
-import sys
-
-class ScoreNormalizer(object):
-    """
-    Class for normalizeing the score for boolean and extractive questions.
-    """
-
-    def __init__(self, model_file_path=None):
-        """
-        Args:
-            score_normalizer_model_path: Path of score normalizer model, a picke file.
-        """
-        self._model_file_path=model_file_path
-        
-
-    def load_model(self):
-        if not self._model_file_path:
-            raise ValueError(f"No score normalizer model path was provided.")
-        try:
-            self._model = pickle.load(open(self._model_file_path, 'rb'))
-        except Exception as ex:
-            raise ValueError(f"Unable to load confidence model from {self._model_file_path}")
-    
-    def normalize_scores(self,input_file : str, output_dir : str,
-                        qtc_is_boolean_label : str = 'boolean',
-                        evc_no_answer_class : str = 'no_answer'):
-        
-        qa_pred_data = create_dataset_from_run_mrc_output(input_file, unpack=True)
-        
-        normalized_predictions=[]
-        for i, qa_pred in enumerate(qa_pred_data):
-           
-            n = self.create_prediction(qtc_is_boolean_label, qa_pred)
-            
-            features = self.create_features(qtc_is_boolean_label, qa_pred)
-            new_score = self._model.predict_proba(features)[0][1]
-            
-            n['confidence_score'] = float(new_score)
-            
-            normalized_predictions.append(n)
-
-        # if the output directory does not exist, create a new directory 
-        if not os.path.exists(output_dir):
-            os.makedirs(output_dir)
-
-        # save the normalized predictions
-        with open(os.path.join(output_dir, 'eval_predictions_processed.json'), 'w') as f:
-            json.dump(normalized_predictions, f, indent=4)
-
-    def create_prediction(self, qtc_is_boolean_label, qa_pred):
-        
-        n={'example_id': qa_pred['example_id'],
-            'start_position': qa_pred['span_answer_start_position'],
-            'end_position': qa_pred['span_answer_end_position'],
-            'passage_index': qa_pred['passage_index'],
-            'yes_no_answer': qa_pred['yes_no_answer']
-            }
-            
-        # Update the prediction to be YES/NO
-        question_label = 1 if qa_pred['question_type_pred'] == qtc_is_boolean_label else 0
-        if question_label == 1:
-            yes_answer = qa_pred['boolean_answer_pred']
-            if yes_answer == "yes": 
-                n['yes_no_answer'] = 3
-            else: 
-                n['yes_no_answer'] =  4
-            n['start_position'] = -1
-            n['end_position'] = -1
-            
-        return n
-
-    def create_features(self, qtc_is_boolean_label, qa_pred):
-        # Apply the score normalizer
-        # qa_conf_score = qa_pred['span_answer_score']
-        # evc_conf_score = float(qa_pred['boolean_answer_scores'][evc_no_answer_class])
-        b_score = qa_pred['start_logit']
-        e_score = qa_pred['end_logit']
-        na_score = qa_pred['target_type_logits'][0] if 'target_type_logits' in  qa_pred else 0.0
-        question_label = 1 if qa_pred['question_type_pred'] == qtc_is_boolean_label else 0
-        feature_list = [question_label,b_score,e_score,na_score]
-        features = numpy.array(feature_list).reshape(1, -1)
-        return features
+import pickle
+import numpy
+import os
+import json
+import importlib
+from primeqa.boolqa.processors.dataset.mrc2dataset import create_dataset_from_run_mrc_output
+import argparse
+import sys
+
+class ScoreNormalizer(object):
+    """
+    Class for normalizeing the score for boolean and extractive questions.
+    """
+
+    def __init__(self, model_file_path=None):
+        """
+        Args:
+            score_normalizer_model_path: Path of score normalizer model, a picke file.
+        """
+        self._model_file_path=model_file_path
+        
+
+    def load_model(self):
+        if not self._model_file_path:
+            raise ValueError(f"No score normalizer model path was provided.")
+        try:
+            self._model = pickle.load(open(self._model_file_path, 'rb'))
+        except Exception as ex:
+            raise ValueError(f"Unable to load confidence model from {self._model_file_path}")
+    
+    def normalize_scores(self,input_file : str, output_dir : str,
+                        qtc_is_boolean_label : str = 'boolean',
+                        evc_no_answer_class : str = 'no_answer'):
+        
+        qa_pred_data = create_dataset_from_run_mrc_output(input_file, unpack=True)
+        
+        normalized_predictions=[]
+        for i, qa_pred in enumerate(qa_pred_data):
+           
+            n = self.create_prediction(qtc_is_boolean_label, qa_pred)
+            
+            features = self.create_features(qtc_is_boolean_label, qa_pred)
+            new_score = self._model.predict_proba(features)[0][1]
+            
+            n['confidence_score'] = float(new_score)
+            
+            normalized_predictions.append(n)
+
+        # if the output directory does not exist, create a new directory 
+        if not os.path.exists(output_dir):
+            os.makedirs(output_dir)
+
+        # save the normalized predictions
+        with open(os.path.join(output_dir, 'eval_predictions_processed.json'), 'w') as f:
+            json.dump(normalized_predictions, f, indent=4)
+
+    def create_prediction(self, qtc_is_boolean_label, qa_pred):
+        
+        n={'example_id': qa_pred['example_id'],
+            'start_position': qa_pred['span_answer_start_position'],
+            'end_position': qa_pred['span_answer_end_position'],
+            'passage_index': qa_pred['passage_index'],
+            'yes_no_answer': qa_pred['yes_no_answer']
+            }
+            
+        # Update the prediction to be YES/NO
+        question_label = 1 if qa_pred['question_type_pred'] == qtc_is_boolean_label else 0
+        if question_label == 1:
+            yes_answer = qa_pred['boolean_answer_pred']
+            if yes_answer == "yes": 
+                n['yes_no_answer'] = 3
+            else: 
+                n['yes_no_answer'] =  4
+            n['start_position'] = -1
+            n['end_position'] = -1
+            
+        return n
+
+    def create_features(self, qtc_is_boolean_label, qa_pred):
+        # Apply the score normalizer
+        # qa_conf_score = qa_pred['span_answer_score']
+        # evc_conf_score = float(qa_pred['boolean_answer_scores'][evc_no_answer_class])
+        b_score = qa_pred['start_logit']
+        e_score = qa_pred['end_logit']
+        na_score = qa_pred['target_type_logits'][0] if 'target_type_logits' in  qa_pred else 0.0
+        question_label = 1 if qa_pred['question_type_pred'] == qtc_is_boolean_label else 0
+        feature_list = [question_label,b_score,e_score,na_score]
+        features = numpy.array(feature_list).reshape(1, -1)
+        return features
```

## primeqa/calibration/confidence_scorer.py

 * *Ordering differences only*

```diff
@@ -1,261 +1,261 @@
-import os
-import json
-import numpy as np
-import joblib
-from joblib import dump, load
-import sklearn
-from sklearn.neural_network import MLPClassifier
-from primeqa.mrc.data_models.target_type import TargetType
-
-
-class ConfidenceScorer(object):
-    """
-    Class for confidence scoring.
-    """
-
-    def __init__(self, confidence_model_path=None):
-        """
-        Args:
-            confidence_model_path: Path of confidence model that contains the model file confidence_model.bin.
-        """
-        if confidence_model_path:
-            try:
-                if os.path.isdir(confidence_model_path):
-                    self._confidence_model = joblib.load(os.path.join(confidence_model_path, 'confidence_model.bin'))
-                else:
-                    self._confidence_model = joblib.load(confidence_model_path)
-            except Exception as ex:
-                raise ValueError(f"Unable to load confidence model from {confidence_model_path}") from ex
-        else:
-            self._confidence_model = None
-
-    def model_exists(self) -> bool:
-        """
-        Check if confidence model exists
-        """
-        return self._confidence_model is not None
-
-    @classmethod
-    def make_features(cls, example_predictions) -> list:
-        """
-        Make confidence features from the predictions (top-k answers) of an example.
-
-        Args:
-            example_predictions: Top-k answers generated by postprocessor ExtractivePostProcessor.
-            Each contains:
-                'example_id',
-                'cls_score',
-                'start_logit',
-                'end_logit',
-                'span_answer': {
-                    "start_position",
-                    "end_position",
-                },
-                'span_answer_score',
-                'start_index',
-                'end_index',
-                'passage_index',
-                'target_type_logits',
-                'span_answer_text',
-                'yes_no_answer',
-                'start_stdev',
-                'end_stdev',
-                'query_passage_similarity'
-
-        Returns:
-            List of features used for confidence scoring.
-
-        """
-
-        # compute minimum risk f1
-        minimum_risk_f1 = []
-        for i in range(len(example_predictions)):
-            sum = 0.0
-            for j in range(len(example_predictions)):
-                if j == i:
-                    continue
-                s1 = example_predictions[i]["span_answer"]["start_position"]
-                e1 = example_predictions[i]["span_answer"]["end_position"]
-                s2 = example_predictions[j]["span_answer"]["start_position"]
-                e2 = example_predictions[j]["span_answer"]["end_position"]
-                if s1 == s2 and e1 == e2:
-                    sum += 1.0
-                elif s1 > e2 or e1 < s2:
-                    continue
-                else:
-                    overlap_start_position = max(s1, s2)
-                    overlap_end_position = min(e1, e2)
-                    precision = (overlap_end_position - overlap_start_position + 1.0) / (e1 - s1 + 1.0)
-                    recall = (overlap_end_position - overlap_start_position + 1.0) / (e2 - s2 + 1.0)
-                    f1 = (2.0 * precision * recall) / (precision + recall)
-                    sum += f1
-            sum /= len(example_predictions)
-            minimum_risk_f1.append(sum)
-
-        # if have span answer
-        have_span_answer = []
-        for pred in example_predictions:
-            if "target_type_logits" not in pred:
-                pred["target_type_logits"] = [0, 0, 0, 0, 0]
-                have_span_answer.append(0.0)
-            elif pred["target_type_logits"][TargetType.SPAN_ANSWER] == max(pred["target_type_logits"]):
-                have_span_answer.append(1.0)
-            else:
-                have_span_answer.append(0.0)
-        if max(have_span_answer) == 1.0:
-            example_have_span_answer = 1.0
-        else:
-            example_have_span_answer = 0.0
-
-        average_norm_span_answer_score = 0.0
-        for pred in example_predictions:
-            average_norm_span_answer_score += pred["normalized_span_answer_score"]
-        average_norm_span_answer_score /= len(example_predictions)
-        features = []
-        for i, pred in enumerate(example_predictions):
-            feat = [
-                pred["span_answer_score"],
-                pred["cls_score"],
-                pred["start_logit"],
-                pred["end_logit"],
-                pred["target_type_logits"][TargetType.NO_ANSWER],   # no answer
-                pred["target_type_logits"][TargetType.SPAN_ANSWER],   # span answer
-                have_span_answer[i],
-                example_have_span_answer,
-                minimum_risk_f1[i],
-                pred["normalized_span_answer_score"],
-                pred["normalized_span_answer_score"] - average_norm_span_answer_score,
-                pred["start_stdev"],
-                pred["end_stdev"],
-                pred["query_passage_similarity"]
-            ]
-            features.append(feat)
-        return features
-
-    def predict_scores(self, example_predictions) -> list:
-        """
-        Compute confidence score for each answer in the top-k predictions.
-
-        Args:
-            example_predictions: Top-k answers generated by postprocessor ExtractivePostProcessor.
-
-        Returns:
-            List of scores for each of the top-k answers.
-
-        """
-
-        if example_predictions is None:
-            return None
-        features = self.make_features(example_predictions)
-        if len(features) == 0:
-            return [0.0] * len(example_predictions)
-        feature_dimension = len(features[0])
-        X = np.zeros((len(features), feature_dimension), dtype=np.double)
-        for i, feat in enumerate(features):
-            X[i, :] = feat
-        if not self.model_exists():
-            return [0.0] * len(example_predictions)
-        scores = self._confidence_model.predict_proba(X)
-        # scores[:,0] : scores for incorrect, scores[:, 1]: score for correct
-        return scores[:, 1]
-
-    @classmethod
-    def reference_prediction_overlap(cls, ground_truth, prediction) -> float:
-        """
-        Calculate the F1-style overlap score between ground truth and prediction.
-
-        Args:
-            ground_truth: List of ground truth each containing "start_position" and "end_position".
-            prediction: Prediction containing "start_position" and "end_position".
-
-        Returns:
-            Overlap score between ground truth and prediction.
-
-        """
-
-        if not prediction or not ground_truth:
-            return 0.0
-        max_overlap_score = 0.0
-        for truth in ground_truth:
-            truth_start_position = truth["start_position"]
-            truth_end_position = truth["end_position"]
-            predicted_start_position = prediction["start_position"]
-            predicted_end_position = prediction["end_position"]
-
-            if truth_start_position == predicted_start_position and truth_end_position == predicted_end_position:
-                return 1.0
-            if truth_start_position < 0 or truth_end_position < 0:
-                continue
-            if predicted_start_position > truth_end_position or predicted_end_position < truth_start_position: # f1 = 0 since no overlap
-                continue
-            overlap_start_position = max(predicted_start_position, truth_start_position)
-            overlap_end_position = min(predicted_end_position, truth_end_position)
-
-            p = float(overlap_end_position - overlap_start_position + 1) / float(predicted_end_position - predicted_start_position + 1)
-            r = float(overlap_end_position - overlap_start_position + 1) / float(truth_end_position - truth_start_position + 1)
-            overlap_score= (2 * p * r) / (p + r)
-            if max_overlap_score < overlap_score:
-                max_overlap_score = overlap_score
-        return max_overlap_score
-
-    @classmethod
-    def make_training_data(cls, prediction_file: str, reference_file: str, overlap_threshold: float = 0.5) -> tuple:
-        """
-        Make training data from prediction file and reference file for confidence model training.
-
-        Args:
-            prediction_file: File containing QA result generated by evaluate() of MRC trainer (i.e. eval_predictions.json).
-            reference_file: File containing the ground truth generated by evaluate() of MRC trainer (i.e. eval_references.json).
-            overlap_threshold: Threshold to determine if a prediction is accepted as correct answer.
-
-        Returns:
-            X: Array of features.
-            Y: Array of class label (0: incorrect, 1: correct).
-        """
-
-        try:
-            with open(reference_file, 'r') as f:
-                raw_references = json.load(f)
-        except:
-            raise ValueError("Unable to load reference file to create training data for confidence model")
-        references = dict()
-        for raw_ref in raw_references:
-            example_id = raw_ref["example_id"][0]
-            language = raw_ref["language"][0]
-            ref = dict()
-            ref["language"] = language
-            ref["span_answer"] = []
-            for i in range(len(raw_ref["start_position"])):
-                span = dict()
-                span["start_position"] = raw_ref["start_position"][i]
-                span["end_position"] = raw_ref["end_position"][i]
-                ref["span_answer"].append(span)
-                references[example_id] = ref
-        try:
-            with open(prediction_file, 'r') as f:
-                raw_predictions = json.load(f)
-        except:
-            raise ValueError("Unable to load prediction file to create training data for confidence model")
-        feature_set = dict()
-        label_set = dict()
-        for example_id in raw_predictions:
-            top_k_predictions = raw_predictions[example_id]
-            features_of_top_k_predictions = cls.make_features(top_k_predictions)
-            # only use top-1 features for training
-            feature_set[example_id] = features_of_top_k_predictions[0]
-
-            overlap_score = cls.reference_prediction_overlap(references[example_id]["span_answer"],
-                                                             top_k_predictions[0]["span_answer"])
-            if overlap_score >= overlap_threshold:
-                label_set[example_id] = 1
-            else:
-                label_set[example_id] = 0
-        for example_id in feature_set:
-            number_features_per_example = len(feature_set[example_id])
-            break
-        X = np.zeros((len(feature_set), number_features_per_example), dtype=np.double)
-        Y = np.zeros((len(feature_set)), dtype=np.int)
-        for i, example_id in enumerate(sorted(feature_set.keys())):
-            X[i, :] = feature_set[example_id]
-            Y[i] = label_set[example_id]
-        return (X, Y)
+import os
+import json
+import numpy as np
+import joblib
+from joblib import dump, load
+import sklearn
+from sklearn.neural_network import MLPClassifier
+from primeqa.mrc.data_models.target_type import TargetType
+
+
+class ConfidenceScorer(object):
+    """
+    Class for confidence scoring.
+    """
+
+    def __init__(self, confidence_model_path=None):
+        """
+        Args:
+            confidence_model_path: Path of confidence model that contains the model file confidence_model.bin.
+        """
+        if confidence_model_path:
+            try:
+                if os.path.isdir(confidence_model_path):
+                    self._confidence_model = joblib.load(os.path.join(confidence_model_path, 'confidence_model.bin'))
+                else:
+                    self._confidence_model = joblib.load(confidence_model_path)
+            except Exception as ex:
+                raise ValueError(f"Unable to load confidence model from {confidence_model_path}") from ex
+        else:
+            self._confidence_model = None
+
+    def model_exists(self) -> bool:
+        """
+        Check if confidence model exists
+        """
+        return self._confidence_model is not None
+
+    @classmethod
+    def make_features(cls, example_predictions) -> list:
+        """
+        Make confidence features from the predictions (top-k answers) of an example.
+
+        Args:
+            example_predictions: Top-k answers generated by postprocessor ExtractivePostProcessor.
+            Each contains:
+                'example_id',
+                'cls_score',
+                'start_logit',
+                'end_logit',
+                'span_answer': {
+                    "start_position",
+                    "end_position",
+                },
+                'span_answer_score',
+                'start_index',
+                'end_index',
+                'passage_index',
+                'target_type_logits',
+                'span_answer_text',
+                'yes_no_answer',
+                'start_stdev',
+                'end_stdev',
+                'query_passage_similarity'
+
+        Returns:
+            List of features used for confidence scoring.
+
+        """
+
+        # compute minimum risk f1
+        minimum_risk_f1 = []
+        for i in range(len(example_predictions)):
+            sum = 0.0
+            for j in range(len(example_predictions)):
+                if j == i:
+                    continue
+                s1 = example_predictions[i]["span_answer"]["start_position"]
+                e1 = example_predictions[i]["span_answer"]["end_position"]
+                s2 = example_predictions[j]["span_answer"]["start_position"]
+                e2 = example_predictions[j]["span_answer"]["end_position"]
+                if s1 == s2 and e1 == e2:
+                    sum += 1.0
+                elif s1 > e2 or e1 < s2:
+                    continue
+                else:
+                    overlap_start_position = max(s1, s2)
+                    overlap_end_position = min(e1, e2)
+                    precision = (overlap_end_position - overlap_start_position + 1.0) / (e1 - s1 + 1.0)
+                    recall = (overlap_end_position - overlap_start_position + 1.0) / (e2 - s2 + 1.0)
+                    f1 = (2.0 * precision * recall) / (precision + recall)
+                    sum += f1
+            sum /= len(example_predictions)
+            minimum_risk_f1.append(sum)
+
+        # if have span answer
+        have_span_answer = []
+        for pred in example_predictions:
+            if "target_type_logits" not in pred:
+                pred["target_type_logits"] = [0, 0, 0, 0, 0]
+                have_span_answer.append(0.0)
+            elif pred["target_type_logits"][TargetType.SPAN_ANSWER] == max(pred["target_type_logits"]):
+                have_span_answer.append(1.0)
+            else:
+                have_span_answer.append(0.0)
+        if max(have_span_answer) == 1.0:
+            example_have_span_answer = 1.0
+        else:
+            example_have_span_answer = 0.0
+
+        average_norm_span_answer_score = 0.0
+        for pred in example_predictions:
+            average_norm_span_answer_score += pred["normalized_span_answer_score"]
+        average_norm_span_answer_score /= len(example_predictions)
+        features = []
+        for i, pred in enumerate(example_predictions):
+            feat = [
+                pred["span_answer_score"],
+                pred["cls_score"],
+                pred["start_logit"],
+                pred["end_logit"],
+                pred["target_type_logits"][TargetType.NO_ANSWER],   # no answer
+                pred["target_type_logits"][TargetType.SPAN_ANSWER],   # span answer
+                have_span_answer[i],
+                example_have_span_answer,
+                minimum_risk_f1[i],
+                pred["normalized_span_answer_score"],
+                pred["normalized_span_answer_score"] - average_norm_span_answer_score,
+                pred["start_stdev"],
+                pred["end_stdev"],
+                pred["query_passage_similarity"]
+            ]
+            features.append(feat)
+        return features
+
+    def predict_scores(self, example_predictions) -> list:
+        """
+        Compute confidence score for each answer in the top-k predictions.
+
+        Args:
+            example_predictions: Top-k answers generated by postprocessor ExtractivePostProcessor.
+
+        Returns:
+            List of scores for each of the top-k answers.
+
+        """
+
+        if example_predictions is None:
+            return None
+        features = self.make_features(example_predictions)
+        if len(features) == 0:
+            return [0.0] * len(example_predictions)
+        feature_dimension = len(features[0])
+        X = np.zeros((len(features), feature_dimension), dtype=np.double)
+        for i, feat in enumerate(features):
+            X[i, :] = feat
+        if not self.model_exists():
+            return [0.0] * len(example_predictions)
+        scores = self._confidence_model.predict_proba(X)
+        # scores[:,0] : scores for incorrect, scores[:, 1]: score for correct
+        return scores[:, 1]
+
+    @classmethod
+    def reference_prediction_overlap(cls, ground_truth, prediction) -> float:
+        """
+        Calculate the F1-style overlap score between ground truth and prediction.
+
+        Args:
+            ground_truth: List of ground truth each containing "start_position" and "end_position".
+            prediction: Prediction containing "start_position" and "end_position".
+
+        Returns:
+            Overlap score between ground truth and prediction.
+
+        """
+
+        if not prediction or not ground_truth:
+            return 0.0
+        max_overlap_score = 0.0
+        for truth in ground_truth:
+            truth_start_position = truth["start_position"]
+            truth_end_position = truth["end_position"]
+            predicted_start_position = prediction["start_position"]
+            predicted_end_position = prediction["end_position"]
+
+            if truth_start_position == predicted_start_position and truth_end_position == predicted_end_position:
+                return 1.0
+            if truth_start_position < 0 or truth_end_position < 0:
+                continue
+            if predicted_start_position > truth_end_position or predicted_end_position < truth_start_position: # f1 = 0 since no overlap
+                continue
+            overlap_start_position = max(predicted_start_position, truth_start_position)
+            overlap_end_position = min(predicted_end_position, truth_end_position)
+
+            p = float(overlap_end_position - overlap_start_position + 1) / float(predicted_end_position - predicted_start_position + 1)
+            r = float(overlap_end_position - overlap_start_position + 1) / float(truth_end_position - truth_start_position + 1)
+            overlap_score= (2 * p * r) / (p + r)
+            if max_overlap_score < overlap_score:
+                max_overlap_score = overlap_score
+        return max_overlap_score
+
+    @classmethod
+    def make_training_data(cls, prediction_file: str, reference_file: str, overlap_threshold: float = 0.5) -> tuple:
+        """
+        Make training data from prediction file and reference file for confidence model training.
+
+        Args:
+            prediction_file: File containing QA result generated by evaluate() of MRC trainer (i.e. eval_predictions.json).
+            reference_file: File containing the ground truth generated by evaluate() of MRC trainer (i.e. eval_references.json).
+            overlap_threshold: Threshold to determine if a prediction is accepted as correct answer.
+
+        Returns:
+            X: Array of features.
+            Y: Array of class label (0: incorrect, 1: correct).
+        """
+
+        try:
+            with open(reference_file, 'r') as f:
+                raw_references = json.load(f)
+        except:
+            raise ValueError("Unable to load reference file to create training data for confidence model")
+        references = dict()
+        for raw_ref in raw_references:
+            example_id = raw_ref["example_id"][0]
+            language = raw_ref["language"][0]
+            ref = dict()
+            ref["language"] = language
+            ref["span_answer"] = []
+            for i in range(len(raw_ref["start_position"])):
+                span = dict()
+                span["start_position"] = raw_ref["start_position"][i]
+                span["end_position"] = raw_ref["end_position"][i]
+                ref["span_answer"].append(span)
+                references[example_id] = ref
+        try:
+            with open(prediction_file, 'r') as f:
+                raw_predictions = json.load(f)
+        except:
+            raise ValueError("Unable to load prediction file to create training data for confidence model")
+        feature_set = dict()
+        label_set = dict()
+        for example_id in raw_predictions:
+            top_k_predictions = raw_predictions[example_id]
+            features_of_top_k_predictions = cls.make_features(top_k_predictions)
+            # only use top-1 features for training
+            feature_set[example_id] = features_of_top_k_predictions[0]
+
+            overlap_score = cls.reference_prediction_overlap(references[example_id]["span_answer"],
+                                                             top_k_predictions[0]["span_answer"])
+            if overlap_score >= overlap_threshold:
+                label_set[example_id] = 1
+            else:
+                label_set[example_id] = 0
+        for example_id in feature_set:
+            number_features_per_example = len(feature_set[example_id])
+            break
+        X = np.zeros((len(feature_set), number_features_per_example), dtype=np.double)
+        Y = np.zeros((len(feature_set)), dtype=np.int)
+        for i, example_id in enumerate(sorted(feature_set.keys())):
+            X[i, :] = feature_set[example_id]
+            Y[i] = label_set[example_id]
+        return (X, Y)
```

## primeqa/calibration/train_confidence_calibrator.py

 * *Ordering differences only*

```diff
@@ -1,532 +1,532 @@
-import logging
-import os
-import sys
-import traceback
-from dataclasses import dataclass, field
-from importlib import import_module
-from operator import attrgetter
-from typing import Optional, Type
-import json
-import joblib
-from joblib import dump, load
-from sklearn.neural_network import MLPClassifier
-from primeqa.calibration.confidence_scorer import ConfidenceScorer
-
-import datasets
-from datasets import DatasetDict, load_from_disk
-from transformers import HfArgumentParser, TrainingArguments, DataCollatorWithPadding, AutoConfig, AutoTokenizer
-from transformers.trainer_utils import get_last_checkpoint, set_seed
-
-from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
-from primeqa.mrc.metrics.tydi_f1.tydi_f1 import TyDiF1
-from primeqa.mrc.models.heads.extractive import EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD
-from primeqa.mrc.models.task_model import ModelForDownstreamTasks
-from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
-from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers
-from primeqa.mrc.processors.preprocessors.tydiqa import TyDiQAPreprocessor
-from primeqa.mrc.trainers.mrc import MRCTrainer
-
-
-def object_reference(reference_as_str: str) -> object:
-    """
-    Given a fully qualified path to a class reference, return a pointer to the reference.
-    This will work with types, functions, methods, and other objects (e.g. dict).
-
-    Args:
-        reference_as_str: the fully qualified path (expects the fully qualified path in dot notation,
-                          e.g. primeqa.mrc.processors.postprocessors.extractive.ExtractivePostProcessor).
-
-    Returns:
-        reference to path given by input
-
-    Raises:
-        TypeError: Unable to resolve input path
-    """
-    def _split_into_class_and_module_name(class_path):
-        modules = class_path.split('.')
-        if len(modules) > 1:
-            return ".".join(modules[:-1]), modules[-1]
-        else:
-            return class_path, None
-
-    try:
-        module_name, object_name = _split_into_class_and_module_name(reference_as_str)
-        module_reference = import_module(module_name)
-        if object_name is None:
-            return module_reference
-        else:
-            return getattr(module_reference, object_name)
-    except Exception as ex:
-        traceback.print_exc()  # Shows additional traceback for why imports fail
-        raise TypeError(f"Unable to resolve the string {reference_as_str} to a fully qualified class path") from ex
-
-
-# modified from
-# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
-@dataclass
-class ModelArguments:
-    """
-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
-    """
-
-    model_name_or_path: str = field(
-        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
-    )
-    config_name: Optional[str] = field(
-        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
-    )
-    tokenizer_name: Optional[str] = field(
-        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
-    )
-    cache_dir: Optional[str] = field(
-        default=None,
-        metadata={"help": "Path to directory to store the pretrained models downloaded from huggingface.co"},
-    )
-
-
-# modified from
-# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
-@dataclass
-class DataTrainingArguments:
-    """
-    Arguments pertaining to what data we are going to input our model for training and eval.
-    """
-
-    dataset_name: str = field(
-        default="tydiqa", metadata={"help": "The name of the dataset to use (via the datasets library)."}
-    )
-    dataset_config_name: str = field(
-        default="primary_task", metadata={
-            "help": "The configuration name of the dataset to use (via the datasets library)."
-        }
-    )
-    overwrite_cache: bool = field(
-        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
-    )
-    relative_confidence_train_size: float = field(
-        default=0.1,
-        metadata={"help": "The relative size of confidence train set split from original train set."}
-    )
-    confidence_dataset_dir: str = field(
-        default=None,
-        metadata={"help": "The directory to save the created confidence datasets."}
-    )
-    preprocessing_num_workers: Optional[int] = field(
-        default=None,
-        metadata={"help": "The number of processes to use for the preprocessing."},
-    )
-    max_seq_length: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "The maximum total input sequence length after tokenization. Sequences longer "
-                    "than this will be truncated, sequences shorter will be padded."
-        },
-    )
-    max_q_char_len: int = field(
-        default=128, metadata={"help": "Max length per question in characters"}
-    )
-    single_context_multiple_passages: bool = field(
-        default=False, metadata={
-            "help": "Allow multiple passages in the same input feature. "
-                    "For an example with question q and context c_{1..n} setting this to True"
-                    "will allow q|c_{i}c_{i+1}; whereas setting this to False enforces q|c_{i} q|c_{i+1}. "
-                    "Note that not all datasets/preprocessors support both values of this parameter. "
-                    "Some preprocessors may override this value."
-            },
-    )
-    max_contexts: Optional[int] = field(
-        default=None, metadata={"help": "Max contexts per consider"}
-    )
-    max_train_samples: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
-                    "value if set."
-        },
-    )
-    max_eval_samples: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
-                    "value if set."
-        },
-    )
-    doc_stride: int = field(
-        default=128,
-        metadata={"help": "When splitting up a long document into chunks, how much stride to take between chunks."},
-    )
-    n_best_size: int = field(
-        default=20,
-        metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
-    )
-    n_best_logits: int = field(
-        default=20,
-        metadata={"help": "The number of logits to consider when searching for start and end position of an answer"}
-    )
-    max_answer_length: int = field(
-        default=32,
-        metadata={
-            "help": "The maximum length of an answer that can be generated. This is needed because the start "
-                    "and end predictions are not conditioned on one another."
-        },
-    )
-    negative_sampling_prob_when_has_answer: float = field(
-        default=0.01,
-        metadata={
-            "help": "Only used when preparing training features, not for decoding. "
-                    "For an example with question q and context c_{1..n} where ∃ answer a ∈ c"
-                    "an input feature span q|c_{i} where a ∉ c_{i} will be kept with this probability."
-                    "Otherwise it will be discarded."
-        },
-    )
-    negative_sampling_prob_when_no_answer: float = field(
-        default=0.04,
-        metadata={
-            "help": "Only used when preparing training features, not for decoding. "
-                    "For an example with question q and context c_{1..n} where ∄ answer a ∈ c"
-                    "an input feature span q|c_{i} will be kept with this probability."
-                    "Otherwise it will be discarded."
-        },
-    )
-
-
-@dataclass
-class TaskArguments:
-    """
-    Task specific arguments.
-    """
-    confidence_model_dir: str = field(
-        metadata={"help": "The directory to save confidence model."}
-    )
-    scorer_type: str = field(
-        default='weighted_sum_target_type_and_score_diff',
-        metadata={"help": "The name of the scorer to compute answer score.",
-                  "choices": SupportedSpanScorers.get_supported()
-                  }
-    )
-    task_heads: object_reference = field(
-        default=None,
-        metadata={"help": "The name of the task head to use.",
-                  "choices": [EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD]
-                  }
-    )
-    preprocessor: object_reference = field(
-        default=TyDiQAPreprocessor,
-        metadata={"help": "The name of the preprocessor to use.",
-                  "choices": [TyDiQAPreprocessor]
-                  }
-    )
-    postprocessor: object_reference = field(
-        default=ExtractivePostProcessor,
-        metadata={"help": "The name of the postprocessor to use.",
-                  "choices": [ExtractivePostProcessor]
-                  }
-    )
-    eval_metrics: object_reference = field(
-        default=TyDiF1,
-        metadata={"help": "The name of the evaluation metric function.",
-                  "choices": [TyDiF1]
-                 }
-    )
-    output_dropout_rate: float = field(
-        default=0.25,
-        metadata={"help": "The dropout probability applied to LM output in "
-                          "order to generate confidence calibration features."
-                  },
-    )
-    decoding_times_with_dropout: int = field(
-        default=5,
-        metadata={"help": "The number of decoding times to generate confidence "
-                          "calibration features with dropout."
-                  },
-    )
-    max_iter_of_confidence_model_training: int = field(
-        default=200,
-        metadata={"help": "The maximum number of iterations for confidence model training."}
-    )
-    prediction_reference_overlap_threshold: float = field(
-        default=0.5,
-        metadata={"help": "The threshold to determine if a prediction is accepted as correct. "
-                            "If the overlap score between prediction and reference is greater "
-                            "than this threshold, the prediction is labeled as correct."
-                  },
-    )
-
-
-    def __post_init__(self):
-        if not self.task_heads:
-            self.task_heads = EXTRACTIVE_HEAD  # cannot directly set mutable value as default
-
-
-def main():
-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments, TaskArguments))
-    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
-        # If we pass only one argument to the script and it's the path to a json file,
-        # let's parse it to get our arguments.
-        model_args, data_args, training_args, task_args = \
-            parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
-    else:
-        model_args, data_args, training_args, task_args = parser.parse_args_into_dataclasses()
-
-    logger = logging.getLogger(__name__)
-    scorer_type = task_args.scorer_type
-    set_seed(training_args.seed)
-
-    # Detecting last checkpoint.
-    last_checkpoint = None
-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
-        last_checkpoint = get_last_checkpoint(training_args.output_dir)
-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
-            raise ValueError(
-                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
-                "Use --overwrite_output_dir to overcome."
-            )
-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
-            logger.info(
-                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
-                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
-            )
-
-    task_heads = task_args.task_heads
-    config = AutoConfig.from_pretrained(
-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
-        cache_dir=model_args.cache_dir,
-    )
-    tokenizer = AutoTokenizer.from_pretrained(
-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
-        cache_dir=model_args.cache_dir,
-        use_fast=True,
-        config=config,
-    )
-
-    config.sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)
-    config.output_dropout_rate = task_args.output_dropout_rate
-    config.decoding_times_with_dropout = task_args.decoding_times_with_dropout
-    model = ModelForDownstreamTasks.from_config(
-        config,
-        model_args.model_name_or_path,
-        task_heads=task_heads,
-        cache_dir=model_args.cache_dir,
-    )
-    model.set_task_head(next(iter(task_heads)))
-
-    confidence_datasets = None
-    if data_args.confidence_dataset_dir:
-        try:
-            logger.info('Loading confidence datasets from disk')
-            confidence_datasets = load_from_disk(data_args.confidence_dataset_dir)
-        except:
-            logger.info('confidence_dataset_dir is neither a dataset directory nor a dataset dict directory')
-    if not confidence_datasets:
-        logger.info('Creating confidence datasets from raw datasets')
-        # load raw dataset
-        logger.info('Loading raw datasets')
-        raw_datasets = datasets.load_dataset(
-            data_args.dataset_name,
-            data_args.dataset_config_name,
-            cache_dir=model_args.cache_dir,
-        )
-
-        original_train_set = raw_datasets["train"]
-        split_train_set = original_train_set.train_test_split(test_size=data_args.relative_confidence_train_size)
-        mrc_train_set = split_train_set["train"]
-        confidence_train_set = split_train_set["test"]
-        validation_set = raw_datasets["validation"]
-
-        confidence_datasets = DatasetDict({
-            "mrc_train": mrc_train_set,
-            "confidence_train": confidence_train_set,
-            "validation": validation_set,
-        })
-        # save new datasets
-        if data_args.confidence_dataset_dir:
-            confidence_datasets.save_to_disk(data_args.confidence_dataset_dir)
-
-    # load preprocessor
-    preprocessor_class = task_args.preprocessor
-    preprocessor = preprocessor_class(
-        stride=data_args.doc_stride,
-        tokenizer=tokenizer,
-        negative_sampling_prob_when_has_answer=data_args.negative_sampling_prob_when_has_answer,
-        negative_sampling_prob_when_no_answer=data_args.negative_sampling_prob_when_no_answer,
-        load_from_cache_file=not data_args.overwrite_cache,
-        max_seq_len=data_args.max_seq_length,
-        num_workers=data_args.preprocessing_num_workers,
-        max_q_char_len=data_args.max_q_char_len,
-        single_context_multiple_passages=data_args.single_context_multiple_passages,
-        max_contexts=data_args.max_contexts,
-    )
-
-    # process train data
-    if training_args.do_train:
-        train_dataset = confidence_datasets["mrc_train"]
-        max_train_samples = data_args.max_train_samples
-        if max_train_samples is not None:
-            # We will select sample from whole data if argument is specified
-            train_dataset = train_dataset.select(range(max_train_samples))
-        # Train Feature Creation
-        with training_args.main_process_first(desc="train dataset map pre-processing"):
-            _, train_dataset = preprocessor.process_train(train_dataset)
-
-    # process val and confidence data
-    if training_args.do_eval:
-        eval_examples = confidence_datasets["validation"]
-        max_eval_samples = data_args.max_eval_samples
-        if max_eval_samples is not None:
-            # We will select sample from whole data if argument is specified
-            eval_examples = eval_examples.select(range(max_eval_samples))
-        # Validation Feature Creation
-        with training_args.main_process_first(desc="validation dataset map pre-processing"):
-            eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)
-
-        conf_examples = confidence_datasets["confidence_train"]
-        max_eval_samples = data_args.max_eval_samples
-        if max_eval_samples is not None:  # data_args.max_eval_samples is not None:
-            # We will select sample from whole data
-            conf_examples = conf_examples.select(range(max_eval_samples))
-        # Validation Feature Creation
-        with training_args.main_process_first(desc="confidence dataset map pre-processing"):
-            conf_examples, conf_dataset = preprocessor.process_eval(conf_examples)
-
-    # If using mixed precision we pad for efficient hardware acceleration
-    using_mixed_precision = any(attrgetter('fp16', 'bf16')(training_args))
-    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=64 if using_mixed_precision else None)
-
-    if task_args.task_heads == EXTRACTIVE_WITH_CONFIDENCE_HEAD:
-        output_confidence_feature = True
-    else:
-        output_confidence_feature = False
-    postprocessor_class = task_args.postprocessor
-
-    # noinspection PyProtectedMember
-    postprocessor = postprocessor_class(
-        k=data_args.n_best_logits,
-        n_best_size=data_args.n_best_size,
-        max_answer_length=data_args.max_answer_length,
-        scorer_type=SupportedSpanScorers(scorer_type),
-        single_context_multiple_passages=preprocessor._single_context_multiple_passages,
-        output_confidence_feature = output_confidence_feature,
-    )
-
-    eval_metrics = task_args.eval_metrics()
-
-    def compute_metrics(p: EvalPredictionWithProcessing):
-        return eval_metrics.compute(predictions=p.processed_predictions, references=p.label_ids)
-
-    trainer = MRCTrainer(
-        model=model,
-        args=training_args,
-        train_dataset=train_dataset if training_args.do_train else None,
-        eval_dataset=eval_dataset if training_args.do_eval else None,
-        eval_examples=eval_examples if training_args.do_eval else None,
-        tokenizer=tokenizer,
-        data_collator=data_collator,
-        post_process_function=postprocessor.process_references_and_predictions,  # see QATrainer in Huggingface
-        compute_metrics=compute_metrics,
-    )
-
-    checkpoint = None
-    if training_args.resume_from_checkpoint is not None:
-        checkpoint = training_args.resume_from_checkpoint
-    elif last_checkpoint is not None:
-        checkpoint = last_checkpoint
-
-    # training
-    if training_args.do_train:
-        train_result = trainer.train(resume_from_checkpoint=checkpoint)
-        trainer.save_model()  # Saves the tokenizer too for easy upload
-
-        metrics = train_result.metrics
-        max_train_samples = (
-            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
-        )
-        metrics["train_samples"] = min(max_train_samples, len(train_dataset))
-
-        trainer.log_metrics("train", metrics)
-        trainer.save_metrics("train", metrics)
-        trainer.save_state()
-
-    # write prediction results of confidence train set and validation set
-    # to different directories
-    base_output_dir = training_args.output_dir
-    validation_set_prediction_output_dir = os.path.join(base_output_dir, "validation_set_predictions")
-    os.makedirs(validation_set_prediction_output_dir, exist_ok=True)
-    confidence_set_prediction_output_dir = os.path.join(base_output_dir, "confidence_set_predictions")
-    os.makedirs(confidence_set_prediction_output_dir, exist_ok=True)
-
-    if training_args.do_eval:
-        logger.info("*** Evaluate on validation set ***")
-        metrics = trainer.evaluate(eval_dataset=eval_dataset, eval_examples=eval_examples)
-        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
-        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
-        trainer.log_metrics("eval", metrics)
-        trainer.save_metrics("eval", metrics)
-        for fn in ["eval_predictions.json", "eval_references.json", "eval_predictions_processed.json"]:
-            if os.path.exists(os.path.join(base_output_dir, fn)):
-                os.replace(os.path.join(base_output_dir, fn), os.path.join(validation_set_prediction_output_dir, fn))
-            else:
-                raise ValueError("Unable to find eval result file {} from {}".format(fn, base_output_dir))
-
-        logger.info("*** Evaluate on confidence train set ***")
-        metrics = trainer.evaluate(eval_dataset=conf_dataset, eval_examples=conf_examples)
-        max_conf_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(conf_dataset)
-        metrics["confidence_samples"] = min(max_conf_samples, len(conf_dataset))
-        trainer.log_metrics("eval", metrics)
-        trainer.save_metrics("eval", metrics)
-        for fn in ["eval_predictions.json", "eval_references.json", "eval_predictions_processed.json"]:
-            if os.path.exists(os.path.join(base_output_dir, fn)):
-                os.replace(os.path.join(base_output_dir, fn), os.path.join(confidence_set_prediction_output_dir, fn))
-            else:
-                raise ValueError("Unable to find eval result file {} from {}".format(fn, base_output_dir))
-
-    # confidence model training
-    confidence_set_prediction_file = os.path.join(confidence_set_prediction_output_dir, "eval_predictions.json")
-    if not os.path.exists(confidence_set_prediction_file):
-        raise ValueError("Unable to find eval_predictions.json file from {} for confidence model training.".format(confidence_set_prediction_output_dir))
-    confidence_set_reference_file = os.path.join(confidence_set_prediction_output_dir, "eval_references.json")
-    if not os.path.exists(confidence_set_reference_file):
-        raise ValueError("Unable to find eval_references.json file from {} for confidence model training.".format(confidence_set_prediction_output_dir))
-
-    confidence_model = MLPClassifier(random_state = 1, activation = 'tanh',
-                                     hidden_layer_sizes=(100,100),
-                                     max_iter=task_args.max_iter_of_confidence_model_training,
-                                     verbose=1)
-    X, Y = ConfidenceScorer.make_training_data(confidence_set_prediction_file, confidence_set_reference_file,
-                                               task_args.prediction_reference_overlap_threshold)
-
-    logger.info("Training confidence model ...")
-    confidence_model.fit(X, Y)
-
-    confidence_model_file = os.path.join(task_args.confidence_model_dir, 'confidence_model.bin')
-    dump(confidence_model, confidence_model_file)
-    logging.info("Saved confidence model to {}".format(confidence_model_file))
-
-    # confidence model eval on validation set
-    confidence_scorer = ConfidenceScorer(task_args.confidence_model_dir)
-    validation_set_prediction_file = os.path.join(validation_set_prediction_output_dir, "eval_predictions.json")
-    if not os.path.exists(validation_set_prediction_file):
-        raise ValueError("Unable to find eval_predictions.json file from {} for confidence model training.".format(validation_set_prediction_output_dir))
-    try:
-        with open(validation_set_prediction_file, 'r') as f:
-            validation_predictions = json.load(f)
-    except:
-        raise ValueError("Unable to load validation predictions from {}".format(validation_set_prediction_file))
-
-    for example_id in validation_predictions:
-        scores = confidence_scorer.predict_scores(validation_predictions[example_id])
-        for i in range(len(validation_predictions[example_id])):
-            validation_predictions[example_id][i]["confidence_score"] = scores[i]
-
-    rescored_prediction_file = os.path.join(validation_set_prediction_output_dir, "eval_predictions.rescored.json")
-    try:
-        with open(rescored_prediction_file, 'w') as f:
-            json.dump(validation_predictions, f, indent=4)
-        logger.info("Saved rescored validation predictions to {}".format(rescored_prediction_file))
-    except:
-        raise ValueError("Unable to save the rescored validation prediction file to {}".format(rescored_prediction_file))
-
-
-
-
-if __name__ == '__main__':
-    main()
+import logging
+import os
+import sys
+import traceback
+from dataclasses import dataclass, field
+from importlib import import_module
+from operator import attrgetter
+from typing import Optional, Type
+import json
+import joblib
+from joblib import dump, load
+from sklearn.neural_network import MLPClassifier
+from primeqa.calibration.confidence_scorer import ConfidenceScorer
+
+import datasets
+from datasets import DatasetDict, load_from_disk
+from transformers import HfArgumentParser, TrainingArguments, DataCollatorWithPadding, AutoConfig, AutoTokenizer
+from transformers.trainer_utils import get_last_checkpoint, set_seed
+
+from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
+from primeqa.mrc.metrics.tydi_f1.tydi_f1 import TyDiF1
+from primeqa.mrc.models.heads.extractive import EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD
+from primeqa.mrc.models.task_model import ModelForDownstreamTasks
+from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
+from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers
+from primeqa.mrc.processors.preprocessors.tydiqa import TyDiQAPreprocessor
+from primeqa.mrc.trainers.mrc import MRCTrainer
+
+
+def object_reference(reference_as_str: str) -> object:
+    """
+    Given a fully qualified path to a class reference, return a pointer to the reference.
+    This will work with types, functions, methods, and other objects (e.g. dict).
+
+    Args:
+        reference_as_str: the fully qualified path (expects the fully qualified path in dot notation,
+                          e.g. primeqa.mrc.processors.postprocessors.extractive.ExtractivePostProcessor).
+
+    Returns:
+        reference to path given by input
+
+    Raises:
+        TypeError: Unable to resolve input path
+    """
+    def _split_into_class_and_module_name(class_path):
+        modules = class_path.split('.')
+        if len(modules) > 1:
+            return ".".join(modules[:-1]), modules[-1]
+        else:
+            return class_path, None
+
+    try:
+        module_name, object_name = _split_into_class_and_module_name(reference_as_str)
+        module_reference = import_module(module_name)
+        if object_name is None:
+            return module_reference
+        else:
+            return getattr(module_reference, object_name)
+    except Exception as ex:
+        traceback.print_exc()  # Shows additional traceback for why imports fail
+        raise TypeError(f"Unable to resolve the string {reference_as_str} to a fully qualified class path") from ex
+
+
+# modified from
+# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
+@dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={"help": "Path to directory to store the pretrained models downloaded from huggingface.co"},
+    )
+
+
+# modified from
+# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
+@dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: str = field(
+        default="tydiqa", metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    dataset_config_name: str = field(
+        default="primary_task", metadata={
+            "help": "The configuration name of the dataset to use (via the datasets library)."
+        }
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    relative_confidence_train_size: float = field(
+        default=0.1,
+        metadata={"help": "The relative size of confidence train set split from original train set."}
+    )
+    confidence_dataset_dir: str = field(
+        default=None,
+        metadata={"help": "The directory to save the created confidence datasets."}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=None,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_seq_length: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "The maximum total input sequence length after tokenization. Sequences longer "
+                    "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    max_q_char_len: int = field(
+        default=128, metadata={"help": "Max length per question in characters"}
+    )
+    single_context_multiple_passages: bool = field(
+        default=False, metadata={
+            "help": "Allow multiple passages in the same input feature. "
+                    "For an example with question q and context c_{1..n} setting this to True"
+                    "will allow q|c_{i}c_{i+1}; whereas setting this to False enforces q|c_{i} q|c_{i+1}. "
+                    "Note that not all datasets/preprocessors support both values of this parameter. "
+                    "Some preprocessors may override this value."
+            },
+    )
+    max_contexts: Optional[int] = field(
+        default=None, metadata={"help": "Max contexts per consider"}
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+                    "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+                    "value if set."
+        },
+    )
+    doc_stride: int = field(
+        default=128,
+        metadata={"help": "When splitting up a long document into chunks, how much stride to take between chunks."},
+    )
+    n_best_size: int = field(
+        default=20,
+        metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
+    )
+    n_best_logits: int = field(
+        default=20,
+        metadata={"help": "The number of logits to consider when searching for start and end position of an answer"}
+    )
+    max_answer_length: int = field(
+        default=32,
+        metadata={
+            "help": "The maximum length of an answer that can be generated. This is needed because the start "
+                    "and end predictions are not conditioned on one another."
+        },
+    )
+    negative_sampling_prob_when_has_answer: float = field(
+        default=0.01,
+        metadata={
+            "help": "Only used when preparing training features, not for decoding. "
+                    "For an example with question q and context c_{1..n} where ∃ answer a ∈ c"
+                    "an input feature span q|c_{i} where a ∉ c_{i} will be kept with this probability."
+                    "Otherwise it will be discarded."
+        },
+    )
+    negative_sampling_prob_when_no_answer: float = field(
+        default=0.04,
+        metadata={
+            "help": "Only used when preparing training features, not for decoding. "
+                    "For an example with question q and context c_{1..n} where ∄ answer a ∈ c"
+                    "an input feature span q|c_{i} will be kept with this probability."
+                    "Otherwise it will be discarded."
+        },
+    )
+
+
+@dataclass
+class TaskArguments:
+    """
+    Task specific arguments.
+    """
+    confidence_model_dir: str = field(
+        metadata={"help": "The directory to save confidence model."}
+    )
+    scorer_type: str = field(
+        default='weighted_sum_target_type_and_score_diff',
+        metadata={"help": "The name of the scorer to compute answer score.",
+                  "choices": SupportedSpanScorers.get_supported()
+                  }
+    )
+    task_heads: object_reference = field(
+        default=None,
+        metadata={"help": "The name of the task head to use.",
+                  "choices": [EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD]
+                  }
+    )
+    preprocessor: object_reference = field(
+        default=TyDiQAPreprocessor,
+        metadata={"help": "The name of the preprocessor to use.",
+                  "choices": [TyDiQAPreprocessor]
+                  }
+    )
+    postprocessor: object_reference = field(
+        default=ExtractivePostProcessor,
+        metadata={"help": "The name of the postprocessor to use.",
+                  "choices": [ExtractivePostProcessor]
+                  }
+    )
+    eval_metrics: object_reference = field(
+        default=TyDiF1,
+        metadata={"help": "The name of the evaluation metric function.",
+                  "choices": [TyDiF1]
+                 }
+    )
+    output_dropout_rate: float = field(
+        default=0.25,
+        metadata={"help": "The dropout probability applied to LM output in "
+                          "order to generate confidence calibration features."
+                  },
+    )
+    decoding_times_with_dropout: int = field(
+        default=5,
+        metadata={"help": "The number of decoding times to generate confidence "
+                          "calibration features with dropout."
+                  },
+    )
+    max_iter_of_confidence_model_training: int = field(
+        default=200,
+        metadata={"help": "The maximum number of iterations for confidence model training."}
+    )
+    prediction_reference_overlap_threshold: float = field(
+        default=0.5,
+        metadata={"help": "The threshold to determine if a prediction is accepted as correct. "
+                            "If the overlap score between prediction and reference is greater "
+                            "than this threshold, the prediction is labeled as correct."
+                  },
+    )
+
+
+    def __post_init__(self):
+        if not self.task_heads:
+            self.task_heads = EXTRACTIVE_HEAD  # cannot directly set mutable value as default
+
+
+def main():
+    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments, TaskArguments))
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args, task_args = \
+            parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args, task_args = parser.parse_args_into_dataclasses()
+
+    logger = logging.getLogger(__name__)
+    scorer_type = task_args.scorer_type
+    set_seed(training_args.seed)
+
+    # Detecting last checkpoint.
+    last_checkpoint = None
+    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
+        last_checkpoint = get_last_checkpoint(training_args.output_dir)
+        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use --overwrite_output_dir to overcome."
+            )
+        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
+            logger.info(
+                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
+                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
+            )
+
+    task_heads = task_args.task_heads
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
+        cache_dir=model_args.cache_dir,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=True,
+        config=config,
+    )
+
+    config.sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)
+    config.output_dropout_rate = task_args.output_dropout_rate
+    config.decoding_times_with_dropout = task_args.decoding_times_with_dropout
+    model = ModelForDownstreamTasks.from_config(
+        config,
+        model_args.model_name_or_path,
+        task_heads=task_heads,
+        cache_dir=model_args.cache_dir,
+    )
+    model.set_task_head(next(iter(task_heads)))
+
+    confidence_datasets = None
+    if data_args.confidence_dataset_dir:
+        try:
+            logger.info('Loading confidence datasets from disk')
+            confidence_datasets = load_from_disk(data_args.confidence_dataset_dir)
+        except:
+            logger.info('confidence_dataset_dir is neither a dataset directory nor a dataset dict directory')
+    if not confidence_datasets:
+        logger.info('Creating confidence datasets from raw datasets')
+        # load raw dataset
+        logger.info('Loading raw datasets')
+        raw_datasets = datasets.load_dataset(
+            data_args.dataset_name,
+            data_args.dataset_config_name,
+            cache_dir=model_args.cache_dir,
+        )
+
+        original_train_set = raw_datasets["train"]
+        split_train_set = original_train_set.train_test_split(test_size=data_args.relative_confidence_train_size)
+        mrc_train_set = split_train_set["train"]
+        confidence_train_set = split_train_set["test"]
+        validation_set = raw_datasets["validation"]
+
+        confidence_datasets = DatasetDict({
+            "mrc_train": mrc_train_set,
+            "confidence_train": confidence_train_set,
+            "validation": validation_set,
+        })
+        # save new datasets
+        if data_args.confidence_dataset_dir:
+            confidence_datasets.save_to_disk(data_args.confidence_dataset_dir)
+
+    # load preprocessor
+    preprocessor_class = task_args.preprocessor
+    preprocessor = preprocessor_class(
+        stride=data_args.doc_stride,
+        tokenizer=tokenizer,
+        negative_sampling_prob_when_has_answer=data_args.negative_sampling_prob_when_has_answer,
+        negative_sampling_prob_when_no_answer=data_args.negative_sampling_prob_when_no_answer,
+        load_from_cache_file=not data_args.overwrite_cache,
+        max_seq_len=data_args.max_seq_length,
+        num_workers=data_args.preprocessing_num_workers,
+        max_q_char_len=data_args.max_q_char_len,
+        single_context_multiple_passages=data_args.single_context_multiple_passages,
+        max_contexts=data_args.max_contexts,
+    )
+
+    # process train data
+    if training_args.do_train:
+        train_dataset = confidence_datasets["mrc_train"]
+        max_train_samples = data_args.max_train_samples
+        if max_train_samples is not None:
+            # We will select sample from whole data if argument is specified
+            train_dataset = train_dataset.select(range(max_train_samples))
+        # Train Feature Creation
+        with training_args.main_process_first(desc="train dataset map pre-processing"):
+            _, train_dataset = preprocessor.process_train(train_dataset)
+
+    # process val and confidence data
+    if training_args.do_eval:
+        eval_examples = confidence_datasets["validation"]
+        max_eval_samples = data_args.max_eval_samples
+        if max_eval_samples is not None:
+            # We will select sample from whole data if argument is specified
+            eval_examples = eval_examples.select(range(max_eval_samples))
+        # Validation Feature Creation
+        with training_args.main_process_first(desc="validation dataset map pre-processing"):
+            eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)
+
+        conf_examples = confidence_datasets["confidence_train"]
+        max_eval_samples = data_args.max_eval_samples
+        if max_eval_samples is not None:  # data_args.max_eval_samples is not None:
+            # We will select sample from whole data
+            conf_examples = conf_examples.select(range(max_eval_samples))
+        # Validation Feature Creation
+        with training_args.main_process_first(desc="confidence dataset map pre-processing"):
+            conf_examples, conf_dataset = preprocessor.process_eval(conf_examples)
+
+    # If using mixed precision we pad for efficient hardware acceleration
+    using_mixed_precision = any(attrgetter('fp16', 'bf16')(training_args))
+    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=64 if using_mixed_precision else None)
+
+    if task_args.task_heads == EXTRACTIVE_WITH_CONFIDENCE_HEAD:
+        output_confidence_feature = True
+    else:
+        output_confidence_feature = False
+    postprocessor_class = task_args.postprocessor
+
+    # noinspection PyProtectedMember
+    postprocessor = postprocessor_class(
+        k=data_args.n_best_logits,
+        n_best_size=data_args.n_best_size,
+        max_answer_length=data_args.max_answer_length,
+        scorer_type=SupportedSpanScorers(scorer_type),
+        single_context_multiple_passages=preprocessor._single_context_multiple_passages,
+        output_confidence_feature = output_confidence_feature,
+    )
+
+    eval_metrics = task_args.eval_metrics()
+
+    def compute_metrics(p: EvalPredictionWithProcessing):
+        return eval_metrics.compute(predictions=p.processed_predictions, references=p.label_ids)
+
+    trainer = MRCTrainer(
+        model=model,
+        args=training_args,
+        train_dataset=train_dataset if training_args.do_train else None,
+        eval_dataset=eval_dataset if training_args.do_eval else None,
+        eval_examples=eval_examples if training_args.do_eval else None,
+        tokenizer=tokenizer,
+        data_collator=data_collator,
+        post_process_function=postprocessor.process_references_and_predictions,  # see QATrainer in Huggingface
+        compute_metrics=compute_metrics,
+    )
+
+    checkpoint = None
+    if training_args.resume_from_checkpoint is not None:
+        checkpoint = training_args.resume_from_checkpoint
+    elif last_checkpoint is not None:
+        checkpoint = last_checkpoint
+
+    # training
+    if training_args.do_train:
+        train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        trainer.save_model()  # Saves the tokenizer too for easy upload
+
+        metrics = train_result.metrics
+        max_train_samples = (
+            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
+        )
+        metrics["train_samples"] = min(max_train_samples, len(train_dataset))
+
+        trainer.log_metrics("train", metrics)
+        trainer.save_metrics("train", metrics)
+        trainer.save_state()
+
+    # write prediction results of confidence train set and validation set
+    # to different directories
+    base_output_dir = training_args.output_dir
+    validation_set_prediction_output_dir = os.path.join(base_output_dir, "validation_set_predictions")
+    os.makedirs(validation_set_prediction_output_dir, exist_ok=True)
+    confidence_set_prediction_output_dir = os.path.join(base_output_dir, "confidence_set_predictions")
+    os.makedirs(confidence_set_prediction_output_dir, exist_ok=True)
+
+    if training_args.do_eval:
+        logger.info("*** Evaluate on validation set ***")
+        metrics = trainer.evaluate(eval_dataset=eval_dataset, eval_examples=eval_examples)
+        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
+        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
+        trainer.log_metrics("eval", metrics)
+        trainer.save_metrics("eval", metrics)
+        for fn in ["eval_predictions.json", "eval_references.json", "eval_predictions_processed.json"]:
+            if os.path.exists(os.path.join(base_output_dir, fn)):
+                os.replace(os.path.join(base_output_dir, fn), os.path.join(validation_set_prediction_output_dir, fn))
+            else:
+                raise ValueError("Unable to find eval result file {} from {}".format(fn, base_output_dir))
+
+        logger.info("*** Evaluate on confidence train set ***")
+        metrics = trainer.evaluate(eval_dataset=conf_dataset, eval_examples=conf_examples)
+        max_conf_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(conf_dataset)
+        metrics["confidence_samples"] = min(max_conf_samples, len(conf_dataset))
+        trainer.log_metrics("eval", metrics)
+        trainer.save_metrics("eval", metrics)
+        for fn in ["eval_predictions.json", "eval_references.json", "eval_predictions_processed.json"]:
+            if os.path.exists(os.path.join(base_output_dir, fn)):
+                os.replace(os.path.join(base_output_dir, fn), os.path.join(confidence_set_prediction_output_dir, fn))
+            else:
+                raise ValueError("Unable to find eval result file {} from {}".format(fn, base_output_dir))
+
+    # confidence model training
+    confidence_set_prediction_file = os.path.join(confidence_set_prediction_output_dir, "eval_predictions.json")
+    if not os.path.exists(confidence_set_prediction_file):
+        raise ValueError("Unable to find eval_predictions.json file from {} for confidence model training.".format(confidence_set_prediction_output_dir))
+    confidence_set_reference_file = os.path.join(confidence_set_prediction_output_dir, "eval_references.json")
+    if not os.path.exists(confidence_set_reference_file):
+        raise ValueError("Unable to find eval_references.json file from {} for confidence model training.".format(confidence_set_prediction_output_dir))
+
+    confidence_model = MLPClassifier(random_state = 1, activation = 'tanh',
+                                     hidden_layer_sizes=(100,100),
+                                     max_iter=task_args.max_iter_of_confidence_model_training,
+                                     verbose=1)
+    X, Y = ConfidenceScorer.make_training_data(confidence_set_prediction_file, confidence_set_reference_file,
+                                               task_args.prediction_reference_overlap_threshold)
+
+    logger.info("Training confidence model ...")
+    confidence_model.fit(X, Y)
+
+    confidence_model_file = os.path.join(task_args.confidence_model_dir, 'confidence_model.bin')
+    dump(confidence_model, confidence_model_file)
+    logging.info("Saved confidence model to {}".format(confidence_model_file))
+
+    # confidence model eval on validation set
+    confidence_scorer = ConfidenceScorer(task_args.confidence_model_dir)
+    validation_set_prediction_file = os.path.join(validation_set_prediction_output_dir, "eval_predictions.json")
+    if not os.path.exists(validation_set_prediction_file):
+        raise ValueError("Unable to find eval_predictions.json file from {} for confidence model training.".format(validation_set_prediction_output_dir))
+    try:
+        with open(validation_set_prediction_file, 'r') as f:
+            validation_predictions = json.load(f)
+    except:
+        raise ValueError("Unable to load validation predictions from {}".format(validation_set_prediction_file))
+
+    for example_id in validation_predictions:
+        scores = confidence_scorer.predict_scores(validation_predictions[example_id])
+        for i in range(len(validation_predictions[example_id])):
+            validation_predictions[example_id][i]["confidence_score"] = scores[i]
+
+    rescored_prediction_file = os.path.join(validation_set_prediction_output_dir, "eval_predictions.rescored.json")
+    try:
+        with open(rescored_prediction_file, 'w') as f:
+            json.dump(validation_predictions, f, indent=4)
+        logger.info("Saved rescored validation predictions to {}".format(rescored_prediction_file))
+    except:
+        raise ValueError("Unable to save the rescored validation prediction file to {}".format(rescored_prediction_file))
+
+
+
+
+if __name__ == '__main__':
+    main()
```

## primeqa/ir/run_bm25_retrieval.py

 * *Ordering differences only*

```diff
@@ -1,107 +1,107 @@
-import os
-import argparse
-import csv
-import json
-from typing import List, Dict
-from primeqa.ir.sparse.retriever import PyseriniRetriever
-import logging
-
-logger = logging.getLogger(__name__)
-logging.basicConfig(level=logging.INFO)
-
-
-def handle_args():
-    parser = argparse.ArgumentParser(description='Run BM25 search'
-)
-    parser.add_argument('--index_path', type=str,required=True,help="Path to a lucene index")
-    parser.add_argument('--queries_file', type=str, required=True, help='Path to queries file in ColBERT tsv [id\tquery] format')
-    parser.add_argument('--top_k', type=int, required=False, default=1000, help='Number of documents/passages to retrieve')
-    parser.add_argument('--xorqa_data_file', type=str, required=False, default=None, help="Required to extract language id for XORQA evaluation." )
-    parser.add_argument('--max_hits_to_output', type=int, required=False, default=100, help='Number of hits to output for XORQA evaluation.')
-    parser.add_argument('--output_dir', type=str, required=True, help='Path to the output directory')
-    args=parser.parse_args()
-    return args
- 
-def load_queries(filepath: str) -> Dict:
-    id_to_question = {}
-    with open(filepath,'r') as f:
-        for line in f:
-            id, question = line.strip().split('\t')
-            id_to_question[id] = question
-    return id_to_question
-
-def run_search(index_path: str, queries: List, top_n: int = 1000, k1: float = 0.9, b: float = 0.4) -> Dict:
-    logger.info(f"Loading index {index_path}")
-    searcher = PyseriniRetriever(index_path, use_bm25=True, k1=k1, b=b)
-    logger.info(f"Running search")
-    id_to_hits = {}
-    for id in queries:
-        question = queries[id]
-        logger.info(f'Running search: {id} {question}')
-        hits = searcher.retrieve(question,top_n)
-        id_to_hits[id] = hits
-    return id_to_hits
-
-def write_colbert_ranking_tsv(output_dir: str , id_to_hits: Dict):
-    output_file = os.path.join(output_dir,'ranking.tsv')
-    search_results = []
-    for id in id_to_hits:
-        for i, hit in enumerate(id_to_hits[id]):
-            result = {
-                "id": id,
-                "docid": hit['doc_id'],
-                "rank": i+1, 
-                "score": hit['score']
-            }
-            search_results.append(result)
-
-    with open(output_file,'w',encoding='utf-8') as f:
-        writer = csv.writer(f, delimiter='\t', lineterminator='\n')
-        for r in search_results:
-            writer.writerow(r.values())
-    logger.info(f"Wrote {output_file}")
-        
-def get_language_id(xorqa_data_file: str) -> Dict:
-    id_to_lang = {}
-    with open(xorqa_data_file,'r') as f:
-        for line in f:
-            data = json.loads(line.strip())
-            id_to_lang[data['id']] = data['lang']
-    return id_to_lang
-
-def write_xorqa_json(output_dir: str, id_to_hits: Dict, top_n: int =100, xorqa_data_file: str = None):
-    id_to_lang = None
-    if xorqa_data_file != None:
-        id_to_lang = get_language_id(xorqa_data_file)
-
-    output_file = os.path.join(output_dir,'ranking_xortydi_format.json')
-    search_results = []
-    for id in id_to_hits:
-        json_data = {
-            "id" : id,
-            "lang" : id_to_lang[id] if id_to_lang != None else "", 
-            "ctxs" : []
-        }
-        for hit in id_to_hits[id][:top_n]:
-            json_data["ctxs"].append(hit['text'])
-        search_results.append(json_data)
-    with open(output_file,'w',encoding='utf-8') as f:
-        json.dump(search_results,f, indent=4)
-    logger.info(f"Wrote {output_file}")
-
-def write_search_results(id_to_hits: Dict, output_dir: str, max_hits_output: int, xorqa_data_file: str =None):
-    if not os.path.exists(output_dir):
-        os.makedirs(output_dir)
-    write_xorqa_json(output_dir, id_to_hits, top_n=max_hits_output, xorqa_data_file=xorqa_data_file)
-    write_colbert_ranking_tsv(output_dir, id_to_hits)
-
-def main():
-    args = handle_args()
-    logger.info("starting")
-    id_to_questions = load_queries(args.queries_file)
-    id_to_hits = run_search(args.index_path, id_to_questions, top_n=1000)
-    write_search_results(id_to_hits, args.output_dir, args.max_hits_to_output, args.xorqa_data_file)
-
-if __name__ == '__main__':
-    main()
+import os
+import argparse
+import csv
+import json
+from typing import List, Dict
+from primeqa.ir.sparse.retriever import PyseriniRetriever
+import logging
+
+logger = logging.getLogger(__name__)
+logging.basicConfig(level=logging.INFO)
+
+
+def handle_args():
+    parser = argparse.ArgumentParser(description='Run BM25 search'
+)
+    parser.add_argument('--index_path', type=str,required=True,help="Path to a lucene index")
+    parser.add_argument('--queries_file', type=str, required=True, help='Path to queries file in ColBERT tsv [id\tquery] format')
+    parser.add_argument('--top_k', type=int, required=False, default=1000, help='Number of documents/passages to retrieve')
+    parser.add_argument('--xorqa_data_file', type=str, required=False, default=None, help="Required to extract language id for XORQA evaluation." )
+    parser.add_argument('--max_hits_to_output', type=int, required=False, default=100, help='Number of hits to output for XORQA evaluation.')
+    parser.add_argument('--output_dir', type=str, required=True, help='Path to the output directory')
+    args=parser.parse_args()
+    return args
+ 
+def load_queries(filepath: str) -> Dict:
+    id_to_question = {}
+    with open(filepath,'r') as f:
+        for line in f:
+            id, question = line.strip().split('\t')
+            id_to_question[id] = question
+    return id_to_question
+
+def run_search(index_path: str, queries: List, top_n: int = 1000, k1: float = 0.9, b: float = 0.4) -> Dict:
+    logger.info(f"Loading index {index_path}")
+    searcher = PyseriniRetriever(index_path, use_bm25=True, k1=k1, b=b)
+    logger.info(f"Running search")
+    id_to_hits = {}
+    for id in queries:
+        question = queries[id]
+        logger.info(f'Running search: {id} {question}')
+        hits = searcher.retrieve(question,top_n)
+        id_to_hits[id] = hits
+    return id_to_hits
+
+def write_colbert_ranking_tsv(output_dir: str , id_to_hits: Dict):
+    output_file = os.path.join(output_dir,'ranking.tsv')
+    search_results = []
+    for id in id_to_hits:
+        for i, hit in enumerate(id_to_hits[id]):
+            result = {
+                "id": id,
+                "docid": hit['doc_id'],
+                "rank": i+1, 
+                "score": hit['score']
+            }
+            search_results.append(result)
+
+    with open(output_file,'w',encoding='utf-8') as f:
+        writer = csv.writer(f, delimiter='\t', lineterminator='\n')
+        for r in search_results:
+            writer.writerow(r.values())
+    logger.info(f"Wrote {output_file}")
+        
+def get_language_id(xorqa_data_file: str) -> Dict:
+    id_to_lang = {}
+    with open(xorqa_data_file,'r') as f:
+        for line in f:
+            data = json.loads(line.strip())
+            id_to_lang[data['id']] = data['lang']
+    return id_to_lang
+
+def write_xorqa_json(output_dir: str, id_to_hits: Dict, top_n: int =100, xorqa_data_file: str = None):
+    id_to_lang = None
+    if xorqa_data_file != None:
+        id_to_lang = get_language_id(xorqa_data_file)
+
+    output_file = os.path.join(output_dir,'ranking_xortydi_format.json')
+    search_results = []
+    for id in id_to_hits:
+        json_data = {
+            "id" : id,
+            "lang" : id_to_lang[id] if id_to_lang != None else "", 
+            "ctxs" : []
+        }
+        for hit in id_to_hits[id][:top_n]:
+            json_data["ctxs"].append(hit['text'])
+        search_results.append(json_data)
+    with open(output_file,'w',encoding='utf-8') as f:
+        json.dump(search_results,f, indent=4)
+    logger.info(f"Wrote {output_file}")
+
+def write_search_results(id_to_hits: Dict, output_dir: str, max_hits_output: int, xorqa_data_file: str =None):
+    if not os.path.exists(output_dir):
+        os.makedirs(output_dir)
+    write_xorqa_json(output_dir, id_to_hits, top_n=max_hits_output, xorqa_data_file=xorqa_data_file)
+    write_colbert_ranking_tsv(output_dir, id_to_hits)
+
+def main():
+    args = handle_args()
+    logger.info("starting")
+    id_to_questions = load_queries(args.queries_file)
+    id_to_hits = run_search(args.index_path, id_to_questions, top_n=1000)
+    write_search_results(id_to_hits, args.output_dir, args.max_hits_to_output, args.xorqa_data_file)
+
+if __name__ == '__main__':
+    main()
     logger.info("Success...")
```

## primeqa/ir/run_ir.py

```diff
@@ -1,158 +1,172 @@
-import logging
-import os
-import sys
-import traceback
-from dataclasses import dataclass, field
-from importlib import import_module
-from operator import attrgetter
-from typing import Optional, Type
-import logging
-
-from transformers import HfArgumentParser
-from primeqa.ir.dense.colbert_top.colbert.infra.config.settings import *
-from primeqa.ir.sparse.config import BM25Config
-from primeqa.ir.sparse.bm25_engine import BM25Engine
-
-logger = logging.getLogger(__name__)
-logging.basicConfig(level=logging.INFO)
-@dataclass
-class ProcessArguments:
-    """
-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
-    """
-
-    engine_type: str = field(
-        metadata={"help": "IR engine type"}
-    )
-    do_train: bool = field(
-        default=False, metadata={"help": "Run model training"}
-    )
-    do_index: bool = field(
-        default=False, metadata={"help": "Run data indexing"}
-    )
-    do_search: bool = field(
-        default=False, metadata={"help": "Run search"}
-    )
-
-@dataclass
-class DPRConfig:
-    '''
-    to be imported from the DPR implementation
-    '''
-    pass
-
-def main():
-
-    parser = HfArgumentParser([ProcessArguments, BM25Config])
-    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
-        # If we pass only one argument to the script and it's the path to a json file,
-        # let's parse it to get our arguments.
-        process_args, model_args, training_args = \
-            parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
-    else:
-        #process_args, colbert_args, dpr_args, bm25_args = parser.parse_args_into_dataclasses()
-        (process_args, bm25_args, remaining_args) = parser.parse_args_into_dataclasses(return_remaining_strings=True)
-
-    if process_args.engine_type == 'ColBERT':
-        logger.info(f"Running ColBERT")
-        from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
-        from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
-
-        # ColBERT argument parser is use here, to allow additional work done in parse()
-        from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
-
-        if hasattr(process_args, 'do_train') and process_args.do_train:
-            from primeqa.ir.dense.colbert_top.colbert.trainer import Trainer
-
-            colbert_parser = Arguments(description='ColBERT training')
-
-            colbert_parser.add_model_parameters()
-            colbert_parser.add_model_training_parameters()
-            colbert_parser.add_training_input()
-            args = colbert_parser.parse()
-
-            assert args.bsize % args.accumsteps == 0, ((args.bsize, args.accumsteps),
-                                                       "The batch size must be divisible by the number of gradient accumulaParsetion steps.")
-            assert args.query_maxlen <= 512
-            assert args.doc_maxlen <= 512
-            args.lazy = args.collection is not None
-
-            args_dict = vars(args)
-            # remove keys not in ColBERTConfig
-            args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed', 'input_arguments', 'engine_type', 'do_train']}
-            colBERTConfig = ColBERTConfig(**args_dict)
-
-            with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
-                trainer = Trainer(args.triples, args.queries, args.collection, colBERTConfig)
-                trainer.train(args.checkpoint)
-                model_fn = trainer.best_checkpoint_path()
-                print('model_fn: ' + model_fn)
-
-        if hasattr(process_args, 'do_index') and process_args.do_index:
-            from primeqa.ir.dense.colbert_top.colbert.indexer import Indexer
-
-            colbert_parser = Arguments(description='ColBERT indexing')
-
-            colbert_parser.add_model_parameters()
-            colbert_parser.add_model_inference_parameters()
-            colbert_parser.add_indexing_input()
-            colbert_parser.add_compressed_index_input()
-            colbert_parser.add_argument('--nway', dest='nway', default=2, type=int)
-            args = colbert_parser.parse()
-
-            args_dict = vars(args)
-            # remove keys not in ColBERTConfig
-            args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'input_arguments']}
-            # args_dict to ColBERTConfig
-            colBERTConfig = ColBERTConfig(**args_dict)
-
-            with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
-                indexer = Indexer(args.checkpoint, colBERTConfig)
-                indexer.index(name=args.index_name, collection=args.collection, overwrite=True)
-
-        if hasattr(process_args, 'do_search') and process_args.do_search:
-            from primeqa.ir.dense.colbert_top.colbert.searcher import Searcher
-            parser = Arguments(description='ColBERT search')
-
-            parser.add_model_parameters()
-            parser.add_model_inference_parameters()
-            parser.add_compressed_index_input()
-            parser.add_ranking_input()
-            parser.add_retrieval_input()
-            args = parser.parse()
-
-            args_dict = vars(args)
-            # remove keys not in ColBERTConfig
-            args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'qrels', 'partitions', 'retrieve_only', 'input_arguments']}
-            colBERTConfig = ColBERTConfig(**args_dict)
-
-            with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
-                searcher = Searcher(args.index_name, checkpoint=args.checkpoint, collection=args.collection, config=colBERTConfig)
-
-                rankings = searcher.search_all(args.queries, args.topK)
-                out_fn = args.ranks_fn
-                rankings.save(out_fn)
-
-    elif process_args.engine_type == 'DPR':
-        pass
-    elif process_args.engine_type == 'BM25':
-        logger.info(f"Running BM25")
-
-        engine = BM25Engine(bm25_args)
-        
-        if hasattr(process_args, 'do_index') and process_args.do_index:
-            logger.info("Running BM25 indexing")
-            engine.do_index()
-            logger.info(f"BM25 indexing finished")
-
-        if hasattr(process_args, 'do_search') and process_args.do_search:
-            logger.info("Running BM25 search")
-            engine.do_search()
-
-            logger.info("BM25 Search finished")
-    else:
-        raise NotImplementedError()
-
-
-if __name__ == '__main__':
-    main()
+import logging
+import os
+import sys
+import traceback
+from dataclasses import dataclass, field
+from importlib import import_module
+from operator import attrgetter
+from typing import Optional, Type
+import logging
+
+from transformers import HfArgumentParser
+from primeqa.ir.dense.colbert_top.colbert.infra.config.settings import *
+from primeqa.ir.sparse.config import BM25Config
+from primeqa.ir.sparse.bm25_engine import BM25Engine
+
+logger = logging.getLogger(__name__)
+
+logging.basicConfig(format='%(asctime)s %(filename)s:%(lineno)d - %(message)s',
+                    datefmt='%m/%d/%Y %H:%M:%S',
+                    level=logging.INFO)
+
+@dataclass
+class ProcessArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    engine_type: str = field(
+        metadata={"help": "IR engine type"}
+    )
+    do_train: bool = field(
+        default=False, metadata={"help": "Run model training"}
+    )
+    do_index: bool = field(
+        default=False, metadata={"help": "Run data indexing"}
+    )
+    do_search: bool = field(
+        default=False, metadata={"help": "Run search"}
+    )
+
+def main():
+    parser = HfArgumentParser([ProcessArguments, BM25Config])
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        process_args, model_args, training_args = \
+            parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
+    else:
+        #process_args, colbert_args, dpr_args, bm25_args = parser.parse_args_into_dataclasses()
+        (process_args, bm25_args, remaining_args) = parser.parse_args_into_dataclasses(return_remaining_strings=True)
+
+    if process_args.engine_type == 'ColBERT':
+        logger.info(f"Running ColBERT")
+        from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
+        from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
+
+        # ColBERT argument parser is use here, to allow additional work done in parse()
+        from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
+
+        if hasattr(process_args, 'do_train') and process_args.do_train:
+            from primeqa.ir.dense.colbert_top.colbert.trainer import Trainer
+
+            colbert_parser = Arguments(description='ColBERT training')
+
+            colbert_parser.add_model_parameters()
+            colbert_parser.add_model_training_parameters()
+            colbert_parser.add_training_input()
+            args = colbert_parser.parse()
+
+            assert args.bsize % args.accumsteps == 0, ((args.bsize, args.accumsteps),
+                                                       "The batch size must be divisible by the number of gradient accumulaParsetion steps.")
+            assert args.query_maxlen <= 512
+            assert args.doc_maxlen <= 512
+            args.lazy = args.collection is not None
+
+            args_dict = vars(args)
+            # remove keys not in ColBERTConfig
+            args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed', 'input_arguments', 'engine_type', 'do_train']}
+            colBERTConfig = ColBERTConfig(**args_dict)
+
+            with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
+                trainer = Trainer(args.triples, args.queries, args.collection, colBERTConfig)
+                trainer.train(args.checkpoint)
+                model_fn = trainer.best_checkpoint_path()
+                print('model_fn: ' + model_fn)
+
+        if hasattr(process_args, 'do_index') and process_args.do_index:
+            from primeqa.ir.dense.colbert_top.colbert.indexer import Indexer
+
+            colbert_parser = Arguments(description='ColBERT indexing')
+
+            colbert_parser.add_model_parameters()
+            colbert_parser.add_model_inference_parameters()
+            colbert_parser.add_indexing_input()
+            colbert_parser.add_compressed_index_input()
+            colbert_parser.add_argument('--nway', dest='nway', default=2, type=int)
+            args = colbert_parser.parse()
+
+            args_dict = vars(args)
+            # remove keys not in ColBERTConfig
+            args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'input_arguments']}
+            # args_dict to ColBERTConfig
+            colBERTConfig = ColBERTConfig(**args_dict)
+
+            with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
+                indexer = Indexer(args.checkpoint, colBERTConfig)
+                indexer.index(name=args.index_name, collection=args.collection, overwrite=True)
+
+        if hasattr(process_args, 'do_search') and process_args.do_search:
+            from primeqa.ir.dense.colbert_top.colbert.searcher import Searcher
+            parser = Arguments(description='ColBERT search')
+
+            parser.add_model_parameters()
+            parser.add_model_inference_parameters()
+            parser.add_compressed_index_input()
+            parser.add_ranking_input()
+            parser.add_retrieval_input()
+            args = parser.parse()
+
+            args_dict = vars(args)
+            # remove keys not in ColBERTConfig
+            args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'qrels', 'partitions', 'retrieve_only', 'input_arguments']}
+            colBERTConfig = ColBERTConfig(**args_dict)
+
+            with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
+                searcher = Searcher(args.index_name, checkpoint=args.checkpoint, collection=args.collection, config=colBERTConfig)
+
+                rankings = searcher.search_all(args.queries, args.topK)
+                out_fn = os.path.join(args.output_dir, 'ranked_passages.tsv')
+                rankings.save(out_fn)
+
+    elif process_args.engine_type == 'DPR':
+        logger.info(f"Running DPR")
+
+        if hasattr(process_args, 'do_train') and process_args.do_train:
+            from primeqa.ir.dense.dpr_top.dpr.biencoder_trainer import BiEncoderTrainer
+            trainer = BiEncoderTrainer()
+            trainer.train()
+
+        if hasattr(process_args, 'do_index') and process_args.do_index:
+            from primeqa.ir.dense.dpr_top.dpr.index_simple_corpus import DPRIndexer
+            indexer = DPRIndexer()
+            indexer.index()
+            # raise NotImplementedError(f"Indexing using the DPR engine is not implemented (yet), but the trained model is compatible with other DPR toolkits.")
+
+        if hasattr(process_args, 'do_search') and process_args.do_search:
+            from primeqa.ir.dense.dpr_top.dpr.searcher import DPRSearcher
+            searcher = DPRSearcher()
+            searcher.search()
+            #raise NotImplementedError(f"Search using the DPR engine is not implemented (yet), but the trained model is compatible with other DPR toolkits.")
+
+    elif process_args.engine_type == 'BM25':
+        logger.info(f"Running BM25")
+
+        engine = BM25Engine(bm25_args)
+        
+        if hasattr(process_args, 'do_index') and process_args.do_index:
+            logger.info("Running BM25 indexing")
+            engine.do_index()
+            logger.info(f"BM25 indexing finished")
+
+        if hasattr(process_args, 'do_search') and process_args.do_search:
+            logger.info("Running BM25 search")
+            engine.do_search()
+
+            logger.info("BM25 Search finished")
+    else:
+        raise NotImplementedError()
+
+
+if __name__ == '__main__':
+    main()
```

## primeqa/ir/dense/__init__.py

```diff
@@ -1 +1 @@
-00000000: 0d0a                                     ..
+00000000: 0a                                       .
```

## primeqa/ir/dense/colbert_top/setup.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-import setuptools
-
-with open('README.md', 'r') as f:
-    long_description = f.read()
-
-setuptools.setup(
-    name='ColBERT',
-    version='0.2.0',
-    author='Omar Khattab',
-    author_email='okhattab@stanford.edu',
-    description="Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
-    long_description=long_description,
-    long_description_content_type='text/markdown',
-    url='https://github.com/stanford-futuredata/ColBERT',
-    packages=setuptools.find_packages(),
-    python_requires='>=3.6',
-)
+import setuptools
+
+with open('README.md', 'r') as f:
+    long_description = f.read()
+
+setuptools.setup(
+    name='ColBERT',
+    version='0.2.0',
+    author='Omar Khattab',
+    author_email='okhattab@stanford.edu',
+    description="Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
+    long_description=long_description,
+    long_description_content_type='text/markdown',
+    url='https://github.com/stanford-futuredata/ColBERT',
+    packages=setuptools.find_packages(),
+    python_requires='>=3.6',
+)
```

## primeqa/ir/dense/colbert_top/colbert/__init__.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-from .trainer import Trainer
-from .indexer import Indexer
-from .searcher import Searcher
-
+from .trainer import Trainer
+from .indexer import Indexer
+from .searcher import Searcher
+
 from .modeling.checkpoint import Checkpoint
```

## primeqa/ir/dense/colbert_top/colbert/index.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-
-
-# TODO: This is the loaded index, underneath a searcher.
-
-
-"""
-## Operations:
-
-index = Index(index='/path/to/index')
-index.load_to_memory()
-
-batch_of_pids = [2324,32432,98743,23432]
-index.lookup(batch_of_pids, device='cuda:0') -> (N, doc_maxlen, dim)
-
-index.iterate_over_parts()
-
-"""
+
+
+# TODO: This is the loaded index, underneath a searcher.
+
+
+"""
+## Operations:
+
+index = Index(index='/path/to/index')
+index.load_to_memory()
+
+batch_of_pids = [2324,32432,98743,23432]
+index.lookup(batch_of_pids, device='cuda:0') -> (N, doc_maxlen, dim)
+
+index.iterate_over_parts()
+
+"""
```

## primeqa/ir/dense/colbert_top/colbert/indexer.py

```diff
@@ -1,102 +1,103 @@
-import os
-import time
-
-import torch
-import random
-import numpy as np
-
-import torch.multiprocessing as mp
-
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
-from primeqa.ir.dense.colbert_top.colbert.infra.launcher import Launcher
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory, print_message
-
-from primeqa.ir.dense.colbert_top.colbert.indexing.collection_indexer import encode
-
-
-class Indexer:
-    def __init__(self, checkpoint, config=None):
-        """
-           Use Run().context() to choose the run's configuration. They are NOT extracted from `config`.
-        """
-        random.seed(12345)
-        np.random.seed(12345)
-        torch.manual_seed(12345)
-        torch.cuda.manual_seed(12345)
-
-        self.index_path = None
-        self.checkpoint = checkpoint
-        self.checkpoint_config = ColBERTConfig.load_from_checkpoint(checkpoint)
-
-        # set model_type from checkpoint's config
-        # config.model_type = self.checkpoint_config.model_type
-
-        self.config = ColBERTConfig.from_existing(self.checkpoint_config, config, Run().config)
-
-        # set model_type from checkpoint's config
-        # self.config.model_type = self.checkpoint_config.model_type
-
-        self.configure(checkpoint=checkpoint)
-
-
-
-    def configure(self, **kw_args):
-        self.config.configure(**kw_args)
-
-    def get_index(self):
-        return self.index_path
-
-    def erase(self):
-        assert self.index_path is not None
-        directory = self.index_path
-        deleted = []
-
-        for filename in sorted(os.listdir(directory)):
-            filename = os.path.join(directory, filename)
-
-            delete = filename.endswith(".json")
-            delete = delete and ('metadata' in filename or 'doclen' in filename or 'plan' in filename)
-            delete = delete or filename.endswith(".pt")
-            
-            if delete:
-                deleted.append(filename)
-        
-        if len(deleted):
-            print_message(f"#> Will delete {len(deleted)} files already at {directory} in 20 seconds...")
-            time.sleep(20)
-
-            for filename in deleted:
-                os.remove(filename)
-
-        return deleted
-
-    def index(self, name, collection, overwrite=False):
-        assert overwrite in [True, False, 'reuse']
-
-        self.configure(collection=collection, index_name=name)
-        self.configure(bsize=64, partitions=None)
-        # self.configure(bsize=1, partitions=None)
-
-        self.index_path = self.config.index_path_
-        index_does_not_exist = (not os.path.exists(self.config.index_path_))
-
-        assert (overwrite in [True, 'reuse']) or index_does_not_exist, self.config.index_path_
-        create_directory(self.config.index_path_)
-
-        if overwrite is True:
-            self.erase()
-
-        if index_does_not_exist or overwrite != 'reuse':
-            self.__launch(collection)
-
-        return self.index_path
-
-    def __launch(self, collection):
-        manager = mp.Manager()
-        shared_lists = [manager.list() for _ in range(self.config.nranks)]
-        shared_queues = [manager.Queue(maxsize=1) for _ in range(self.config.nranks)]
-
-        launcher = Launcher(encode)
-        launcher.launch(self.config, collection, shared_lists, shared_queues)
+import os
+import time
+
+import torch
+import random
+
+import numpy as np
+
+import torch.multiprocessing as mp
+
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
+from primeqa.ir.dense.colbert_top.colbert.infra.launcher import Launcher
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory, print_message
+
+from primeqa.ir.dense.colbert_top.colbert.indexing.collection_indexer import encode
+
+
+class Indexer:
+    def __init__(self, checkpoint, config=None):
+        """
+           Use Run().context() to choose the run's configuration. They are NOT extracted from `config`.
+        """
+        random.seed(12345)
+        np.random.seed(12345)
+        torch.manual_seed(12345)
+        torch.cuda.manual_seed(12345)
+
+        self.index_path = None
+        self.checkpoint = checkpoint
+        self.checkpoint_config = ColBERTConfig.load_from_checkpoint(checkpoint)
+
+        # set model_type from checkpoint's config
+        # config.model_type = self.checkpoint_config.model_type
+
+        self.config = ColBERTConfig.from_existing(Run().config, self.checkpoint_config, config)
+
+        # set model_type from checkpoint's config
+        # self.config.model_type = self.checkpoint_config.model_type
+
+        self.configure(checkpoint=checkpoint)
+
+
+
+    def configure(self, **kw_args):
+        self.config.configure(**kw_args)
+
+    def get_index(self):
+        return self.index_path
+
+    def erase(self):
+        assert self.index_path is not None
+        directory = self.index_path
+        deleted = []
+
+        for filename in sorted(os.listdir(directory)):
+            filename = os.path.join(directory, filename)
+
+            delete = filename.endswith(".json")
+            delete = delete and ('metadata' in filename or 'doclen' in filename or 'plan' in filename)
+            delete = delete or filename.endswith(".pt")
+            
+            if delete:
+                deleted.append(filename)
+        
+        if len(deleted):
+            print_message(f"#> Will delete {len(deleted)} files already at {directory} in 20 seconds...")
+            time.sleep(20)
+
+            for filename in deleted:
+                os.remove(filename)
+
+        return deleted
+
+    def index(self, name, collection, overwrite=False):
+        assert overwrite in [True, False, 'reuse']
+
+        self.configure(collection=collection, index_name=name)
+        self.configure(bsize=64, partitions=None)
+        # self.configure(bsize=1, partitions=None)
+
+        self.index_path = self.config.index_path_
+        index_does_not_exist = (not os.path.exists(self.config.index_path_))
+
+        assert (overwrite in [True, 'reuse']) or index_does_not_exist, self.config.index_path_
+        create_directory(self.config.index_path_)
+
+        if overwrite is True:
+            self.erase()
+
+        if index_does_not_exist or overwrite != 'reuse':
+            self.__launch(collection)
+
+        return self.index_path
+
+    def __launch(self, collection):
+        manager = mp.Manager()
+        shared_lists = [manager.list() for _ in range(self.config.nranks)]
+        shared_queues = [manager.Queue(maxsize=1) for _ in range(self.config.nranks)]
+
+        launcher = Launcher(encode)
+        launcher.launch(self.config, collection, shared_lists, shared_queues)
```

## primeqa/ir/dense/colbert_top/colbert/parameters.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-import torch
-
-if torch.cuda.is_available():
-    DEVICE = torch.device("cuda")
-else:
-    DEVICE = torch.device("cpu")
-
-
-SAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 250*1000, 300*1000, 400*1000]
-SAVED_CHECKPOINTS += [10*1000, 20*1000, 30*1000, 40*1000, 50*1000, 60*1000, 70*1000, 80*1000, 90*1000]
-SAVED_CHECKPOINTS += [25*1000, 50*1000, 75*1000]
-
-# SAVED_CHECKPOINTS += [10]
-
-SAVED_CHECKPOINTS = set(SAVED_CHECKPOINTS)
-SAVED_STEPS_PROGRESS = 2000
-
+import torch
+
+if torch.cuda.is_available():
+    DEVICE = torch.device("cuda")
+else:
+    DEVICE = torch.device("cpu")
+
+
+SAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 250*1000, 300*1000, 400*1000]
+SAVED_CHECKPOINTS += [10*1000, 20*1000, 30*1000, 40*1000, 50*1000, 60*1000, 70*1000, 80*1000, 90*1000]
+SAVED_CHECKPOINTS += [25*1000, 50*1000, 75*1000]
+
+# SAVED_CHECKPOINTS += [10]
+
+SAVED_CHECKPOINTS = set(SAVED_CHECKPOINTS)
+SAVED_STEPS_PROGRESS = 2000
+
 # TODO:  final_ckpt    2k, 5k, 10k   20k, 50k, 100k  150k  200k, 500k, 1M       2M, 5M, 10M
```

## primeqa/ir/dense/colbert_top/colbert/run_indexer.py

 * *Ordering differences only*

```diff
@@ -1,33 +1,33 @@
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
-
-from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
-from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
-from primeqa.ir.dense.colbert_top.colbert.indexer import Indexer
-
-def main():
-    parser = Arguments(description='Training ColBERT with <query, positive passage, negative passage> triples.')
-
-    parser.add_model_parameters()
-    parser.add_model_inference_parameters()
-    parser.add_indexing_input()
-    parser.add_compressed_index_input()
-
-    parser.add_argument('--kmeans_niters', dest='kmeans_niters', default=20, type=int)
-    parser.add_argument('--nway', dest='nway', default=2, type=int)
-
-    args = parser.parse()
-
-    # Namespace to dict
-    args_dict = vars(args)
-    # remove keys not in ColBERTConfig
-    args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'input_arguments']}
-    # args_dict to ColBERTConfig
-    colBERTConfig = ColBERTConfig(**args_dict)
-
-    with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
-        indexer = Indexer(args.checkpoint, colBERTConfig)
-        indexer.index(name=args.index_name, collection=args.collection, overwrite=True)
-
-
-if __name__ == "__main__":
-    main()
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
+
+from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
+from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
+from primeqa.ir.dense.colbert_top.colbert.indexer import Indexer
+
+def main():
+    parser = Arguments(description='Training ColBERT with <query, positive passage, negative passage> triples.')
+
+    parser.add_model_parameters()
+    parser.add_model_inference_parameters()
+    parser.add_indexing_input()
+    parser.add_compressed_index_input()
+
+    parser.add_argument('--kmeans_niters', dest='kmeans_niters', default=20, type=int)
+    parser.add_argument('--nway', dest='nway', default=2, type=int)
+
+    args = parser.parse()
+
+    # Namespace to dict
+    args_dict = vars(args)
+    # remove keys not in ColBERTConfig
+    args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'input_arguments']}
+    # args_dict to ColBERTConfig
+    colBERTConfig = ColBERTConfig(**args_dict)
+
+    with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
+        indexer = Indexer(args.checkpoint, colBERTConfig)
+        indexer.index(name=args.index_name, collection=args.collection, overwrite=True)
+
+
+if __name__ == "__main__":
+    main()
```

## primeqa/ir/dense/colbert_top/colbert/run_searcher.py

```diff
@@ -1,38 +1,37 @@
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
-
-from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
-from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
-from primeqa.ir.dense.colbert_top.colbert.searcher import Searcher
-
-def main():
-    parser = Arguments(description='run ')
-
-    parser.add_model_parameters()
-    parser.add_model_inference_parameters()
-    parser.add_compressed_index_input()
-    parser.add_ranking_input()
-    parser.add_retrieval_input()
-
-    # parser.add_argument('--ranks_fn', dest='ranks_fn', required=True)
-    # parser.add_argument('--topk', dest='topK', default=1000)
-
-    args = parser.parse()
-
-    # Namespace to dict
-    args_dict = vars(args)
-    # remove keys not in ColBERTConfig
-    # args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'qrels', 'partitions', 'retrieve_only', 'ranks_fn', 'topK', 'input_arguments']}
-    # need to keep ranks_fn and topK arguments to save the ranking results
-    args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'qrels', 'partitions', 'retrieve_only', 'input_arguments']}
-    # args_dict to ColBERTConfig
-    colBERTConfig = ColBERTConfig(**args_dict)
-
-    with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
-        searcher = Searcher(args.index_name, checkpoint=args.checkpoint, collection=args.collection, config=colBERTConfig)
-
-        rankings = searcher.search_all(args.queries, args.topK)
-        out_fn = args.ranks_fn
-        rankings.save(out_fn)
-
-if __name__ == "__main__":
-    main()
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
+
+from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
+from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
+from primeqa.ir.dense.colbert_top.colbert.searcher import Searcher
+
+def main():
+    parser = Arguments(description='run ')
+
+    parser.add_model_parameters()
+    parser.add_model_inference_parameters()
+    parser.add_compressed_index_input()
+    parser.add_ranking_input()
+    parser.add_retrieval_input()
+
+    # parser.add_argument('--ranks_fn', dest='ranks_fn', required=True)
+    # parser.add_argument('--topk', dest='topK', default=1000)
+
+    args = parser.parse()
+
+    # Namespace to dict
+    args_dict = vars(args)
+    # remove keys not in ColBERTConfig
+    # need to keep ranks_fn and topK arguments to save the ranking results
+    args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'nthreads', 'distributed', 'compression_level', 'qrels', 'partitions', 'input_arguments']}
+    # args_dict to ColBERTConfig
+    colBERTConfig = ColBERTConfig(**args_dict)
+
+    with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
+        searcher = Searcher(args.index_name, checkpoint=args.checkpoint, collection=args.collection, config=colBERTConfig)
+
+        rankings = searcher.search_all(args.queries, args.topK)
+        out_fn = args.ranks_fn
+        rankings.save(out_fn)
+
+if __name__ == "__main__":
+    main()
```

## primeqa/ir/dense/colbert_top/colbert/run_trainer.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
-
-from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
-from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
-from primeqa.ir.dense.colbert_top.colbert.trainer import Trainer
-
-def main():
-    parser = Arguments(description='Training ColBERT with <query, positive passage, negative passage> triples.')
-
-    parser.add_model_parameters()
-    parser.add_model_training_parameters()
-    parser.add_training_input()
-    # parser.add_argument('--model_type', dest='model_type', default='bert')
-    # comment out as we define the argument at model training parameters
-
-    args = parser.parse()
-
-    assert args.bsize % args.accumsteps == 0, ((args.bsize, args.accumsteps),
-                                               "The batch size must be divisible by the number of gradient accumulation steps.")
-    assert args.query_maxlen <= 512
-    assert args.doc_maxlen <= 512
-
-    args.lazy = args.collection is not None
-
-    # Namespace to dict
-    args_dict = vars(args)
-    # remove keys not in ColBERTConfig
-    # args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed', 'resume_optimizer', 'model_type','input_arguments']}
-    args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed', 'input_arguments']}
-    # args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed']}
-    colBERTConfig = ColBERTConfig(**args_dict)
-
-    with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
-        trainer = Trainer(args.triples, args.queries, args.collection, colBERTConfig)
-        trainer.train(args.checkpoint)
-
-
-if __name__ == "__main__":
-    main()
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
+
+from primeqa.ir.dense.colbert_top.colbert.utils.parser import Arguments
+from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
+from primeqa.ir.dense.colbert_top.colbert.trainer import Trainer
+
+def main():
+    parser = Arguments(description='Training ColBERT with <query, positive passage, negative passage> triples.')
+
+    parser.add_model_parameters()
+    parser.add_model_training_parameters()
+    parser.add_training_input()
+    # parser.add_argument('--model_type', dest='model_type', default='bert')
+    # comment out as we define the argument at model training parameters
+
+    args = parser.parse()
+
+    assert args.bsize % args.accumsteps == 0, ((args.bsize, args.accumsteps),
+                                               "The batch size must be divisible by the number of gradient accumulation steps.")
+    assert args.query_maxlen <= 512
+    assert args.doc_maxlen <= 512
+
+    args.lazy = args.collection is not None
+
+    # Namespace to dict
+    args_dict = vars(args)
+    # remove keys not in ColBERTConfig
+    # args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed', 'resume_optimizer', 'model_type','input_arguments']}
+    args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed', 'input_arguments']}
+    # args_dict = {key: args_dict[key] for key in args_dict if key not in ['run', 'lazy', 'nthreads', 'distributed']}
+    colBERTConfig = ColBERTConfig(**args_dict)
+
+    with Run().context(RunConfig(root=args.root, experiment=args.experiment, nranks=args.nranks, amp=args.amp)):
+        trainer = Trainer(args.triples, args.queries, args.collection, colBERTConfig)
+        trainer.train(args.checkpoint)
+
+
+if __name__ == "__main__":
+    main()
```

## primeqa/ir/dense/colbert_top/colbert/searcher.py

```diff
@@ -1,108 +1,120 @@
-import os
-import torch
-
-from tqdm import tqdm
-from typing import Union
-
-from primeqa.ir.dense.colbert_top.colbert.data import Collection, Queries, Ranking
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.checkpoint import Checkpoint
-from primeqa.ir.dense.colbert_top.colbert.search.index_storage import IndexScorer
-
-from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
-from primeqa.ir.dense.colbert_top.colbert.infra.launcher import print_memory_stats
-
-TextQueries = Union[str, 'list[str]', 'dict[int, str]', Queries]
-
-
-class Searcher:
-    def __init__(self, index, checkpoint=None, collection=None, config=None):
-        print_memory_stats()
-
-        initial_config = ColBERTConfig.from_existing(config, Run().config)
-
-        default_index_root = initial_config.index_root_
-        self.index = os.path.join(default_index_root, index)
-        self.index_config = ColBERTConfig.load_from_index(self.index)
-
-        self.checkpoint = checkpoint or self.index_config.checkpoint
-        self.checkpoint_config = ColBERTConfig.load_from_checkpoint(self.checkpoint)
-        self.config = ColBERTConfig.from_existing(self.checkpoint_config, self.index_config, initial_config)
-
-        self.collection = Collection.cast(collection or self.config.collection)
-        self.configure(checkpoint=self.checkpoint, collection=self.collection)
-
-        self.checkpoint = Checkpoint(self.checkpoint, colbert_config=self.config)
-        use_gpu = torch.cuda.is_available()
-        if use_gpu:
-            self.checkpoint = self.checkpoint.cuda()
-
-        self.ranker = IndexScorer(self.index, use_gpu)
-
-        print_memory_stats()
-
-    def configure(self, **kw_args):
-        self.config.configure(**kw_args)
-
-    def encode(self, text: TextQueries):
-        queries = text if type(text) is list else [text]
-        bsize = 128 if len(queries) > 128 else None
-
-        self.checkpoint.query_tokenizer.query_maxlen = self.config.query_maxlen
-        Q = self.checkpoint.queryFromText(queries, bsize=bsize, to_cpu=True)
-
-        return Q
-
-    def search(self, text: str, k=10):
-        return self.dense_search(self.encode(text), k)
-
-    def search_all(self, queries: TextQueries, k=10):
-        queries = Queries.cast(queries)
-        queries_ = list(queries.values())
-
-        Q = self.encode(queries_)
-
-        return self._search_all_Q(queries, Q, k)
-
-    def _search_all_Q(self, queries, Q, k):
-        all_scored_pids = [list(zip(*self.dense_search(Q[query_idx:query_idx+1], k=k)))
-                           for query_idx in tqdm(range(Q.size(0)))]
-
-        data = {qid: val for qid, val in zip(queries.keys(), all_scored_pids)}
-
-        provenance = Provenance()
-        provenance.source = 'Searcher::search_all'
-        provenance.queries = queries.provenance()
-        provenance.config = self.config.export()
-        provenance.k = k
-
-        return Ranking(data=data, provenance=provenance)
-
-    def dense_search(self, Q: torch.Tensor, k=10):
-        if k <= 10:
-            if self.config.ncells is None:
-                self.configure(ncells=1)
-            if self.config.centroid_score_threshold is None:
-                self.configure(centroid_score_threshold=0.5)
-            if self.config.ndocs is None:
-                self.configure(ndocs=256)
-        elif k <= 100:
-            if self.config.ncells is None:
-                self.configure(ncells=2)
-            if self.config.centroid_score_threshold is None:
-                self.configure(centroid_score_threshold=0.45)
-            if self.config.ndocs is None:
-                self.configure(ndocs=1024)
-        else:
-            if self.config.ncells is None:
-                self.configure(ncells=4)
-            if self.config.centroid_score_threshold is None:
-                self.configure(centroid_score_threshold=0.4)
-            if self.config.ndocs is None:
-                self.configure(ndocs=max(k * 4, 4096))
-
-        pids, scores = self.ranker.rank(self.config, Q, k)
-
-        return pids[:k], list(range(1, k+1)), scores[:k]
+import os
+import torch
+
+from tqdm import tqdm
+from typing import Union, List, Dict
+
+
+from primeqa.ir.dense.colbert_top.colbert.data import Collection, Queries, Ranking
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.checkpoint import Checkpoint
+from primeqa.ir.dense.colbert_top.colbert.search.index_storage import IndexScorer
+
+from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.infra.launcher import print_memory_stats
+
+TextQueries = Union[str, List[str], Dict[int, str], Queries]
+
+
+class Searcher:
+    def __init__(self, index, checkpoint=None, collection=None, config=None):
+        print_memory_stats()
+        
+        initial_config = ColBERTConfig.from_existing(Run().config, config)
+
+        if initial_config.index_location is not None:
+            self.index = initial_config.index_location
+        else:
+            self.index = (
+                initial_config.index_path
+                if initial_config.index_path
+                else (
+                    os.path.join(initial_config.index_root, index)
+                    if initial_config.index_root
+                    else os.path.join(initial_config.index_root_, index)
+                )
+            )
+ 
+        self.index_config = ColBERTConfig.load_from_index(self.index)
+
+        self.checkpoint = checkpoint or self.index_config.checkpoint
+        self.checkpoint_config = ColBERTConfig.load_from_checkpoint(self.checkpoint)
+        self.config = ColBERTConfig.from_existing(self.checkpoint_config, self.index_config, initial_config)
+
+        self.collection = None
+        self.configure(checkpoint=self.checkpoint, collection=self.collection)
+
+        self.checkpoint = Checkpoint(self.checkpoint, colbert_config=self.config)
+        use_gpu = torch.cuda.is_available()
+        if use_gpu:
+            self.checkpoint = self.checkpoint.cuda()
+
+        self.ranker = IndexScorer(self.index, use_gpu)
+
+        print_memory_stats()
+
+    def configure(self, **kw_args):
+        self.config.configure(**kw_args)
+
+    def encode(self, text: TextQueries):
+        queries = text if isinstance(text, list) else [text]
+        bsize = 128 if len(queries) > 128 else None
+
+        self.checkpoint.query_tokenizer.query_maxlen = self.config.query_maxlen
+        Q = self.checkpoint.queryFromText(queries, bsize=bsize, to_cpu=True)
+
+        return Q
+
+    def search(self, text: str, k=10):
+        return self.dense_search(self.encode(text), k)
+
+    def search_all(self, queries: TextQueries, k=10):
+        queries = Queries.cast(queries)
+        queries_ = list(queries.values())
+
+        Q = self.encode(queries_)
+
+        return self._search_all_Q(queries, Q, k)
+
+    def _search_all_Q(self, queries, Q, k):
+        all_scored_pids = [list(zip(*self.dense_search(Q[query_idx:query_idx+1], k=k)))
+                           for query_idx in tqdm(range(Q.size(0)))]
+
+        data = {qid: val for qid, val in zip(queries.keys(), all_scored_pids)}
+
+        provenance = Provenance()
+        provenance.source = 'Searcher::search_all'
+        provenance.queries = queries.provenance()
+        provenance.config = self.config.export()
+        provenance.k = k
+
+        return Ranking(data=data, provenance=provenance)
+
+    def dense_search(self, Q: torch.Tensor, k=10):
+        if k <= 10:
+            if self.config.ncells is None:
+                self.configure(ncells=1)
+            if self.config.centroid_score_threshold is None:
+                self.configure(centroid_score_threshold=0.5)
+            if self.config.ndocs is None:
+                self.configure(ndocs=256)
+        elif k <= 100:
+            if self.config.ncells is None:
+                self.configure(ncells=2)
+            if self.config.centroid_score_threshold is None:
+                self.configure(centroid_score_threshold=0.45)
+            if self.config.ndocs is None:
+                self.configure(ndocs=1024)
+        else:
+            if self.config.ncells is None:
+                self.configure(ncells=4)
+            if self.config.centroid_score_threshold is None:
+                self.configure(centroid_score_threshold=0.4)
+            if self.config.ndocs is None:
+                self.configure(ndocs=max(k * 4, 4096))
+
+        pids, scores = self.ranker.rank(self.config, Q, k)
+
+        return pids[:k], list(range(1, k+1)), scores[:k]
```

## primeqa/ir/dense/colbert_top/colbert/trainer.py

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-from primeqa.ir.dense.colbert_top.colbert.infra.launcher import Launcher
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
-
-from primeqa.ir.dense.colbert_top.colbert.training.training import train
-
-
-class Trainer:
-    def __init__(self, triples, queries, collection, config=None):
-        self.config = ColBERTConfig.from_existing(config, Run().config)
-
-        self.triples = triples
-        self.queries = queries
-        self.collection = collection
-
-    def configure(self, **kw_args):
-        self.config.configure(**kw_args)
-
-    def train(self, checkpoint='bert-base-uncased'):
-        """
-            Note that config.checkpoint is ignored. Only the supplied checkpoint here is used.
-        """
-
-        # Resources don't come from the config object. They come from the input parameters.
-        # TODO: After the API stabilizes, make this "self.config.assign()" to emphasize this distinction.
-        self.configure(triples=self.triples, queries=self.queries, collection=self.collection)
-        self.configure(checkpoint=checkpoint)
-
-        launcher = Launcher(train)
-
-        self._best_checkpoint_path = launcher.launch(self.config, self.triples, self.queries, self.collection)
-
-
-    def best_checkpoint_path(self):
-        return self._best_checkpoint_path
-
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+from primeqa.ir.dense.colbert_top.colbert.infra.launcher import Launcher
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig, RunConfig
+
+from primeqa.ir.dense.colbert_top.colbert.training.training import train
+
+
+class Trainer:
+    def __init__(self, triples, queries, collection, config=None):
+        self.config = ColBERTConfig.from_existing(config, Run().config)
+
+        self.triples = triples
+        self.queries = queries
+        self.collection = collection
+
+    def configure(self, **kw_args):
+        self.config.configure(**kw_args)
+
+    def train(self, checkpoint='bert-base-uncased'):
+        """
+            Note that config.checkpoint is ignored. Only the supplied checkpoint here is used.
+        """
+
+        # Resources don't come from the config object. They come from the input parameters.
+        # TODO: After the API stabilizes, make this "self.config.assign()" to emphasize this distinction.
+        self.configure(triples=self.triples, queries=self.queries, collection=self.collection)
+        self.configure(checkpoint=checkpoint)
+
+        launcher = Launcher(train)
+
+        self._best_checkpoint_path = launcher.launch(self.config, self.triples, self.queries, self.collection)
+
+
+    def best_checkpoint_path(self):
+        return self._best_checkpoint_path
+
```

## primeqa/ir/dense/colbert_top/colbert/data/__init__.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-from .collection import *
-from .queries import *
-
-from .ranking import *
-from .examples import *
+from .collection import *
+from .queries import *
+
+from .ranking import *
+from .examples import *
```

## primeqa/ir/dense/colbert_top/colbert/data/collection.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-
-# Could be .tsv or .json. The latter always allows more customization via optional parameters.
-# I think it could be worth doing some kind of parallel reads too, if the file exceeds 1 GiBs.
-# Just need to use a datastructure that shares things across processes without too much pickling.
-# I think multiprocessing.Manager can do that!
-
-import os
-import itertools
-
-from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_collection
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-
-
-class Collection:
-    def __init__(self, path=None, data=None):
-        self.path = path
-        self.data = data or self._load_file(path)
-
-    def __iter__(self):
-        # TODO: If __data isn't there, stream from disk!
-        return self.data.__iter__()
-
-    def __getitem__(self, item):
-        # TODO: Load from disk the first time this is called. Unless self.data is already not None.
-        return self.data[item]
-
-    def __len__(self):
-        # TODO: Load here too. Basically, let's make data a property function and, on first call, either load or get __data.
-        return len(self.data)
-
-    def _load_file(self, path):
-        self.path = path
-        return self._load_tsv(path) if path.endswith('.tsv') else self._load_jsonl(path)
-
-    def _load_tsv(self, path):
-        return load_collection(path)
-
-    def _load_jsonl(self, path):
-        raise NotImplementedError()
-
-    def provenance(self):
-        return self.path
-    
-    def toDict(self):
-        return {'provenance': self.provenance()}
-
-    def save(self, new_path):
-        assert new_path.endswith('.tsv'), "TODO: Support .json[l] too."
-        assert not os.path.exists(new_path), new_path
-
-        with Run().open(new_path, 'w') as f:
-            # TODO: expects content to always be a string here; no separate title!
-            for pid, content in enumerate(self.data):
-                content = f'{pid}\t{content}\n'
-                f.write(content)
-            
-            return f.name
-
-    def enumerate(self, rank):
-        for _, offset, passages in self.enumerate_batches(rank=rank):
-            for idx, passage in enumerate(passages):
-                yield (offset + idx, passage)
-
-    def enumerate_batches(self, rank, chunksize=None):
-        assert rank is not None, "TODO: Add support for the rank=None case."
-
-        chunksize = chunksize or self.get_chunksize()
-
-        offset = 0
-        iterator = iter(self)
-
-        for chunk_idx, owner in enumerate(itertools.cycle(range(Run().nranks))):
-            L = [line for _, line in zip(range(chunksize), iterator)]
-
-            if len(L) > 0 and owner == rank:
-                yield (chunk_idx, offset, L)
-
-            offset += len(L)
-
-            if len(L) < chunksize:
-                return
-    
-    def get_chunksize(self):
-        return min(25_000, 1 + len(self) // Run().nranks)  # 25k is great, 10k allows things to reside on GPU??
-
-    @classmethod
-    def cast(cls, obj):
-        if type(obj) is str:
-            return cls(path=obj)
-
-        if type(obj) is list:
-            return cls(data=obj)
-
-        if type(obj) is cls:
-            return obj
-
-        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
-
-
-# TODO: Look up path in some global [per-thread or thread-safe] list.
+
+# Could be .tsv or .json. The latter always allows more customization via optional parameters.
+# I think it could be worth doing some kind of parallel reads too, if the file exceeds 1 GiBs.
+# Just need to use a datastructure that shares things across processes without too much pickling.
+# I think multiprocessing.Manager can do that!
+
+import os
+import itertools
+
+from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_collection
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+
+
+class Collection:
+    def __init__(self, path=None, data=None):
+        self.path = path
+        self.data = data or self._load_file(path)
+
+    def __iter__(self):
+        # TODO: If __data isn't there, stream from disk!
+        return self.data.__iter__()
+
+    def __getitem__(self, item):
+        # TODO: Load from disk the first time this is called. Unless self.data is already not None.
+        return self.data[item]
+
+    def __len__(self):
+        # TODO: Load here too. Basically, let's make data a property function and, on first call, either load or get __data.
+        return len(self.data)
+
+    def _load_file(self, path):
+        self.path = path
+        return self._load_tsv(path) if path.endswith('.tsv') else self._load_jsonl(path)
+
+    def _load_tsv(self, path):
+        return load_collection(path)
+
+    def _load_jsonl(self, path):
+        raise NotImplementedError()
+
+    def provenance(self):
+        return self.path
+    
+    def toDict(self):
+        return {'provenance': self.provenance()}
+
+    def save(self, new_path):
+        assert new_path.endswith('.tsv'), "TODO: Support .json[l] too."
+        assert not os.path.exists(new_path), new_path
+
+        with Run().open(new_path, 'w') as f:
+            # TODO: expects content to always be a string here; no separate title!
+            for pid, content in enumerate(self.data):
+                content = f'{pid}\t{content}\n'
+                f.write(content)
+            
+            return f.name
+
+    def enumerate(self, rank):
+        for _, offset, passages in self.enumerate_batches(rank=rank):
+            for idx, passage in enumerate(passages):
+                yield (offset + idx, passage)
+
+    def enumerate_batches(self, rank, chunksize=None):
+        assert rank is not None, "TODO: Add support for the rank=None case."
+
+        chunksize = chunksize or self.get_chunksize()
+
+        offset = 0
+        iterator = iter(self)
+
+        for chunk_idx, owner in enumerate(itertools.cycle(range(Run().nranks))):
+            L = [line for _, line in zip(range(chunksize), iterator)]
+
+            if len(L) > 0 and owner == rank:
+                yield (chunk_idx, offset, L)
+
+            offset += len(L)
+
+            if len(L) < chunksize:
+                return
+    
+    def get_chunksize(self):
+        return min(25_000, 1 + len(self) // Run().nranks)  # 25k is great, 10k allows things to reside on GPU??
+
+    @classmethod
+    def cast(cls, obj):
+        if type(obj) is str:
+            return cls(path=obj)
+
+        if type(obj) is list:
+            return cls(data=obj)
+
+        if type(obj) is cls:
+            return obj
+
+        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
+
+
+# TODO: Look up path in some global [per-thread or thread-safe] list.
```

## primeqa/ir/dense/colbert_top/colbert/data/dataset.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-
-
-# Not just the corpus, but also an arbitrary number of query sets, indexed by name in a dictionary/dotdict.
-# And also query sets with top-k PIDs.
-# QAs too? TripleSets too?
-
-
-class Dataset:
-    def __init__(self):
-        pass
-
-    def select(self, key):
-        # Select the {corpus, queryset, tripleset, rankingset} determined by uniqueness or by key and return a "unique" dataset (e.g., for key=train)
-        pass
+
+
+# Not just the corpus, but also an arbitrary number of query sets, indexed by name in a dictionary/dotdict.
+# And also query sets with top-k PIDs.
+# QAs too? TripleSets too?
+
+
+class Dataset:
+    def __init__(self):
+        pass
+
+    def select(self, key):
+        # Select the {corpus, queryset, tripleset, rankingset} determined by uniqueness or by key and return a "unique" dataset (e.g., for key=train)
+        pass
```

## primeqa/ir/dense/colbert_top/colbert/data/examples.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-import os
-import ujson
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
-from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
-
-
-class Examples:
-    def __init__(self, path=None, data=None, nway=None, provenance=None):
-        self.__provenance = provenance or path or Provenance()
-        self.nway = nway
-        self.path = path
-        self.data = data or self._load_file(path)
-
-    def provenance(self):
-        return self.__provenance
-    
-    def toDict(self):
-        return self.provenance()
-
-    def _load_file(self, path):
-        nway = self.nway + 1 if self.nway else self.nway
-        examples = []
-
-        with open(path) as f:
-            for line in f:
-                example = ujson.loads(line)[:nway]
-                examples.append(example)
-
-        return examples
-
-    def tolist(self, rank=None, nranks=None):
-        """
-        NOTE: For distributed sampling, this isn't equivalent to perfectly uniform sampling.
-        In particular, each subset is perfectly represented in every batch! However, since we never
-        repeat passes over the data, we never repeat any particular triple, and the split across
-        nodes is random (since the underlying file is pre-shuffled), there's no concern here.
-        """
-
-        if rank or nranks:
-            assert rank in range(nranks), (rank, nranks)
-            return [self.data[idx] for idx in range(0, len(self.data), nranks)]  # if line_idx % nranks == rank
-
-        return list(self.data)
-
-    def save(self, new_path):
-        assert 'json' in new_path.strip('/').split('/')[-1].split('.'), "TODO: Support .json[l] too."
-
-        print_message(f"#> Writing {len(self.data) / 1000_000.0}M examples to {new_path}")
-
-        with Run().open(new_path, 'w') as f:
-            for example in self.data:
-                ujson.dump(example, f)
-                f.write('\n')
-
-            output_path = f.name
-            print_message(f"#> Saved examples with {len(self.data)} lines to {f.name}")
-        
-        with Run().open(f'{new_path}.meta', 'w') as f:
-            d = {}
-            d['metadata'] = get_metadata_only()
-            d['provenance'] = self.provenance()
-            line = ujson.dumps(d, indent=4)
-            f.write(line)
-
-        return output_path
-
-    @classmethod
-    def cast(cls, obj, nway=None):
-        if type(obj) is str:
-            return cls(path=obj, nway=nway)
-
-        if isinstance(obj, list):
-            return cls(data=obj, nway=nway)
-
-        if type(obj) is cls:
-            assert nway is None, nway
-            return obj
-
-        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+import os
+import ujson
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
+from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
+
+
+class Examples:
+    def __init__(self, path=None, data=None, nway=None, provenance=None):
+        self.__provenance = provenance or path or Provenance()
+        self.nway = nway
+        self.path = path
+        self.data = data or self._load_file(path)
+
+    def provenance(self):
+        return self.__provenance
+    
+    def toDict(self):
+        return self.provenance()
+
+    def _load_file(self, path):
+        nway = self.nway + 1 if self.nway else self.nway
+        examples = []
+
+        with open(path) as f:
+            for line in f:
+                example = ujson.loads(line)[:nway]
+                examples.append(example)
+
+        return examples
+
+    def tolist(self, rank=None, nranks=None):
+        """
+        NOTE: For distributed sampling, this isn't equivalent to perfectly uniform sampling.
+        In particular, each subset is perfectly represented in every batch! However, since we never
+        repeat passes over the data, we never repeat any particular triple, and the split across
+        nodes is random (since the underlying file is pre-shuffled), there's no concern here.
+        """
+
+        if rank or nranks:
+            assert rank in range(nranks), (rank, nranks)
+            return [self.data[idx] for idx in range(0, len(self.data), nranks)]  # if line_idx % nranks == rank
+
+        return list(self.data)
+
+    def save(self, new_path):
+        assert 'json' in new_path.strip('/').split('/')[-1].split('.'), "TODO: Support .json[l] too."
+
+        print_message(f"#> Writing {len(self.data) / 1000_000.0}M examples to {new_path}")
+
+        with Run().open(new_path, 'w') as f:
+            for example in self.data:
+                ujson.dump(example, f)
+                f.write('\n')
+
+            output_path = f.name
+            print_message(f"#> Saved examples with {len(self.data)} lines to {f.name}")
+        
+        with Run().open(f'{new_path}.meta', 'w') as f:
+            d = {}
+            d['metadata'] = get_metadata_only()
+            d['provenance'] = self.provenance()
+            line = ujson.dumps(d, indent=4)
+            f.write(line)
+
+        return output_path
+
+    @classmethod
+    def cast(cls, obj, nway=None):
+        if type(obj) is str:
+            return cls(path=obj, nway=nway)
+
+        if isinstance(obj, list):
+            return cls(data=obj, nway=nway)
+
+        if type(obj) is cls:
+            assert nway is None, nway
+            return obj
+
+        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
```

## primeqa/ir/dense/colbert_top/colbert/data/queries.py

 * *Ordering differences only*

```diff
@@ -1,163 +1,163 @@
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-import os
-import ujson
-
-from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_queries
-
-# TODO: Look up path in some global [per-thread or thread-safe] list.
-# TODO: path could be a list of paths...? But then how can we tell it's not a list of queries..
-
-
-class Queries:
-    def __init__(self, path=None, data=None):
-        self.path = path
-
-        if data:
-            assert isinstance(data, dict), type(data)
-        self._load_data(data) or self._load_file(path)
-    
-    def __len__(self):
-        return len(self.data)
-
-    def __iter__(self):
-        return iter(self.data.items())
-
-    def provenance(self):
-        return self.path
-    
-    def toDict(self):
-        return {'provenance': self.provenance()}
-
-    def _load_data(self, data):
-        if data is None:
-            return None
-
-        self.data = {}
-        self._qas = {}
-
-        for qid, content in data.items():
-            if isinstance(content, dict):
-                self.data[qid] = content['question']
-                self._qas[qid] = content
-            else:
-                self.data[qid] = content
-
-        if len(self._qas) == 0:
-            del self._qas
-
-        return True
-
-    def _load_file(self, path):
-        if not path.endswith('.json'):
-            self.data = load_queries(path)
-            return True
-        
-        # Load QAs
-        self.data = {}
-        self._qas = {}
-
-        with open(path) as f:
-            for line in f:
-                qa = ujson.loads(line)
-
-                assert qa['qid'] not in self.data
-                self.data[qa['qid']] = qa['question']
-                self._qas[qa['qid']] = qa
-
-        return self.data
-
-    def qas(self):
-        return dict(self._qas)
-
-    def __getitem__(self, key):
-        return self.data[key]
-
-    def keys(self):
-        return self.data.keys()
-
-    def values(self):
-        return self.data.values()
-
-    def items(self):
-        return self.data.items()
-
-    def save(self, new_path):
-        assert new_path.endswith('.tsv')
-        assert not os.path.exists(new_path), new_path
-
-        with Run().open(new_path, 'w') as f:
-            for qid, content in self.data.items():
-                content = f'{qid}\t{content}\n'
-                f.write(content)
-            
-            return f.name
-
-    def save_qas(self, new_path):
-        assert new_path.endswith('.json')
-        assert not os.path.exists(new_path), new_path
-
-        with open(new_path, 'w') as f:
-            for qid, qa in self._qas.items():
-                qa['qid'] = qid
-                f.write(ujson.dumps(qa) + '\n')
-
-    def _load_tsv(self, path):
-        raise NotImplementedError
-
-    def _load_jsonl(self, path):
-        raise NotImplementedError
-
-    @classmethod
-    def cast(cls, obj):
-        if type(obj) is str:
-            return cls(path=obj)
-
-        if isinstance(obj, dict) or isinstance(obj, list):
-            return cls(data=obj)
-
-        if type(obj) is cls:
-            return obj
-
-        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
-
-
-# class QuerySet:
-#     def __init__(self, *paths, renumber=False):
-#         self.paths = paths
-#         self.original_queries = [load_queries(path) for path in paths]
-
-#         if renumber:
-#             self.queries = flatten([q.values() for q in self.original_queries])
-#             self.queries = {idx: text for idx, text in enumerate(self.queries)}
-
-#         else:
-#             self.queries = {}
-
-#             for queries in self.original_queries:
-#                 assert len(set.intersection(set(queries.keys()), set(self.queries.keys()))) == 0, \
-#                     "renumber=False requires non-overlapping query IDs"
-
-#                 self.queries.update(queries)
-
-#         assert len(self.queries) == sum(map(len, self.original_queries))
-
-#     def todict(self):
-#         return dict(self.queries)
-
-#     def tolist(self):
-#         return list(self.queries.values())
-
-#     def query_sets(self):
-#         return self.original_queries
-
-#     def split_rankings(self, rankings):
-#         assert type(rankings) is list
-#         assert len(rankings) == len(self.queries)
-
-#         sub_rankings = []
-#         offset = 0
-#         for source in self.original_queries:
-#             sub_rankings.append(rankings[offset:offset+len(source)])
-#             offset += len(source)
-
-#         return sub_rankings
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+import os
+import ujson
+
+from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_queries
+
+# TODO: Look up path in some global [per-thread or thread-safe] list.
+# TODO: path could be a list of paths...? But then how can we tell it's not a list of queries..
+
+
+class Queries:
+    def __init__(self, path=None, data=None):
+        self.path = path
+
+        if data:
+            assert isinstance(data, dict), type(data)
+        self._load_data(data) or self._load_file(path)
+    
+    def __len__(self):
+        return len(self.data)
+
+    def __iter__(self):
+        return iter(self.data.items())
+
+    def provenance(self):
+        return self.path
+    
+    def toDict(self):
+        return {'provenance': self.provenance()}
+
+    def _load_data(self, data):
+        if data is None:
+            return None
+
+        self.data = {}
+        self._qas = {}
+
+        for qid, content in data.items():
+            if isinstance(content, dict):
+                self.data[qid] = content['question']
+                self._qas[qid] = content
+            else:
+                self.data[qid] = content
+
+        if len(self._qas) == 0:
+            del self._qas
+
+        return True
+
+    def _load_file(self, path):
+        if not path.endswith('.json'):
+            self.data = load_queries(path)
+            return True
+        
+        # Load QAs
+        self.data = {}
+        self._qas = {}
+
+        with open(path) as f:
+            for line in f:
+                qa = ujson.loads(line)
+
+                assert qa['qid'] not in self.data
+                self.data[qa['qid']] = qa['question']
+                self._qas[qa['qid']] = qa
+
+        return self.data
+
+    def qas(self):
+        return dict(self._qas)
+
+    def __getitem__(self, key):
+        return self.data[key]
+
+    def keys(self):
+        return self.data.keys()
+
+    def values(self):
+        return self.data.values()
+
+    def items(self):
+        return self.data.items()
+
+    def save(self, new_path):
+        assert new_path.endswith('.tsv')
+        assert not os.path.exists(new_path), new_path
+
+        with Run().open(new_path, 'w') as f:
+            for qid, content in self.data.items():
+                content = f'{qid}\t{content}\n'
+                f.write(content)
+            
+            return f.name
+
+    def save_qas(self, new_path):
+        assert new_path.endswith('.json')
+        assert not os.path.exists(new_path), new_path
+
+        with open(new_path, 'w') as f:
+            for qid, qa in self._qas.items():
+                qa['qid'] = qid
+                f.write(ujson.dumps(qa) + '\n')
+
+    def _load_tsv(self, path):
+        raise NotImplementedError
+
+    def _load_jsonl(self, path):
+        raise NotImplementedError
+
+    @classmethod
+    def cast(cls, obj):
+        if type(obj) is str:
+            return cls(path=obj)
+
+        if isinstance(obj, dict) or isinstance(obj, list):
+            return cls(data=obj)
+
+        if type(obj) is cls:
+            return obj
+
+        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
+
+
+# class QuerySet:
+#     def __init__(self, *paths, renumber=False):
+#         self.paths = paths
+#         self.original_queries = [load_queries(path) for path in paths]
+
+#         if renumber:
+#             self.queries = flatten([q.values() for q in self.original_queries])
+#             self.queries = {idx: text for idx, text in enumerate(self.queries)}
+
+#         else:
+#             self.queries = {}
+
+#             for queries in self.original_queries:
+#                 assert len(set.intersection(set(queries.keys()), set(self.queries.keys()))) == 0, \
+#                     "renumber=False requires non-overlapping query IDs"
+
+#                 self.queries.update(queries)
+
+#         assert len(self.queries) == sum(map(len, self.original_queries))
+
+#     def todict(self):
+#         return dict(self.queries)
+
+#     def tolist(self):
+#         return list(self.queries.values())
+
+#     def query_sets(self):
+#         return self.original_queries
+
+#     def split_rankings(self, rankings):
+#         assert type(rankings) is list
+#         assert len(rankings) == len(self.queries)
+
+#         sub_rankings = []
+#         offset = 0
+#         for source in self.original_queries:
+#             sub_rankings.append(rankings[offset:offset+len(source)])
+#             offset += len(source)
+
+#         return sub_rankings
```

## primeqa/ir/dense/colbert_top/colbert/data/ranking.py

```diff
@@ -1,94 +1,95 @@
-import os
-import tqdm
-import ujson
-from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
-
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, groupby_first_item
-from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
-
-
-def numericize(v):
-    if '.' in v:
-        return float(v)
-
-    return int(v)
-
-
-def load_ranking(path):  # works with annotated and un-annotated ranked lists
-    print_message("#> Loading the ranked lists from", path)
-
-    with open(path) as f:
-        return [list(map(numericize, line.strip().split('\t'))) for line in f]
-
-
-class Ranking:
-    def __init__(self, path=None, data=None, metrics=None, provenance=None):
-        self.__provenance = provenance or path or Provenance()
-        self.data = self._prepare_data(data or self._load_file(path))
-
-    def provenance(self):
-        return self.__provenance
-    
-    def toDict(self):
-        return {'provenance': self.provenance()}
-
-    def _prepare_data(self, data):
-        # TODO: Handle list of lists???
-        if isinstance(data, dict):
-            self.flat_ranking = [(qid, *rest) for qid, subranking in data.items() for rest in subranking]
-            return data
-
-        self.flat_ranking = data
-        return groupby_first_item(tqdm.tqdm(self.flat_ranking))
-
-    def _load_file(self, path):
-        return load_ranking(path)
-
-    def todict(self):
-        return dict(self.data)
-
-    def tolist(self):
-        return list(self.flat_ranking)
-
-    def items(self):
-        return self.data.items()
-
-    def _load_tsv(self, path):
-        raise NotImplementedError
-
-    def _load_jsonl(self, path):
-        raise NotImplementedError
-
-    def save(self, new_path):
-        assert 'tsv' in new_path.strip('/').split('/')[-1].split('.'), "TODO: Support .json[l] too."
-
-        with Run().open(new_path, 'w') as f:
-            for items in self.flat_ranking:
-                line = '\t'.join(map(lambda x: str(int(x) if type(x) is bool else x), items)) + '\n'
-                f.write(line)
-
-            output_path = f.name
-            print_message(f"#> Saved ranking of {len(self.data)} queries and {len(self.flat_ranking)} lines to {f.name}")
-        
-        with Run().open(f'{new_path}.meta', 'w') as f:
-            d = {}
-            d['metadata'] = get_metadata_only()
-            d['provenance'] = self.provenance()
-            line = ujson.dumps(d, indent=4)
-            f.write(line)
-        
-        return output_path
-
-    @classmethod
-    def cast(cls, obj):
-        if type(obj) is str:
-            return cls(path=obj)
-
-        if isinstance(obj, dict) or isinstance(obj, list):
-            return cls(data=obj)
-
-        if type(obj) is cls:
-            return obj
-
-        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
+import os
+import tqdm
+import ujson
+from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
+
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, groupby_first_item, create_directory
+from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
+
+
+def numericize(v):
+    if '.' in v:
+        return float(v)
+
+    return int(v)
+
+
+def load_ranking(path):  # works with annotated and un-annotated ranked lists
+    print_message("#> Loading the ranked lists from", path)
+
+    with open(path) as f:
+        return [list(map(numericize, line.strip().split('\t'))) for line in f]
+
+
+class Ranking:
+    def __init__(self, path=None, data=None, metrics=None, provenance=None):
+        self.__provenance = provenance or path or Provenance()
+        self.data = self._prepare_data(data or self._load_file(path))
+
+    def provenance(self):
+        return self.__provenance
+    
+    def toDict(self):
+        return {'provenance': self.provenance()}
+
+    def _prepare_data(self, data):
+        # TODO: Handle list of lists???
+        if isinstance(data, dict):
+            self.flat_ranking = [(qid, *rest) for qid, subranking in data.items() for rest in subranking]
+            return data
+
+        self.flat_ranking = data
+        return groupby_first_item(tqdm.tqdm(self.flat_ranking))
+
+    def _load_file(self, path):
+        return load_ranking(path)
+
+    def todict(self):
+        return dict(self.data)
+
+    def tolist(self):
+        return list(self.flat_ranking)
+
+    def items(self):
+        return self.data.items()
+
+    def _load_tsv(self, path):
+        raise NotImplementedError
+
+    def _load_jsonl(self, path):
+        raise NotImplementedError
+
+    def save(self, new_path):
+        assert 'tsv' in new_path.strip('/').split('/')[-1].split('.'), "TODO: Support .json[l] too."
+        create_directory(os.path.dirname(new_path))
+
+        with Run().open(new_path, 'w') as f:
+            for items in self.flat_ranking:
+                line = '\t'.join(map(lambda x: str(int(x) if type(x) is bool else x), items)) + '\n'
+                f.write(line)
+
+            output_path = f.name
+            print_message(f"#> Saved ranking of {len(self.data)} queries and {len(self.flat_ranking)} lines to {f.name}")
+        
+        with Run().open(f'{new_path}.meta', 'w') as f:
+            d = {}
+            d['metadata'] = get_metadata_only()
+            d['provenance'] = self.provenance()
+            line = ujson.dumps(d, indent=4)
+            f.write(line)
+        
+        return output_path
+
+    @classmethod
+    def cast(cls, obj):
+        if type(obj) is str:
+            return cls(path=obj)
+
+        if isinstance(obj, dict) or isinstance(obj, list):
+            return cls(data=obj)
+
+        if type(obj) is cls:
+            return obj
+
+        assert False, f"obj has type {type(obj)} which is not compatible with cast()"
```

## primeqa/ir/dense/colbert_top/colbert/distillation/ranking_scorer.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-import tqdm
-import ujson
-
-from collections import defaultdict
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, zipstar
-from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
-
-from primeqa.ir.dense.colbert_top.colbert.infra import Run
-from primeqa.ir.dense.colbert_top.colbert.data import Ranking
-from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
-from primeqa.ir.dense.colbert_top.colbert.distillation.scorer import Scorer
-
-
-class RankingScorer:
-    def __init__(self, scorer: Scorer, ranking: Ranking):
-        self.scorer = scorer
-        self.ranking = ranking.tolist()
-        self.__provenance = Provenance()
-
-        print_message(f"#> Loaded ranking with {len(self.ranking)} qid--pid pairs!")
-    
-    def provenance(self):
-        return self.__provenance
-
-    def run(self):
-        print_message(f"#> Starting..")
-
-        qids, pids, *_ = zipstar(self.ranking)
-        distillation_scores = self.scorer.launch(qids, pids)
-
-        scores_by_qid = defaultdict(list)
-
-        for qid, pid, score in tqdm.tqdm(zip(qids, pids, distillation_scores)):
-            scores_by_qid[qid].append((score, pid))
-
-        with Run().open('distillation_scores.json', 'w') as f:
-            for qid in tqdm.tqdm(scores_by_qid):
-                obj = (qid, scores_by_qid[qid])
-                f.write(ujson.dumps(obj) + '\n')
-
-            output_path = f.name
-            print_message(f'#> Saved the distillation_scores to {output_path}')
-
-        with Run().open(f'{output_path}.meta', 'w') as f:
-            d = {}
-            d['metadata'] = get_metadata_only()
-            d['provenance'] = self.provenance()
-            line = ujson.dumps(d, indent=4)
-            f.write(line)
-
-        return output_path
+import tqdm
+import ujson
+
+from collections import defaultdict
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, zipstar
+from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
+
+from primeqa.ir.dense.colbert_top.colbert.infra import Run
+from primeqa.ir.dense.colbert_top.colbert.data import Ranking
+from primeqa.ir.dense.colbert_top.colbert.infra.provenance import Provenance
+from primeqa.ir.dense.colbert_top.colbert.distillation.scorer import Scorer
+
+
+class RankingScorer:
+    def __init__(self, scorer: Scorer, ranking: Ranking):
+        self.scorer = scorer
+        self.ranking = ranking.tolist()
+        self.__provenance = Provenance()
+
+        print_message(f"#> Loaded ranking with {len(self.ranking)} qid--pid pairs!")
+    
+    def provenance(self):
+        return self.__provenance
+
+    def run(self):
+        print_message(f"#> Starting..")
+
+        qids, pids, *_ = zipstar(self.ranking)
+        distillation_scores = self.scorer.launch(qids, pids)
+
+        scores_by_qid = defaultdict(list)
+
+        for qid, pid, score in tqdm.tqdm(zip(qids, pids, distillation_scores)):
+            scores_by_qid[qid].append((score, pid))
+
+        with Run().open('distillation_scores.json', 'w') as f:
+            for qid in tqdm.tqdm(scores_by_qid):
+                obj = (qid, scores_by_qid[qid])
+                f.write(ujson.dumps(obj) + '\n')
+
+            output_path = f.name
+            print_message(f'#> Saved the distillation_scores to {output_path}')
+
+        with Run().open(f'{output_path}.meta', 'w') as f:
+            d = {}
+            d['metadata'] = get_metadata_only()
+            d['provenance'] = self.provenance()
+            line = ujson.dumps(d, indent=4)
+            f.write(line)
+
+        return output_path
```

## primeqa/ir/dense/colbert_top/colbert/distillation/scorer.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-import torch
-import tqdm
-
-from transformers import AutoTokenizer, AutoModelForSequenceClassification
-
-from primeqa.ir.dense.colbert_top.colbert.infra.launcher import Launcher
-from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
-from primeqa.ir.dense.colbert_top.colbert.modeling.reranker.electra import ElectraReranker
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten
-
-
-DEFAULT_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'
-
-
-class Scorer:
-    def __init__(self, queries, collection, model=DEFAULT_MODEL, maxlen=180, bsize=256):
-        self.queries = queries
-        self.collection = collection
-        self.model = model
-
-        self.maxlen = maxlen
-        self.bsize = bsize
-
-    def launch(self, qids, pids):
-        launcher = Launcher(self._score_pairs_process, return_all=True)
-        outputs = launcher.launch(Run().config, qids, pids)
-
-        return flatten(outputs)
-
-    def _score_pairs_process(self, config, qids, pids):
-        assert len(qids) == len(pids), (len(qids), len(pids))
-        share = 1 + len(qids) // config.nranks
-        offset = config.rank * share
-        endpos = (1 + config.rank) * share
-
-        return self._score_pairs(qids[offset:endpos], pids[offset:endpos], show_progress=(config.rank < 1))
-
-    def _score_pairs(self, qids, pids, show_progress=False):
-        tokenizer = AutoTokenizer.from_pretrained(self.model)
-        model = AutoModelForSequenceClassification.from_pretrained(self.model).cuda()
-
-        assert len(qids) == len(pids), (len(qids), len(pids))
-
-        scores = []
-
-        model.eval()
-        with torch.inference_mode():
-            with torch.cuda.amp.autocast():
-                for offset in tqdm.tqdm(range(0, len(qids), self.bsize), disable=(not show_progress)):
-                    endpos = offset + self.bsize
-
-                    queries_ = [self.queries[qid] for qid in qids[offset:endpos]]
-                    passages_ = [self.collection[pid] for pid in pids[offset:endpos]]
-
-                    features = tokenizer(queries_, passages_, padding='longest', truncation=True,
-                                            return_tensors='pt', max_length=self.maxlen).to(model.device)
-
-                    scores.append(model(**features).logits.flatten())
-
-        scores = torch.cat(scores)
-        scores = scores.tolist()
-
-        Run().print(f'Returning with {len(scores)} scores')
-
-        return scores
-
-
-# LONG-TERM TODO: This can be sped up by sorting by length in advance.
+import torch
+import tqdm
+
+from transformers import AutoTokenizer, AutoModelForSequenceClassification
+
+from primeqa.ir.dense.colbert_top.colbert.infra.launcher import Launcher
+from primeqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig
+from primeqa.ir.dense.colbert_top.colbert.modeling.reranker.electra import ElectraReranker
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten
+
+
+DEFAULT_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'
+
+
+class Scorer:
+    def __init__(self, queries, collection, model=DEFAULT_MODEL, maxlen=180, bsize=256):
+        self.queries = queries
+        self.collection = collection
+        self.model = model
+
+        self.maxlen = maxlen
+        self.bsize = bsize
+
+    def launch(self, qids, pids):
+        launcher = Launcher(self._score_pairs_process, return_all=True)
+        outputs = launcher.launch(Run().config, qids, pids)
+
+        return flatten(outputs)
+
+    def _score_pairs_process(self, config, qids, pids):
+        assert len(qids) == len(pids), (len(qids), len(pids))
+        share = 1 + len(qids) // config.nranks
+        offset = config.rank * share
+        endpos = (1 + config.rank) * share
+
+        return self._score_pairs(qids[offset:endpos], pids[offset:endpos], show_progress=(config.rank < 1))
+
+    def _score_pairs(self, qids, pids, show_progress=False):
+        tokenizer = AutoTokenizer.from_pretrained(self.model)
+        model = AutoModelForSequenceClassification.from_pretrained(self.model).cuda()
+
+        assert len(qids) == len(pids), (len(qids), len(pids))
+
+        scores = []
+
+        model.eval()
+        with torch.inference_mode():
+            with torch.cuda.amp.autocast():
+                for offset in tqdm.tqdm(range(0, len(qids), self.bsize), disable=(not show_progress)):
+                    endpos = offset + self.bsize
+
+                    queries_ = [self.queries[qid] for qid in qids[offset:endpos]]
+                    passages_ = [self.collection[pid] for pid in pids[offset:endpos]]
+
+                    features = tokenizer(queries_, passages_, padding='longest', truncation=True,
+                                            return_tensors='pt', max_length=self.maxlen).to(model.device)
+
+                    scores.append(model(**features).logits.flatten())
+
+        scores = torch.cat(scores)
+        scores = scores.tolist()
+
+        Run().print(f'Returning with {len(scores)} scores')
+
+        return scores
+
+
+# LONG-TERM TODO: This can be sped up by sorting by length in advance.
```

## primeqa/ir/dense/colbert_top/colbert/evaluation/load_model.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-import os
-import ujson
-import torch
-import random
-
-from collections import defaultdict, OrderedDict
-
-from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
-from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, load_checkpoint
-
-
-def load_model(args, do_print=True):
-    colbert = ColBERT.from_pretrained('bert-base-uncased',
-                                      query_maxlen=args.query_maxlen,
-                                      doc_maxlen=args.doc_maxlen,
-                                      dim=args.dim,
-                                      similarity_metric=args.similarity,
-                                      mask_punctuation=args.mask_punctuation)
-    colbert = colbert.to(DEVICE)
-
-    print_message("#> Loading model checkpoint.", condition=do_print)
-
-    checkpoint = load_checkpoint(args.checkpoint, colbert, do_print=do_print)
-
-    colbert.eval()
-
-    return colbert, checkpoint
+import os
+import ujson
+import torch
+import random
+
+from collections import defaultdict, OrderedDict
+
+from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
+from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, load_checkpoint
+
+
+def load_model(args, do_print=True):
+    colbert = ColBERT.from_pretrained('bert-base-uncased',
+                                      query_maxlen=args.query_maxlen,
+                                      doc_maxlen=args.doc_maxlen,
+                                      dim=args.dim,
+                                      similarity_metric=args.similarity,
+                                      mask_punctuation=args.mask_punctuation)
+    colbert = colbert.to(DEVICE)
+
+    print_message("#> Loading model checkpoint.", condition=do_print)
+
+    checkpoint = load_checkpoint(args.checkpoint, colbert, do_print=do_print)
+
+    colbert.eval()
+
+    return colbert, checkpoint
```

## primeqa/ir/dense/colbert_top/colbert/evaluation/loaders.py

 * *Ordering differences only*

```diff
@@ -1,209 +1,209 @@
-import os
-import ujson
-import torch
-import random
-
-from collections import defaultdict, OrderedDict
-
-from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
-from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, load_checkpoint, remove_first_and_last_quote
-from primeqa.ir.dense.colbert_top.colbert.evaluation.load_model import load_model
-from primeqa.ir.dense.colbert_top.colbert.utils.runs import Run
-
-
-def load_queries(queries_path):
-    queries = OrderedDict()
-
-    print_message("#> Loading the queries from", queries_path, "...")
-
-    with open(queries_path) as f:
-        for line in f:
-            qid, query, *_ = line.strip().split('\t')
-            qid = int(qid)
-
-            # removing (") at query
-            # query = remove_first_and_last_quote(query)
-
-            assert (qid not in queries), ("Query QID", qid, "is repeated!")
-            queries[qid] = query
-
-    print_message("#> Got", len(queries), "queries. All QIDs are unique.\n")
-
-    return queries
-
-
-def load_qrels(qrels_path):
-    if qrels_path is None:
-        return None
-
-    print_message("#> Loading qrels from", qrels_path, "...")
-
-    qrels = OrderedDict()
-    with open(qrels_path, mode='r', encoding="utf-8") as f:
-        for line in f:
-            qid, x, pid, y = map(int, line.strip().split('\t'))
-            assert x == 0 and y == 1
-            qrels[qid] = qrels.get(qid, [])
-            qrels[qid].append(pid)
-
-    # assert all(len(qrels[qid]) == len(set(qrels[qid])) for qid in qrels)
-    for qid in qrels:
-        qrels[qid] = list(set(qrels[qid]))
-
-    avg_positive = round(sum(len(qrels[qid]) for qid in qrels) / len(qrels), 2)
-
-    print_message("#> Loaded qrels for", len(qrels), "unique queries with",
-                  avg_positive, "positives per query on average.\n")
-
-    return qrels
-
-
-def load_topK(topK_path):
-    queries = OrderedDict()
-    topK_docs = OrderedDict()
-    topK_pids = OrderedDict()
-
-    print_message("#> Loading the top-k per query from", topK_path, "...")
-
-    with open(topK_path) as f:
-        for line_idx, line in enumerate(f):
-            if line_idx and line_idx % (10*1000*1000) == 0:
-                print(line_idx, end=' ', flush=True)
-
-            qid, pid, query, passage = line.split('\t')
-            qid, pid = int(qid), int(pid)
-
-            assert (qid not in queries) or (queries[qid] == query)
-            queries[qid] = query
-            topK_docs[qid] = topK_docs.get(qid, [])
-            topK_docs[qid].append(passage)
-            topK_pids[qid] = topK_pids.get(qid, [])
-            topK_pids[qid].append(pid)
-
-        print()
-
-    assert all(len(topK_pids[qid]) == len(set(topK_pids[qid])) for qid in topK_pids)
-
-    Ks = [len(topK_pids[qid]) for qid in topK_pids]
-
-    print_message("#> max(Ks) =", max(Ks), ", avg(Ks) =", round(sum(Ks) / len(Ks), 2))
-    print_message("#> Loaded the top-k per query for", len(queries), "unique queries.\n")
-
-    return queries, topK_docs, topK_pids
-
-
-def load_topK_pids(topK_path, qrels):
-    topK_pids = defaultdict(list)
-    topK_positives = defaultdict(list)
-
-    print_message("#> Loading the top-k PIDs per query from", topK_path, "...")
-
-    with open(topK_path) as f:
-        for line_idx, line in enumerate(f):
-            if line_idx and line_idx % (10*1000*1000) == 0:
-                print(line_idx, end=' ', flush=True)
-
-            qid, pid, *rest = line.strip().split('\t')
-            qid, pid = int(qid), int(pid)
-
-            topK_pids[qid].append(pid)
-
-            assert len(rest) in [1, 2, 3]
-
-            if len(rest) > 1:
-                *_, label = rest
-                label = int(label)
-                assert label in [0, 1]
-
-                if label >= 1:
-                    topK_positives[qid].append(pid)
-
-        print()
-
-    assert all(len(topK_pids[qid]) == len(set(topK_pids[qid])) for qid in topK_pids)
-    assert all(len(topK_positives[qid]) == len(set(topK_positives[qid])) for qid in topK_positives)
-
-    # Make them sets for fast lookups later
-    topK_positives = {qid: set(topK_positives[qid]) for qid in topK_positives}
-
-    Ks = [len(topK_pids[qid]) for qid in topK_pids]
-
-    print_message("#> max(Ks) =", max(Ks), ", avg(Ks) =", round(sum(Ks) / len(Ks), 2))
-    print_message("#> Loaded the top-k per query for", len(topK_pids), "unique queries.\n")
-
-    if len(topK_positives) == 0:
-        topK_positives = None
-    else:
-        assert len(topK_pids) >= len(topK_positives)
-
-        for qid in set.difference(set(topK_pids.keys()), set(topK_positives.keys())):
-            topK_positives[qid] = []
-
-        assert len(topK_pids) == len(topK_positives)
-
-        avg_positive = round(sum(len(topK_positives[qid]) for qid in topK_positives) / len(topK_pids), 2)
-
-        print_message("#> Concurrently got annotations for", len(topK_positives), "unique queries with",
-                      avg_positive, "positives per query on average.\n")
-
-    assert qrels is None or topK_positives is None, "Cannot have both qrels and an annotated top-K file!"
-
-    if topK_positives is None:
-        topK_positives = qrels
-
-    return topK_pids, topK_positives
-
-
-def load_collection(collection_path):
-    print_message("#> Loading collection...")
-
-    collection = []
-
-    with open(collection_path) as f:
-        for line_idx, line in enumerate(f):
-            if line_idx % (1000*1000) == 0:
-                print(f'{line_idx // 1000 // 1000}M', end=' ', flush=True)
-
-            pid, passage, *rest = line.strip('\n\r ').split('\t')
-            # pid, passage, *rest = line.strip().split('\t')
-            assert pid == 'id' or int(pid) == line_idx
-
-            # if pid == 'id':
-            #     continue
-
-            if len(rest) >= 1:
-                title = rest[0]
-                passage = title + ' | ' + passage
-                # Don't add | between title and passage
-                # remove (") at passage and add with space
-                # passage = remove_first_and_last_quote(passage)
-                # passage = remove_first_and_last_quote(title) + ' | ' + passage
-
-            collection.append(passage)
-
-    print()
-
-    return collection
-
-
-def load_colbert(args, do_print=True):
-    colbert, checkpoint = load_model(args, do_print)
-
-    # TODO: If the parameters below were not specified on the command line, their *checkpoint* values should be used.
-    # I.e., not their purely (i.e., training) default values.
-
-    for k in ['query_maxlen', 'doc_maxlen', 'dim', 'similarity', 'amp']:
-        if 'arguments' in checkpoint and hasattr(args, k):
-            if k in checkpoint['arguments'] and checkpoint['arguments'][k] != getattr(args, k):
-                a, b = checkpoint['arguments'][k], getattr(args, k)
-                Run.warn(f"Got checkpoint['arguments']['{k}'] != args.{k} (i.e., {a} != {b})")
-
-    if 'arguments' in checkpoint:
-        if args.rank < 1:
-            print(ujson.dumps(checkpoint['arguments'], indent=4))
-
-    if do_print:
-        print('\n')
-
-    return colbert, checkpoint
+import os
+import ujson
+import torch
+import random
+
+from collections import defaultdict, OrderedDict
+
+from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
+from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, load_checkpoint, remove_first_and_last_quote
+from primeqa.ir.dense.colbert_top.colbert.evaluation.load_model import load_model
+from primeqa.ir.dense.colbert_top.colbert.utils.runs import Run
+
+
+def load_queries(queries_path):
+    queries = OrderedDict()
+
+    print_message("#> Loading the queries from", queries_path, "...")
+
+    with open(queries_path) as f:
+        for line in f:
+            qid, query, *_ = line.strip().split('\t')
+            qid = int(qid)
+
+            # removing (") at query
+            # query = remove_first_and_last_quote(query)
+
+            assert (qid not in queries), ("Query QID", qid, "is repeated!")
+            queries[qid] = query
+
+    print_message("#> Got", len(queries), "queries. All QIDs are unique.\n")
+
+    return queries
+
+
+def load_qrels(qrels_path):
+    if qrels_path is None:
+        return None
+
+    print_message("#> Loading qrels from", qrels_path, "...")
+
+    qrels = OrderedDict()
+    with open(qrels_path, mode='r', encoding="utf-8") as f:
+        for line in f:
+            qid, x, pid, y = map(int, line.strip().split('\t'))
+            assert x == 0 and y == 1
+            qrels[qid] = qrels.get(qid, [])
+            qrels[qid].append(pid)
+
+    # assert all(len(qrels[qid]) == len(set(qrels[qid])) for qid in qrels)
+    for qid in qrels:
+        qrels[qid] = list(set(qrels[qid]))
+
+    avg_positive = round(sum(len(qrels[qid]) for qid in qrels) / len(qrels), 2)
+
+    print_message("#> Loaded qrels for", len(qrels), "unique queries with",
+                  avg_positive, "positives per query on average.\n")
+
+    return qrels
+
+
+def load_topK(topK_path):
+    queries = OrderedDict()
+    topK_docs = OrderedDict()
+    topK_pids = OrderedDict()
+
+    print_message("#> Loading the top-k per query from", topK_path, "...")
+
+    with open(topK_path) as f:
+        for line_idx, line in enumerate(f):
+            if line_idx and line_idx % (10*1000*1000) == 0:
+                print(line_idx, end=' ', flush=True)
+
+            qid, pid, query, passage = line.split('\t')
+            qid, pid = int(qid), int(pid)
+
+            assert (qid not in queries) or (queries[qid] == query)
+            queries[qid] = query
+            topK_docs[qid] = topK_docs.get(qid, [])
+            topK_docs[qid].append(passage)
+            topK_pids[qid] = topK_pids.get(qid, [])
+            topK_pids[qid].append(pid)
+
+        print()
+
+    assert all(len(topK_pids[qid]) == len(set(topK_pids[qid])) for qid in topK_pids)
+
+    Ks = [len(topK_pids[qid]) for qid in topK_pids]
+
+    print_message("#> max(Ks) =", max(Ks), ", avg(Ks) =", round(sum(Ks) / len(Ks), 2))
+    print_message("#> Loaded the top-k per query for", len(queries), "unique queries.\n")
+
+    return queries, topK_docs, topK_pids
+
+
+def load_topK_pids(topK_path, qrels):
+    topK_pids = defaultdict(list)
+    topK_positives = defaultdict(list)
+
+    print_message("#> Loading the top-k PIDs per query from", topK_path, "...")
+
+    with open(topK_path) as f:
+        for line_idx, line in enumerate(f):
+            if line_idx and line_idx % (10*1000*1000) == 0:
+                print(line_idx, end=' ', flush=True)
+
+            qid, pid, *rest = line.strip().split('\t')
+            qid, pid = int(qid), int(pid)
+
+            topK_pids[qid].append(pid)
+
+            assert len(rest) in [1, 2, 3]
+
+            if len(rest) > 1:
+                *_, label = rest
+                label = int(label)
+                assert label in [0, 1]
+
+                if label >= 1:
+                    topK_positives[qid].append(pid)
+
+        print()
+
+    assert all(len(topK_pids[qid]) == len(set(topK_pids[qid])) for qid in topK_pids)
+    assert all(len(topK_positives[qid]) == len(set(topK_positives[qid])) for qid in topK_positives)
+
+    # Make them sets for fast lookups later
+    topK_positives = {qid: set(topK_positives[qid]) for qid in topK_positives}
+
+    Ks = [len(topK_pids[qid]) for qid in topK_pids]
+
+    print_message("#> max(Ks) =", max(Ks), ", avg(Ks) =", round(sum(Ks) / len(Ks), 2))
+    print_message("#> Loaded the top-k per query for", len(topK_pids), "unique queries.\n")
+
+    if len(topK_positives) == 0:
+        topK_positives = None
+    else:
+        assert len(topK_pids) >= len(topK_positives)
+
+        for qid in set.difference(set(topK_pids.keys()), set(topK_positives.keys())):
+            topK_positives[qid] = []
+
+        assert len(topK_pids) == len(topK_positives)
+
+        avg_positive = round(sum(len(topK_positives[qid]) for qid in topK_positives) / len(topK_pids), 2)
+
+        print_message("#> Concurrently got annotations for", len(topK_positives), "unique queries with",
+                      avg_positive, "positives per query on average.\n")
+
+    assert qrels is None or topK_positives is None, "Cannot have both qrels and an annotated top-K file!"
+
+    if topK_positives is None:
+        topK_positives = qrels
+
+    return topK_pids, topK_positives
+
+
+def load_collection(collection_path):
+    print_message("#> Loading collection...")
+
+    collection = []
+
+    with open(collection_path) as f:
+        for line_idx, line in enumerate(f):
+            if line_idx % (1000*1000) == 0:
+                print(f'{line_idx // 1000 // 1000}M', end=' ', flush=True)
+
+            pid, passage, *rest = line.strip('\n\r ').split('\t')
+            # pid, passage, *rest = line.strip().split('\t')
+            assert pid == 'id' or int(pid) == line_idx
+
+            # if pid == 'id':
+            #     continue
+
+            if len(rest) >= 1:
+                title = rest[0]
+                passage = title + ' | ' + passage
+                # Don't add | between title and passage
+                # remove (") at passage and add with space
+                # passage = remove_first_and_last_quote(passage)
+                # passage = remove_first_and_last_quote(title) + ' | ' + passage
+
+            collection.append(passage)
+
+    print()
+
+    return collection
+
+
+def load_colbert(args, do_print=True):
+    colbert, checkpoint = load_model(args, do_print)
+
+    # TODO: If the parameters below were not specified on the command line, their *checkpoint* values should be used.
+    # I.e., not their purely (i.e., training) default values.
+
+    for k in ['query_maxlen', 'doc_maxlen', 'dim', 'similarity', 'amp']:
+        if 'arguments' in checkpoint and hasattr(args, k):
+            if k in checkpoint['arguments'] and checkpoint['arguments'][k] != getattr(args, k):
+                a, b = checkpoint['arguments'][k], getattr(args, k)
+                Run.warn(f"Got checkpoint['arguments']['{k}'] != args.{k} (i.e., {a} != {b})")
+
+    if 'arguments' in checkpoint:
+        if args.rank < 1:
+            print(ujson.dumps(checkpoint['arguments'], indent=4))
+
+    if do_print:
+        print('\n')
+
+    return colbert, checkpoint
```

## primeqa/ir/dense/colbert_top/colbert/evaluation/metrics.py

 * *Ordering differences only*

```diff
@@ -1,114 +1,114 @@
-import ujson
-
-from collections import defaultdict
-from primeqa.ir.dense.colbert_top.colbert.utils.runs import Run
-
-
-class Metrics:
-    def __init__(self, mrr_depths: set, recall_depths: set, success_depths: set, total_queries=None):
-        self.results = {}
-        self.mrr_sums = {depth: 0.0 for depth in mrr_depths}
-        self.recall_sums = {depth: 0.0 for depth in recall_depths}
-        self.success_sums = {depth: 0.0 for depth in success_depths}
-        self.total_queries = total_queries
-
-        self.max_query_idx = -1
-        self.num_queries_added = 0
-
-    def add(self, query_idx, query_key, ranking, gold_positives):
-        self.num_queries_added += 1
-
-        assert query_key not in self.results
-        assert len(self.results) <= query_idx
-        assert len(set(gold_positives)) == len(gold_positives)
-        assert len(set([pid for _, pid, _ in ranking])) == len(ranking)
-
-        self.results[query_key] = ranking
-
-        positives = [i for i, (_, pid, _) in enumerate(ranking) if pid in gold_positives]
-
-        if len(positives) == 0:
-            return
-
-        for depth in self.mrr_sums:
-            first_positive = positives[0]
-            self.mrr_sums[depth] += (1.0 / (first_positive+1.0)) if first_positive < depth else 0.0
-
-        for depth in self.success_sums:
-            first_positive = positives[0]
-            self.success_sums[depth] += 1.0 if first_positive < depth else 0.0
-
-        for depth in self.recall_sums:
-            num_positives_up_to_depth = len([pos for pos in positives if pos < depth])
-            self.recall_sums[depth] += num_positives_up_to_depth / len(gold_positives)
-
-    def print_metrics(self, query_idx):
-        for depth in sorted(self.mrr_sums):
-            print("MRR@" + str(depth), "=", self.mrr_sums[depth] / (query_idx+1.0))
-
-        for depth in sorted(self.success_sums):
-            print("Success@" + str(depth), "=", self.success_sums[depth] / (query_idx+1.0))
-
-        for depth in sorted(self.recall_sums):
-            print("Recall@" + str(depth), "=", self.recall_sums[depth] / (query_idx+1.0))
-
-    def log(self, query_idx):
-        assert query_idx >= self.max_query_idx
-        self.max_query_idx = query_idx
-
-        Run.log_metric("ranking/max_query_idx", query_idx, query_idx)
-        Run.log_metric("ranking/num_queries_added", self.num_queries_added, query_idx)
-
-        for depth in sorted(self.mrr_sums):
-            score = self.mrr_sums[depth] / (query_idx+1.0)
-            Run.log_metric("ranking/MRR." + str(depth), score, query_idx)
-
-        for depth in sorted(self.success_sums):
-            score = self.success_sums[depth] / (query_idx+1.0)
-            Run.log_metric("ranking/Success." + str(depth), score, query_idx)
-
-        for depth in sorted(self.recall_sums):
-            score = self.recall_sums[depth] / (query_idx+1.0)
-            Run.log_metric("ranking/Recall." + str(depth), score, query_idx)
-
-    def output_final_metrics(self, path, query_idx, num_queries):
-        assert query_idx + 1 == num_queries
-        assert num_queries == self.total_queries
-
-        if self.max_query_idx < query_idx:
-            self.log(query_idx)
-
-        self.print_metrics(query_idx)
-
-        output = defaultdict(dict)
-
-        for depth in sorted(self.mrr_sums):
-            score = self.mrr_sums[depth] / (query_idx+1.0)
-            output['mrr'][depth] = score
-
-        for depth in sorted(self.success_sums):
-            score = self.success_sums[depth] / (query_idx+1.0)
-            output['success'][depth] = score
-
-        for depth in sorted(self.recall_sums):
-            score = self.recall_sums[depth] / (query_idx+1.0)
-            output['recall'][depth] = score
-
-        with open(path, 'w') as f:
-            ujson.dump(output, f, indent=4)
-            f.write('\n')
-
-
-def evaluate_recall(qrels, queries, topK_pids):
-    if qrels is None:
-        return
-
-    assert set(qrels.keys()) == set(queries.keys())
-    recall_at_k = [len(set.intersection(set(qrels[qid]), set(topK_pids[qid]))) / max(1.0, len(qrels[qid]))
-                   for qid in qrels]
-    recall_at_k = sum(recall_at_k) / len(qrels)
-    recall_at_k = round(recall_at_k, 3)
-    print("Recall @ maximum depth =", recall_at_k)
-
-
-# TODO: If implicit qrels are used (for re-ranking), warn if a recall metric is requested + add an asterisk to output.
+import ujson
+
+from collections import defaultdict
+from primeqa.ir.dense.colbert_top.colbert.utils.runs import Run
+
+
+class Metrics:
+    def __init__(self, mrr_depths: set, recall_depths: set, success_depths: set, total_queries=None):
+        self.results = {}
+        self.mrr_sums = {depth: 0.0 for depth in mrr_depths}
+        self.recall_sums = {depth: 0.0 for depth in recall_depths}
+        self.success_sums = {depth: 0.0 for depth in success_depths}
+        self.total_queries = total_queries
+
+        self.max_query_idx = -1
+        self.num_queries_added = 0
+
+    def add(self, query_idx, query_key, ranking, gold_positives):
+        self.num_queries_added += 1
+
+        assert query_key not in self.results
+        assert len(self.results) <= query_idx
+        assert len(set(gold_positives)) == len(gold_positives)
+        assert len(set([pid for _, pid, _ in ranking])) == len(ranking)
+
+        self.results[query_key] = ranking
+
+        positives = [i for i, (_, pid, _) in enumerate(ranking) if pid in gold_positives]
+
+        if len(positives) == 0:
+            return
+
+        for depth in self.mrr_sums:
+            first_positive = positives[0]
+            self.mrr_sums[depth] += (1.0 / (first_positive+1.0)) if first_positive < depth else 0.0
+
+        for depth in self.success_sums:
+            first_positive = positives[0]
+            self.success_sums[depth] += 1.0 if first_positive < depth else 0.0
+
+        for depth in self.recall_sums:
+            num_positives_up_to_depth = len([pos for pos in positives if pos < depth])
+            self.recall_sums[depth] += num_positives_up_to_depth / len(gold_positives)
+
+    def print_metrics(self, query_idx):
+        for depth in sorted(self.mrr_sums):
+            print("MRR@" + str(depth), "=", self.mrr_sums[depth] / (query_idx+1.0))
+
+        for depth in sorted(self.success_sums):
+            print("Success@" + str(depth), "=", self.success_sums[depth] / (query_idx+1.0))
+
+        for depth in sorted(self.recall_sums):
+            print("Recall@" + str(depth), "=", self.recall_sums[depth] / (query_idx+1.0))
+
+    def log(self, query_idx):
+        assert query_idx >= self.max_query_idx
+        self.max_query_idx = query_idx
+
+        Run.log_metric("ranking/max_query_idx", query_idx, query_idx)
+        Run.log_metric("ranking/num_queries_added", self.num_queries_added, query_idx)
+
+        for depth in sorted(self.mrr_sums):
+            score = self.mrr_sums[depth] / (query_idx+1.0)
+            Run.log_metric("ranking/MRR." + str(depth), score, query_idx)
+
+        for depth in sorted(self.success_sums):
+            score = self.success_sums[depth] / (query_idx+1.0)
+            Run.log_metric("ranking/Success." + str(depth), score, query_idx)
+
+        for depth in sorted(self.recall_sums):
+            score = self.recall_sums[depth] / (query_idx+1.0)
+            Run.log_metric("ranking/Recall." + str(depth), score, query_idx)
+
+    def output_final_metrics(self, path, query_idx, num_queries):
+        assert query_idx + 1 == num_queries
+        assert num_queries == self.total_queries
+
+        if self.max_query_idx < query_idx:
+            self.log(query_idx)
+
+        self.print_metrics(query_idx)
+
+        output = defaultdict(dict)
+
+        for depth in sorted(self.mrr_sums):
+            score = self.mrr_sums[depth] / (query_idx+1.0)
+            output['mrr'][depth] = score
+
+        for depth in sorted(self.success_sums):
+            score = self.success_sums[depth] / (query_idx+1.0)
+            output['success'][depth] = score
+
+        for depth in sorted(self.recall_sums):
+            score = self.recall_sums[depth] / (query_idx+1.0)
+            output['recall'][depth] = score
+
+        with open(path, 'w') as f:
+            ujson.dump(output, f, indent=4)
+            f.write('\n')
+
+
+def evaluate_recall(qrels, queries, topK_pids):
+    if qrels is None:
+        return
+
+    assert set(qrels.keys()) == set(queries.keys())
+    recall_at_k = [len(set.intersection(set(qrels[qid]), set(topK_pids[qid]))) / max(1.0, len(qrels[qid]))
+                   for qid in qrels]
+    recall_at_k = sum(recall_at_k) / len(qrels)
+    recall_at_k = round(recall_at_k, 3)
+    print("Recall @ maximum depth =", recall_at_k)
+
+
+# TODO: If implicit qrels are used (for re-ranking), warn if a recall metric is requested + add an asterisk to output.
```

## primeqa/ir/dense/colbert_top/colbert/indexing/collection_encoder.py

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-import torch
-
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, batch
-
-
-class CollectionEncoder():
-    def __init__(self, config, checkpoint):
-        self.config = config
-        self.checkpoint = checkpoint
-
-    def encode_passages(self, passages):
-        Run().print(f"#> Encoding {len(passages)} passages..")
-
-        if len(passages) == 0:
-            return None, None
-
-        with torch.inference_mode():
-            embs, doclens = [], []
-
-            # Batch here to avoid OOM from storing intermediate embeddings on GPU.
-            # Storing on the GPU helps with speed of masking, etc.
-            # But ideally this batching happens internally inside docFromText.
-            for passages_batch in batch(passages, self.config.bsize * 50):
-            # for passages_batch in batch(passages, self.config.bsize * 1):
-                embs_, doclens_ = self.checkpoint.docFromText(passages_batch, bsize=self.config.bsize,
-                                                              keep_dims='flatten', showprogress=False)
-                embs.append(embs_)
-                doclens.extend(doclens_)
-
-            embs = torch.cat(embs)
-
-            # embs, doclens = self.checkpoint.docFromText(passages, bsize=self.config.bsize,
-            #                                                   keep_dims='flatten', showprogress=(self.config.rank < 1))
-
-        # with torch.inference_mode():
-        #     embs = self.checkpoint.docFromText(passages, bsize=self.config.bsize,
-        #                                        keep_dims=False, showprogress=(self.config.rank < 1))
-        #     assert type(embs) is list
-        #     assert len(embs) == len(passages)
-
-        #     doclens = [d.size(0) for d in embs]
-        #     embs = torch.cat(embs)
-
-        return embs, doclens
+import torch
+
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, batch
+
+
+class CollectionEncoder():
+    def __init__(self, config, checkpoint):
+        self.config = config
+        self.checkpoint = checkpoint
+
+    def encode_passages(self, passages):
+        Run().print(f"#> Encoding {len(passages)} passages..")
+
+        if len(passages) == 0:
+            return None, None
+
+        with torch.inference_mode():
+            embs, doclens = [], []
+
+            # Batch here to avoid OOM from storing intermediate embeddings on GPU.
+            # Storing on the GPU helps with speed of masking, etc.
+            # But ideally this batching happens internally inside docFromText.
+            for passages_batch in batch(passages, self.config.bsize * 50):
+            # for passages_batch in batch(passages, self.config.bsize * 1):
+                embs_, doclens_ = self.checkpoint.docFromText(passages_batch, bsize=self.config.bsize,
+                                                              keep_dims='flatten', showprogress=False)
+                embs.append(embs_)
+                doclens.extend(doclens_)
+
+            embs = torch.cat(embs)
+
+            # embs, doclens = self.checkpoint.docFromText(passages, bsize=self.config.bsize,
+            #                                                   keep_dims='flatten', showprogress=(self.config.rank < 1))
+
+        # with torch.inference_mode():
+        #     embs = self.checkpoint.docFromText(passages, bsize=self.config.bsize,
+        #                                        keep_dims=False, showprogress=(self.config.rank < 1))
+        #     assert type(embs) is list
+        #     assert len(embs) == len(passages)
+
+        #     doclens = [d.size(0) for d in embs]
+        #     embs = torch.cat(embs)
+
+        return embs, doclens
```

## primeqa/ir/dense/colbert_top/colbert/indexing/collection_indexer.py

 * *Ordering differences only*

```diff
@@ -1,431 +1,431 @@
-import os
-import tqdm
-import time
-import ujson
-import faiss
-import torch
-import random
-
-import numpy as np
-import torch.multiprocessing as mp
-from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
-
-import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
-
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-from primeqa.ir.dense.colbert_top.colbert.infra.launcher import print_memory_stats
-from primeqa.ir.dense.colbert_top.colbert.modeling.checkpoint import Checkpoint
-from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
-
-from primeqa.ir.dense.colbert_top.colbert.indexing.collection_encoder import CollectionEncoder
-from primeqa.ir.dense.colbert_top.colbert.indexing.index_saver import IndexSaver
-from primeqa.ir.dense.colbert_top.colbert.indexing.utils import optimize_ivf
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message
-
-from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual import ResidualCodec
-
-
-def encode(config, collection, shared_lists, shared_queues):
-    encoder = CollectionIndexer(config=config, collection=collection)
-    encoder.run(shared_lists)
-
-
-class CollectionIndexer():
-    def __init__(self, config: ColBERTConfig, collection):
-        self.config = config
-        self.rank, self.nranks = self.config.rank, self.config.nranks
-        self.num_partitions_max = self.config.num_partitions_max
-
-        if self.config.rank == 0:
-            self.config.help()
-
-        self.collection = Collection.cast(collection)
-        if torch.cuda.is_available():
-            self.checkpoint = Checkpoint(self.config.checkpoint, colbert_config=self.config).cuda()
-        else:
-            self.checkpoint = Checkpoint(self.config.checkpoint, colbert_config=self.config).cpu()
-
-        self.encoder = CollectionEncoder(config, self.checkpoint)
-        self.saver = IndexSaver(config)
-
-        print_memory_stats(f'RANK:{self.rank}')
-
-    def run(self, shared_lists):
-        with torch.inference_mode():
-            self.setup()
-            distributed.barrier(self.rank)
-            print_memory_stats(f'RANK:{self.rank}')
-
-            self.train(shared_lists)
-            distributed.barrier(self.rank)
-            print_memory_stats(f'RANK:{self.rank}')
-
-            self.index()
-            distributed.barrier(self.rank)
-            print_memory_stats(f'RANK:{self.rank}')
-
-            self.finalize()
-            distributed.barrier(self.rank)
-            print_memory_stats(f'RANK:{self.rank}')
-
-    def setup(self):
-        self.num_chunks = int(np.ceil(len(self.collection) / self.collection.get_chunksize()))
-
-        sampled_pids = self._sample_pids()
-        avg_doclen_est = self._sample_embeddings(sampled_pids)
-
-        # Select the number of partitions
-        num_passages = len(self.collection)
-        self.num_embeddings_est = num_passages * avg_doclen_est
-        # self.num_partitions = int(2 ** np.floor(np.log2(16 * np.sqrt(self.num_embeddings_est))))
-        # 16 --> 4 suggested by @Omar Khattab, reduce the numbers of centroids
-        # self.num_partitions = int(2 ** np.floor(np.log2(4 * np.sqrt(self.num_embeddings_est))))
-        num_partitions_multiplier = 8
-        self.num_partitions = int(2 ** np.floor(np.log2(num_partitions_multiplier * np.sqrt(self.num_embeddings_est))))
-        print_message(f'>> num_partitions_multiplier = {num_partitions_multiplier}, self.num_partitions = {self.num_partitions}')
-        # num_partitions_max = 50000
-        if self.num_partitions > self.num_partitions_max:
-            self.num_partitions = self.num_partitions_max
-            print_message(f'>> num_partitions limited to: self.num_partitions = {self.num_partitions}')
-
-        Run().print_main(f'Creaing {self.num_partitions:,} partitions.')
-        Run().print_main(f'*Estimated* {int(self.num_embeddings_est):,} embeddings.')
-
-        self._save_plan()
-
-    def _sample_pids(self):
-        num_passages = len(self.collection)
-
-        # Simple alternative: < 100k: 100%, < 1M: 15%, < 10M: 7%, < 100M: 3%, > 100M: 1%
-        # Keep in mind that, say, 15% still means at least 100k.
-        # So the formula is max(100% * min(total, 100k), 15% * min(total, 1M), ...)
-        # Then we subsample the vectors to 100 * num_partitions
-
-        typical_doclen = 120  # let's keep sampling independent of the actual doc_maxlen
-        sampled_pids = 16 * np.sqrt(typical_doclen * num_passages)
-        # sampled_pids = int(2 ** np.floor(np.log2(1 + sampled_pids)))
-        sampled_pids = min(1 + int(sampled_pids), num_passages)
-
-        sampled_pids = random.sample(range(num_passages), sampled_pids)
-        Run().print_main(f"# of sampled PIDs = {len(sampled_pids)} \t sampled_pids[:3] = {sampled_pids[:3]}")
-
-        return set(sampled_pids)
-
-    def _sample_embeddings(self, sampled_pids):
-        local_pids = self.collection.enumerate(rank=self.rank)
-        local_sample = [passage for pid, passage in local_pids if pid in sampled_pids]
-
-        local_sample_embs, doclens = self.encoder.encode_passages(local_sample)
-
-        if torch.cuda.is_available():
-            self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
-            torch.distributed.all_reduce(self.num_sample_embs)
-
-            avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
-            avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
-            torch.distributed.all_reduce(avg_doclen_est)
-
-            nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
-            torch.distributed.all_reduce(nonzero_ranks)
-        else:
-            if torch.distributed.is_initialized():
-                self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
-                torch.distributed.all_reduce(self.num_sample_embs)
-
-                avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
-                avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
-                torch.distributed.all_reduce(avg_doclen_est)
-
-                nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
-                torch.distributed.all_reduce(nonzero_ranks)
-            else:
-                self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
-
-                avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
-                avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
-
-                nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
-
-        avg_doclen_est = avg_doclen_est.item() / nonzero_ranks.item()
-        self.avg_doclen_est = avg_doclen_est
-
-        Run().print(f'avg_doclen_est = {avg_doclen_est} \t len(local_sample) = {len(local_sample):,}')
-
-        torch.save(local_sample_embs, os.path.join(self.config.index_path_, f'sample.{self.rank}.pt'))
-
-        return avg_doclen_est
-
-    def _save_plan(self):
-        if self.rank < 1:
-            config = self.config
-            self.plan_path = os.path.join(config.index_path_, 'plan.json')
-            Run().print("#> Saving the indexing plan to", self.plan_path, "..")
-
-            with open(self.plan_path, 'w') as f:
-                d = {'config': config.export()}
-                d['num_chunks'] = self.num_chunks
-                d['num_partitions'] = self.num_partitions
-                d['num_embeddings_est'] = self.num_embeddings_est
-                d['avg_doclen_est'] = self.avg_doclen_est
-
-                f.write(ujson.dumps(d, indent=4) + '\n')
-
-    def train(self, shared_lists):
-        if self.rank > 0:
-            return
-
-        sample, heldout = self._concatenate_and_split_sample()
-
-        centroids = self._train_kmeans(sample, shared_lists)
-
-        print_memory_stats(f'RANK:{self.rank}')
-        del sample
-
-        bucket_cutoffs, bucket_weights, avg_residual = self._compute_avg_residual(centroids, heldout)
-
-        print_message(f'avg_residual = {avg_residual}')
-
-        codec = ResidualCodec(config=self.config, centroids=centroids, avg_residual=avg_residual,
-                              bucket_cutoffs=bucket_cutoffs, bucket_weights=bucket_weights)
-        self.saver.save_codec(codec)
-
-    def _concatenate_and_split_sample(self):
-        print_memory_stats(f'***1*** \t RANK:{self.rank}')
-
-        # TODO: Allocate a float16 array. Load the samples from disk, copy to array.
-        sample = torch.empty(self.num_sample_embs, self.config.dim, dtype=torch.float16)
-
-        offset = 0
-        for r in range(self.nranks):
-            sub_sample_path = os.path.join(self.config.index_path_, f'sample.{r}.pt')
-            sub_sample = torch.load(sub_sample_path)
-            os.remove(sub_sample_path)
-
-            endpos = offset + sub_sample.size(0)
-            sample[offset:endpos] = sub_sample
-            offset = endpos
-
-        assert endpos == sample.size(0), (endpos, sample.size())
-
-        print_memory_stats(f'***2*** \t RANK:{self.rank}')
-
-        # Shuffle and split out a 5% "heldout" sub-sample [up to 50k elements]
-        sample = sample[torch.randperm(sample.size(0))]
-
-        print_memory_stats(f'***3*** \t RANK:{self.rank}')
-
-        heldout_fraction = 0.05 # 5%
-        heldout_size = int(min(heldout_fraction * sample.size(0), 50_000))
-        sample, sample_heldout = sample.split([sample.size(0) - heldout_size, heldout_size], dim=0)
-
-        print_memory_stats(f'***4*** \t RANK:{self.rank}')
-
-        return sample, sample_heldout
-
-    def _train_kmeans(self, sample, shared_lists):
-        if torch.cuda.is_available():
-            torch.cuda.empty_cache()
-
-        do_fork_for_faiss = False  # set to True to free faiss GPU-0 memory at the cost of one more copy of `sample`.
-
-        args_ = [self.config.dim, self.num_partitions, self.config.kmeans_niters]
-
-        if do_fork_for_faiss:
-            # For this to work reliably, write the sample to disk. Pickle may not handle >4GB of data.
-            # Delete the sample file after work is done.
-
-            shared_lists[0][0] = sample
-            return_value_queue = mp.Queue()
-
-            args_ = args_ + [shared_lists, return_value_queue]
-            proc = mp.Process(target=compute_faiss_kmeans, args=args_)
-
-            proc.start()
-            centroids = return_value_queue.get()
-            proc.join()
-
-        else:
-            args_ = args_ + [[[sample]]]
-            centroids = compute_faiss_kmeans(*args_)
-
-        centroids = torch.nn.functional.normalize(centroids, dim=-1).half()
-
-        return centroids
-
-    def _compute_avg_residual(self, centroids, heldout):
-        compressor = ResidualCodec(config=self.config, centroids=centroids, avg_residual=None)
-
-        if torch.cuda.is_available():
-            heldout_reconstruct = compressor.compress_into_codes(heldout, out_device='cuda')
-            heldout_reconstruct = compressor.lookup_centroids(heldout_reconstruct, out_device='cuda')
-            heldout_avg_residual = heldout.cuda() - heldout_reconstruct
-        else:
-            heldout_reconstruct = compressor.compress_into_codes(heldout, out_device='cpu')
-            heldout_reconstruct = compressor.lookup_centroids(heldout_reconstruct, out_device='cpu')
-            heldout_avg_residual = heldout - heldout_reconstruct
-
-        avg_residual = torch.abs(heldout_avg_residual).mean(dim=0).cpu()
-        print([round(x, 3) for x in avg_residual.squeeze().tolist()])
-
-        num_options = 2 ** self.config.nbits
-        quantiles = torch.arange(0, num_options, device=heldout_avg_residual.device) * (1 / num_options)
-        bucket_cutoffs_quantiles, bucket_weights_quantiles = quantiles[1:], quantiles + (0.5 / num_options)
-
-        bucket_cutoffs = heldout_avg_residual.float().quantile(bucket_cutoffs_quantiles)
-        bucket_weights = heldout_avg_residual.float().quantile(bucket_weights_quantiles)
-
-        print_message(
-            f"#> Got bucket_cutoffs_quantiles = {bucket_cutoffs_quantiles} and bucket_weights_quantiles = {bucket_weights_quantiles}")
-        print_message(f"#> Got bucket_cutoffs = {bucket_cutoffs} and bucket_weights = {bucket_weights}")
-
-        return bucket_cutoffs, bucket_weights, avg_residual.mean()
-
-        # EVENTAULLY: Compare the above with non-heldout sample. If too different, we can do better!
-        # sample = sample[subsample_idxs]
-        # sample_reconstruct = get_centroids_for(centroids, sample)
-        # sample_avg_residual = (sample - sample_reconstruct).mean(dim=0)
-
-    def index(self):
-        with self.saver.thread():
-            batches = self.collection.enumerate_batches(rank=self.rank)
-            for chunk_idx, offset, passages in tqdm.tqdm(batches, disable=self.rank > 0):
-                embs, doclens = self.encoder.encode_passages(passages)
-                if torch.cuda.is_available():
-                    assert embs.dtype == torch.float16, embs.dtype
-                else:
-                    assert embs.dtype == torch.float32, embs.dtype
-                    embs = embs.half()
-
-                Run().print_main(f"#> Saving chunk {chunk_idx}: \t {len(passages):,} passages "
-                                 f"and {embs.size(0):,} embeddings. From #{offset:,} onward.")
-
-                self.saver.save_chunk(chunk_idx, offset, embs, doclens)
-                del embs, doclens
-
-    def finalize(self):
-        if self.rank > 0:
-            return
-
-        self._check_all_files_are_saved()
-        self._collect_embedding_id_offset()
-
-        self._build_ivf()
-        self._update_metadata()
-
-    def _check_all_files_are_saved(self):
-        for chunk_idx in range(self.num_chunks):
-            # EVENTUALLY: Check those files!
-            pass
-
-    def _collect_embedding_id_offset(self):
-        passage_offset = 0
-        embedding_offset = 0
-
-        self.embedding_offsets = []
-
-        for chunk_idx in range(self.num_chunks):
-            metadata_path = os.path.join(self.config.index_path_, f'{chunk_idx}.metadata.json')
-
-            with open(metadata_path) as f:
-                chunk_metadata = ujson.load(f)
-
-                chunk_metadata['embedding_offset'] = embedding_offset
-                self.embedding_offsets.append(embedding_offset)
-
-                assert chunk_metadata['passage_offset'] == passage_offset, (chunk_idx, passage_offset, chunk_metadata)
-
-                passage_offset += chunk_metadata['num_passages']
-                embedding_offset += chunk_metadata['num_embeddings']
-
-            with open(metadata_path, 'w') as f:
-                f.write(ujson.dumps(chunk_metadata, indent=4) + '\n')
-
-        self.num_embeddings = embedding_offset
-        assert len(self.embedding_offsets) == self.num_chunks, len(self.embedding_offsets)
-
-    def _build_ivf(self):
-        # Maybe we should several small IVFs? Every 250M embeddings, so that's every 1 GB.
-        # It would save *memory* here and *disk space* regarding the int64.
-        # But we'd have to decide how many IVFs to use during retrieval: many (loop) or one?
-        # A loop seems nice if we can find a size that's large enough for speed yet small enough to fit on GPU!
-        # Then it would help nicely for batching later: 1GB.
-
-        codes = torch.empty(self.num_embeddings,)
-        print_memory_stats(f'RANK:{self.rank}')
-
-        for chunk_idx in range(self.num_chunks):
-            offset = self.embedding_offsets[chunk_idx]
-            chunk_codes = ResidualCodec.Embeddings.load_codes(self.config.index_path_, chunk_idx)
-
-            codes[offset:offset+chunk_codes.size(0)] = chunk_codes
-
-        print_message(f"offset: {offset}")
-        print_message(f"chunk codes size(0): {chunk_codes.size(0)}")
-        print_message(f"codes size(0): {codes.size(0)}")
-        print_message(f"codes size(): {codes.size()}")
-
-
-        assert offset+chunk_codes.size(0) == codes.size(0), (offset, chunk_codes.size(0), codes.size())
-
-
-        print_memory_stats(f'RANK:{self.rank}')
-
-        codes = codes.sort()
-        ivf, values = codes.indices, codes.values
-
-        print_memory_stats(f'RANK:{self.rank}')
-
-        partitions, ivf_lengths = values.unique_consecutive(return_counts=True)
-
-
-        print_message(f">>>>partition.size(0): {partitions.size(0)}")
-        print_message(f">>>>num_partition: {self.num_partitions}")
-
-        # All partitions should be non-empty. (We can use torch.histc otherwise.)
-        assert partitions.size(0) == self.num_partitions, (partitions.size(), self.num_partitions)
-
-        print_memory_stats(f'RANK:{self.rank}')
-
-        _, _ = optimize_ivf(ivf, ivf_lengths, self.config.index_path_)
-
-    def _update_metadata(self):
-        config = self.config
-        self.metadata_path = os.path.join(config.index_path_, 'metadata.json')
-        Run().print("#> Saving the indexing metadata to", self.metadata_path, "..")
-
-        with open(self.metadata_path, 'w') as f:
-            d = {'config': config.export()}
-            d['num_chunks'] = self.num_chunks
-            d['num_partitions'] = self.num_partitions
-            d['num_embeddings'] = self.num_embeddings
-            d['avg_doclen'] = self.num_embeddings / len(self.collection)
-
-            f.write(ujson.dumps(d, indent=4) + '\n')
-
-
-def compute_faiss_kmeans(dim, num_partitions, kmeans_niters, shared_lists, return_value_queue=None):
-    kmeans = faiss.Kmeans(dim, num_partitions, niter=kmeans_niters, gpu=torch.cuda.is_available(), verbose=True, seed=123)
-
-    sample = shared_lists[0][0]
-    sample = sample.float().numpy()
-    kmeans.train(sample)
-
-    centroids = torch.from_numpy(kmeans.centroids)
-
-    print_memory_stats(f'RANK:0*')
-
-    if return_value_queue is not None:
-        return_value_queue.put(centroids)
-
-    return centroids
-
-
-"""
-TODOs:
-
-1. Notice we're using self.config.bsize.
-
-2. Consider saving/using heldout_avg_residual as a vector --- that is, using 128 averages!
-
-3. Consider the operations with .cuda() tensors. Are all of them good for OOM?
-"""
+import os
+import tqdm
+import time
+import ujson
+import faiss
+import torch
+import random
+
+import numpy as np
+import torch.multiprocessing as mp
+from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
+
+import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
+
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+from primeqa.ir.dense.colbert_top.colbert.infra.launcher import print_memory_stats
+from primeqa.ir.dense.colbert_top.colbert.modeling.checkpoint import Checkpoint
+from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
+
+from primeqa.ir.dense.colbert_top.colbert.indexing.collection_encoder import CollectionEncoder
+from primeqa.ir.dense.colbert_top.colbert.indexing.index_saver import IndexSaver
+from primeqa.ir.dense.colbert_top.colbert.indexing.utils import optimize_ivf
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message
+
+from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual import ResidualCodec
+
+
+def encode(config, collection, shared_lists, shared_queues):
+    encoder = CollectionIndexer(config=config, collection=collection)
+    encoder.run(shared_lists)
+
+
+class CollectionIndexer():
+    def __init__(self, config: ColBERTConfig, collection):
+        self.config = config
+        self.rank, self.nranks = self.config.rank, self.config.nranks
+        self.num_partitions_max = self.config.num_partitions_max
+
+        if self.config.rank == 0:
+            self.config.help()
+
+        self.collection = Collection.cast(collection)
+        if torch.cuda.is_available():
+            self.checkpoint = Checkpoint(self.config.checkpoint, colbert_config=self.config).cuda()
+        else:
+            self.checkpoint = Checkpoint(self.config.checkpoint, colbert_config=self.config).cpu()
+
+        self.encoder = CollectionEncoder(config, self.checkpoint)
+        self.saver = IndexSaver(config)
+
+        print_memory_stats(f'RANK:{self.rank}')
+
+    def run(self, shared_lists):
+        with torch.inference_mode():
+            self.setup()
+            distributed.barrier(self.rank)
+            print_memory_stats(f'RANK:{self.rank}')
+
+            self.train(shared_lists)
+            distributed.barrier(self.rank)
+            print_memory_stats(f'RANK:{self.rank}')
+
+            self.index()
+            distributed.barrier(self.rank)
+            print_memory_stats(f'RANK:{self.rank}')
+
+            self.finalize()
+            distributed.barrier(self.rank)
+            print_memory_stats(f'RANK:{self.rank}')
+
+    def setup(self):
+        self.num_chunks = int(np.ceil(len(self.collection) / self.collection.get_chunksize()))
+
+        sampled_pids = self._sample_pids()
+        avg_doclen_est = self._sample_embeddings(sampled_pids)
+
+        # Select the number of partitions
+        num_passages = len(self.collection)
+        self.num_embeddings_est = num_passages * avg_doclen_est
+        # self.num_partitions = int(2 ** np.floor(np.log2(16 * np.sqrt(self.num_embeddings_est))))
+        # 16 --> 4 suggested by @Omar Khattab, reduce the numbers of centroids
+        # self.num_partitions = int(2 ** np.floor(np.log2(4 * np.sqrt(self.num_embeddings_est))))
+        num_partitions_multiplier = 8
+        self.num_partitions = int(2 ** np.floor(np.log2(num_partitions_multiplier * np.sqrt(self.num_embeddings_est))))
+        print_message(f'>> num_partitions_multiplier = {num_partitions_multiplier}, self.num_partitions = {self.num_partitions}')
+        # num_partitions_max = 50000
+        if self.num_partitions > self.num_partitions_max:
+            self.num_partitions = self.num_partitions_max
+            print_message(f'>> num_partitions limited to: self.num_partitions = {self.num_partitions}')
+
+        Run().print_main(f'Creaing {self.num_partitions:,} partitions.')
+        Run().print_main(f'*Estimated* {int(self.num_embeddings_est):,} embeddings.')
+
+        self._save_plan()
+
+    def _sample_pids(self):
+        num_passages = len(self.collection)
+
+        # Simple alternative: < 100k: 100%, < 1M: 15%, < 10M: 7%, < 100M: 3%, > 100M: 1%
+        # Keep in mind that, say, 15% still means at least 100k.
+        # So the formula is max(100% * min(total, 100k), 15% * min(total, 1M), ...)
+        # Then we subsample the vectors to 100 * num_partitions
+
+        typical_doclen = 120  # let's keep sampling independent of the actual doc_maxlen
+        sampled_pids = 16 * np.sqrt(typical_doclen * num_passages)
+        # sampled_pids = int(2 ** np.floor(np.log2(1 + sampled_pids)))
+        sampled_pids = min(1 + int(sampled_pids), num_passages)
+
+        sampled_pids = random.sample(range(num_passages), sampled_pids)
+        Run().print_main(f"# of sampled PIDs = {len(sampled_pids)} \t sampled_pids[:3] = {sampled_pids[:3]}")
+
+        return set(sampled_pids)
+
+    def _sample_embeddings(self, sampled_pids):
+        local_pids = self.collection.enumerate(rank=self.rank)
+        local_sample = [passage for pid, passage in local_pids if pid in sampled_pids]
+
+        local_sample_embs, doclens = self.encoder.encode_passages(local_sample)
+
+        if torch.cuda.is_available():
+            self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cuda()
+            torch.distributed.all_reduce(self.num_sample_embs)
+
+            avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
+            avg_doclen_est = torch.tensor([avg_doclen_est]).cuda()
+            torch.distributed.all_reduce(avg_doclen_est)
+
+            nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cuda()
+            torch.distributed.all_reduce(nonzero_ranks)
+        else:
+            if torch.distributed.is_initialized():
+                self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
+                torch.distributed.all_reduce(self.num_sample_embs)
+
+                avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
+                avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
+                torch.distributed.all_reduce(avg_doclen_est)
+
+                nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
+                torch.distributed.all_reduce(nonzero_ranks)
+            else:
+                self.num_sample_embs = torch.tensor([local_sample_embs.size(0)]).cpu()
+
+                avg_doclen_est = sum(doclens) / len(doclens) if doclens else 0
+                avg_doclen_est = torch.tensor([avg_doclen_est]).cpu()
+
+                nonzero_ranks = torch.tensor([float(len(local_sample) > 0)]).cpu()
+
+        avg_doclen_est = avg_doclen_est.item() / nonzero_ranks.item()
+        self.avg_doclen_est = avg_doclen_est
+
+        Run().print(f'avg_doclen_est = {avg_doclen_est} \t len(local_sample) = {len(local_sample):,}')
+
+        torch.save(local_sample_embs, os.path.join(self.config.index_path_, f'sample.{self.rank}.pt'))
+
+        return avg_doclen_est
+
+    def _save_plan(self):
+        if self.rank < 1:
+            config = self.config
+            self.plan_path = os.path.join(config.index_path_, 'plan.json')
+            Run().print("#> Saving the indexing plan to", self.plan_path, "..")
+
+            with open(self.plan_path, 'w') as f:
+                d = {'config': config.export()}
+                d['num_chunks'] = self.num_chunks
+                d['num_partitions'] = self.num_partitions
+                d['num_embeddings_est'] = self.num_embeddings_est
+                d['avg_doclen_est'] = self.avg_doclen_est
+
+                f.write(ujson.dumps(d, indent=4) + '\n')
+
+    def train(self, shared_lists):
+        if self.rank > 0:
+            return
+
+        sample, heldout = self._concatenate_and_split_sample()
+
+        centroids = self._train_kmeans(sample, shared_lists)
+
+        print_memory_stats(f'RANK:{self.rank}')
+        del sample
+
+        bucket_cutoffs, bucket_weights, avg_residual = self._compute_avg_residual(centroids, heldout)
+
+        print_message(f'avg_residual = {avg_residual}')
+
+        codec = ResidualCodec(config=self.config, centroids=centroids, avg_residual=avg_residual,
+                              bucket_cutoffs=bucket_cutoffs, bucket_weights=bucket_weights)
+        self.saver.save_codec(codec)
+
+    def _concatenate_and_split_sample(self):
+        print_memory_stats(f'***1*** \t RANK:{self.rank}')
+
+        # TODO: Allocate a float16 array. Load the samples from disk, copy to array.
+        sample = torch.empty(self.num_sample_embs, self.config.dim, dtype=torch.float16)
+
+        offset = 0
+        for r in range(self.nranks):
+            sub_sample_path = os.path.join(self.config.index_path_, f'sample.{r}.pt')
+            sub_sample = torch.load(sub_sample_path)
+            os.remove(sub_sample_path)
+
+            endpos = offset + sub_sample.size(0)
+            sample[offset:endpos] = sub_sample
+            offset = endpos
+
+        assert endpos == sample.size(0), (endpos, sample.size())
+
+        print_memory_stats(f'***2*** \t RANK:{self.rank}')
+
+        # Shuffle and split out a 5% "heldout" sub-sample [up to 50k elements]
+        sample = sample[torch.randperm(sample.size(0))]
+
+        print_memory_stats(f'***3*** \t RANK:{self.rank}')
+
+        heldout_fraction = 0.05 # 5%
+        heldout_size = int(min(heldout_fraction * sample.size(0), 50_000))
+        sample, sample_heldout = sample.split([sample.size(0) - heldout_size, heldout_size], dim=0)
+
+        print_memory_stats(f'***4*** \t RANK:{self.rank}')
+
+        return sample, sample_heldout
+
+    def _train_kmeans(self, sample, shared_lists):
+        if torch.cuda.is_available():
+            torch.cuda.empty_cache()
+
+        do_fork_for_faiss = False  # set to True to free faiss GPU-0 memory at the cost of one more copy of `sample`.
+
+        args_ = [self.config.dim, self.num_partitions, self.config.kmeans_niters]
+
+        if do_fork_for_faiss:
+            # For this to work reliably, write the sample to disk. Pickle may not handle >4GB of data.
+            # Delete the sample file after work is done.
+
+            shared_lists[0][0] = sample
+            return_value_queue = mp.Queue()
+
+            args_ = args_ + [shared_lists, return_value_queue]
+            proc = mp.Process(target=compute_faiss_kmeans, args=args_)
+
+            proc.start()
+            centroids = return_value_queue.get()
+            proc.join()
+
+        else:
+            args_ = args_ + [[[sample]]]
+            centroids = compute_faiss_kmeans(*args_)
+
+        centroids = torch.nn.functional.normalize(centroids, dim=-1).half()
+
+        return centroids
+
+    def _compute_avg_residual(self, centroids, heldout):
+        compressor = ResidualCodec(config=self.config, centroids=centroids, avg_residual=None)
+
+        if torch.cuda.is_available():
+            heldout_reconstruct = compressor.compress_into_codes(heldout, out_device='cuda')
+            heldout_reconstruct = compressor.lookup_centroids(heldout_reconstruct, out_device='cuda')
+            heldout_avg_residual = heldout.cuda() - heldout_reconstruct
+        else:
+            heldout_reconstruct = compressor.compress_into_codes(heldout, out_device='cpu')
+            heldout_reconstruct = compressor.lookup_centroids(heldout_reconstruct, out_device='cpu')
+            heldout_avg_residual = heldout - heldout_reconstruct
+
+        avg_residual = torch.abs(heldout_avg_residual).mean(dim=0).cpu()
+        print([round(x, 3) for x in avg_residual.squeeze().tolist()])
+
+        num_options = 2 ** self.config.nbits
+        quantiles = torch.arange(0, num_options, device=heldout_avg_residual.device) * (1 / num_options)
+        bucket_cutoffs_quantiles, bucket_weights_quantiles = quantiles[1:], quantiles + (0.5 / num_options)
+
+        bucket_cutoffs = heldout_avg_residual.float().quantile(bucket_cutoffs_quantiles)
+        bucket_weights = heldout_avg_residual.float().quantile(bucket_weights_quantiles)
+
+        print_message(
+            f"#> Got bucket_cutoffs_quantiles = {bucket_cutoffs_quantiles} and bucket_weights_quantiles = {bucket_weights_quantiles}")
+        print_message(f"#> Got bucket_cutoffs = {bucket_cutoffs} and bucket_weights = {bucket_weights}")
+
+        return bucket_cutoffs, bucket_weights, avg_residual.mean()
+
+        # EVENTAULLY: Compare the above with non-heldout sample. If too different, we can do better!
+        # sample = sample[subsample_idxs]
+        # sample_reconstruct = get_centroids_for(centroids, sample)
+        # sample_avg_residual = (sample - sample_reconstruct).mean(dim=0)
+
+    def index(self):
+        with self.saver.thread():
+            batches = self.collection.enumerate_batches(rank=self.rank)
+            for chunk_idx, offset, passages in tqdm.tqdm(batches, disable=self.rank > 0):
+                embs, doclens = self.encoder.encode_passages(passages)
+                if torch.cuda.is_available():
+                    assert embs.dtype == torch.float16, embs.dtype
+                else:
+                    assert embs.dtype == torch.float32, embs.dtype
+                    embs = embs.half()
+
+                Run().print_main(f"#> Saving chunk {chunk_idx}: \t {len(passages):,} passages "
+                                 f"and {embs.size(0):,} embeddings. From #{offset:,} onward.")
+
+                self.saver.save_chunk(chunk_idx, offset, embs, doclens)
+                del embs, doclens
+
+    def finalize(self):
+        if self.rank > 0:
+            return
+
+        self._check_all_files_are_saved()
+        self._collect_embedding_id_offset()
+
+        self._build_ivf()
+        self._update_metadata()
+
+    def _check_all_files_are_saved(self):
+        for chunk_idx in range(self.num_chunks):
+            # EVENTUALLY: Check those files!
+            pass
+
+    def _collect_embedding_id_offset(self):
+        passage_offset = 0
+        embedding_offset = 0
+
+        self.embedding_offsets = []
+
+        for chunk_idx in range(self.num_chunks):
+            metadata_path = os.path.join(self.config.index_path_, f'{chunk_idx}.metadata.json')
+
+            with open(metadata_path) as f:
+                chunk_metadata = ujson.load(f)
+
+                chunk_metadata['embedding_offset'] = embedding_offset
+                self.embedding_offsets.append(embedding_offset)
+
+                assert chunk_metadata['passage_offset'] == passage_offset, (chunk_idx, passage_offset, chunk_metadata)
+
+                passage_offset += chunk_metadata['num_passages']
+                embedding_offset += chunk_metadata['num_embeddings']
+
+            with open(metadata_path, 'w') as f:
+                f.write(ujson.dumps(chunk_metadata, indent=4) + '\n')
+
+        self.num_embeddings = embedding_offset
+        assert len(self.embedding_offsets) == self.num_chunks, len(self.embedding_offsets)
+
+    def _build_ivf(self):
+        # Maybe we should several small IVFs? Every 250M embeddings, so that's every 1 GB.
+        # It would save *memory* here and *disk space* regarding the int64.
+        # But we'd have to decide how many IVFs to use during retrieval: many (loop) or one?
+        # A loop seems nice if we can find a size that's large enough for speed yet small enough to fit on GPU!
+        # Then it would help nicely for batching later: 1GB.
+
+        codes = torch.empty(self.num_embeddings,)
+        print_memory_stats(f'RANK:{self.rank}')
+
+        for chunk_idx in range(self.num_chunks):
+            offset = self.embedding_offsets[chunk_idx]
+            chunk_codes = ResidualCodec.Embeddings.load_codes(self.config.index_path_, chunk_idx)
+
+            codes[offset:offset+chunk_codes.size(0)] = chunk_codes
+
+        print_message(f"offset: {offset}")
+        print_message(f"chunk codes size(0): {chunk_codes.size(0)}")
+        print_message(f"codes size(0): {codes.size(0)}")
+        print_message(f"codes size(): {codes.size()}")
+
+
+        assert offset+chunk_codes.size(0) == codes.size(0), (offset, chunk_codes.size(0), codes.size())
+
+
+        print_memory_stats(f'RANK:{self.rank}')
+
+        codes = codes.sort()
+        ivf, values = codes.indices, codes.values
+
+        print_memory_stats(f'RANK:{self.rank}')
+
+        partitions, ivf_lengths = values.unique_consecutive(return_counts=True)
+
+
+        print_message(f">>>>partition.size(0): {partitions.size(0)}")
+        print_message(f">>>>num_partition: {self.num_partitions}")
+
+        # All partitions should be non-empty. (We can use torch.histc otherwise.)
+        assert partitions.size(0) == self.num_partitions, (partitions.size(), self.num_partitions)
+
+        print_memory_stats(f'RANK:{self.rank}')
+
+        _, _ = optimize_ivf(ivf, ivf_lengths, self.config.index_path_)
+
+    def _update_metadata(self):
+        config = self.config
+        self.metadata_path = os.path.join(config.index_path_, 'metadata.json')
+        Run().print("#> Saving the indexing metadata to", self.metadata_path, "..")
+
+        with open(self.metadata_path, 'w') as f:
+            d = {'config': config.export()}
+            d['num_chunks'] = self.num_chunks
+            d['num_partitions'] = self.num_partitions
+            d['num_embeddings'] = self.num_embeddings
+            d['avg_doclen'] = self.num_embeddings / len(self.collection)
+
+            f.write(ujson.dumps(d, indent=4) + '\n')
+
+
+def compute_faiss_kmeans(dim, num_partitions, kmeans_niters, shared_lists, return_value_queue=None):
+    kmeans = faiss.Kmeans(dim, num_partitions, niter=kmeans_niters, gpu=torch.cuda.is_available(), verbose=True, seed=123)
+
+    sample = shared_lists[0][0]
+    sample = sample.float().numpy()
+    kmeans.train(sample)
+
+    centroids = torch.from_numpy(kmeans.centroids)
+
+    print_memory_stats(f'RANK:0*')
+
+    if return_value_queue is not None:
+        return_value_queue.put(centroids)
+
+    return centroids
+
+
+"""
+TODOs:
+
+1. Notice we're using self.config.bsize.
+
+2. Consider saving/using heldout_avg_residual as a vector --- that is, using 128 averages!
+
+3. Consider the operations with .cuda() tensors. Are all of them good for OOM?
+"""
```

## primeqa/ir/dense/colbert_top/colbert/indexing/index_manager.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-import torch
-import numpy as np
-
-from bitarray import bitarray
-
-
-class IndexManager():
-    def __init__(self, dim):
-        self.dim = dim
-
-    def save(self, tensor, path_prefix):
-        torch.save(tensor, path_prefix)
-
-    def save_bitarray(self, bitarray, path_prefix):
-        with open(path_prefix, "wb") as f:
-            bitarray.tofile(f)
-
-
-def load_index_part(filename, verbose=True):
-    part = torch.load(filename)
-
-    if type(part) == list:  # for backward compatibility
-        part = torch.cat(part)
-
-    return part
-
-
-def load_compressed_index_part(filename, dim, bits):
-    a = bitarray()
-
-    with open(filename, "rb") as f:
-        a.fromfile(f)
-
-    n = len(a) // dim // bits
-    part = torch.tensor(np.frombuffer(a.tobytes(), dtype=np.uint8))  # TODO: isn't from_numpy(.) faster?
-    part = part.reshape((n, int(np.ceil(dim * bits / 8))))
-
-    return part
+import torch
+import numpy as np
+
+from bitarray import bitarray
+
+
+class IndexManager():
+    def __init__(self, dim):
+        self.dim = dim
+
+    def save(self, tensor, path_prefix):
+        torch.save(tensor, path_prefix)
+
+    def save_bitarray(self, bitarray, path_prefix):
+        with open(path_prefix, "wb") as f:
+            bitarray.tofile(f)
+
+
+def load_index_part(filename, verbose=True):
+    part = torch.load(filename)
+
+    if type(part) == list:  # for backward compatibility
+        part = torch.cat(part)
+
+    return part
+
+
+def load_compressed_index_part(filename, dim, bits):
+    a = bitarray()
+
+    with open(filename, "rb") as f:
+        a.fromfile(f)
+
+    n = len(a) // dim // bits
+    part = torch.tensor(np.frombuffer(a.tobytes(), dtype=np.uint8))  # TODO: isn't from_numpy(.) faster?
+    part = part.reshape((n, int(np.ceil(dim * bits / 8))))
+
+    return part
```

## primeqa/ir/dense/colbert_top/colbert/indexing/index_saver.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-import os
-import queue
-import ujson
-import threading
-
-from contextlib import contextmanager
-
-from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual import ResidualCodec
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-
-class IndexSaver():
-    def __init__(self, config):
-        self.config = config
-
-    def save_codec(self, codec):
-        codec.save(index_path=self.config.index_path_)
-
-    def load_codec(self):
-        return ResidualCodec.load(index_path=self.config.index_path_)
-
-    @contextmanager
-    def thread(self):
-        self.codec = self.load_codec()
-
-        self.saver_queue = queue.Queue(maxsize=3)
-        thread = threading.Thread(target=self._saver_thread)
-        thread.start()
-
-        try:
-            yield
-
-        finally:
-            self.saver_queue.put(None)
-            thread.join()
-
-            del self.saver_queue
-            del self.codec
-
-    def save_chunk(self, chunk_idx, offset, embs, doclens):
-        compressed_embs = self.codec.compress(embs)
-
-        self.saver_queue.put((chunk_idx, offset, compressed_embs, doclens))
-
-    def _saver_thread(self):
-        for args in iter(self.saver_queue.get, None):
-            self._write_chunk_to_disk(*args)
-
-    def _write_chunk_to_disk(self, chunk_idx, offset, compressed_embs, doclens):
-        path_prefix = os.path.join(self.config.index_path_, str(chunk_idx))
-        compressed_embs.save(path_prefix)
-
-        doclens_path = os.path.join(self.config.index_path_, f'doclens.{chunk_idx}.json')
-        with open(doclens_path, 'w') as output_doclens:
-            ujson.dump(doclens, output_doclens)
-
-        metadata_path = os.path.join(self.config.index_path_, f'{chunk_idx}.metadata.json')
-        with open(metadata_path, 'w') as output_metadata:
-            metadata = {'passage_offset': offset, 'num_passages': len(doclens), 'num_embeddings': len(compressed_embs)}
-            ujson.dump(metadata, output_metadata)
+import os
+import queue
+import ujson
+import threading
+
+from contextlib import contextmanager
+
+from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual import ResidualCodec
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+
+class IndexSaver():
+    def __init__(self, config):
+        self.config = config
+
+    def save_codec(self, codec):
+        codec.save(index_path=self.config.index_path_)
+
+    def load_codec(self):
+        return ResidualCodec.load(index_path=self.config.index_path_)
+
+    @contextmanager
+    def thread(self):
+        self.codec = self.load_codec()
+
+        self.saver_queue = queue.Queue(maxsize=3)
+        thread = threading.Thread(target=self._saver_thread)
+        thread.start()
+
+        try:
+            yield
+
+        finally:
+            self.saver_queue.put(None)
+            thread.join()
+
+            del self.saver_queue
+            del self.codec
+
+    def save_chunk(self, chunk_idx, offset, embs, doclens):
+        compressed_embs = self.codec.compress(embs)
+
+        self.saver_queue.put((chunk_idx, offset, compressed_embs, doclens))
+
+    def _saver_thread(self):
+        for args in iter(self.saver_queue.get, None):
+            self._write_chunk_to_disk(*args)
+
+    def _write_chunk_to_disk(self, chunk_idx, offset, compressed_embs, doclens):
+        path_prefix = os.path.join(self.config.index_path_, str(chunk_idx))
+        compressed_embs.save(path_prefix)
+
+        doclens_path = os.path.join(self.config.index_path_, f'doclens.{chunk_idx}.json')
+        with open(doclens_path, 'w') as output_doclens:
+            ujson.dump(doclens, output_doclens)
+
+        metadata_path = os.path.join(self.config.index_path_, f'{chunk_idx}.metadata.json')
+        with open(metadata_path, 'w') as output_metadata:
+            metadata = {'passage_offset': offset, 'num_passages': len(doclens), 'num_embeddings': len(compressed_embs)}
+            ujson.dump(metadata, output_metadata)
```

## primeqa/ir/dense/colbert_top/colbert/indexing/loaders.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-import re
-import os
-import ujson
-
-
-def get_parts(directory):
-    extension = '.pt'
-
-    parts = sorted([int(filename[: -1 * len(extension)]) for filename in os.listdir(directory)
-                    if filename.endswith(extension)])
-
-    assert list(range(len(parts))) == parts, parts
-
-    # Integer-sortedness matters.
-    parts_paths = [os.path.join(directory, '{}{}'.format(filename, extension)) for filename in parts]
-    samples_paths = [os.path.join(directory, '{}.sample'.format(filename)) for filename in parts]
-
-    return parts, parts_paths, samples_paths
-
-
-def load_doclens(directory, flatten=True):
-    doclens_filenames = {}
-
-    for filename in os.listdir(directory):
-        match = re.match("doclens.(\d+).json", filename)
-
-        if match is not None:
-            doclens_filenames[int(match.group(1))] = filename
-
-    doclens_filenames = [os.path.join(directory, doclens_filenames[i]) for i in sorted(doclens_filenames.keys())]
-
-    all_doclens = [ujson.load(open(filename)) for filename in doclens_filenames]
-
-    if flatten:
-        all_doclens = [x for sub_doclens in all_doclens for x in sub_doclens]
-    
-    if len(all_doclens) == 0:
-        raise ValueError("Could not load doclens")
-
-    return all_doclens
-
-
-def get_deltas(directory):
-    extension = '.residuals.pt'
-
-    parts = sorted([int(filename[: -1 * len(extension)]) for filename in os.listdir(directory)
-                    if filename.endswith(extension)])
-
-    assert list(range(len(parts))) == parts, parts
-
-    # Integer-sortedness matters.
-    parts_paths = [os.path.join(directory, '{}{}'.format(filename, extension)) for filename in parts]
-
-    return parts, parts_paths
-
-
-# def load_compression_data(level, path):
-#     with open(path, "r") as f:
-#         for line in f:
-#             line = line.split(',')
-#             bits = int(line[0])
-
-#             if bits == level:
-#                 return [float(v) for v in line[1:]]
-
-#     raise ValueError(f"No data found for {level}-bit compression")
+import re
+import os
+import ujson
+
+
+def get_parts(directory):
+    extension = '.pt'
+
+    parts = sorted([int(filename[: -1 * len(extension)]) for filename in os.listdir(directory)
+                    if filename.endswith(extension)])
+
+    assert list(range(len(parts))) == parts, parts
+
+    # Integer-sortedness matters.
+    parts_paths = [os.path.join(directory, '{}{}'.format(filename, extension)) for filename in parts]
+    samples_paths = [os.path.join(directory, '{}.sample'.format(filename)) for filename in parts]
+
+    return parts, parts_paths, samples_paths
+
+
+def load_doclens(directory, flatten=True):
+    doclens_filenames = {}
+
+    for filename in os.listdir(directory):
+        match = re.match("doclens.(\d+).json", filename)
+
+        if match is not None:
+            doclens_filenames[int(match.group(1))] = filename
+
+    doclens_filenames = [os.path.join(directory, doclens_filenames[i]) for i in sorted(doclens_filenames.keys())]
+
+    all_doclens = [ujson.load(open(filename)) for filename in doclens_filenames]
+
+    if flatten:
+        all_doclens = [x for sub_doclens in all_doclens for x in sub_doclens]
+    
+    if len(all_doclens) == 0:
+        raise ValueError("Could not load doclens")
+
+    return all_doclens
+
+
+def get_deltas(directory):
+    extension = '.residuals.pt'
+
+    parts = sorted([int(filename[: -1 * len(extension)]) for filename in os.listdir(directory)
+                    if filename.endswith(extension)])
+
+    assert list(range(len(parts))) == parts, parts
+
+    # Integer-sortedness matters.
+    parts_paths = [os.path.join(directory, '{}{}'.format(filename, extension)) for filename in parts]
+
+    return parts, parts_paths
+
+
+# def load_compression_data(level, path):
+#     with open(path, "r") as f:
+#         for line in f:
+#             line = line.split(',')
+#             bits = int(line[0])
+
+#             if bits == level:
+#                 return [float(v) for v in line[1:]]
+
+#     raise ValueError(f"No data found for {level}-bit compression")
```

## primeqa/ir/dense/colbert_top/colbert/indexing/utils.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-import os
-import torch
-import tqdm
-
-from primeqa.ir.dense.colbert_top.colbert.indexing.loaders import load_doclens
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, flatten
-
-def optimize_ivf(orig_ivf, orig_ivf_lengths, index_path):
-    print_message("#> Optimizing IVF to store map from centroids to list of pids..")
-
-    print_message("#> Building the emb2pid mapping..")
-    all_doclens = load_doclens(index_path, flatten=False)
-
-    all_doclens = flatten(all_doclens)
-    total_num_embeddings = sum(all_doclens)
-
-    emb2pid = torch.zeros(total_num_embeddings, dtype=torch.int)
-
-    """
-    EVENTUALLY: Use two tensors. emb2pid_offsets will have every 256th element.
-    emb2pid_delta will have the delta from the corresponding offset,
-    """
-
-    offset_doclens = 0
-    for pid, dlength in enumerate(all_doclens):
-        emb2pid[offset_doclens: offset_doclens + dlength] = pid
-        offset_doclens += dlength
-
-    print_message("len(emb2pid) =", len(emb2pid))
-
-    ivf = emb2pid[orig_ivf]
-    unique_pids_per_centroid = []
-    ivf_lengths = []
-
-    offset = 0
-    for length in tqdm.tqdm(orig_ivf_lengths.tolist()):
-        pids = torch.unique(ivf[offset:offset+length])
-        unique_pids_per_centroid.append(pids)
-        ivf_lengths.append(pids.shape[0])
-        offset += length
-    ivf = torch.cat(unique_pids_per_centroid)
-    ivf_lengths = torch.tensor(ivf_lengths)
-
-    original_ivf_path = os.path.join(index_path, 'ivf.pt')
-    optimized_ivf_path = os.path.join(index_path, 'ivf.pid.pt')
-    torch.save((ivf, ivf_lengths), optimized_ivf_path)
-    print_message(f"#> Saved optimized IVF to {optimized_ivf_path}")
-    if os.path.exists(original_ivf_path):
-        print_message(f"#> Original IVF at path \"{original_ivf_path}\" can now be removed")
-
-    return ivf, ivf_lengths
-
-
+import os
+import torch
+import tqdm
+
+from primeqa.ir.dense.colbert_top.colbert.indexing.loaders import load_doclens
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, flatten
+
+def optimize_ivf(orig_ivf, orig_ivf_lengths, index_path):
+    print_message("#> Optimizing IVF to store map from centroids to list of pids..")
+
+    print_message("#> Building the emb2pid mapping..")
+    all_doclens = load_doclens(index_path, flatten=False)
+
+    all_doclens = flatten(all_doclens)
+    total_num_embeddings = sum(all_doclens)
+
+    emb2pid = torch.zeros(total_num_embeddings, dtype=torch.int)
+
+    """
+    EVENTUALLY: Use two tensors. emb2pid_offsets will have every 256th element.
+    emb2pid_delta will have the delta from the corresponding offset,
+    """
+
+    offset_doclens = 0
+    for pid, dlength in enumerate(all_doclens):
+        emb2pid[offset_doclens: offset_doclens + dlength] = pid
+        offset_doclens += dlength
+
+    print_message("len(emb2pid) =", len(emb2pid))
+
+    ivf = emb2pid[orig_ivf]
+    unique_pids_per_centroid = []
+    ivf_lengths = []
+
+    offset = 0
+    for length in tqdm.tqdm(orig_ivf_lengths.tolist()):
+        pids = torch.unique(ivf[offset:offset+length])
+        unique_pids_per_centroid.append(pids)
+        ivf_lengths.append(pids.shape[0])
+        offset += length
+    ivf = torch.cat(unique_pids_per_centroid)
+    ivf_lengths = torch.tensor(ivf_lengths)
+
+    original_ivf_path = os.path.join(index_path, 'ivf.pt')
+    optimized_ivf_path = os.path.join(index_path, 'ivf.pid.pt')
+    torch.save((ivf, ivf_lengths), optimized_ivf_path)
+    print_message(f"#> Saved optimized IVF to {optimized_ivf_path}")
+    if os.path.exists(original_ivf_path):
+        print_message(f"#> Original IVF at path \"{original_ivf_path}\" can now be removed")
+
+    return ivf, ivf_lengths
+
+
```

## primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual.py

 * *Ordering differences only*

```diff
@@ -1,299 +1,299 @@
-"""
-EVENTUALLY: Tune the batch sizes selected here for a good balance of speed and generality.
-"""
-
-import os
-import torch
-import numpy as np
-from itertools import product
-import sys
-
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings import ResidualEmbeddings
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, print_torch_extension_error_message
-
-import pathlib
-from torch.utils.cpp_extension import load
-
-
-class ResidualCodec:
-    Embeddings = ResidualEmbeddings
-
-    def __init__(self, config, centroids, avg_residual=None, bucket_cutoffs=None, bucket_weights=None):
-        self.dim, self.nbits = config.dim, config.nbits
-
-        self.use_gpu = torch.cuda.is_available()
-
-        ResidualCodec.try_load_torch_extensions(self.use_gpu)
-
-        if self.use_gpu:
-            self.centroids = centroids.half().cuda()
-        else:
-            self.centroids = centroids.float().cpu()
-
-        self.avg_residual = avg_residual
-
-        if torch.is_tensor(self.avg_residual):
-
-            if self.use_gpu:
-                self.avg_residual = self.avg_residual.half().cuda()
-            else:
-                self.avg_residual = self.avg_residual.float().cpu()
-
-        if torch.is_tensor(bucket_cutoffs):
-
-            if self.use_gpu:
-                bucket_cutoffs = bucket_cutoffs.cuda()
-                bucket_weights = bucket_weights.half().cuda()
-            else:
-                bucket_cutoffs = bucket_cutoffs.cpu()
-                bucket_weights = bucket_weights.float().cpu()
-
-        self.bucket_cutoffs = bucket_cutoffs
-        self.bucket_weights = bucket_weights
-
-        if self.use_gpu:
-            self.arange_bits = torch.arange(0, self.nbits, device='cuda', dtype=torch.uint8)
-        else:
-            self.arange_bits = torch.arange(0, self.nbits, device='cpu', dtype=torch.uint8)
-
-        # We reverse the residual bits because arange_bits as
-        # currently constructed produces results with the reverse
-        # of the expected endianness
-        self.reversed_bit_map = []
-        mask = (1 << self.nbits) - 1
-        for i in range(256):
-            # The reversed byte
-            z = 0
-            for j in range(8, 0, -self.nbits):
-                # Extract a subsequence of length n bits
-                x = (i >> (j - self.nbits)) & mask
-
-                # Reverse the endianness of each bit subsequence (e.g. 10 -> 01)
-                y = 0
-                for k in range(self.nbits - 1, -1, -1):
-                    y += ((x >> (self.nbits - k - 1)) & 1) * (2 ** k)
-
-                # Set the corresponding bits in the output byte
-                z |= y
-                if j > self.nbits:
-                    z <<= self.nbits
-            self.reversed_bit_map.append(z)
-        self.reversed_bit_map = torch.tensor(self.reversed_bit_map).to(torch.uint8)
-
-        # A table of all possible lookup orders into bucket_weights
-        # given n bits per lookup
-        keys_per_byte = 8 // self.nbits
-        if self.bucket_weights is not None:
-            self.decompression_lookup_table = (
-                torch.tensor(
-                    list(
-                        product(
-                            list(range(len(self.bucket_weights))),
-                            repeat=keys_per_byte
-                        )
-                    )
-                )
-                .to(torch.uint8)
-            )
-        else:
-            self.decompression_lookup_table = None
-        if self.use_gpu:
-            self.reversed_bit_map = self.reversed_bit_map.cuda()
-            if self.decompression_lookup_table is not None:
-                self.decompression_lookup_table = self.decompression_lookup_table.cuda()
-
-    @classmethod
-    def try_load_torch_extensions(cls, use_gpu):
-        if hasattr(cls, "loaded_extensions") or not use_gpu:
-            return
-
-        verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True"
-
-        print_message(f"Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
-        try:
-            decompress_residuals_cpp = load(
-                name="decompress_residuals_cpp",
-                sources=[
-                    os.path.join(
-                        pathlib.Path(__file__).parent.resolve(), "decompress_residuals.cpp"
-                    ),
-                    os.path.join(
-                        pathlib.Path(__file__).parent.resolve(), "decompress_residuals.cu"
-                    ),
-                ],
-                extra_cflags=["-O3"],
-                verbose=verbose,
-            )
-        except (RuntimeError, KeyboardInterrupt) as e:
-            if not verbose:
-                import traceback
-                traceback.print_exc()
-            print_torch_extension_error_message()
-            sys.exit(1)
-        cls.decompress_residuals = decompress_residuals_cpp.decompress_residuals_cpp
-
-        print_message(f"Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
-        try:
-            packbits_cpp = load(
-                name="packbits_cpp",
-                sources=[
-                    os.path.join(
-                        pathlib.Path(__file__).parent.resolve(), "packbits.cpp"
-                    ),
-                    os.path.join(
-                        pathlib.Path(__file__).parent.resolve(), "packbits.cu"
-                    ),
-                ],
-                extra_cflags=["-O3"],
-                verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True",
-            )
-        except (RuntimeError, KeyboardInterrupt) as e:
-            if not verbose:
-                import traceback
-                traceback.print_exc()
-            print_torch_extension_error_message()
-            sys.exit(1)
-        cls.packbits = packbits_cpp.packbits_cpp
-
-        cls.loaded_extensions = True
-
-    @classmethod
-    def load(cls, index_path):
-        config = ColBERTConfig.load_from_index(index_path)
-        centroids_path = os.path.join(index_path, 'centroids.pt')
-        avgresidual_path = os.path.join(index_path, 'avg_residual.pt')
-        buckets_path = os.path.join(index_path, 'buckets.pt')
-
-        centroids = torch.load(centroids_path, map_location='cpu')
-        avg_residual = torch.load(avgresidual_path, map_location='cpu')
-        bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')
-
-        if avg_residual.dim() == 0:
-            avg_residual = avg_residual.item()
-
-        return cls(config=config, centroids=centroids, avg_residual=avg_residual, bucket_cutoffs=bucket_cutoffs, bucket_weights=bucket_weights)
-
-    def save(self, index_path):
-        assert self.avg_residual is not None
-        assert torch.is_tensor(self.bucket_cutoffs), self.bucket_cutoffs
-        assert torch.is_tensor(self.bucket_weights), self.bucket_weights
-
-        centroids_path = os.path.join(index_path, 'centroids.pt')
-        avgresidual_path = os.path.join(index_path, 'avg_residual.pt')
-        buckets_path = os.path.join(index_path, 'buckets.pt')
-
-        torch.save(self.centroids.half(), centroids_path)
-        torch.save((self.bucket_cutoffs.half(), self.bucket_weights), buckets_path)
-
-        if torch.is_tensor(self.avg_residual):
-            torch.save(self.avg_residual.half(), avgresidual_path)
-        else:
-            torch.save(torch.tensor([self.avg_residual], dtype=torch.float16), avgresidual_path)
-
-    def compress(self, embs):
-        codes, residuals = [], []
-
-        for batch in embs.split(1 << 18):
-
-            if self.use_gpu:
-                batch = batch.cuda().half()
-            else:
-                batch = batch.cpu().half()
-
-            codes_ = self.compress_into_codes(batch, out_device=batch.device)
-            centroids_ = self.lookup_centroids(codes_, out_device=batch.device)
-
-            residuals_ = (batch - centroids_)
-
-            codes.append(codes_.cpu())
-            residuals.append(self.binarize(residuals_).cpu())
-
-        codes = torch.cat(codes)
-        residuals = torch.cat(residuals)
-
-        return ResidualCodec.Embeddings(codes, residuals)
-
-    def binarize(self, residuals):
-        residuals = torch.bucketize(residuals.float(), self.bucket_cutoffs).to(dtype=torch.uint8)
-        residuals = residuals.unsqueeze(-1).expand(*residuals.size(), self.nbits)  # add a new nbits-wide dim
-        residuals = residuals >> self.arange_bits  # divide by 2^bit for each bit position
-        residuals = residuals & 1  # apply mod 2 to binarize
-
-        assert self.dim % 8 == 0, self.dim
-        assert self.dim % (self.nbits * 8) == 0, (self.dim, self.nbits)
-
-        if self.use_gpu:
-            residuals_packed = ResidualCodec.packbits(residuals.contiguous().flatten())
-        else:
-            residuals_packed = np.packbits(np.asarray(residuals.contiguous().flatten()))
-        residuals_packed = torch.as_tensor(residuals_packed, dtype=torch.uint8)
-
-        residuals_packed = residuals_packed.reshape(residuals.size(0), self.dim // 8 * self.nbits)
-
-        return residuals_packed
-
-    def compress_into_codes(self, embs, out_device):
-        """
-            EVENTUALLY: Fusing the kernels or otherwise avoiding materalizing the entire matrix before max(dim=0)
-                        seems like it would help here a lot.
-        """
-
-        codes = []
-
-        bsize = (1 << 29) // self.centroids.size(0)
-        for batch in embs.split(bsize):
-
-            if self.use_gpu:
-                indices = (self.centroids @ batch.T.cuda().half()).max(dim=0).indices.to(device=out_device)
-            else:
-                indices = (self.centroids @ batch.T.cpu().float()).max(dim=0).indices.to(device=out_device)
-
-            codes.append(indices)
-
-        return torch.cat(codes)
-
-    def lookup_centroids(self, codes, out_device):
-        """
-            Handles multi-dimensional codes too.
-
-            EVENTUALLY: The .split() below should happen on a flat view.
-        """
-
-        centroids = []
-
-        for batch in codes.split(1 << 20):
-
-            if self.use_gpu:
-                centroids.append(self.centroids[batch.cuda().long()].to(device=out_device))
-            else:
-                centroids.append(self.centroids[batch.cpu().long()].to(device=out_device))
-
-        return torch.cat(centroids)
-
-    def decompress(self, compressed_embs: Embeddings):
-        """
-            We batch below even if the target device is CUDA to avoid large temporary buffers causing OOM.
-        """
-
-        assert self.use_gpu
-
-        codes, residuals = compressed_embs.codes, compressed_embs.residuals
-
-        D = []
-        for codes_, residuals_ in zip(codes.split(1 << 15), residuals.split(1 << 15)):
-            codes_, residuals_ = codes_.cuda(), residuals_.cuda()
-            centroids_ = ResidualCodec.decompress_residuals(
-                residuals_,
-                self.bucket_weights,
-                self.reversed_bit_map,
-                self.decompression_lookup_table,
-                codes_,
-                self.centroids,
-                self.dim,
-                self.nbits,
-            ).cuda()
-            D_ = torch.nn.functional.normalize(centroids_, p=2, dim=-1).half()
-            D.append(D_)
-
-        return torch.cat(D)
+"""
+EVENTUALLY: Tune the batch sizes selected here for a good balance of speed and generality.
+"""
+
+import os
+import torch
+import numpy as np
+from itertools import product
+import sys
+
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings import ResidualEmbeddings
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, print_torch_extension_error_message
+
+import pathlib
+from torch.utils.cpp_extension import load
+
+
+class ResidualCodec:
+    Embeddings = ResidualEmbeddings
+
+    def __init__(self, config, centroids, avg_residual=None, bucket_cutoffs=None, bucket_weights=None):
+        self.dim, self.nbits = config.dim, config.nbits
+
+        self.use_gpu = torch.cuda.is_available()
+
+        ResidualCodec.try_load_torch_extensions(self.use_gpu)
+
+        if self.use_gpu:
+            self.centroids = centroids.half().cuda()
+        else:
+            self.centroids = centroids.float().cpu()
+
+        self.avg_residual = avg_residual
+
+        if torch.is_tensor(self.avg_residual):
+
+            if self.use_gpu:
+                self.avg_residual = self.avg_residual.half().cuda()
+            else:
+                self.avg_residual = self.avg_residual.float().cpu()
+
+        if torch.is_tensor(bucket_cutoffs):
+
+            if self.use_gpu:
+                bucket_cutoffs = bucket_cutoffs.cuda()
+                bucket_weights = bucket_weights.half().cuda()
+            else:
+                bucket_cutoffs = bucket_cutoffs.cpu()
+                bucket_weights = bucket_weights.float().cpu()
+
+        self.bucket_cutoffs = bucket_cutoffs
+        self.bucket_weights = bucket_weights
+
+        if self.use_gpu:
+            self.arange_bits = torch.arange(0, self.nbits, device='cuda', dtype=torch.uint8)
+        else:
+            self.arange_bits = torch.arange(0, self.nbits, device='cpu', dtype=torch.uint8)
+
+        # We reverse the residual bits because arange_bits as
+        # currently constructed produces results with the reverse
+        # of the expected endianness
+        self.reversed_bit_map = []
+        mask = (1 << self.nbits) - 1
+        for i in range(256):
+            # The reversed byte
+            z = 0
+            for j in range(8, 0, -self.nbits):
+                # Extract a subsequence of length n bits
+                x = (i >> (j - self.nbits)) & mask
+
+                # Reverse the endianness of each bit subsequence (e.g. 10 -> 01)
+                y = 0
+                for k in range(self.nbits - 1, -1, -1):
+                    y += ((x >> (self.nbits - k - 1)) & 1) * (2 ** k)
+
+                # Set the corresponding bits in the output byte
+                z |= y
+                if j > self.nbits:
+                    z <<= self.nbits
+            self.reversed_bit_map.append(z)
+        self.reversed_bit_map = torch.tensor(self.reversed_bit_map).to(torch.uint8)
+
+        # A table of all possible lookup orders into bucket_weights
+        # given n bits per lookup
+        keys_per_byte = 8 // self.nbits
+        if self.bucket_weights is not None:
+            self.decompression_lookup_table = (
+                torch.tensor(
+                    list(
+                        product(
+                            list(range(len(self.bucket_weights))),
+                            repeat=keys_per_byte
+                        )
+                    )
+                )
+                .to(torch.uint8)
+            )
+        else:
+            self.decompression_lookup_table = None
+        if self.use_gpu:
+            self.reversed_bit_map = self.reversed_bit_map.cuda()
+            if self.decompression_lookup_table is not None:
+                self.decompression_lookup_table = self.decompression_lookup_table.cuda()
+
+    @classmethod
+    def try_load_torch_extensions(cls, use_gpu):
+        if hasattr(cls, "loaded_extensions") or not use_gpu:
+            return
+
+        verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True"
+
+        print_message(f"Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
+        try:
+            decompress_residuals_cpp = load(
+                name="decompress_residuals_cpp",
+                sources=[
+                    os.path.join(
+                        pathlib.Path(__file__).parent.resolve(), "decompress_residuals.cpp"
+                    ),
+                    os.path.join(
+                        pathlib.Path(__file__).parent.resolve(), "decompress_residuals.cu"
+                    ),
+                ],
+                extra_cflags=["-O3"],
+                verbose=verbose,
+            )
+        except (RuntimeError, KeyboardInterrupt) as e:
+            if not verbose:
+                import traceback
+                traceback.print_exc()
+            print_torch_extension_error_message()
+            sys.exit(1)
+        cls.decompress_residuals = decompress_residuals_cpp.decompress_residuals_cpp
+
+        print_message(f"Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
+        try:
+            packbits_cpp = load(
+                name="packbits_cpp",
+                sources=[
+                    os.path.join(
+                        pathlib.Path(__file__).parent.resolve(), "packbits.cpp"
+                    ),
+                    os.path.join(
+                        pathlib.Path(__file__).parent.resolve(), "packbits.cu"
+                    ),
+                ],
+                extra_cflags=["-O3"],
+                verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True",
+            )
+        except (RuntimeError, KeyboardInterrupt) as e:
+            if not verbose:
+                import traceback
+                traceback.print_exc()
+            print_torch_extension_error_message()
+            sys.exit(1)
+        cls.packbits = packbits_cpp.packbits_cpp
+
+        cls.loaded_extensions = True
+
+    @classmethod
+    def load(cls, index_path):
+        config = ColBERTConfig.load_from_index(index_path)
+        centroids_path = os.path.join(index_path, 'centroids.pt')
+        avgresidual_path = os.path.join(index_path, 'avg_residual.pt')
+        buckets_path = os.path.join(index_path, 'buckets.pt')
+
+        centroids = torch.load(centroids_path, map_location='cpu')
+        avg_residual = torch.load(avgresidual_path, map_location='cpu')
+        bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')
+
+        if avg_residual.dim() == 0:
+            avg_residual = avg_residual.item()
+
+        return cls(config=config, centroids=centroids, avg_residual=avg_residual, bucket_cutoffs=bucket_cutoffs, bucket_weights=bucket_weights)
+
+    def save(self, index_path):
+        assert self.avg_residual is not None
+        assert torch.is_tensor(self.bucket_cutoffs), self.bucket_cutoffs
+        assert torch.is_tensor(self.bucket_weights), self.bucket_weights
+
+        centroids_path = os.path.join(index_path, 'centroids.pt')
+        avgresidual_path = os.path.join(index_path, 'avg_residual.pt')
+        buckets_path = os.path.join(index_path, 'buckets.pt')
+
+        torch.save(self.centroids.half(), centroids_path)
+        torch.save((self.bucket_cutoffs.half(), self.bucket_weights), buckets_path)
+
+        if torch.is_tensor(self.avg_residual):
+            torch.save(self.avg_residual.half(), avgresidual_path)
+        else:
+            torch.save(torch.tensor([self.avg_residual], dtype=torch.float16), avgresidual_path)
+
+    def compress(self, embs):
+        codes, residuals = [], []
+
+        for batch in embs.split(1 << 18):
+
+            if self.use_gpu:
+                batch = batch.cuda().half()
+            else:
+                batch = batch.cpu().half()
+
+            codes_ = self.compress_into_codes(batch, out_device=batch.device)
+            centroids_ = self.lookup_centroids(codes_, out_device=batch.device)
+
+            residuals_ = (batch - centroids_)
+
+            codes.append(codes_.cpu())
+            residuals.append(self.binarize(residuals_).cpu())
+
+        codes = torch.cat(codes)
+        residuals = torch.cat(residuals)
+
+        return ResidualCodec.Embeddings(codes, residuals)
+
+    def binarize(self, residuals):
+        residuals = torch.bucketize(residuals.float(), self.bucket_cutoffs).to(dtype=torch.uint8)
+        residuals = residuals.unsqueeze(-1).expand(*residuals.size(), self.nbits)  # add a new nbits-wide dim
+        residuals = residuals >> self.arange_bits  # divide by 2^bit for each bit position
+        residuals = residuals & 1  # apply mod 2 to binarize
+
+        assert self.dim % 8 == 0, self.dim
+        assert self.dim % (self.nbits * 8) == 0, (self.dim, self.nbits)
+
+        if self.use_gpu:
+            residuals_packed = ResidualCodec.packbits(residuals.contiguous().flatten())
+        else:
+            residuals_packed = np.packbits(np.asarray(residuals.contiguous().flatten()))
+        residuals_packed = torch.as_tensor(residuals_packed, dtype=torch.uint8)
+
+        residuals_packed = residuals_packed.reshape(residuals.size(0), self.dim // 8 * self.nbits)
+
+        return residuals_packed
+
+    def compress_into_codes(self, embs, out_device):
+        """
+            EVENTUALLY: Fusing the kernels or otherwise avoiding materalizing the entire matrix before max(dim=0)
+                        seems like it would help here a lot.
+        """
+
+        codes = []
+
+        bsize = (1 << 29) // self.centroids.size(0)
+        for batch in embs.split(bsize):
+
+            if self.use_gpu:
+                indices = (self.centroids @ batch.T.cuda().half()).max(dim=0).indices.to(device=out_device)
+            else:
+                indices = (self.centroids @ batch.T.cpu().float()).max(dim=0).indices.to(device=out_device)
+
+            codes.append(indices)
+
+        return torch.cat(codes)
+
+    def lookup_centroids(self, codes, out_device):
+        """
+            Handles multi-dimensional codes too.
+
+            EVENTUALLY: The .split() below should happen on a flat view.
+        """
+
+        centroids = []
+
+        for batch in codes.split(1 << 20):
+
+            if self.use_gpu:
+                centroids.append(self.centroids[batch.cuda().long()].to(device=out_device))
+            else:
+                centroids.append(self.centroids[batch.cpu().long()].to(device=out_device))
+
+        return torch.cat(centroids)
+
+    def decompress(self, compressed_embs: Embeddings):
+        """
+            We batch below even if the target device is CUDA to avoid large temporary buffers causing OOM.
+        """
+
+        assert self.use_gpu
+
+        codes, residuals = compressed_embs.codes, compressed_embs.residuals
+
+        D = []
+        for codes_, residuals_ in zip(codes.split(1 << 15), residuals.split(1 << 15)):
+            codes_, residuals_ = codes_.cuda(), residuals_.cuda()
+            centroids_ = ResidualCodec.decompress_residuals(
+                residuals_,
+                self.bucket_weights,
+                self.reversed_bit_map,
+                self.decompression_lookup_table,
+                codes_,
+                self.centroids,
+                self.dim,
+                self.nbits,
+            ).cuda()
+            D_ = torch.nn.functional.normalize(centroids_, p=2, dim=-1).half()
+            D.append(D_)
+
+        return torch.cat(D)
```

## primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-import os
-import torch
-import ujson
-
-from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings_strided import ResidualEmbeddingsStrided
-
-
-class ResidualEmbeddings:
-    Strided = ResidualEmbeddingsStrided
-
-    def __init__(self, codes, residuals):
-        """
-            Supply the already compressed residuals.
-        """
-
-        # assert isinstance(residuals, bitarray), type(residuals)
-        assert codes.size(0) == residuals.size(0), (codes.size(), residuals.size())
-        assert codes.dim() == 1 and residuals.dim() == 2, (codes.size(), residuals.size())
-        assert residuals.dtype == torch.uint8, residuals.dtype
-
-        self.codes = codes.to(torch.int32)  # (num_embeddings,) int32
-        self.residuals = residuals   # (num_embeddings, compressed_dim) uint8
-
-    @classmethod
-    def load_chunks(cls, index_path, chunk_idxs, num_embeddings):
-        num_embeddings += 512  # pad for access with strides
-
-        dim, nbits = get_dim_and_nbits(index_path)
-
-        codes = torch.empty(num_embeddings, dtype=torch.int32)
-        residuals = torch.empty(num_embeddings, dim // 8 * nbits, dtype=torch.uint8)
-
-        codes_offset = 0
-
-        for chunk_idx in chunk_idxs:
-            chunk = cls.load(index_path, chunk_idx)
-
-            codes_endpos = codes_offset + chunk.codes.size(0)
-
-            # Copy the values over to the allocated space
-            codes[codes_offset:codes_endpos] = chunk.codes
-            residuals[codes_offset:codes_endpos] = chunk.residuals
-
-            codes_offset = codes_endpos
-
-        # codes, residuals = codes.cuda(), residuals.cuda()  # FIXME: REMOVE THIS LINE!
-
-        return cls(codes, residuals)
-
-    @classmethod
-    def load(cls, index_path, chunk_idx):
-        codes = cls.load_codes(index_path, chunk_idx)
-        residuals = cls.load_residuals(index_path, chunk_idx)
-
-        return cls(codes, residuals)
-
-    @classmethod
-    def load_codes(self, index_path, chunk_idx):
-        codes_path = os.path.join(index_path, f'{chunk_idx}.codes.pt')
-        return torch.load(codes_path, map_location='cpu')
-
-    @classmethod
-    def load_residuals(self, index_path, chunk_idx):
-        residuals_path = os.path.join(index_path, f'{chunk_idx}.residuals.pt')  # f'{chunk_idx}.residuals.bn'
-        # return _load_bitarray(residuals_path)
-
-        return torch.load(residuals_path, map_location='cpu')
-
-    def save(self, path_prefix):
-        codes_path = f'{path_prefix}.codes.pt'
-        residuals_path = f'{path_prefix}.residuals.pt'  # f'{path_prefix}.residuals.bn'
-
-        torch.save(self.codes, codes_path)
-        torch.save(self.residuals, residuals_path)
-        # _save_bitarray(self.residuals, residuals_path)
-
-    def __len__(self):
-        return self.codes.size(0)
-
-
-def get_dim_and_nbits(index_path):
-    # TODO: Ideally load this using ColBERTConfig.load_from_index!
-    with open(os.path.join(index_path, 'metadata.json')) as f:
-        metadata = ujson.load(f)['config']
-
-    dim = metadata['dim']
-    nbits = metadata['nbits']
-
-    assert (dim * nbits) % 8 == 0, (dim, nbits, dim * nbits)
-
-    return dim, nbits
+import os
+import torch
+import ujson
+
+from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings_strided import ResidualEmbeddingsStrided
+
+
+class ResidualEmbeddings:
+    Strided = ResidualEmbeddingsStrided
+
+    def __init__(self, codes, residuals):
+        """
+            Supply the already compressed residuals.
+        """
+
+        # assert isinstance(residuals, bitarray), type(residuals)
+        assert codes.size(0) == residuals.size(0), (codes.size(), residuals.size())
+        assert codes.dim() == 1 and residuals.dim() == 2, (codes.size(), residuals.size())
+        assert residuals.dtype == torch.uint8, residuals.dtype
+
+        self.codes = codes.to(torch.int32)  # (num_embeddings,) int32
+        self.residuals = residuals   # (num_embeddings, compressed_dim) uint8
+
+    @classmethod
+    def load_chunks(cls, index_path, chunk_idxs, num_embeddings):
+        num_embeddings += 512  # pad for access with strides
+
+        dim, nbits = get_dim_and_nbits(index_path)
+
+        codes = torch.empty(num_embeddings, dtype=torch.int32)
+        residuals = torch.empty(num_embeddings, dim // 8 * nbits, dtype=torch.uint8)
+
+        codes_offset = 0
+
+        for chunk_idx in chunk_idxs:
+            chunk = cls.load(index_path, chunk_idx)
+
+            codes_endpos = codes_offset + chunk.codes.size(0)
+
+            # Copy the values over to the allocated space
+            codes[codes_offset:codes_endpos] = chunk.codes
+            residuals[codes_offset:codes_endpos] = chunk.residuals
+
+            codes_offset = codes_endpos
+
+        # codes, residuals = codes.cuda(), residuals.cuda()  # FIXME: REMOVE THIS LINE!
+
+        return cls(codes, residuals)
+
+    @classmethod
+    def load(cls, index_path, chunk_idx):
+        codes = cls.load_codes(index_path, chunk_idx)
+        residuals = cls.load_residuals(index_path, chunk_idx)
+
+        return cls(codes, residuals)
+
+    @classmethod
+    def load_codes(self, index_path, chunk_idx):
+        codes_path = os.path.join(index_path, f'{chunk_idx}.codes.pt')
+        return torch.load(codes_path, map_location='cpu')
+
+    @classmethod
+    def load_residuals(self, index_path, chunk_idx):
+        residuals_path = os.path.join(index_path, f'{chunk_idx}.residuals.pt')  # f'{chunk_idx}.residuals.bn'
+        # return _load_bitarray(residuals_path)
+
+        return torch.load(residuals_path, map_location='cpu')
+
+    def save(self, path_prefix):
+        codes_path = f'{path_prefix}.codes.pt'
+        residuals_path = f'{path_prefix}.residuals.pt'  # f'{path_prefix}.residuals.bn'
+
+        torch.save(self.codes, codes_path)
+        torch.save(self.residuals, residuals_path)
+        # _save_bitarray(self.residuals, residuals_path)
+
+    def __len__(self):
+        return self.codes.size(0)
+
+
+def get_dim_and_nbits(index_path):
+    # TODO: Ideally load this using ColBERTConfig.load_from_index!
+    with open(os.path.join(index_path, 'metadata.json')) as f:
+        metadata = ujson.load(f)['config']
+
+    dim = metadata['dim']
+    nbits = metadata['nbits']
+
+    assert (dim * nbits) % 8 == 0, (dim, nbits, dim * nbits)
+
+    return dim, nbits
```

## primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings_strided.py

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-import primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings as residual_embeddings
-
-from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
-
-class ResidualEmbeddingsStrided:
-    def __init__(self, codec, embeddings, doclens):
-        self.codec = codec
-        self.codes = embeddings.codes
-        self.residuals = embeddings.residuals
-        self.use_gpu = self.codec.use_gpu
-
-        self.codes_strided = StridedTensor(self.codes, doclens, use_gpu=self.use_gpu)
-        self.residuals_strided = StridedTensor(self.residuals, doclens, use_gpu=self.use_gpu)
-
-    def lookup_eids(self, embedding_ids, codes=None, out_device='cuda'):
-        codes = self.codes[embedding_ids] if codes is None else codes
-        residuals = self.residuals[embedding_ids]
-
-        return self.codec.decompress(residual_embeddings.ResidualEmbeddings(codes, residuals))
-
-    def lookup_pids(self, passage_ids, out_device='cuda'):
-        codes_packed, codes_lengths = self.codes_strided.lookup(passage_ids)
-        residuals_packed, _ = self.residuals_strided.lookup(passage_ids)
-
-        embeddings_packed = self.codec.decompress(residual_embeddings.ResidualEmbeddings(codes_packed, residuals_packed))
-
-        return embeddings_packed, codes_lengths
-
-    def lookup_codes(self, passage_ids):
-        return self.codes_strided.lookup(passage_ids)
+import primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings as residual_embeddings
+
+from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
+
+class ResidualEmbeddingsStrided:
+    def __init__(self, codec, embeddings, doclens):
+        self.codec = codec
+        self.codes = embeddings.codes
+        self.residuals = embeddings.residuals
+        self.use_gpu = self.codec.use_gpu
+
+        self.codes_strided = StridedTensor(self.codes, doclens, use_gpu=self.use_gpu)
+        self.residuals_strided = StridedTensor(self.residuals, doclens, use_gpu=self.use_gpu)
+
+    def lookup_eids(self, embedding_ids, codes=None, out_device='cuda'):
+        codes = self.codes[embedding_ids] if codes is None else codes
+        residuals = self.residuals[embedding_ids]
+
+        return self.codec.decompress(residual_embeddings.ResidualEmbeddings(codes, residuals))
+
+    def lookup_pids(self, passage_ids, out_device='cuda'):
+        codes_packed, codes_lengths = self.codes_strided.lookup(passage_ids)
+        residuals_packed, _ = self.residuals_strided.lookup(passage_ids)
+
+        embeddings_packed = self.codec.decompress(residual_embeddings.ResidualEmbeddings(codes_packed, residuals_packed))
+
+        return embeddings_packed, codes_lengths
+
+    def lookup_codes(self, passage_ids):
+        return self.codes_strided.lookup(passage_ids)
```

## primeqa/ir/dense/colbert_top/colbert/infra/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .run import *
+from .run import *
 from .config import *
```

## primeqa/ir/dense/colbert_top/colbert/infra/launcher.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-import os
-import time
-import torch
-import random
-
-import torch.multiprocessing as mp
-import numpy as np
-
-try:
-    mp.set_start_method('spawn', force=True)
-except RuntimeError:
-    pass
-
-import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
-
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-from primeqa.ir.dense.colbert_top.colbert.infra.config import BaseConfig, RunConfig, RunSettings
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-
-class Launcher:
-    def __init__(self, callee, run_config=None, return_all=False):
-        self.callee = callee
-        self.return_all = return_all
-
-        self.run_config = RunConfig.from_existing(Run().config, run_config)
-        self.nranks = self.run_config.nranks
-
-    def launch(self, custom_config, *args):
-        return_value_queue = mp.Queue()
-
-        rng = random.Random(time.time())
-        port = str(12355 + rng.randint(0, 1000))  # randomize the port to avoid collision on launching several jobs.
-
-        all_procs = []
-        for new_rank in range(0, self.nranks):
-            assert isinstance(custom_config, BaseConfig)
-            assert isinstance(custom_config, RunSettings)
-
-            new_config = type(custom_config).from_existing(custom_config, self.run_config, RunConfig(rank=new_rank))
-
-            args_ = (self.callee, port, return_value_queue, new_config, *args)
-            all_procs.append(mp.Process(target=setup_new_process, args=args_))
-
-        # Clear GPU space (e.g., after a `Searcher` on GPU-0 is deleted)
-        # TODO: Generalize this from GPU-0 only!
-        # TODO: Move this to a function. And call that function from __del__ in a class that's inherited by Searcher, Indexer, etc.
-
-        # t = torch.cuda.get_device_properties(0).total_memory
-        # r = torch.cuda.memory_reserved(0)
-        # a = torch.cuda.memory_allocated(0)
-        # f = r-a
-
-        # print_message(f"[Pre-Emptying] GPU memory check: r={r}, a={a}, f={f}")
-
-        torch.cuda.empty_cache()
-
-        # t = torch.cuda.get_device_properties(0).total_memory
-        # r = torch.cuda.memory_reserved(0)
-        # a = torch.cuda.memory_allocated(0)
-        # f = r-a
-
-        # print_message(f"[Post-Emptying] GPU memory check: r={r}, a={a}, f={f}")
-
-        print_memory_stats('MAIN')
-
-        for proc in all_procs:
-            print("#> Starting...")
-            proc.start()
-
-        print_memory_stats('MAIN')
-
-        # TODO: If the processes crash upon join, raise an exception and don't block on .get() below!
-
-        return_values = sorted([return_value_queue.get() for _ in all_procs])
-        return_values = [val for rank, val in return_values]
-
-        if not self.return_all:
-            return_values = return_values[0]
-        
-        for proc in all_procs:
-            proc.join()
-            print("#> Joined...")
-
-        print_memory_stats('MAIN')
-        
-        return return_values
-
-
-def setup_new_process(callee, port, return_value_queue, config, *args):
-    print_memory_stats()
-
-    random.seed(config.rng_seed)
-    np.random.seed(config.rng_seed)
-    torch.manual_seed(config.rng_seed)
-    torch.cuda.manual_seed_all(config.rng_seed)
-
-    rank, nranks = config.rank, config.nranks
-
-    os.environ["MASTER_ADDR"] = "localhost"
-    os.environ["MASTER_PORT"] = port
-    os.environ["WORLD_SIZE"] = str(config.nranks)
-    os.environ["RANK"] = str(config.rank)
-
-    # TODO: Ideally the gpus "getter" handles this max-nranks thing!
-    os.environ["CUDA_VISIBLE_DEVICES"] = ','.join(map(str, config.gpus_[:nranks]))
-
-    nranks_, distributed_ = distributed.init(rank)
-    assert nranks_ == nranks
-
-    # Run.init(args.rank, args.root, args.experiment, args.run)
-
-    with Run().context(config, inherit_config=False):
-        return_val = callee(config, *args)
-
-    return_value_queue.put((rank, return_val))
-
-
-def print_memory_stats(message=''):
-    return  # FIXME: Add this back before release.
-
-    import psutil  # Remove before releases? Or at least make optional with try/except.
-
-    global_info = psutil.virtual_memory()
-    total, available, used, free = global_info.total, global_info.available, global_info.used, global_info.free
-
-    info = psutil.Process().memory_info()
-    rss, vms, shared = info.rss, info.vms, info.shared
-    uss = psutil.Process().memory_full_info().uss
-
-    gib = 1024 ** 3
-
-    summary = f"""
-    "[PID: {os.getpid()}]
-    [{message}]
-    Available: {available / gib:,.1f} / {total / gib:,.1f}
-    Free: {free / gib:,.1f} / {total / gib:,.1f}
-    Usage: {used / gib:,.1f} / {total / gib:,.1f}
-
-    RSS: {rss  / gib:,.1f}
-    VMS: {vms  / gib:,.1f}
-    USS: {uss  / gib:,.1f}
-    SHARED: {shared  / gib:,.1f}
-    """.strip().replace('\n', '\t')
-
-    print_message(summary, pad=True)
+import os
+import time
+import torch
+import random
+
+import torch.multiprocessing as mp
+import numpy as np
+
+try:
+    mp.set_start_method('spawn', force=True)
+except RuntimeError:
+    pass
+
+import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
+
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+from primeqa.ir.dense.colbert_top.colbert.infra.config import BaseConfig, RunConfig, RunSettings
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+
+class Launcher:
+    def __init__(self, callee, run_config=None, return_all=False):
+        self.callee = callee
+        self.return_all = return_all
+
+        self.run_config = RunConfig.from_existing(Run().config, run_config)
+        self.nranks = self.run_config.nranks
+
+    def launch(self, custom_config, *args):
+        return_value_queue = mp.Queue()
+
+        rng = random.Random(time.time())
+        port = str(12355 + rng.randint(0, 1000))  # randomize the port to avoid collision on launching several jobs.
+
+        all_procs = []
+        for new_rank in range(0, self.nranks):
+            assert isinstance(custom_config, BaseConfig)
+            assert isinstance(custom_config, RunSettings)
+
+            new_config = type(custom_config).from_existing(custom_config, self.run_config, RunConfig(rank=new_rank))
+
+            args_ = (self.callee, port, return_value_queue, new_config, *args)
+            all_procs.append(mp.Process(target=setup_new_process, args=args_))
+
+        # Clear GPU space (e.g., after a `Searcher` on GPU-0 is deleted)
+        # TODO: Generalize this from GPU-0 only!
+        # TODO: Move this to a function. And call that function from __del__ in a class that's inherited by Searcher, Indexer, etc.
+
+        # t = torch.cuda.get_device_properties(0).total_memory
+        # r = torch.cuda.memory_reserved(0)
+        # a = torch.cuda.memory_allocated(0)
+        # f = r-a
+
+        # print_message(f"[Pre-Emptying] GPU memory check: r={r}, a={a}, f={f}")
+
+        torch.cuda.empty_cache()
+
+        # t = torch.cuda.get_device_properties(0).total_memory
+        # r = torch.cuda.memory_reserved(0)
+        # a = torch.cuda.memory_allocated(0)
+        # f = r-a
+
+        # print_message(f"[Post-Emptying] GPU memory check: r={r}, a={a}, f={f}")
+
+        print_memory_stats('MAIN')
+
+        for proc in all_procs:
+            print("#> Starting...")
+            proc.start()
+
+        print_memory_stats('MAIN')
+
+        # TODO: If the processes crash upon join, raise an exception and don't block on .get() below!
+
+        return_values = sorted([return_value_queue.get() for _ in all_procs])
+        return_values = [val for rank, val in return_values]
+
+        if not self.return_all:
+            return_values = return_values[0]
+        
+        for proc in all_procs:
+            proc.join()
+            print("#> Joined...")
+
+        print_memory_stats('MAIN')
+        
+        return return_values
+
+
+def setup_new_process(callee, port, return_value_queue, config, *args):
+    print_memory_stats()
+
+    random.seed(config.rng_seed)
+    np.random.seed(config.rng_seed)
+    torch.manual_seed(config.rng_seed)
+    torch.cuda.manual_seed_all(config.rng_seed)
+
+    rank, nranks = config.rank, config.nranks
+
+    os.environ["MASTER_ADDR"] = "localhost"
+    os.environ["MASTER_PORT"] = port
+    os.environ["WORLD_SIZE"] = str(config.nranks)
+    os.environ["RANK"] = str(config.rank)
+
+    # TODO: Ideally the gpus "getter" handles this max-nranks thing!
+    os.environ["CUDA_VISIBLE_DEVICES"] = ','.join(map(str, config.gpus_[:nranks]))
+
+    nranks_, distributed_ = distributed.init(rank)
+    assert nranks_ == nranks
+
+    # Run.init(args.rank, args.root, args.experiment, args.run)
+
+    with Run().context(config, inherit_config=False):
+        return_val = callee(config, *args)
+
+    return_value_queue.put((rank, return_val))
+
+
+def print_memory_stats(message=''):
+    return  # FIXME: Add this back before release.
+
+    import psutil  # Remove before releases? Or at least make optional with try/except.
+
+    global_info = psutil.virtual_memory()
+    total, available, used, free = global_info.total, global_info.available, global_info.used, global_info.free
+
+    info = psutil.Process().memory_info()
+    rss, vms, shared = info.rss, info.vms, info.shared
+    uss = psutil.Process().memory_full_info().uss
+
+    gib = 1024 ** 3
+
+    summary = f"""
+    "[PID: {os.getpid()}]
+    [{message}]
+    Available: {available / gib:,.1f} / {total / gib:,.1f}
+    Free: {free / gib:,.1f} / {total / gib:,.1f}
+    Usage: {used / gib:,.1f} / {total / gib:,.1f}
+
+    RSS: {rss  / gib:,.1f}
+    VMS: {vms  / gib:,.1f}
+    USS: {uss  / gib:,.1f}
+    SHARED: {shared  / gib:,.1f}
+    """.strip().replace('\n', '\t')
+
+    print_message(summary, pad=True)
```

## primeqa/ir/dense/colbert_top/colbert/infra/provenance.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-import sys
-import traceback
-import inspect
-
-
-class Provenance:
-    def __init__(self) -> None:
-        self.initial_stacktrace = self.stacktrace()
-
-    def stacktrace(self):
-        trace = inspect.stack()
-        output = []
-
-        for frame in trace[2:-1]:
-            try:
-                frame = f'{frame.filename}:{frame.lineno}:{frame.function}:   {frame.code_context[0].strip()}'
-                output.append(frame)
-            except:
-                output.append(None)
-
-        return output
-
-    def toDict(self):  # for ujson
-        self.serialization_stacktrace = self.stacktrace()
-        return dict(self.__dict__)
-
-
-if __name__ == '__main__':
-    p = Provenance()
-    print(p.toDict().keys())
-
-    import ujson
-    print(ujson.dumps(p, indent=4))
-
-
-    class X:
-        def __init__(self) -> None:
-            pass
-        
-        def toDict(self):
-            return {'key': 1}
-    
+import sys
+import traceback
+import inspect
+
+
+class Provenance:
+    def __init__(self) -> None:
+        self.initial_stacktrace = self.stacktrace()
+
+    def stacktrace(self):
+        trace = inspect.stack()
+        output = []
+
+        for frame in trace[2:-1]:
+            try:
+                frame = f'{frame.filename}:{frame.lineno}:{frame.function}:   {frame.code_context[0].strip()}'
+                output.append(frame)
+            except:
+                output.append(None)
+
+        return output
+
+    def toDict(self):  # for ujson
+        self.serialization_stacktrace = self.stacktrace()
+        return dict(self.__dict__)
+
+
+if __name__ == '__main__':
+    p = Provenance()
+    print(p.toDict().keys())
+
+    import ujson
+    print(ujson.dumps(p, indent=4))
+
+
+    class X:
+        def __init__(self) -> None:
+            pass
+        
+        def toDict(self):
+            return {'key': 1}
+    
     print(ujson.dumps(X()))
```

## primeqa/ir/dense/colbert_top/colbert/infra/run.py

 * *Ordering differences only*

```diff
@@ -1,92 +1,92 @@
-import os
-import atexit
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory, print_message, timestamp
-from contextlib import contextmanager
-
-from primeqa.ir.dense.colbert_top.colbert.infra.config import RunConfig
-
-
-class Run(object):
-    _instance = None
-
-    os.environ["TOKENIZERS_PARALLELISM"] = "true"  # NOTE: If a deadlock arises, switch to false!!
-
-    def __new__(cls):
-        """
-        Singleton Pattern. See https://python-patterns.guide/gang-of-four/singleton/
-        """
-        if cls._instance is None:
-            cls._instance = super().__new__(cls)
-            cls._instance.stack = []
-
-            # TODO: Save a timestamp here! And re-use it! But allow the user to override it on calling Run().context a second time.
-            run_config = RunConfig()
-            run_config.assign_defaults()
-            
-            cls._instance.__append(run_config)
-
-        # TODO: atexit.register(all_done)
-
-        return cls._instance
-
-    @property
-    def config(self):
-        return self.stack[-1]
-
-    def __getattr__(self, name):
-        if hasattr(self.config, name):
-            return getattr(self.config, name)
-
-        super().__getattr__(name)
-
-    def __append(self, runconfig: RunConfig):
-        # runconfig.disallow_writes(readonly=True)
-        self.stack.append(runconfig)
-
-    def __pop(self):
-        self.stack.pop()
-
-    @contextmanager
-    def context(self, runconfig: RunConfig, inherit_config=True):
-        if inherit_config:
-            runconfig = RunConfig.from_existing(self.config, runconfig)
-
-        self.__append(runconfig)
-
-        try:
-            yield
-        finally:
-            self.__pop()
-        
-    def open(self, path, mode='r'):
-        path = os.path.join(self.path_, path)
-
-        if not os.path.exists(self.path_):
-            create_directory(self.path_)
-
-        if ('w' in mode or 'a' in mode) and not self.overwrite:
-            assert not os.path.exists(path), (self.overwrite, path)
-
-        return open(path, mode=mode)
-    
-    def print(self, *args):
-        print_message("[" + str(self.rank) + "]", "\t\t", *args)
-
-    def print_main(self, *args):
-        if self.rank == 0:
-            self.print(*args)
-
-
-if __name__ == '__main__':
-    print(Run().root, '!')
-
-    with Run().context(RunConfig(rank=0, nranks=1)):
-        with Run().context(RunConfig(experiment='newproject')):
-            print(Run().nranks, '!')
-
-        print(Run().config, '!')
-        print(Run().rank)
-
-
+import os
+import atexit
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory, print_message, timestamp
+from contextlib import contextmanager
+
+from primeqa.ir.dense.colbert_top.colbert.infra.config import RunConfig
+
+
+class Run(object):
+    _instance = None
+
+    os.environ["TOKENIZERS_PARALLELISM"] = "true"  # NOTE: If a deadlock arises, switch to false!!
+
+    def __new__(cls):
+        """
+        Singleton Pattern. See https://python-patterns.guide/gang-of-four/singleton/
+        """
+        if cls._instance is None:
+            cls._instance = super().__new__(cls)
+            cls._instance.stack = []
+
+            # TODO: Save a timestamp here! And re-use it! But allow the user to override it on calling Run().context a second time.
+            run_config = RunConfig()
+            run_config.assign_defaults()
+            
+            cls._instance.__append(run_config)
+
+        # TODO: atexit.register(all_done)
+
+        return cls._instance
+
+    @property
+    def config(self):
+        return self.stack[-1]
+
+    def __getattr__(self, name):
+        if hasattr(self.config, name):
+            return getattr(self.config, name)
+
+        super().__getattr__(name)
+
+    def __append(self, runconfig: RunConfig):
+        # runconfig.disallow_writes(readonly=True)
+        self.stack.append(runconfig)
+
+    def __pop(self):
+        self.stack.pop()
+
+    @contextmanager
+    def context(self, runconfig: RunConfig, inherit_config=True):
+        if inherit_config:
+            runconfig = RunConfig.from_existing(self.config, runconfig)
+
+        self.__append(runconfig)
+
+        try:
+            yield
+        finally:
+            self.__pop()
+        
+    def open(self, path, mode='r'):
+        path = os.path.join(self.path_, path)
+
+        if not os.path.exists(self.path_):
+            create_directory(self.path_)
+
+        if ('w' in mode or 'a' in mode) and not self.overwrite:
+            assert not os.path.exists(path), (self.overwrite, path)
+
+        return open(path, mode=mode)
+    
+    def print(self, *args):
+        print_message("[" + str(self.rank) + "]", "\t\t", *args)
+
+    def print_main(self, *args):
+        if self.rank == 0:
+            self.print(*args)
+
+
+if __name__ == '__main__':
+    print(Run().root, '!')
+
+    with Run().context(RunConfig(rank=0, nranks=1)):
+        with Run().context(RunConfig(experiment='newproject')):
+            print(Run().nranks, '!')
+
+        print(Run().config, '!')
+        print(Run().rank)
+
+
 # TODO: Handle logging all prints to a file. There should be a way to determine the level of logs that go to stdout.
```

## primeqa/ir/dense/colbert_top/colbert/infra/config/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .config import *
+from .config import *
 from .settings import *
```

## primeqa/ir/dense/colbert_top/colbert/infra/config/base_config.py

 * *Ordering differences only*

```diff
@@ -1,122 +1,122 @@
-import os
-import torch
-import ujson
-import dataclasses
-
-from typing import Any
-from collections import defaultdict
-from dataclasses import dataclass, fields
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp, torch_load_dnn
-
-from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
-from .core_config import *
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-import ujson
-
-@dataclass
-class BaseConfig(CoreConfig):
-    @classmethod
-    def from_existing(cls, *sources):
-        kw_args = {}
-
-        for source in sources:
-            if source is None:
-                continue
-                
-            local_kw_args = dataclasses.asdict(source)
-            local_kw_args = {k: local_kw_args[k] for k in source.assigned}
-            kw_args = {**kw_args, **local_kw_args}
-
-        obj = cls(**kw_args)
-
-        return obj
-
-    @classmethod
-    def from_deprecated_args(cls, args):
-        obj = cls()
-        ignored = obj.configure(ignore_unrecognized=True, **args)
-
-        return obj, ignored
-
-    @classmethod
-    def from_path(cls, name):
-
-        print_message(f"#> base_config.py from_path {name}")
-
-        with open(name) as f:
-
-            args = ujson.load(f)
-
-            print_message(f"#> base_config.py from_path args loaded! ")
-
-            if 'config' in args:
-                args = args['config']
-
-                print_message(f"#> base_config.py from_path args replaced ! ")
-
-
-        return cls.from_deprecated_args(args)  # the new, non-deprecated version functions the same at this level.
-    
-    @classmethod
-    def load_from_checkpoint(cls, checkpoint_path):
-        if checkpoint_path.endswith('.dnn') or checkpoint_path.endswith('.model'):
-            dnn = torch_load_dnn(checkpoint_path)
-            config, _ = cls.from_deprecated_args(dnn.get('arguments', {}))
-
-            # get model type from a dnn file
-            if dnn['model_type'] is not None:
-                config.set('model_type', dnn['model_type'])
-
-            config.set('checkpoint', checkpoint_path)
-
-            return config
-
-        loaded_config_path = os.path.join(checkpoint_path, 'artifact.metadata')
-
-        print_message(f"#> base_config.py load_from_checkpoint {checkpoint_path}")
-        print_message(f"#> base_config.py load_from_checkpoint {loaded_config_path}")
-
-        if os.path.exists(loaded_config_path):
-            loaded_config, _ = cls.from_path(loaded_config_path)
-            loaded_config.set('checkpoint', checkpoint_path)
-
-            return loaded_config
-
-        return None  # can happen if checkpoint_path is something like 'bert-base-uncased'
-
-    @classmethod
-    def load_from_index(cls, index_path):
-        # FIXME: We should start here with initial_config = ColBERTConfig(config, Run().config).
-        # This should allow us to say initial_config.index_root. Then, below, set config = Config(..., initial_c)
-
-        # default_index_root = os.path.join(Run().root, Run().experiment, 'indexes/')
-        # index_path = os.path.join(default_index_root, index_path)
-
-        # CONSIDER: No more plan/metadata.json. Only metadata.json to avoid weird issues when loading an index.
-
-        try:
-            metadata_path = os.path.join(index_path, 'metadata.json')
-            loaded_config, _ = cls.from_path(metadata_path)
-        except:
-            metadata_path = os.path.join(index_path, 'plan.json')
-            loaded_config, _ = cls.from_path(metadata_path)
-        
-        return loaded_config
-
-    def save(self, path, overwrite=False):
-        assert overwrite or not os.path.exists(path), path
-
-        with open(path, 'w') as f:
-            args = self.export()  # dict(self.__config)
-            args['meta'] = get_metadata_only()
-            args['meta']['version'] = 'colbert-v0.4'
-            # TODO: Add git_status details.. It can't be too large! It should be a path that Runs() saves on exit, maybe!
-
-            f.write(ujson.dumps(args, indent=4) + '\n')
-
-    def save_for_checkpoint(self, checkpoint_path):
-        assert not checkpoint_path.endswith('.dnn'), \
-            f"{checkpoint_path}: We reserve *.dnn names for the deprecated checkpoint format."
-
-        output_config_path = os.path.join(checkpoint_path, 'artifact.metadata')
-        self.save(output_config_path, overwrite=True)
+import os
+import torch
+import ujson
+import dataclasses
+
+from typing import Any
+from collections import defaultdict
+from dataclasses import dataclass, fields
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp, torch_load_dnn
+
+from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
+from .core_config import *
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+import ujson
+
+@dataclass
+class BaseConfig(CoreConfig):
+    @classmethod
+    def from_existing(cls, *sources):
+        kw_args = {}
+
+        for source in sources:
+            if source is None:
+                continue
+                
+            local_kw_args = dataclasses.asdict(source)
+            local_kw_args = {k: local_kw_args[k] for k in source.assigned}
+            kw_args = {**kw_args, **local_kw_args}
+
+        obj = cls(**kw_args)
+
+        return obj
+
+    @classmethod
+    def from_deprecated_args(cls, args):
+        obj = cls()
+        ignored = obj.configure(ignore_unrecognized=True, **args)
+
+        return obj, ignored
+
+    @classmethod
+    def from_path(cls, name):
+
+        print_message(f"#> base_config.py from_path {name}")
+
+        with open(name) as f:
+
+            args = ujson.load(f)
+
+            print_message(f"#> base_config.py from_path args loaded! ")
+
+            if 'config' in args:
+                args = args['config']
+
+                print_message(f"#> base_config.py from_path args replaced ! ")
+
+
+        return cls.from_deprecated_args(args)  # the new, non-deprecated version functions the same at this level.
+    
+    @classmethod
+    def load_from_checkpoint(cls, checkpoint_path):
+        if checkpoint_path.endswith('.dnn') or checkpoint_path.endswith('.model'):
+            dnn = torch_load_dnn(checkpoint_path)
+            config, _ = cls.from_deprecated_args(dnn.get('arguments', {}))
+
+            # get model type from a dnn file
+            if dnn['model_type'] is not None:
+                config.set('model_type', dnn['model_type'])
+
+            config.set('checkpoint', checkpoint_path)
+
+            return config
+
+        loaded_config_path = os.path.join(checkpoint_path, 'artifact.metadata')
+
+        print_message(f"#> base_config.py load_from_checkpoint {checkpoint_path}")
+        print_message(f"#> base_config.py load_from_checkpoint {loaded_config_path}")
+
+        if os.path.exists(loaded_config_path):
+            loaded_config, _ = cls.from_path(loaded_config_path)
+            loaded_config.set('checkpoint', checkpoint_path)
+
+            return loaded_config
+
+        return None  # can happen if checkpoint_path is something like 'bert-base-uncased'
+
+    @classmethod
+    def load_from_index(cls, index_path):
+        # FIXME: We should start here with initial_config = ColBERTConfig(config, Run().config).
+        # This should allow us to say initial_config.index_root. Then, below, set config = Config(..., initial_c)
+
+        # default_index_root = os.path.join(Run().root, Run().experiment, 'indexes/')
+        # index_path = os.path.join(default_index_root, index_path)
+
+        # CONSIDER: No more plan/metadata.json. Only metadata.json to avoid weird issues when loading an index.
+
+        try:
+            metadata_path = os.path.join(index_path, 'metadata.json')
+            loaded_config, _ = cls.from_path(metadata_path)
+        except:
+            metadata_path = os.path.join(index_path, 'plan.json')
+            loaded_config, _ = cls.from_path(metadata_path)
+        
+        return loaded_config
+
+    def save(self, path, overwrite=False):
+        assert overwrite or not os.path.exists(path), path
+
+        with open(path, 'w') as f:
+            args = self.export()  # dict(self.__config)
+            args['meta'] = get_metadata_only()
+            args['meta']['version'] = 'colbert-v0.4'
+            # TODO: Add git_status details.. It can't be too large! It should be a path that Runs() saves on exit, maybe!
+
+            f.write(ujson.dumps(args, indent=4) + '\n')
+
+    def save_for_checkpoint(self, checkpoint_path):
+        assert not checkpoint_path.endswith('.dnn'), \
+            f"{checkpoint_path}: We reserve *.dnn names for the deprecated checkpoint format."
+
+        output_config_path = os.path.join(checkpoint_path, 'artifact.metadata')
+        self.save(output_config_path, overwrite=True)
```

## primeqa/ir/dense/colbert_top/colbert/infra/config/config.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-from dataclasses import dataclass
-
-from .base_config import BaseConfig
-from .settings import *
-
-
-@dataclass
-class RunConfig(BaseConfig, RunSettings):
-    pass
-
-
-@dataclass
-class ColBERTConfig(RunSettings, ResourceSettings, DocSettings, QuerySettings, TrainingSettings,
-                    IndexingSettings, SearchSettings, BaseConfig):
-    pass
+from dataclasses import dataclass
+
+from .base_config import BaseConfig
+from .settings import *
+
+
+@dataclass
+class RunConfig(BaseConfig, RunSettings):
+    pass
+
+
+@dataclass
+class ColBERTConfig(RunSettings, ResourceSettings, DocSettings, QuerySettings, TrainingSettings,
+                    IndexingSettings, SearchSettings, BaseConfig):
+    pass
```

## primeqa/ir/dense/colbert_top/colbert/infra/config/core_config.py

 * *Ordering differences only*

```diff
@@ -1,86 +1,86 @@
-import os
-import torch
-import ujson
-import dataclasses
-
-from typing import Any
-from collections import defaultdict
-from dataclasses import dataclass, fields
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp, torch_load_dnn
-
-from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
-
-
-@dataclass
-class DefaultVal:
-    val: Any
-
-
-@dataclass
-class CoreConfig:
-    def __post_init__(self):
-        """
-        Source: https://stackoverflow.com/a/58081120/1493011
-        """
-
-        self.assigned = {}
-
-        for field in fields(self):
-            field_val = getattr(self, field.name)
-
-            if isinstance(field_val, DefaultVal) or field_val is None:
-                setattr(self, field.name, field.default.val)
-
-            if not isinstance(field_val, DefaultVal):
-                self.assigned[field.name] = True
-    
-    def assign_defaults(self):
-        for field in fields(self):
-            setattr(self, field.name, field.default.val)
-            self.assigned[field.name] = True
-
-    def configure(self, ignore_unrecognized=True, **kw_args):
-        ignored = set()
-
-        for key, value in kw_args.items():
-            self.set(key, value, ignore_unrecognized) or ignored.update({key})
-
-        return ignored
-
-        """
-        # TODO: Take a config object, not kw_args.
-
-        for key in config.assigned:
-            value = getattr(config, key)
-        """
-
-    def set(self, key, value, ignore_unrecognized=False):
-        if hasattr(self, key):
-            setattr(self, key, value)
-            self.assigned[key] = True
-            return True
-
-        if not ignore_unrecognized:
-            raise Exception(f"Unrecognized key `{key}` for {type(self)}")
-
-    def help(self):
-        print(ujson.dumps(dataclasses.asdict(self), indent=4))
-
-    def __export_value(self, v):
-        v = v.provenance() if hasattr(v, 'provenance') else v
-
-        if isinstance(v, list) and len(v) > 100:
-            v = (f"list with {len(v)} elements starting with...", v[:3])
-
-        if isinstance(v, dict) and len(v) > 100:
-            v = (f"dict with {len(v)} keys starting with...", list(v.keys())[:3])
-
-        return v
-
-    def export(self):
-        d = dataclasses.asdict(self)
-
-        for k, v in d.items():
-            d[k] = self.__export_value(v)
-
-        return d
+import os
+import torch
+import ujson
+import dataclasses
+
+from typing import Any
+from collections import defaultdict
+from dataclasses import dataclass, fields
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp, torch_load_dnn
+
+from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import get_metadata_only
+
+
+@dataclass
+class DefaultVal:
+    val: Any
+
+
+@dataclass
+class CoreConfig:
+    def __post_init__(self):
+        """
+        Source: https://stackoverflow.com/a/58081120/1493011
+        """
+
+        self.assigned = {}
+
+        for field in fields(self):
+            field_val = getattr(self, field.name)
+
+            if isinstance(field_val, DefaultVal) or field_val is None:
+                setattr(self, field.name, field.default.val)
+
+            if not isinstance(field_val, DefaultVal):
+                self.assigned[field.name] = True
+    
+    def assign_defaults(self):
+        for field in fields(self):
+            setattr(self, field.name, field.default.val)
+            self.assigned[field.name] = True
+
+    def configure(self, ignore_unrecognized=True, **kw_args):
+        ignored = set()
+
+        for key, value in kw_args.items():
+            self.set(key, value, ignore_unrecognized) or ignored.update({key})
+
+        return ignored
+
+        """
+        # TODO: Take a config object, not kw_args.
+
+        for key in config.assigned:
+            value = getattr(config, key)
+        """
+
+    def set(self, key, value, ignore_unrecognized=False):
+        if hasattr(self, key):
+            setattr(self, key, value)
+            self.assigned[key] = True
+            return True
+
+        if not ignore_unrecognized:
+            raise Exception(f"Unrecognized key `{key}` for {type(self)}")
+
+    def help(self):
+        print(ujson.dumps(dataclasses.asdict(self), indent=4))
+
+    def __export_value(self, v):
+        v = v.provenance() if hasattr(v, 'provenance') else v
+
+        if isinstance(v, list) and len(v) > 100:
+            v = (f"list with {len(v)} elements starting with...", v[:3])
+
+        if isinstance(v, dict) and len(v) > 100:
+            v = (f"dict with {len(v)} keys starting with...", list(v.keys())[:3])
+
+        return v
+
+    def export(self):
+        d = dataclasses.asdict(self)
+
+        for k, v in d.items():
+            d[k] = self.__export_value(v)
+
+        return d
```

## primeqa/ir/dense/colbert_top/colbert/infra/config/settings.py

```diff
@@ -1,192 +1,193 @@
-import os
-import torch
-
-import __main__
-from dataclasses import dataclass
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp
-
-from .core_config import DefaultVal
-
-
-@dataclass
-class RunSettings:
-    """
-        The defaults here have a special status in Run(), which initially calls assign_defaults(),
-        so these aren't soft defaults in that specific context.
-    """
-
-    overwrite: bool = DefaultVal(False)
-
-    root: str = DefaultVal(os.path.join(os.getcwd(), 'experiments'))
-    experiment: str = DefaultVal('default')
-
-    index_root: str = DefaultVal(None)
-    name: str = DefaultVal(timestamp(daydir=True))
-
-    rank: int = DefaultVal(0)
-    nranks: int = DefaultVal(1)
-    amp: bool = DefaultVal(True)
-
-    total_visible_gpus = torch.cuda.device_count()
-    gpus: int = DefaultVal(total_visible_gpus)
-
-    @property
-    def gpus_(self):
-        value = self.gpus
-
-        if isinstance(value, int):
-            value = list(range(value))
-
-        if isinstance(value, str):
-            value = value.split(',')
-
-        value = list(map(int, value))
-        value = sorted(list(set(value)))
-
-        assert all(device_idx in range(0, self.total_visible_gpus) for device_idx in value), value
-
-        return value
-
-    @property
-    def index_root_(self):
-        return self.index_root or os.path.join(self.root, self.experiment, 'indexes/')
-
-    @property
-    def script_name_(self):
-        if '__file__' in dir(__main__):
-            cwd = os.path.abspath(os.getcwd())
-            script_path = os.path.abspath(__main__.__file__)
-            root_path = os.path.abspath(self.root)
-
-            if script_path.startswith(cwd):
-                script_path = script_path[len(cwd):]
-
-            else:
-                try:
-                    commonpath = os.path.commonpath([script_path, root_path])
-                    script_path = script_path[len(commonpath):]
-                except:
-                    pass
-
-
-            if script_path.endswith('bin/pytest'):
-                script_path = script_path + '.py'
-            assert script_path.endswith('.py'), (script_path, cwd)
-            script_name = script_path.replace('/', '.').strip('.')[:-3]
-
-            assert len(script_name) > 0, (script_name, script_path, cwd)
-
-            return script_name
-
-        return 'none'
-
-    @property
-    def path_(self):
-        return os.path.join(self.root, self.experiment, self.script_name_, self.name)
-
-    @property
-    def device_(self):
-        return self.gpus_[self.rank % self.nranks]
-
-
-@dataclass
-class ResourceSettings:
-    checkpoint: str = DefaultVal(None)
-    teacher_checkpoint: str = DefaultVal(None)
-    triples: str = DefaultVal(None)
-    teacher_triples: str = DefaultVal(None)
-    collection: str = DefaultVal(None)
-    queries: str = DefaultVal(None)
-    index_name: str = DefaultVal(None)
-
-
-@dataclass
-class DocSettings:
-    dim: int = DefaultVal(128)
-    doc_maxlen: int = DefaultVal(220)
-    mask_punctuation: bool = DefaultVal(True)
-
-
-@dataclass
-class QuerySettings:
-    query_maxlen: int = DefaultVal(32)
-    attend_to_mask_tokens : bool = DefaultVal(False)
-    interaction: str = DefaultVal('colbert')
-
-
-@dataclass
-class TrainingSettings:
-    similarity: str = DefaultVal('cosine')
-
-    bsize: int = DefaultVal(32)
-
-    accumsteps: int = DefaultVal(1)
-
-    lr: float = DefaultVal(3e-06)
-
-    maxsteps: int = DefaultVal(500_000)
-
-    save_every: int = DefaultVal(None)
-
-    resume: bool = DefaultVal(False)
-    resume_optimizer: bool = DefaultVal(False)
-
-    ## NEW:
-    warmup: int = DefaultVal(None)
-
-    warmup_bert: int = DefaultVal(None)
-
-    relu: bool = DefaultVal(False)
-
-    nway: int = DefaultVal(2)
-
-    use_ib_negatives: bool = DefaultVal(False)
-
-    reranker: bool = DefaultVal(False)
-
-    distillation_alpha: float = DefaultVal(1.0)
-
-    ignore_scores: bool = DefaultVal(False)
-
-    shuffle_every_epoch: bool = DefaultVal(False)
-
-    save_steps: int = DefaultVal(2000)
-    save_epochs: int = DefaultVal(-1)
-    epochs: int = DefaultVal(10)
-    input_arguments: dict = DefaultVal({})
-    model_type: str = DefaultVal('bert-base-uncased')
-    init_from_lm: str = DefaultVal(None)
-    local_models_repository: str = DefaultVal(None)
-    ranks_fn: str = DefaultVal(None)
-    topK: int = DefaultVal(100)
-
-    # used in distillation (Student/Teacher) training
-    student_teacher_temperature: float = DefaultVal(1.0)
-    student_teacher_top_loss_weight: float = DefaultVal(0.5)
-    teacher_model_type: str = DefaultVal('xlm-roberta-base')
-    teacher_doc_maxlen: int = DefaultVal(180)
-    distill_query_passage_separately: bool = DefaultVal(False)
-    query_only: bool = DefaultVal(False)
-    loss_function: str = DefaultVal(None)
-    query_weight: float = DefaultVal(0.5)
-
-    rng_seed: int = DefaultVal(12345)
-
-@dataclass
-class IndexingSettings:
-    index_path: str = DefaultVal(None)
-
-    nbits: int = DefaultVal(1)
-
-    kmeans_niters: int = DefaultVal(20)
-
-    num_partitions_max: int = DefaultVal(10000000)
-    @property
-    def index_path_(self):
-        return self.index_path or os.path.join(self.index_root_, self.index_name)
-
-@dataclass
-class SearchSettings:
-    ncells: int = DefaultVal(None)
-    centroid_score_threshold: float = DefaultVal(None)
-    ndocs: int = DefaultVal(None)
+import os
+import torch
+
+import __main__
+from dataclasses import dataclass
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp
+
+from .core_config import DefaultVal
+
+
+@dataclass
+class RunSettings:
+    """
+        The defaults here have a special status in Run(), which initially calls assign_defaults(),
+        so these aren't soft defaults in that specific context.
+    """
+
+    overwrite: bool = DefaultVal(False)
+
+    root: str = DefaultVal(os.path.join(os.getcwd(), 'experiments'))
+    experiment: str = DefaultVal('default')
+
+    index_root: str = DefaultVal(None)
+    name: str = DefaultVal(timestamp(daydir=True))
+
+    rank: int = DefaultVal(0)
+    nranks: int = DefaultVal(1)
+    amp: bool = DefaultVal(True)
+
+    total_visible_gpus = torch.cuda.device_count()
+    gpus: int = DefaultVal(total_visible_gpus)
+
+    @property
+    def gpus_(self):
+        value = self.gpus
+
+        if isinstance(value, int):
+            value = list(range(value))
+
+        if isinstance(value, str):
+            value = value.split(',')
+
+        value = list(map(int, value))
+        value = sorted(list(set(value)))
+
+        assert all(device_idx in range(0, self.total_visible_gpus) for device_idx in value), value
+
+        return value
+
+    @property
+    def index_root_(self):
+        return self.index_root or os.path.join(self.root, self.experiment, 'indexes/')
+
+    @property
+    def script_name_(self):
+        if '__file__' in dir(__main__):
+            cwd = os.path.abspath(os.getcwd())
+            script_path = os.path.abspath(__main__.__file__)
+            root_path = os.path.abspath(self.root)
+
+            if script_path.startswith(cwd):
+                script_path = script_path[len(cwd):]
+
+            else:
+                try:
+                    commonpath = os.path.commonpath([script_path, root_path])
+                    script_path = script_path[len(commonpath):]
+                except:
+                    pass
+
+
+            if script_path.endswith('bin/pytest'):
+                script_path = script_path + '.py'
+            assert script_path.endswith('.py'), (script_path, cwd)
+            script_name = script_path.replace('/', '.').strip('.')[:-3]
+
+            assert len(script_name) > 0, (script_name, script_path, cwd)
+
+            return script_name
+
+        return None
+
+    @property
+    def path_(self):
+        return os.path.join(self.root, self.experiment, self.script_name_, self.name) if self.script_name_ is not None else os.path.join(self.root, self.experiment, self.name)
+
+    @property
+    def device_(self):
+        return self.gpus_[self.rank % self.nranks]
+
+
+@dataclass
+class ResourceSettings:
+    checkpoint: str = DefaultVal(None)
+    teacher_checkpoint: str = DefaultVal(None)
+    triples: str = DefaultVal(None)
+    teacher_triples: str = DefaultVal(None)
+    collection: str = DefaultVal(None)
+    queries: str = DefaultVal(None)
+    index_name: str = DefaultVal(None)
+
+@dataclass
+class DocSettings:
+    dim: int = DefaultVal(128)
+    doc_maxlen: int = DefaultVal(180)
+    mask_punctuation: bool = DefaultVal(True)
+
+
+@dataclass
+class QuerySettings:
+    query_maxlen: int = DefaultVal(32)
+    attend_to_mask_tokens : bool = DefaultVal(False)
+    interaction: str = DefaultVal('colbert')
+
+
+@dataclass
+class TrainingSettings:
+    similarity: str = DefaultVal('cosine')
+
+    bsize: int = DefaultVal(32)
+
+    accumsteps: int = DefaultVal(1)
+
+    lr: float = DefaultVal(3e-06)
+
+    maxsteps: int = DefaultVal(500_000)
+
+    save_every: int = DefaultVal(None)
+
+    resume: bool = DefaultVal(False)
+    resume_optimizer: bool = DefaultVal(False)
+
+    ## NEW:
+    warmup: int = DefaultVal(None)
+
+    warmup_bert: int = DefaultVal(None)
+
+    relu: bool = DefaultVal(False)
+
+    nway: int = DefaultVal(2)
+
+    use_ib_negatives: bool = DefaultVal(False)
+
+    reranker: bool = DefaultVal(False)
+
+    distillation_alpha: float = DefaultVal(1.0)
+
+    ignore_scores: bool = DefaultVal(False)
+
+    shuffle_every_epoch: bool = DefaultVal(False)
+
+    save_steps: int = DefaultVal(2000)
+    save_epochs: int = DefaultVal(-1)
+    epochs: int = DefaultVal(10)
+    input_arguments: dict = DefaultVal({})
+    model_type: str = DefaultVal('bert-base-uncased')
+    init_from_lm: str = DefaultVal(None)
+    local_models_repository: str = DefaultVal(None)
+    ranks_fn: str = DefaultVal(None)
+    output_dir: str = DefaultVal(None)
+    topK: int = DefaultVal(100)
+
+    # used in distillation (Student/Teacher) training
+    student_teacher_temperature: float = DefaultVal(1.0)
+    student_teacher_top_loss_weight: float = DefaultVal(0.5)
+    teacher_model_type: str = DefaultVal('xlm-roberta-base')
+    teacher_doc_maxlen: int = DefaultVal(180)
+    distill_query_passage_separately: bool = DefaultVal(False)
+    query_only: bool = DefaultVal(False)
+    loss_function: str = DefaultVal(None)
+    query_weight: float = DefaultVal(0.5)
+
+    rng_seed: int = DefaultVal(12345)
+
+@dataclass
+class IndexingSettings:
+    index_path: str = DefaultVal(None)
+    index_location: str = DefaultVal(None)
+
+    nbits: int = DefaultVal(1)
+
+    kmeans_niters: int = DefaultVal(20)
+
+    num_partitions_max: int = DefaultVal(10000000)
+    @property
+    def index_path_(self):
+        return self.index_path or os.path.join(self.index_root_, self.index_name)
+
+@dataclass
+class SearchSettings:
+    ncells: int = DefaultVal(None)
+    centroid_score_threshold: float = DefaultVal(None)
+    ndocs: int = DefaultVal(None)
```

## primeqa/ir/dense/colbert_top/colbert/infra/utilities/create_triples.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-import random
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import save_metadata
-from primeqa.ir.dense.colbert_top.utility.supervision.triples import sample_for_query
-
-from primeqa.ir.dense.colbert_top.colbert.data.ranking import Ranking
-from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
-
-MAX_NUM_TRIPLES = 40_000_000
-
-
-class Triples:
-    def __init__(self, ranking, seed=12345):
-        random.seed(seed)  # TODO: Use internal RNG instead..
-        self.qid2rankings = Ranking.cast(ranking).todict()
-
-    def create(self, positives, depth):
-        assert all(len(x) == 2 for x in positives)
-        assert all(maxBest <= maxDepth for maxBest, maxDepth in positives), positives
-
-        Triples = []
-        NonEmptyQIDs = 0
-
-        for processing_idx, qid in enumerate(self.qid2rankings):
-            l = sample_for_query(qid, self.qid2rankings[qid], positives, depth, False, None)
-            NonEmptyQIDs += (len(l) > 0)
-            Triples.extend(l)
-
-            if processing_idx % (10_000) == 0:
-                print_message(f"#> Done with {processing_idx+1} questions!\t\t "
-                              f"{str(len(Triples) / 1000)}k triples for {NonEmptyQIDs} unqiue QIDs.")
-
-        print_message(f"#> Sub-sample the triples (if > {MAX_NUM_TRIPLES})..")
-        print_message(f"#> len(Triples) = {len(Triples)}")
-
-        if len(Triples) > MAX_NUM_TRIPLES:
-            Triples = random.sample(Triples, MAX_NUM_TRIPLES)
-
-        ### Prepare the triples ###
-        print_message("#> Shuffling the triples...")
-        random.shuffle(Triples)
-
-        self.Triples = Examples(data=Triples)
-
-        return Triples
-
-    def save(self, new_path):
-        Examples(data=self.Triples).save(new_path)
-
-        # save_metadata(f'{output}.meta', args)  # TODO: What args to save?? {seed, positives, depth, rankings if path or else whatever provenance the rankings object shares}
-
+import random
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import save_metadata
+from primeqa.ir.dense.colbert_top.utility.supervision.triples import sample_for_query
+
+from primeqa.ir.dense.colbert_top.colbert.data.ranking import Ranking
+from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
+
+MAX_NUM_TRIPLES = 40_000_000
+
+
+class Triples:
+    def __init__(self, ranking, seed=12345):
+        random.seed(seed)  # TODO: Use internal RNG instead..
+        self.qid2rankings = Ranking.cast(ranking).todict()
+
+    def create(self, positives, depth):
+        assert all(len(x) == 2 for x in positives)
+        assert all(maxBest <= maxDepth for maxBest, maxDepth in positives), positives
+
+        Triples = []
+        NonEmptyQIDs = 0
+
+        for processing_idx, qid in enumerate(self.qid2rankings):
+            l = sample_for_query(qid, self.qid2rankings[qid], positives, depth, False, None)
+            NonEmptyQIDs += (len(l) > 0)
+            Triples.extend(l)
+
+            if processing_idx % (10_000) == 0:
+                print_message(f"#> Done with {processing_idx+1} questions!\t\t "
+                              f"{str(len(Triples) / 1000)}k triples for {NonEmptyQIDs} unqiue QIDs.")
+
+        print_message(f"#> Sub-sample the triples (if > {MAX_NUM_TRIPLES})..")
+        print_message(f"#> len(Triples) = {len(Triples)}")
+
+        if len(Triples) > MAX_NUM_TRIPLES:
+            Triples = random.sample(Triples, MAX_NUM_TRIPLES)
+
+        ### Prepare the triples ###
+        print_message("#> Shuffling the triples...")
+        random.shuffle(Triples)
+
+        self.Triples = Examples(data=Triples)
+
+        return Triples
+
+    def save(self, new_path):
+        Examples(data=self.Triples).save(new_path)
+
+        # save_metadata(f'{output}.meta', args)  # TODO: What args to save?? {seed, positives, depth, rankings if path or else whatever provenance the rankings object shares}
+
```

## primeqa/ir/dense/colbert_top/colbert/infra/utilities/minicorpus.py

 * *Ordering differences only*

```diff
@@ -1,64 +1,64 @@
-import os
-import random
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory
-
-from primeqa.ir.dense.colbert_top.colbert.data import Collection, Queries, Ranking
-
-
-def sample_minicorpus(name, factor, topk=30, maxdev=3000):
-    """
-    Factor:
-        * nano=1
-        * micro=10
-        * mini=100
-        * small=100 with topk=100
-        * medium=150 with topk=300
-    """
-
-    random.seed(12345)
-
-    # Load collection
-    collection = Collection(path='/dfs/scratch0/okhattab/OpenQA/collection.tsv')
-
-    # Load train and dev queries
-    qas_train = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/qas.json').qas()
-    qas_dev = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/qas.json').qas()
-
-    # Load train and dev C3 rankings
-    ranking_train = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/rankings/C3.tsv.annotated').todict()
-    ranking_dev = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/rankings/C3.tsv.annotated').todict()
-
-    # Sample NT and ND queries from each, keep only the top-k passages for those
-    sample_train = random.sample(list(qas_train.keys()), min(len(qas_train.keys()), 300*factor))
-    sample_dev = random.sample(list(qas_dev.keys()), min(len(qas_dev.keys()), maxdev, 30*factor))
-
-    train_pids = [pid for qid in sample_train for qpids in ranking_train[qid][:topk] for pid in qpids]
-    dev_pids = [pid for qid in sample_dev for qpids in ranking_dev[qid][:topk] for pid in qpids]
-
-    sample_pids = sorted(list(set(train_pids + dev_pids)))
-    print(f'len(sample_pids) = {len(sample_pids)}')
-
-    # Save the new query sets: train and dev
-    ROOT = f'/future/u/okhattab/root/unit/data/NQ-{name}'
-
-    create_directory(os.path.join(ROOT, 'train'))
-    create_directory(os.path.join(ROOT, 'dev'))
-
-    new_train = Queries(data={qid: qas_train[qid] for qid in sample_train})
-    new_train.save(os.path.join(ROOT, 'train/questions.tsv'))
-    new_train.save_qas(os.path.join(ROOT, 'train/qas.json'))
-
-    new_dev = Queries(data={qid: qas_dev[qid] for qid in sample_dev})
-    new_dev.save(os.path.join(ROOT, 'dev/questions.tsv'))
-    new_dev.save_qas(os.path.join(ROOT, 'dev/qas.json'))
-
-    # Save the new collection
-    print(f"Saving to {os.path.join(ROOT, 'collection.tsv')}")
-    Collection(data=[collection[pid] for pid in sample_pids]).save(os.path.join(ROOT, 'collection.tsv'))
-
-    print('#> Done!')
-
-
-if __name__ == '__main__':
-    sample_minicorpus('medium', 150, topk=300)
+import os
+import random
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory
+
+from primeqa.ir.dense.colbert_top.colbert.data import Collection, Queries, Ranking
+
+
+def sample_minicorpus(name, factor, topk=30, maxdev=3000):
+    """
+    Factor:
+        * nano=1
+        * micro=10
+        * mini=100
+        * small=100 with topk=100
+        * medium=150 with topk=300
+    """
+
+    random.seed(12345)
+
+    # Load collection
+    collection = Collection(path='/dfs/scratch0/okhattab/OpenQA/collection.tsv')
+
+    # Load train and dev queries
+    qas_train = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/qas.json').qas()
+    qas_dev = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/qas.json').qas()
+
+    # Load train and dev C3 rankings
+    ranking_train = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/rankings/C3.tsv.annotated').todict()
+    ranking_dev = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/rankings/C3.tsv.annotated').todict()
+
+    # Sample NT and ND queries from each, keep only the top-k passages for those
+    sample_train = random.sample(list(qas_train.keys()), min(len(qas_train.keys()), 300*factor))
+    sample_dev = random.sample(list(qas_dev.keys()), min(len(qas_dev.keys()), maxdev, 30*factor))
+
+    train_pids = [pid for qid in sample_train for qpids in ranking_train[qid][:topk] for pid in qpids]
+    dev_pids = [pid for qid in sample_dev for qpids in ranking_dev[qid][:topk] for pid in qpids]
+
+    sample_pids = sorted(list(set(train_pids + dev_pids)))
+    print(f'len(sample_pids) = {len(sample_pids)}')
+
+    # Save the new query sets: train and dev
+    ROOT = f'/future/u/okhattab/root/unit/data/NQ-{name}'
+
+    create_directory(os.path.join(ROOT, 'train'))
+    create_directory(os.path.join(ROOT, 'dev'))
+
+    new_train = Queries(data={qid: qas_train[qid] for qid in sample_train})
+    new_train.save(os.path.join(ROOT, 'train/questions.tsv'))
+    new_train.save_qas(os.path.join(ROOT, 'train/qas.json'))
+
+    new_dev = Queries(data={qid: qas_dev[qid] for qid in sample_dev})
+    new_dev.save(os.path.join(ROOT, 'dev/questions.tsv'))
+    new_dev.save_qas(os.path.join(ROOT, 'dev/qas.json'))
+
+    # Save the new collection
+    print(f"Saving to {os.path.join(ROOT, 'collection.tsv')}")
+    Collection(data=[collection[pid] for pid in sample_pids]).save(os.path.join(ROOT, 'collection.tsv'))
+
+    print('#> Done!')
+
+
+if __name__ == '__main__':
+    sample_minicorpus('medium', 150, topk=300)
```

## primeqa/ir/dense/colbert_top/colbert/modeling/base_colbert.py

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-import os
-import torch
-
-from transformers import AutoTokenizer
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_colbert_from_pretrained
-from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
-
-class BaseColBERT(torch.nn.Module):
-    """
-    Shallow module that wraps the ColBERT parameters, custom configuration, and underlying tokenizer.
-    This class provides direct instantiation and saving of the model/colbert_config/tokenizer package.
-
-    Like HF, evaluation mode is the default.
-    """
-
-    def __init__(self, name, colbert_config=None):
-        super().__init__()
-
-        print_message(f"#>>>>> at BaseColBERT name (model type) : {name}")
-
-        self.name = name
-        self.colbert_config = ColBERTConfig.from_existing(ColBERTConfig.load_from_checkpoint(name), colbert_config)
-        # self.colbert_config = colbert_config
-        # checkpoint_config = ColBERTConfig.load_from_checkpoint(name)
-        # self.colbert_config.model_type = checkpoint_config.model_type
-
-        self.model = get_colbert_from_pretrained(name, colbert_config=self.colbert_config)
-
-        self.raw_tokenizer = AutoTokenizer.from_pretrained(self.model.base)
-        # self.raw_tokenizer = None
-        # TEMP fix
-        # self.raw_tokenizer = get_doc_tokenizer(colbert_config.model_type, colbert_config.doc_maxlen)
-
-        self.eval()
-
-    @property
-    def device(self):
-        return self.model.device
-
-    @property
-    def bert(self):
-        return self.model.bert
-
-    @property
-    def linear(self):
-        return self.model.linear
-    
-    @property
-    def score_scaler(self):
-        return self.model.score_scaler
-
-    def save(self, path):
-        assert not path.endswith('.dnn'), f"{path}: We reserve *.dnn names for the deprecated checkpoint format."
-
-        self.model.save_pretrained(path)
-        self.raw_tokenizer.save_pretrained(path)
-
-        self.colbert_config.save_for_checkpoint(path)
-
-
+import os
+import torch
+
+from transformers import AutoTokenizer
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+from primeqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_colbert_from_pretrained
+from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
+
+class BaseColBERT(torch.nn.Module):
+    """
+    Shallow module that wraps the ColBERT parameters, custom configuration, and underlying tokenizer.
+    This class provides direct instantiation and saving of the model/colbert_config/tokenizer package.
+
+    Like HF, evaluation mode is the default.
+    """
+
+    def __init__(self, name, colbert_config=None):
+        super().__init__()
+
+        print_message(f"#>>>>> at BaseColBERT name (model type) : {name}")
+
+        self.name = name
+        self.colbert_config = ColBERTConfig.from_existing(ColBERTConfig.load_from_checkpoint(name), colbert_config)
+        # self.colbert_config = colbert_config
+        # checkpoint_config = ColBERTConfig.load_from_checkpoint(name)
+        # self.colbert_config.model_type = checkpoint_config.model_type
+
+        self.model = get_colbert_from_pretrained(name, colbert_config=self.colbert_config)
+
+        self.raw_tokenizer = AutoTokenizer.from_pretrained(self.model.base)
+        # self.raw_tokenizer = None
+        # TEMP fix
+        # self.raw_tokenizer = get_doc_tokenizer(colbert_config.model_type, colbert_config.doc_maxlen)
+
+        self.eval()
+
+    @property
+    def device(self):
+        return self.model.device
+
+    @property
+    def bert(self):
+        return self.model.bert
+
+    @property
+    def linear(self):
+        return self.model.linear
+    
+    @property
+    def score_scaler(self):
+        return self.model.score_scaler
+
+    def save(self, path):
+        assert not path.endswith('.dnn'), f"{path}: We reserve *.dnn names for the deprecated checkpoint format."
+
+        self.model.save_pretrained(path)
+        self.raw_tokenizer.save_pretrained(path)
+
+        self.colbert_config.save_for_checkpoint(path)
+
+
```

## primeqa/ir/dense/colbert_top/colbert/modeling/checkpoint.py

 * *Ordering differences only*

```diff
@@ -1,165 +1,165 @@
-import torch
-
-from tqdm import tqdm
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import QueryTokenizer, DocTokenizer
-from primeqa.ir.dense.colbert_top.colbert.utils.amp import MixedPrecisionManager
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
-from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-class Checkpoint(ColBERT):
-    """
-        Easy inference with ColBERT.
-
-        TODO: Add .cast() accepting [also] an object instance-of(Checkpoint) as first argument.
-    """
-
-    def __init__(self, name, colbert_config=None):
-
-        super().__init__(name, colbert_config)
-        assert self.training is False
-
-        # get model type from checkpoint
-        if name.endswith('.dnn') or name.endswith('.model'):
-            dnn_checkpoint = torch_load_dnn(colbert_config.checkpoint)
-            model_type = dnn_checkpoint['model_type']
-        else:
-            model_type=name
-
-        self.query_tokenizer = get_query_tokenizer(model_type, colbert_config.query_maxlen, colbert_config.attend_to_mask_tokens)
-        self.doc_tokenizer = get_doc_tokenizer(model_type, colbert_config.doc_maxlen)
-
-        self.amp_manager = MixedPrecisionManager(True)
-
-        self.docFromText_used = False
-
-    def query(self, *args, to_cpu=False, **kw_args):
-        with torch.no_grad():
-            with self.amp_manager.context():
-                Q = super().query(*args, **kw_args)
-                return Q.cpu() if to_cpu else Q
-
-    def doc(self, *args, to_cpu=False, **kw_args):
-        with torch.no_grad():
-            with self.amp_manager.context():
-                D = super().doc(*args, **kw_args)
-
-                if to_cpu:
-                    return (D[0].cpu(), *D[1:]) if isinstance(D, tuple) else D.cpu()
-
-                return D
-
-    def queryFromText(self, queries, bsize=None, to_cpu=False, context=None):
-        if bsize:
-            batches = self.query_tokenizer.tensorize(queries, context=context, bsize=bsize)
-            batches = [self.query(input_ids, attention_mask, to_cpu=to_cpu) for input_ids, attention_mask in batches]
-            return torch.cat(batches)
-
-        input_ids, attention_mask = self.query_tokenizer.tensorize(queries, context=context)
-        return self.query(input_ids, attention_mask)
-
-    def docFromText(self, docs, bsize=None, keep_dims=True, to_cpu=False, showprogress=False, return_tokens=False):
-        assert keep_dims in [True, False, 'flatten']
-
-        if not self.docFromText_used:
-            print_message(f"#> checkpoint, docFromText, Input: {docs[0]}, \t\t {bsize}")
-
-        if bsize:
-            text_batches, reverse_indices = self.doc_tokenizer.tensorize(docs, bsize=bsize)
-
-            if not self.docFromText_used:
-                print_message(f"#> checkpoint, docFromText, Output IDs: {text_batches[0]}")
-                self.docFromText_used = True
-
-            returned_text = []
-            if return_tokens:
-                returned_text = [text for batch in text_batches for text in batch[0]]
-                returned_text = [returned_text[idx] for idx in reverse_indices.tolist()]
-                returned_text = [returned_text]
-
-            keep_dims_ = 'return_mask' if keep_dims == 'flatten' else keep_dims
-            batches = [self.doc(input_ids, attention_mask, keep_dims=keep_dims_, to_cpu=to_cpu)
-                       for input_ids, attention_mask in tqdm(text_batches, disable=not showprogress)]
-
-            if keep_dims is True:
-                D = _stack_3D_tensors(batches)
-                return (D[reverse_indices], *returned_text)
-
-            elif keep_dims == 'flatten':
-                D, mask = [], []
-
-                for D_, mask_ in batches:
-                    D.append(D_)
-                    mask.append(mask_)
-
-                D, mask = torch.cat(D)[reverse_indices], torch.cat(mask)[reverse_indices]
-
-                doclens = mask.squeeze(-1).sum(-1).tolist()
-
-                D = D.view(-1, self.colbert_config.dim)
-                D = D[mask.bool().flatten()].cpu()
-
-                return (D, doclens, *returned_text)
-
-            assert keep_dims is False
-
-            D = [d for batch in batches for d in batch]
-            return ([D[idx] for idx in reverse_indices.tolist()], *returned_text)
-
-        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)
-        return self.doc(input_ids, attention_mask, keep_dims=keep_dims, to_cpu=to_cpu)
-
-    def lazy_rank(self, queries, docs):
-        Q = self.queryFromText(queries, bsize=128, to_cpu=True)
-        D = self.docFromText(docs, bsize=128, to_cpu=True)
-
-        assert False, "Implement scoring"
-
-    def score(self, Q, D, mask=None, lengths=None):
-        assert False, "Call colbert_score"
-        # EVENTUALLY: Just call the colbert_score function!
-
-        if lengths is not None:
-            assert mask is None, "don't supply both mask and lengths"
-
-            mask = torch.arange(D.size(1), device=self.device) + 1
-            mask = mask.unsqueeze(0) <= lengths.to(self.device).unsqueeze(-1)
-
-        scores = (D @ Q)
-        scores = scores if mask is None else scores * mask.unsqueeze(-1)
-        scores = scores.max(1)
-
-        return scores.values.sum(-1).cpu()
-
-
-def _stack_3D_tensors(groups):
-    bsize = sum([x.size(0) for x in groups])
-    maxlen = max([x.size(1) for x in groups])
-    hdim = groups[0].size(2)
-
-    output = torch.zeros(bsize, maxlen, hdim, device=groups[0].device, dtype=groups[0].dtype)
-
-    offset = 0
-    for x in groups:
-        endpos = offset + x.size(0)
-        output[offset:endpos, :x.size(1)] = x
-        offset = endpos
-
-    return output
-
-
-"""
-TODO:
-
-def tokenize_and_encode(checkpoint, passages):
-    embeddings, token_ids = checkpoint.docFromText(passages, bsize=128, keep_dims=False, showprogress=True, return_tokens=True)
-    tokens = [checkpoint.doc_tokenizer.tok.convert_ids_to_tokens(ids.tolist()) for ids in token_ids]
-    tokens = [tokens[:tokens.index('[PAD]') if '[PAD]' in tokens else -1] for tokens in tokens]
-    tokens = [[tok for tok in tokens if tok not in checkpoint.skiplist] for tokens in tokens]
-
-    return embeddings, tokens
-
-"""
+import torch
+
+from tqdm import tqdm
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import QueryTokenizer, DocTokenizer
+from primeqa.ir.dense.colbert_top.colbert.utils.amp import MixedPrecisionManager
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
+from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+class Checkpoint(ColBERT):
+    """
+        Easy inference with ColBERT.
+
+        TODO: Add .cast() accepting [also] an object instance-of(Checkpoint) as first argument.
+    """
+
+    def __init__(self, name, colbert_config=None):
+
+        super().__init__(name, colbert_config)
+        assert self.training is False
+
+        # get model type from checkpoint
+        if name.endswith('.dnn') or name.endswith('.model'):
+            dnn_checkpoint = torch_load_dnn(colbert_config.checkpoint)
+            model_type = dnn_checkpoint['model_type']
+        else:
+            model_type=name
+
+        self.query_tokenizer = get_query_tokenizer(model_type, colbert_config.query_maxlen, colbert_config.attend_to_mask_tokens)
+        self.doc_tokenizer = get_doc_tokenizer(model_type, colbert_config.doc_maxlen)
+
+        self.amp_manager = MixedPrecisionManager(True)
+
+        self.docFromText_used = False
+
+    def query(self, *args, to_cpu=False, **kw_args):
+        with torch.no_grad():
+            with self.amp_manager.context():
+                Q = super().query(*args, **kw_args)
+                return Q.cpu() if to_cpu else Q
+
+    def doc(self, *args, to_cpu=False, **kw_args):
+        with torch.no_grad():
+            with self.amp_manager.context():
+                D = super().doc(*args, **kw_args)
+
+                if to_cpu:
+                    return (D[0].cpu(), *D[1:]) if isinstance(D, tuple) else D.cpu()
+
+                return D
+
+    def queryFromText(self, queries, bsize=None, to_cpu=False, context=None):
+        if bsize:
+            batches = self.query_tokenizer.tensorize(queries, context=context, bsize=bsize)
+            batches = [self.query(input_ids, attention_mask, to_cpu=to_cpu) for input_ids, attention_mask in batches]
+            return torch.cat(batches)
+
+        input_ids, attention_mask = self.query_tokenizer.tensorize(queries, context=context)
+        return self.query(input_ids, attention_mask)
+
+    def docFromText(self, docs, bsize=None, keep_dims=True, to_cpu=False, showprogress=False, return_tokens=False):
+        assert keep_dims in [True, False, 'flatten']
+
+        if not self.docFromText_used:
+            print_message(f"#> checkpoint, docFromText, Input: {docs[0]}, \t\t {bsize}")
+
+        if bsize:
+            text_batches, reverse_indices = self.doc_tokenizer.tensorize(docs, bsize=bsize)
+
+            if not self.docFromText_used:
+                print_message(f"#> checkpoint, docFromText, Output IDs: {text_batches[0]}")
+                self.docFromText_used = True
+
+            returned_text = []
+            if return_tokens:
+                returned_text = [text for batch in text_batches for text in batch[0]]
+                returned_text = [returned_text[idx] for idx in reverse_indices.tolist()]
+                returned_text = [returned_text]
+
+            keep_dims_ = 'return_mask' if keep_dims == 'flatten' else keep_dims
+            batches = [self.doc(input_ids, attention_mask, keep_dims=keep_dims_, to_cpu=to_cpu)
+                       for input_ids, attention_mask in tqdm(text_batches, disable=not showprogress)]
+
+            if keep_dims is True:
+                D = _stack_3D_tensors(batches)
+                return (D[reverse_indices], *returned_text)
+
+            elif keep_dims == 'flatten':
+                D, mask = [], []
+
+                for D_, mask_ in batches:
+                    D.append(D_)
+                    mask.append(mask_)
+
+                D, mask = torch.cat(D)[reverse_indices], torch.cat(mask)[reverse_indices]
+
+                doclens = mask.squeeze(-1).sum(-1).tolist()
+
+                D = D.view(-1, self.colbert_config.dim)
+                D = D[mask.bool().flatten()].cpu()
+
+                return (D, doclens, *returned_text)
+
+            assert keep_dims is False
+
+            D = [d for batch in batches for d in batch]
+            return ([D[idx] for idx in reverse_indices.tolist()], *returned_text)
+
+        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)
+        return self.doc(input_ids, attention_mask, keep_dims=keep_dims, to_cpu=to_cpu)
+
+    def lazy_rank(self, queries, docs):
+        Q = self.queryFromText(queries, bsize=128, to_cpu=True)
+        D = self.docFromText(docs, bsize=128, to_cpu=True)
+
+        assert False, "Implement scoring"
+
+    def score(self, Q, D, mask=None, lengths=None):
+        assert False, "Call colbert_score"
+        # EVENTUALLY: Just call the colbert_score function!
+
+        if lengths is not None:
+            assert mask is None, "don't supply both mask and lengths"
+
+            mask = torch.arange(D.size(1), device=self.device) + 1
+            mask = mask.unsqueeze(0) <= lengths.to(self.device).unsqueeze(-1)
+
+        scores = (D @ Q)
+        scores = scores if mask is None else scores * mask.unsqueeze(-1)
+        scores = scores.max(1)
+
+        return scores.values.sum(-1).cpu()
+
+
+def _stack_3D_tensors(groups):
+    bsize = sum([x.size(0) for x in groups])
+    maxlen = max([x.size(1) for x in groups])
+    hdim = groups[0].size(2)
+
+    output = torch.zeros(bsize, maxlen, hdim, device=groups[0].device, dtype=groups[0].dtype)
+
+    offset = 0
+    for x in groups:
+        endpos = offset + x.size(0)
+        output[offset:endpos, :x.size(1)] = x
+        offset = endpos
+
+    return output
+
+
+"""
+TODO:
+
+def tokenize_and_encode(checkpoint, passages):
+    embeddings, token_ids = checkpoint.docFromText(passages, bsize=128, keep_dims=False, showprogress=True, return_tokens=True)
+    tokens = [checkpoint.doc_tokenizer.tok.convert_ids_to_tokens(ids.tolist()) for ids in token_ids]
+    tokens = [tokens[:tokens.index('[PAD]') if '[PAD]' in tokens else -1] for tokens in tokens]
+    tokens = [[tok for tok in tokens if tok not in checkpoint.skiplist] for tokens in tokens]
+
+    return embeddings, tokens
+
+"""
```

## primeqa/ir/dense/colbert_top/colbert/modeling/colbert.py

```diff
@@ -1,268 +1,269 @@
-from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, flatten, print_torch_extension_error_message
-from primeqa.ir.dense.colbert_top.colbert.modeling.base_colbert import BaseColBERT
-from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
-
-import torch
-import string
-
-import random
-import numpy as np
-
-import os
-import pathlib
-from torch.utils.cpp_extension import load
-import sys
-
-class ColBERT(BaseColBERT):
-    """
-        This class handles the basic encoding and scoring operations in ColBERT. It is used for training.
-    """
-
-    def __init__(self, name='bert-base-uncased', colbert_config=None):
-
-        print_message(f"#>>>>> at ColBERT name (model type) : {name}")
-
-        super().__init__(name, colbert_config)
-        self.use_gpu = torch.cuda.is_available()
-
-        ColBERT.try_load_torch_extensions(self.use_gpu)
-
-        if self.colbert_config.mask_punctuation:
-            self.skiplist = {w: True
-                             for symbol in string.punctuation
-                             for w in [symbol, self.raw_tokenizer.encode(symbol, add_special_tokens=False)[0]]}
-        self.query_used = False
-        self.doc_used = False
-
-    @classmethod
-    def try_load_torch_extensions(cls, use_gpu):
-        if hasattr(cls, "loaded_extensions") or use_gpu:
-            return
-
-        verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True"
-
-        print_message(f"Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
-        try:
-            segmented_maxsim_cpp = load(
-                name="segmented_maxsim_cpp",
-                sources=[
-                    os.path.join(
-                        pathlib.Path(__file__).parent.resolve(), "segmented_maxsim.cpp"
-                    ),
-                ],
-                extra_cflags=["-O3"],
-                verbose=verbose,
-            )
-        except (RuntimeError, KeyboardInterrupt) as e:
-            if not verbose:
-                import traceback
-                traceback.print_exc()
-            print_torch_extension_error_message()
-            sys.exit(1)
-        cls.segmented_maxsim = segmented_maxsim_cpp.segmented_maxsim_cpp
-
-        cls.loaded_extensions = True
-
-    def forward(self, Q, D):
-        Q = self.query(*Q)
-        D, D_mask = self.doc(*D, keep_dims='return_mask')
-
-        # Repeat each query encoding for every corresponding document.
-        Q_duplicated = Q.repeat_interleave(self.colbert_config.nway, dim=0).contiguous()
-        scores = self.score(Q_duplicated, D, D_mask)
-
-        if self.colbert_config.distill_query_passage_separately :
-            if self.colbert_config.query_only:
-                return Q
-            else :
-                return scores, Q_duplicated, D
-
-        if self.colbert_config.use_ib_negatives:
-            ib_loss = self.compute_ib_loss(Q, D, D_mask)
-            return scores, ib_loss
-
-        return scores
-
-    def compute_ib_loss(self, Q, D, D_mask):
-        # TODO: Organize the code below! Quite messy.
-        if DEVICE == torch.device("cuda"):
-            scores = (D.unsqueeze(0) @ Q.permute(0, 2, 1).unsqueeze(1)).flatten(0, 1)  # query-major unsqueeze
-        else:
-            scores = (D.unsqueeze(0).float() @ Q.permute(0, 2, 1).unsqueeze(1)).flatten(0, 1)
-
-        scores = colbert_score_reduce(scores, D_mask.repeat(Q.size(0), 1, 1), self.colbert_config)
-
-        nway = self.colbert_config.nway
-        all_except_self_negatives = [list(range(qidx*D.size(0), qidx*D.size(0) + nway*qidx+1)) +
-                                     list(range(qidx*D.size(0) + nway * (qidx+1), qidx*D.size(0) + D.size(0)))
-                                     for qidx in range(Q.size(0))]
-
-        scores = scores[flatten(all_except_self_negatives)]
-        scores = scores.view(Q.size(0), -1)  # D.size(0) - self.colbert_config.nway + 1)
-
-        labels = torch.arange(0, Q.size(0), device=scores.device) * (self.colbert_config.nway)
-
-        return torch.nn.CrossEntropyLoss()(scores, labels)
-
-    def query(self, input_ids, attention_mask):
-        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(self.device)
-        # print query input_ids
-        if not self.query_used:
-
-            print_message("#>>>> colbert query ==")
-            print_message(f"#>>>>> input_ids: {input_ids[0].size()}, {input_ids[0]}")
-
-        Q = self.bert(input_ids, attention_mask=attention_mask)[0]
-
-        # print out Q
-        if not self.query_used:
-            print_message("#>>>> before linear query ==")
-            print_message(f"#>>>>> Q: {Q[0].size()}, {Q[0]}")
-            print_message(f"#>>>>> self.linear query : {self.linear.weight}")
-
-        Q = self.linear(Q)
-
-        if not self.query_used:
-            self.query_used = True
-
-            print_message("#>>>> colbert query ==")
-            print_message(f"#>>>>> Q: {Q[0].size()}, {Q[0]}")
-
-
-        mask = torch.tensor(self.mask(input_ids, skiplist=[]), device=self.device).unsqueeze(2).float()
-        Q = Q * mask
-
-        return torch.nn.functional.normalize(Q, p=2, dim=2)
-
-    def doc(self, input_ids, attention_mask, keep_dims=True):
-        assert keep_dims in [True, False, 'return_mask']
-
-        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(self.device)
-        # print doc input_ids
-        # [     0,   9749,   4960,  40455,      6,  58745,  48302,    136,   4343,   71,
-        if not self.doc_used:
-            print_message("#>>>> colbert doc ==")
-            print_message(f"#>>>>> input_ids: {input_ids[0].size()}, {input_ids[0]}")
-
-        D = self.bert(input_ids, attention_mask=attention_mask)[0]
-
-        if not self.doc_used:
-            print_message("#>>>> before linear doc ==")
-            print_message(f"#>>>>> D: {D[0].size()}, {D[0]}")
-
-            print_message(f"#>>>>> self.linear doc : {self.linear.weight}")
-
-        D = self.linear(D)
-
-        # print out D
-        if not self.doc_used:
-            self.doc_used = True
-
-            print_message("#>>>> colbert doc ==")
-            print_message(f"#>>>>> D: {D[0].size()}, {D[0]}")
-
-
-        mask = torch.tensor(self.mask(input_ids, skiplist=self.skiplist), device=self.device).unsqueeze(2).float()
-        D = D * mask
-
-        D = torch.nn.functional.normalize(D, p=2, dim=2)
-        if self.use_gpu:
-            D = D.half()
-
-        if keep_dims is False:
-            D, mask = D.cpu(), mask.bool().cpu().squeeze(-1)
-            D = [d[mask[idx]] for idx, d in enumerate(D)]
-
-        elif keep_dims == 'return_mask':
-            return D, mask.bool()
-
-        return D
-
-    def score(self, Q, D_padded, D_mask):
-        # assert self.colbert_config.similarity == 'cosine'
-
-        if self.colbert_config.similarity == 'l2':
-            assert self.colbert_config.interaction == 'colbert'
-            return (-1.0 * ((Q.unsqueeze(2) - D_padded.unsqueeze(1))**2).sum(-1)).max(-1).values.sum(-1)
-
-        return colbert_score(Q, D_padded, D_mask, config=self.colbert_config)
-
-    def mask(self, input_ids, skiplist):
-        mask = [[(x not in skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]
-        return mask
-
-
-# TODO: In Query/DocTokenizer, use colbert.raw_tokenizer
-
-# TODO: The masking below might also be applicable in the kNN part
-def colbert_score_reduce(scores_padded, D_mask, config: ColBERTConfig):
-    D_padding = ~D_mask.view(scores_padded.size(0), scores_padded.size(1)).bool()
-    scores_padded[D_padding] = -9999
-    scores = scores_padded.max(1).values
-
-    assert config.interaction in ['colbert', 'flipr'], config.interaction
-
-    if config.interaction == 'flipr':
-        assert config.query_maxlen == 64, ("for now", config)
-        # assert scores.size(1) == config.query_maxlen, scores.size()
-
-        K1 = config.query_maxlen // 2
-        K2 = 8
-
-        A = scores[:, :config.query_maxlen].topk(K1, dim=-1).values.sum(-1)
-        B = 0
-
-        if K2 <= scores.size(1) - config.query_maxlen:
-            B = scores[:, config.query_maxlen:].topk(K2, dim=-1).values.sum(1)
-
-        return A + B
-
-    return scores.sum(-1)
-
-
-# TODO: Wherever this is called, pass `config=`
-def colbert_score(Q, D_padded, D_mask, config=ColBERTConfig()):
-    """
-        Supply sizes Q = (1 | num_docs, *, dim) and D = (num_docs, *, dim).
-        If Q.size(0) is 1, the matrix will be compared with all passages.
-        Otherwise, each query matrix will be compared against the *aligned* passage.
-
-        EVENTUALLY: Consider masking with -inf for the maxsim (or enforcing a ReLU).
-    """
-
-    use_gpu = torch.cuda.is_available()
-    if use_gpu:
-        Q, D_padded, D_mask = Q.cuda(), D_padded.cuda(), D_mask.cuda()
-
-    assert Q.dim() == 3, Q.size()
-    assert D_padded.dim() == 3, D_padded.size()
-    assert Q.size(0) in [1, D_padded.size(0)]
-
-    scores = D_padded @ Q.to(dtype=D_padded.dtype).permute(0, 2, 1)
-
-    return colbert_score_reduce(scores, D_mask, config)
-
-
-def colbert_score_packed(Q, D_packed, D_lengths, config=ColBERTConfig()):
-    """
-        Works with a single query only.
-    """
-    use_gpu = torch.cuda.is_available()
-    if use_gpu:
-        Q, D_packed, D_lengths = Q.cuda(), D_packed.cuda(), D_lengths.cuda()
-
-    Q = Q.squeeze(0)
-
-    assert Q.dim() == 2, Q.size()
-    assert D_packed.dim() == 2, D_packed.size()
-
-    scores = D_packed @ Q.to(dtype=D_packed.dtype).T
-
-    if use_gpu or config.interaction == "flipr":
-        scores_padded, scores_mask = StridedTensor(scores, D_lengths, use_gpu=use_gpu).as_padded_tensor()
-
-        return colbert_score_reduce(scores_padded, scores_mask, config)
-    else:
-        return ColBERT.segmented_maxsim(scores, D_lengths)
+from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, flatten, print_torch_extension_error_message
+from primeqa.ir.dense.colbert_top.colbert.modeling.base_colbert import BaseColBERT
+from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
+
+import torch
+import string
+
+import random
+import numpy as np
+
+import os
+import pathlib
+from torch.utils.cpp_extension import load
+import sys
+
+class ColBERT(BaseColBERT):
+    """
+        This class handles the basic encoding and scoring operations in ColBERT. It is used for training.
+    """
+
+    def __init__(self, name='bert-base-uncased', colbert_config=None):
+
+        print_message(f"#>>>>> at ColBERT name (model type) : {name}")
+
+        super().__init__(name, colbert_config)
+        self.use_gpu = torch.cuda.is_available()
+
+        ColBERT.try_load_torch_extensions(self.use_gpu)
+
+        if self.colbert_config.mask_punctuation:
+            self.skiplist = {w: True
+                             for symbol in string.punctuation
+                             for w in [symbol, self.raw_tokenizer.encode(symbol, add_special_tokens=False)[0]]}
+        self.query_used = False
+        self.doc_used = False
+
+    @classmethod
+    def try_load_torch_extensions(cls, use_gpu):
+        if hasattr(cls, "loaded_extensions") or use_gpu:
+            return
+
+        verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True"
+
+        print_message(f"Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
+        try:
+            segmented_maxsim_cpp = load(
+                name="segmented_maxsim_cpp",
+                sources=[
+                    os.path.join(
+                        pathlib.Path(__file__).parent.resolve(), "segmented_maxsim.cpp"
+                    ),
+                ],
+                extra_cflags=["-O3"],
+                verbose=verbose,
+            )
+        except (RuntimeError, KeyboardInterrupt) as e:
+            if not verbose:
+                import traceback
+                traceback.print_exc()
+            print_torch_extension_error_message()
+            sys.exit(1)
+        cls.segmented_maxsim = segmented_maxsim_cpp.segmented_maxsim_cpp
+
+        cls.loaded_extensions = True
+
+    def forward(self, Q, D):
+        Q = self.query(*Q)
+        D, D_mask = self.doc(*D, keep_dims='return_mask')
+
+        # Repeat each query encoding for every corresponding document.
+        Q_duplicated = Q.repeat_interleave(self.colbert_config.nway, dim=0).contiguous()
+        scores = self.score(Q_duplicated, D, D_mask)
+
+        if self.colbert_config.distill_query_passage_separately :
+            if self.colbert_config.query_only:
+                return Q
+            else :
+                return scores, Q_duplicated, D
+
+        if self.colbert_config.use_ib_negatives:
+            ib_loss = self.compute_ib_loss(Q, D, D_mask)
+            return scores, ib_loss
+
+        return scores
+
+    def compute_ib_loss(self, Q, D, D_mask):
+        # TODO: Organize the code below! Quite messy.
+        if DEVICE == torch.device("cuda"):
+            scores = (D.unsqueeze(0) @ Q.permute(0, 2, 1).unsqueeze(1)).flatten(0, 1)  # query-major unsqueeze
+        else:
+            scores = (D.unsqueeze(0).float() @ Q.permute(0, 2, 1).unsqueeze(1)).flatten(0, 1)
+
+        scores = colbert_score_reduce(scores, D_mask.repeat(Q.size(0), 1, 1), self.colbert_config)
+
+        nway = self.colbert_config.nway
+        all_except_self_negatives = [list(range(qidx*D.size(0), qidx*D.size(0) + nway*qidx+1)) +
+                                     list(range(qidx*D.size(0) + nway * (qidx+1), qidx*D.size(0) + D.size(0)))
+                                     for qidx in range(Q.size(0))]
+
+        scores = scores[flatten(all_except_self_negatives)]
+        scores = scores.view(Q.size(0), -1)  # D.size(0) - self.colbert_config.nway + 1)
+
+        labels = torch.arange(0, Q.size(0), device=scores.device) * (self.colbert_config.nway)
+
+        return torch.nn.CrossEntropyLoss()(scores, labels)
+
+    def query(self, input_ids, attention_mask):
+        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(self.device)
+        # print query input_ids
+        if not self.query_used:
+
+            print_message("#>>>> colbert query ==")
+            print_message(f"#>>>>> input_ids: {input_ids[0].size()}, {input_ids[0]}")
+
+        Q = self.bert(input_ids, attention_mask=attention_mask)[0]
+
+        # print out Q
+        if not self.query_used:
+            print_message("#>>>> before linear query ==")
+            print_message(f"#>>>>> Q: {Q[0].size()}, {Q[0]}")
+            print_message(f"#>>>>> self.linear query : {self.linear.weight}")
+
+        Q = self.linear(Q)
+
+        if not self.query_used:
+            self.query_used = True
+
+            print_message("#>>>> colbert query ==")
+            print_message(f"#>>>>> Q: {Q[0].size()}, {Q[0]}")
+
+
+        mask = torch.tensor(self.mask(input_ids, skiplist=[]), device=self.device).unsqueeze(2).float()
+        Q = Q * mask
+
+        return torch.nn.functional.normalize(Q, p=2, dim=2)
+
+    def doc(self, input_ids, attention_mask, keep_dims=True):
+        assert keep_dims in [True, False, 'return_mask']
+
+        input_ids, attention_mask = input_ids.to(self.device), attention_mask.to(self.device)
+        # print doc input_ids
+        # [     0,   9749,   4960,  40455,      6,  58745,  48302,    136,   4343,   71,
+        if not self.doc_used:
+            print_message("#>>>> colbert doc ==")
+            print_message(f"#>>>>> input_ids: {input_ids[0].size()}, {input_ids[0]}")
+
+        D = self.bert(input_ids, attention_mask=attention_mask)[0]
+
+        if not self.doc_used:
+            print_message("#>>>> before linear doc ==")
+            print_message(f"#>>>>> D: {D[0].size()}, {D[0]}")
+
+            print_message(f"#>>>>> self.linear doc : {self.linear.weight}")
+
+        D = self.linear(D)
+
+        # print out D
+        if not self.doc_used:
+            self.doc_used = True
+
+            print_message("#>>>> colbert doc ==")
+            print_message(f"#>>>>> D: {D[0].size()}, {D[0]}")
+
+
+        mask = torch.tensor(self.mask(input_ids, skiplist=self.skiplist), device=self.device).unsqueeze(2).float()
+        D = D * mask
+
+        D = torch.nn.functional.normalize(D, p=2, dim=2)
+        if self.use_gpu:
+            D = D.half()
+
+        if keep_dims is False:
+            D, mask = D.cpu(), mask.bool().cpu().squeeze(-1)
+            D = [d[mask[idx]] for idx, d in enumerate(D)]
+
+        elif keep_dims == 'return_mask':
+            return D, mask.bool()
+
+        return D
+
+    def score(self, Q, D_padded, D_mask):
+        # assert self.colbert_config.similarity == 'cosine'
+
+        if self.colbert_config.similarity == 'l2':
+            assert False, 'l2 similarity is not supported'
+            assert self.colbert_config.interaction == 'colbert'
+            return (-1.0 * ((Q.unsqueeze(2) - D_padded.unsqueeze(1))**2).sum(-1)).max(-1).values.sum(-1)
+
+        return colbert_score(Q, D_padded, D_mask, config=self.colbert_config)
+
+    def mask(self, input_ids, skiplist):
+        mask = [[(x not in skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]
+        return mask
+
+
+# TODO: In Query/DocTokenizer, use colbert.raw_tokenizer
+
+# TODO: The masking below might also be applicable in the kNN part
+def colbert_score_reduce(scores_padded, D_mask, config: ColBERTConfig):
+    D_padding = ~D_mask.view(scores_padded.size(0), scores_padded.size(1)).bool()
+    scores_padded[D_padding] = -9999
+    scores = scores_padded.max(1).values
+
+    assert config.interaction in ['colbert', 'flipr'], config.interaction
+
+    if config.interaction == 'flipr':
+        assert config.query_maxlen == 64, ("for now", config)
+        # assert scores.size(1) == config.query_maxlen, scores.size()
+
+        K1 = config.query_maxlen // 2
+        K2 = 8
+
+        A = scores[:, :config.query_maxlen].topk(K1, dim=-1).values.sum(-1)
+        B = 0
+
+        if K2 <= scores.size(1) - config.query_maxlen:
+            B = scores[:, config.query_maxlen:].topk(K2, dim=-1).values.sum(1)
+
+        return A + B
+
+    return scores.sum(-1)
+
+
+# TODO: Wherever this is called, pass `config=`
+def colbert_score(Q, D_padded, D_mask, config=ColBERTConfig()):
+    """
+        Supply sizes Q = (1 | num_docs, *, dim) and D = (num_docs, *, dim).
+        If Q.size(0) is 1, the matrix will be compared with all passages.
+        Otherwise, each query matrix will be compared against the *aligned* passage.
+
+        EVENTUALLY: Consider masking with -inf for the maxsim (or enforcing a ReLU).
+    """
+
+    use_gpu = torch.cuda.is_available()
+    if use_gpu:
+        Q, D_padded, D_mask = Q.cuda(), D_padded.cuda(), D_mask.cuda()
+
+    assert Q.dim() == 3, Q.size()
+    assert D_padded.dim() == 3, D_padded.size()
+    assert Q.size(0) in [1, D_padded.size(0)]
+
+    scores = D_padded @ Q.to(dtype=D_padded.dtype).permute(0, 2, 1)
+
+    return colbert_score_reduce(scores, D_mask, config)
+
+
+def colbert_score_packed(Q, D_packed, D_lengths, config=ColBERTConfig()):
+    """
+        Works with a single query only.
+    """
+    use_gpu = torch.cuda.is_available()
+    if use_gpu:
+        Q, D_packed, D_lengths = Q.cuda(), D_packed.cuda(), D_lengths.cuda()
+
+    Q = Q.squeeze(0)
+
+    assert Q.dim() == 2, Q.size()
+    assert D_packed.dim() == 2, D_packed.size()
+
+    scores = D_packed @ Q.to(dtype=D_packed.dtype).T
+
+    if use_gpu or config.interaction == "flipr":
+        scores_padded, scores_mask = StridedTensor(scores, D_lengths, use_gpu=use_gpu).as_padded_tensor()
+
+        return colbert_score_reduce(scores_padded, scores_mask, config)
+    else:
+        return ColBERT.segmented_maxsim(scores, D_lengths)
```

## primeqa/ir/dense/colbert_top/colbert/modeling/factory.py

 * *Ordering differences only*

```diff
@@ -1,107 +1,107 @@
-# bert imports
-# from colbert.modeling.colbert import ColBERT
-from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert import HF_ColBERT
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import QueryTokenizer, DocTokenizer
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-
-# xlmr imports
-from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert_xlmr import HF_ColBERT_XLMR
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.doc_tokenization_xlmr import DocTokenizerXLMR
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.query_tokenization_xlmr import QueryTokenizerXLMR
-
-import os
-import json
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
-
-# Based on model type to associate to a proper model and tokennizers(query, doc)
-#----------------------------------------------------------------
-def get_colbert_from_pretrained(name, colbert_config):
-    # in V2, these come from
-    # training::colbert = ColBERT(name=config.checkpoint, colbert_config=config)
-
-    # currently, support bert and xlmr, ONLY and tinybert is hard wired.
-
-    local_models_repository = colbert_config.local_models_repository
-    model_type = name
-
-    if colbert_config.model_type is not None:
-        model_type = colbert_config.model_type
-
-    # if it is a directory, load json file to get the model type,or  if it is a dnn file
-    if os.path.isdir(name):
-        json_file= name + '/config.json'
-        print_message(f"json file (get_colbert_from_pretrained): {json_file}")
-        with open(json_file) as file:
-            data = json.load(file)
-        assert model_type == data["_name_or_path"], f"model type in {name} not matching"
-        # model_type = data["_name_or_path"]
-    elif name.endswith('.dnn') or name.endswith('.model'):
-        dnn_checkpoint = torch_load_dnn(name)
-        assert model_type == dnn_checkpoint['model_type'], f"model type in {name} not matching"
-        # model_type = dnn_checkpoint['model_type']
-
-    print_message(f"factory model type: {model_type}")
-
-    if model_type=='bert-base-uncased' or model_type=='bert-large-uncased':
-        colbert = HF_ColBERT.from_pretrained(name, colbert_config)
-    elif model_type == 'tinybert':
-        if not local_models_repository:
-            raise ValueError("Please specify the local repository for additional models.")
-        #  hard wired for local Tinybert model
-        colbert = HF_ColBERT.from_pretrained(os.path.join(local_models_repository, 'tinybert/TinyBERT_General_4L_312D'), colbert_config)
-        # e.g. from https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/tree/main
-    elif model_type=='xlm-roberta-base' or model_type=='xlm-roberta-large':
-        colbert = HF_ColBERT_XLMR.from_pretrained(name, colbert_config)
-    else:
-        raise NotImplementedError
-
-    colbert.model_type=model_type
-    return colbert
-
-#----------------------------------------------------------------
-def get_query_tokenizer(model_type, maxlen, attend_to_mask_tokens):
-
-
-    # if it is a directory, load json file to get the model type
-    if os.path.isdir(model_type):
-        json_file = model_type + '/config.json'
-        print_message(f"json file (get_query_tokenizer): {json_file}")
-        with open(json_file) as file:
-            data = json.load(file)
-        model_type = data["_name_or_path"]
-
-    print_message(f"get query model type: {model_type}")
-
-    if model_type=='bert-base-uncased' or model_type=='bert-large-uncased':
-        return QueryTokenizer(maxlen,model_type, attend_to_mask_tokens)
-    elif model_type=='tinybert':
-        return QueryTokenizer(maxlen, 'bert-base-uncased',attend_to_mask_tokens)
-    elif model_type=='xlm-roberta-base' or model_type=='xlm-roberta-large':
-        return QueryTokenizerXLMR(maxlen, model_type)
-    else:
-        raise NotImplementedError
-
-#----------------------------------------------------------------
-def get_doc_tokenizer(model_type, maxlen):
-
-
-    # if it is a directory, load json file to get the model type
-    if os.path.isdir(model_type):
-        json_file = model_type + '/config.json'
-        print_message(f"json file (get_doc_tokenizer): {json_file}")
-        with open(json_file) as file:
-            data = json.load(file)
-        model_type = data["_name_or_path"]
-
-
-    print_message(f"get doc model type: {model_type}")
-
-    if model_type=='bert-base-uncased' or model_type=='bert-large-uncased':
-        return DocTokenizer(maxlen, model_type)
-    elif model_type=='tinybert':
-        return DocTokenizer(maxlen, 'bert-base-uncased')
-    elif model_type=='xlm-roberta-base' or model_type=='xlm-roberta-large':
-        return DocTokenizerXLMR(maxlen, model_type)
-    else:
-        raise NotImplementedError
+# bert imports
+# from colbert.modeling.colbert import ColBERT
+from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert import HF_ColBERT
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import QueryTokenizer, DocTokenizer
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+
+# xlmr imports
+from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert_xlmr import HF_ColBERT_XLMR
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.doc_tokenization_xlmr import DocTokenizerXLMR
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.query_tokenization_xlmr import QueryTokenizerXLMR
+
+import os
+import json
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
+
+# Based on model type to associate to a proper model and tokennizers(query, doc)
+#----------------------------------------------------------------
+def get_colbert_from_pretrained(name, colbert_config):
+    # in V2, these come from
+    # training::colbert = ColBERT(name=config.checkpoint, colbert_config=config)
+
+    # currently, support bert and xlmr, ONLY and tinybert is hard wired.
+
+    local_models_repository = colbert_config.local_models_repository
+    model_type = name
+
+    if colbert_config.model_type is not None:
+        model_type = colbert_config.model_type
+
+    # if it is a directory, load json file to get the model type,or  if it is a dnn file
+    if os.path.isdir(name):
+        json_file= name + '/config.json'
+        print_message(f"json file (get_colbert_from_pretrained): {json_file}")
+        with open(json_file) as file:
+            data = json.load(file)
+        assert model_type == data["_name_or_path"], f"model type in {name} not matching"
+        # model_type = data["_name_or_path"]
+    elif name.endswith('.dnn') or name.endswith('.model'):
+        dnn_checkpoint = torch_load_dnn(name)
+        assert model_type == dnn_checkpoint['model_type'], f"model type in {name} not matching"
+        # model_type = dnn_checkpoint['model_type']
+
+    print_message(f"factory model type: {model_type}")
+
+    if model_type=='bert-base-uncased' or model_type=='bert-large-uncased':
+        colbert = HF_ColBERT.from_pretrained(name, colbert_config)
+    elif model_type == 'tinybert':
+        if not local_models_repository:
+            raise ValueError("Please specify the local repository for additional models.")
+        #  hard wired for local Tinybert model
+        colbert = HF_ColBERT.from_pretrained(os.path.join(local_models_repository, 'tinybert/TinyBERT_General_4L_312D'), colbert_config)
+        # e.g. from https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/tree/main
+    elif model_type=='xlm-roberta-base' or model_type=='xlm-roberta-large':
+        colbert = HF_ColBERT_XLMR.from_pretrained(name, colbert_config)
+    else:
+        raise NotImplementedError
+
+    colbert.model_type=model_type
+    return colbert
+
+#----------------------------------------------------------------
+def get_query_tokenizer(model_type, maxlen, attend_to_mask_tokens):
+
+
+    # if it is a directory, load json file to get the model type
+    if os.path.isdir(model_type):
+        json_file = model_type + '/config.json'
+        print_message(f"json file (get_query_tokenizer): {json_file}")
+        with open(json_file) as file:
+            data = json.load(file)
+        model_type = data["_name_or_path"]
+
+    print_message(f"get query model type: {model_type}")
+
+    if model_type=='bert-base-uncased' or model_type=='bert-large-uncased':
+        return QueryTokenizer(maxlen,model_type, attend_to_mask_tokens)
+    elif model_type=='tinybert':
+        return QueryTokenizer(maxlen, 'bert-base-uncased',attend_to_mask_tokens)
+    elif model_type=='xlm-roberta-base' or model_type=='xlm-roberta-large':
+        return QueryTokenizerXLMR(maxlen, model_type)
+    else:
+        raise NotImplementedError
+
+#----------------------------------------------------------------
+def get_doc_tokenizer(model_type, maxlen):
+
+
+    # if it is a directory, load json file to get the model type
+    if os.path.isdir(model_type):
+        json_file = model_type + '/config.json'
+        print_message(f"json file (get_doc_tokenizer): {json_file}")
+        with open(json_file) as file:
+            data = json.load(file)
+        model_type = data["_name_or_path"]
+
+
+    print_message(f"get doc model type: {model_type}")
+
+    if model_type=='bert-base-uncased' or model_type=='bert-large-uncased':
+        return DocTokenizer(maxlen, model_type)
+    elif model_type=='tinybert':
+        return DocTokenizer(maxlen, 'bert-base-uncased')
+    elif model_type=='xlm-roberta-base' or model_type=='xlm-roberta-large':
+        return DocTokenizerXLMR(maxlen, model_type)
+    else:
+        raise NotImplementedError
```

## primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert.py

```diff
@@ -1,77 +1,80 @@
-import torch.nn as nn
-
-from transformers import BertPreTrainedModel, BertModel, AutoTokenizer
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
-
-
-class HF_ColBERT(BertPreTrainedModel):
-    """
-        Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
-        
-        This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
-    """
-    _keys_to_ignore_on_load_unexpected = [r"cls"]
-
-    def __init__(self, config, colbert_config):
-        super().__init__(config)
-
-        self.dim = colbert_config.dim
-        self.bert = BertModel(config)
-        self.linear = nn.Linear(config.hidden_size, colbert_config.dim, bias=False)
-
-        # if colbert_config.relu:
-        #     self.score_scaler = nn.Linear(1, 1)
-
-        self.init_weights()
-
-        # if colbert_config.relu:
-        #     self.score_scaler.weight.data.fill_(1.0)
-        #     self.score_scaler.bias.data.fill_(-8.0)
-
-    @classmethod
-    def from_pretrained(cls, name_or_path, colbert_config):
-        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
-        # if True:  # name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
-            # name_or_path = '/dccstor/colbert-ir/weizhong/experiments/Apr3_2022_v2_txt_xmlr_BERT/none/2022-04/03/13.20.02/checkpoints/colbert_75000.dnn'
-            dnn = torch_load_dnn(name_or_path)
-            base = dnn.get('arguments', {}).get('model', 'bert-base-uncased')
-
-            state_dict=dnn['model_state_dict']
-            from collections import OrderedDict
-            import re
-            state_dict = OrderedDict([(re.sub(r'^model.bert.', 'bert.', key), value) for key, value in state_dict.items()])
-            state_dict = OrderedDict([(re.sub(r'^model.linear.', 'linear.', key), value) for key, value in state_dict.items()])
-            obj = super().from_pretrained(base, state_dict=state_dict, colbert_config=colbert_config)
-            #obj = super().from_pretrained(base, state_dict=dnn['model_state_dict'], colbert_config=colbert_config)
-            obj.base = base
-
-            return obj
-
-        obj = super().from_pretrained(name_or_path, colbert_config=colbert_config)
-        obj.base = name_or_path
-
-        return obj
-
-    @staticmethod
-    def raw_tokenizer_from_pretrained(name_or_path):
-        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
-            dnn = torch_load_dnn(name_or_path)
-            base = dnn.get('arguments', {}).get('model', 'bert-base-uncased')
-
-            obj = AutoTokenizer.from_pretrained(base)
-            obj.base = base
-
-            return obj
-
-        obj = AutoTokenizer.from_pretrained(name_or_path)
-        obj.base = name_or_path
-
-        return obj
-
-"""
-TODO: It's easy to write a class generator that takes "name_or_path" and loads AutoConfig to check the Architecture's
-      name, finds that name's *PreTrainedModel and *Model in dir(transformers), and then basically repeats the above.
-
-      It's easy for the BaseColBERT class to instantiate things from there.
-"""
-
+import torch.nn as nn
+
+from transformers import BertPreTrainedModel, BertModel, AutoTokenizer
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+class HF_ColBERT(BertPreTrainedModel):
+    """
+        Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
+        
+        This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
+    """
+    _keys_to_ignore_on_load_unexpected = [r"cls"]
+
+    def __init__(self, config, colbert_config):
+        super().__init__(config)
+
+        self.dim = colbert_config.dim
+        self.bert = BertModel(config)
+        self.linear = nn.Linear(config.hidden_size, colbert_config.dim, bias=False)
+
+        # if colbert_config.relu:
+        #     self.score_scaler = nn.Linear(1, 1)
+
+        self.init_weights()
+
+        # if colbert_config.relu:
+        #     self.score_scaler.weight.data.fill_(1.0)
+        #     self.score_scaler.bias.data.fill_(-8.0)
+
+    @classmethod
+    def from_pretrained(cls, name_or_path, colbert_config):
+        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
+            dnn = torch_load_dnn(name_or_path)
+
+            base_default = 'bert-base-uncased'
+            if (not dnn.get('arguments') or dnn.get('arguments').get('model')) and (not dnn.get('model_type')):
+                print_message(f"[WARNING] Using default model type (base) {base_default}")
+            base = dnn.get('arguments', {}).get('model', base_default) if dnn.get('arguments') else dnn.get('model_type', base_default)
+
+            state_dict=dnn['model_state_dict']
+            from collections import OrderedDict
+            import re
+            state_dict = OrderedDict([(re.sub(r'^model.bert.', 'bert.', key), value) for key, value in state_dict.items()])
+            state_dict = OrderedDict([(re.sub(r'^model.linear.', 'linear.', key), value) for key, value in state_dict.items()])
+            obj = super().from_pretrained(base, state_dict=state_dict, colbert_config=colbert_config)
+            #obj = super().from_pretrained(base, state_dict=dnn['model_state_dict'], colbert_config=colbert_config)
+            obj.base = base
+
+            return obj
+
+        obj = super().from_pretrained(name_or_path, colbert_config=colbert_config)
+        obj.base = name_or_path
+
+        return obj
+
+    @staticmethod
+    def raw_tokenizer_from_pretrained(name_or_path):
+        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
+            dnn = torch_load_dnn(name_or_path)
+            base = dnn.get('arguments', {}).get('model', 'bert-base-uncased')
+
+            obj = AutoTokenizer.from_pretrained(base)
+            obj.base = base
+
+            return obj
+
+        obj = AutoTokenizer.from_pretrained(name_or_path)
+        obj.base = name_or_path
+
+        return obj
+
+"""
+TODO: It's easy to write a class generator that takes "name_or_path" and loads AutoConfig to check the Architecture's
+      name, finds that name's *PreTrainedModel and *Model in dir(transformers), and then basically repeats the above.
+
+      It's easy for the BaseColBERT class to instantiate things from there.
+"""
+
```

## primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert_xlmr.py

```diff
@@ -1,93 +1,96 @@
-import torch.nn as nn
-
-from transformers import BertPreTrainedModel, BertModel, AutoTokenizer
-from transformers import AutoModel
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
-from transformers import PreTrainedModel
-from transformers import XLMRobertaModel
-
-class HF_ColBERT_XLMR(XLMRobertaModel):
-#class HF_ColBERT_XLMR(PreTrainedModel):
-    """
-        Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
-        
-        This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
-    """
-
-    def __init__(self, config, colbert_config):
-        super().__init__(config)
-
-        self.dim = colbert_config.dim
-        # resolve conflict between bert and roberta
-        # self.roberta = XLMRobertaModel(config)
-        # self.bert = self.roberta
-        self.encoder = None
-        self.embeddings = None
-        self.pooler = None
-
-        self.roberta = XLMRobertaModel(config)
-        self.bert = self.roberta
-        #self.bert = XLMRobertaModel(config)
-
-        self.linear = nn.Linear(config.hidden_size, colbert_config.dim, bias=False)
-
-        # if colbert_config.relu:
-        #     self.score_scaler = nn.Linear(1, 1)
-
-        self.init_weights()
-
-        # if colbert_config.relu:
-        #     self.score_scaler.weight.data.fill_(1.0)
-        #     self.score_scaler.bias.data.fill_(-8.0)
-
-    @classmethod
-    def from_pretrained(cls, name_or_path, colbert_config):
-        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
-            dnn = torch_load_dnn(name_or_path)
-            base = dnn.get('arguments', {}).get('model', 'xlm-roberta-base')  # TODO: how about other lm-roberta-XXX?
-
-            state_dict=dnn['model_state_dict']
-            from collections import OrderedDict
-            import re
-
-            # for reading V1
-            # state_dict = OrderedDict([(re.sub(r'^roberta.', 'bert.', key), value) for key, value in state_dict.items()])
-
-            # for reading V2
-            state_dict = OrderedDict([(re.sub(r'^model.', '', key), value) for key, value in state_dict.items()])
-
-            obj = super().from_pretrained(base, state_dict=state_dict, colbert_config=colbert_config)
-            #obj = super().from_pretrained(base, state_dict=dnn['model_state_dict'], colbert_config=colbert_config)
-            obj.base = base
-
-            return obj
-
-        obj = super().from_pretrained(name_or_path, colbert_config=colbert_config)  # <<<< HERE
-
-        obj.base = name_or_path
-
-        return obj
-
-    @staticmethod
-    def raw_tokenizer_from_pretrained(name_or_path):
-        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
-            dnn = torch_load_dnn(name_or_path)
-            base = dnn.get('arguments', {}).get('model',  'xlm-roberta-base')  # TODO: how about other lm-roberta-XXX?
-
-            obj = AutoTokenizer.from_pretrained(base)
-            obj.base = base
-
-            return obj
-
-        obj = AutoTokenizer.from_pretrained(name_or_path)
-        obj.base = name_or_path
-
-        return obj
-
-"""
-TODO: It's easy to write a class generator that takes "name_or_path" and loads AutoConfig to check the Architecture's
-      name, finds that name's *PreTrainedModel and *Model in dir(transformers), and then basically repeats the above.
-
-      It's easy for the BaseColBERT class to instantiate things from there.
-"""
-
+import torch.nn as nn
+
+from transformers import AutoTokenizer, XLMRobertaModel
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+class HF_ColBERT_XLMR(XLMRobertaModel):
+#class HF_ColBERT_XLMR(PreTrainedModel):
+    """
+        Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
+        
+        This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
+    """
+
+    def __init__(self, config, colbert_config):
+        super().__init__(config)
+
+        self.dim = colbert_config.dim
+        # resolve conflict between bert and roberta
+        # self.roberta = XLMRobertaModel(config)
+        # self.bert = self.roberta
+        self.encoder = None
+        self.embeddings = None
+        self.pooler = None
+
+        self.roberta = XLMRobertaModel(config)
+        self.bert = self.roberta
+        #self.bert = XLMRobertaModel(config)
+
+        self.linear = nn.Linear(config.hidden_size, colbert_config.dim, bias=False)
+
+        # if colbert_config.relu:
+        #     self.score_scaler = nn.Linear(1, 1)
+
+        self.init_weights()
+
+        # if colbert_config.relu:
+        #     self.score_scaler.weight.data.fill_(1.0)
+        #     self.score_scaler.bias.data.fill_(-8.0)
+
+    @classmethod
+    def from_pretrained(cls, name_or_path, colbert_config):
+        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
+            dnn = torch_load_dnn(name_or_path)
+
+            base_default = 'xlm-roberta-base'
+            if (not dnn.get('arguments') or dnn.get('arguments').get('model')) and (not dnn.get('model_type')):
+                print_message(f"[WARNING] Using default model type (base) {base_default}")
+            base = dnn.get('arguments', {}).get('model', base_default) if dnn.get('arguments') else dnn.get('model_type', base_default)
+
+            state_dict=dnn['model_state_dict']
+            from collections import OrderedDict
+            import re
+
+            # for reading V1
+            # state_dict = OrderedDict([(re.sub(r'^roberta.', 'bert.', key), value) for key, value in state_dict.items()])
+
+            # for reading V2
+            state_dict = OrderedDict([(re.sub(r'^model.', '', key), value) for key, value in state_dict.items()])
+
+            obj = super().from_pretrained(base, state_dict=state_dict, colbert_config=colbert_config)
+            #obj = super().from_pretrained(base, state_dict=dnn['model_state_dict'], colbert_config=colbert_config)
+            obj.base = base
+
+            return obj
+
+        obj = super().from_pretrained(name_or_path, colbert_config=colbert_config)  # <<<< HERE
+
+        obj.base = name_or_path
+
+        return obj
+
+    @staticmethod
+    def raw_tokenizer_from_pretrained(name_or_path):
+        if name_or_path.endswith('.dnn') or name_or_path.endswith('.model'):
+            dnn = torch_load_dnn(name_or_path)
+            base = dnn.get('arguments', {}).get('model',  'xlm-roberta-base')  # TODO: how about other lm-roberta-XXX?
+
+            obj = AutoTokenizer.from_pretrained(base)
+            obj.base = base
+
+            return obj
+
+        obj = AutoTokenizer.from_pretrained(name_or_path)
+        obj.base = name_or_path
+
+        return obj
+
+"""
+TODO: It's easy to write a class generator that takes "name_or_path" and loads AutoConfig to check the Architecture's
+      name, finds that name's *PreTrainedModel and *Model in dir(transformers), and then basically repeats the above.
+
+      It's easy for the BaseColBERT class to instantiate things from there.
+"""
+
```

## primeqa/ir/dense/colbert_top/colbert/modeling/reranker/electra.py

 * *Ordering differences only*

```diff
@@ -1,35 +1,35 @@
-import torch.nn as nn
-
-from transformers import ElectraPreTrainedModel, ElectraModel, AutoTokenizer
-
-class ElectraReranker(ElectraPreTrainedModel):
-    """
-        Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
-        
-        This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
-    """
-    _keys_to_ignore_on_load_unexpected = [r"cls"]
-
-    def __init__(self, config):
-        super().__init__(config)
-
-        self.electra = ElectraModel(config)
-        self.linear = nn.Linear(config.hidden_size, 1)
-        self.raw_tokenizer = AutoTokenizer.from_pretrained('google/electra-large-discriminator')
-
-        self.init_weights()
-
-    def forward(self, encoding):
-        outputs = self.electra(encoding.input_ids,
-                               attention_mask=encoding.attention_mask,
-                               token_type_ids=encoding.token_type_ids)[0]
-
-        scores = self.linear(outputs[:, 0]).squeeze(-1)
-
-        return scores
-    
-    def save(self, path):
-        assert not path.endswith('.dnn'), f"{path}: We reserve *.dnn names for the deprecated checkpoint format."
-
-        self.save_pretrained(path)
+import torch.nn as nn
+
+from transformers import ElectraPreTrainedModel, ElectraModel, AutoTokenizer
+
+class ElectraReranker(ElectraPreTrainedModel):
+    """
+        Shallow wrapper around HuggingFace transformers. All new parameters should be defined at this level.
+        
+        This makes sure `{from,save}_pretrained` and `init_weights` are applied to new parameters correctly.
+    """
+    _keys_to_ignore_on_load_unexpected = [r"cls"]
+
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.electra = ElectraModel(config)
+        self.linear = nn.Linear(config.hidden_size, 1)
+        self.raw_tokenizer = AutoTokenizer.from_pretrained('google/electra-large-discriminator')
+
+        self.init_weights()
+
+    def forward(self, encoding):
+        outputs = self.electra(encoding.input_ids,
+                               attention_mask=encoding.attention_mask,
+                               token_type_ids=encoding.token_type_ids)[0]
+
+        scores = self.linear(outputs[:, 0]).squeeze(-1)
+
+        return scores
+    
+    def save(self, path):
+        assert not path.endswith('.dnn'), f"{path}: We reserve *.dnn names for the deprecated checkpoint format."
+
+        self.save_pretrained(path)
         self.raw_tokenizer.save_pretrained(path)
```

## primeqa/ir/dense/colbert_top/colbert/modeling/reranker/tokenizer.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-from transformers import AutoTokenizer
-
-class RerankerTokenizer():
-    def __init__(self, total_maxlen, base):
-        self.total_maxlen = total_maxlen
-        self.tok = AutoTokenizer.from_pretrained(base)
-
-    def tensorize(self, questions, passages):
-        assert type(questions) in [list, tuple], type(questions)
-        assert type(passages) in [list, tuple], type(passages)
-
-        encoding = self.tok(questions, passages, padding='longest', truncation='longest_first',
-                            return_tensors='pt', max_length=self.total_maxlen, add_special_tokens=True)
-
-        return encoding
+from transformers import AutoTokenizer
+
+class RerankerTokenizer():
+    def __init__(self, total_maxlen, base):
+        self.total_maxlen = total_maxlen
+        self.tok = AutoTokenizer.from_pretrained(base)
+
+    def tensorize(self, questions, passages):
+        assert type(questions) in [list, tuple], type(questions)
+        assert type(passages) in [list, tuple], type(passages)
+
+        encoding = self.tok(questions, passages, padding='longest', truncation='longest_first',
+                            return_tensors='pt', max_length=self.total_maxlen, add_special_tokens=True)
+
+        return encoding
```

## primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.query_tokenization import *
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.doc_tokenization import *
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import tensorize_triples
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.query_tokenization import *
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.doc_tokenization import *
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import tensorize_triples
```

## primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization.py

 * *Ordering differences only*

```diff
@@ -1,81 +1,81 @@
-import torch
-
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert import HF_ColBERT
-from primeqa.ir.dense.colbert_top.colbert.infra import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-class DocTokenizer():
-    # def __init__(self, config: ColBERTConfig):
-    def __init__(self, doc_maxlen, model_type):
-        # self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
-        # assert False
-        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(model_type)
-
-        # self.config = config
-        # self.doc_maxlen = config.doc_maxlen
-        self.doc_maxlen = doc_maxlen
-
-        self.D_marker_token, self.D_marker_token_id = '[D]', self.tok.convert_tokens_to_ids('[unused1]')
-        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
-        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
-
-        assert self.D_marker_token_id == 2
-        self.used = False
-
-    def tokenize(self, batch_text, add_special_tokens=False):
-        # assert False
-
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
-
-        if not add_special_tokens:
-            return tokens
-
-        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]
-        tokens = [prefix + lst + suffix for lst in tokens]
-
-        return tokens
-
-    def encode(self, batch_text, add_special_tokens=False):
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']
-
-        if not add_special_tokens:
-            return ids
-
-        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]
-        ids = [prefix + lst + suffix for lst in ids]
-
-        return ids
-
-    def tensorize(self, batch_text, bsize=None):
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        # add placehold for the [D] marker
-        batch_text = ['. ' + x for x in batch_text]
-
-        obj = self.tok(batch_text, padding='longest', truncation='longest_first',
-                       return_tensors='pt', max_length=self.doc_maxlen)
-
-        ids, mask = obj['input_ids'], obj['attention_mask']
-
-        # postprocess for the [D] marker
-        ids[:, 1] = self.D_marker_token_id
-
-        if not self.used:
-            self.used = True
-            print_message("#> BERT DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
-            print_message(f"#> Input: {batch_text[0]}, \t\t {bsize}")
-            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
-            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
-
-        if bsize:
-            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)
-            batches = _split_into_batches(ids, mask, bsize)
-            return batches, reverse_indices
-
-        return ids, mask
+import torch
+
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert import HF_ColBERT
+from primeqa.ir.dense.colbert_top.colbert.infra import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+class DocTokenizer():
+    # def __init__(self, config: ColBERTConfig):
+    def __init__(self, doc_maxlen, model_type):
+        # self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
+        # assert False
+        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(model_type)
+
+        # self.config = config
+        # self.doc_maxlen = config.doc_maxlen
+        self.doc_maxlen = doc_maxlen
+
+        self.D_marker_token, self.D_marker_token_id = '[D]', self.tok.convert_tokens_to_ids('[unused1]')
+        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
+        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
+
+        assert self.D_marker_token_id == 2
+        self.used = False
+
+    def tokenize(self, batch_text, add_special_tokens=False):
+        # assert False
+
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
+
+        if not add_special_tokens:
+            return tokens
+
+        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]
+        tokens = [prefix + lst + suffix for lst in tokens]
+
+        return tokens
+
+    def encode(self, batch_text, add_special_tokens=False):
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']
+
+        if not add_special_tokens:
+            return ids
+
+        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]
+        ids = [prefix + lst + suffix for lst in ids]
+
+        return ids
+
+    def tensorize(self, batch_text, bsize=None):
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        # add placehold for the [D] marker
+        batch_text = ['. ' + x for x in batch_text]
+
+        obj = self.tok(batch_text, padding='longest', truncation='longest_first',
+                       return_tensors='pt', max_length=self.doc_maxlen)
+
+        ids, mask = obj['input_ids'], obj['attention_mask']
+
+        # postprocess for the [D] marker
+        ids[:, 1] = self.D_marker_token_id
+
+        if not self.used:
+            self.used = True
+            print_message("#> BERT DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
+            print_message(f"#> Input: {batch_text[0]}, \t\t {bsize}")
+            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
+            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
+
+        if bsize:
+            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)
+            batches = _split_into_batches(ids, mask, bsize)
+            return batches, reverse_indices
+
+        return ids, mask
```

## primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization_xlmr.py

 * *Ordering differences only*

```diff
@@ -1,79 +1,79 @@
-import torch
-
-from transformers import XLMRobertaTokenizer # there's no Fast version
-from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert_xlmr import HF_ColBERT_XLMR
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-class DocTokenizerXLMR():
-    def __init__(self, doc_maxlen, model_type):
-        # self.tok = XLMRobertaTokenizer.from_pretrained(model_type)
-        self.tok = HF_ColBERT_XLMR.raw_tokenizer_from_pretrained(model_type)
-
-        self.doc_maxlen = doc_maxlen
-
-        self.Q_marker_token, self.D_marker_token_id = '?', 9749  # Hot Beverage
-        self.used = False
-
-    # tokenizer is not used colbert code base, but is implemented in DocTokenizer
-    def tokenize(self, batch_text, add_special_tokens=False):
-        raise NotImplementedError()
-
-    # encode is not used colbert code base, but is implemented in DocTokenizer
-    def encode(self, batch_text, add_special_tokens=False):
-        raise NotImplementedError()
-
-    def tensorize(self, batch_text, bsize=None):
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        # add placehold for the [D] marker
-        # strangely, prefixing with '. ' introduces _two_ extra tokens [5,6]
-        # it seems that 6 is the empty string
-        # into the output - I don't understand why ...
-        batch_text = ['$ ' + x for x in batch_text]
-
-        obj = self.tok(batch_text, padding='longest', truncation='longest_first',
-                       return_tensors='pt', max_length=self.doc_maxlen)
-
-        ids, mask = obj['input_ids'], obj['attention_mask']
-
-        # postprocess for the [D] marker
-        ids[:, 1] = self.D_marker_token_id
-
-        if self.used is False:
-            self.used = True
-            # firstbg = (context is None) or context[0]
-            # print()
-            print_message("#> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
-            # print(f"#> Input: {batch_text[0]}, \t\t {firstbg}, \t\t {bsize}")
-            print_message(f"#> Input: {batch_text[0]}, \t\t {bsize}")
-            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
-            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
-            # print()
-
-        if bsize:
-            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)
-            batches = _split_into_batches(ids, mask, bsize)
-            return batches, reverse_indices
-
-        return ids, mask
-
-
-# In [1]: from colbert.modeling.tokenization import DocTokenizerXLMR
-
-# In [2]: t=DocTokenizerXLMR(50)
-
-# In [3]: t.tensorize(['Here is the answer.', 'Another longer answer is here.'])
-# (tensor([[     0,   9749,  11853,     83,     70,  35166,      5,      2,      1],
-#          [     0,   9749, 116267,  51713,  35166,     83,   3688,      5,      2]]),
-#  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
-#          [1, 1, 1, 1, 1, 1, 1, 1, 1]]))
-
-
-# In [4]:  t.D_marker_token_id
-# Out[4]: 9749
-
-
-# In [5]: t.tok.decode(range(6))
-# Out[6]: '<s><pad></s><unk>,.'
-
+import torch
+
+from transformers import XLMRobertaTokenizer # there's no Fast version
+from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert_xlmr import HF_ColBERT_XLMR
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+class DocTokenizerXLMR():
+    def __init__(self, doc_maxlen, model_type):
+        # self.tok = XLMRobertaTokenizer.from_pretrained(model_type)
+        self.tok = HF_ColBERT_XLMR.raw_tokenizer_from_pretrained(model_type)
+
+        self.doc_maxlen = doc_maxlen
+
+        self.Q_marker_token, self.D_marker_token_id = '?', 9749  # Hot Beverage
+        self.used = False
+
+    # tokenizer is not used colbert code base, but is implemented in DocTokenizer
+    def tokenize(self, batch_text, add_special_tokens=False):
+        raise NotImplementedError()
+
+    # encode is not used colbert code base, but is implemented in DocTokenizer
+    def encode(self, batch_text, add_special_tokens=False):
+        raise NotImplementedError()
+
+    def tensorize(self, batch_text, bsize=None):
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        # add placehold for the [D] marker
+        # strangely, prefixing with '. ' introduces _two_ extra tokens [5,6]
+        # it seems that 6 is the empty string
+        # into the output - I don't understand why ...
+        batch_text = ['$ ' + x for x in batch_text]
+
+        obj = self.tok(batch_text, padding='longest', truncation='longest_first',
+                       return_tensors='pt', max_length=self.doc_maxlen)
+
+        ids, mask = obj['input_ids'], obj['attention_mask']
+
+        # postprocess for the [D] marker
+        ids[:, 1] = self.D_marker_token_id
+
+        if self.used is False:
+            self.used = True
+            # firstbg = (context is None) or context[0]
+            # print()
+            print_message("#> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
+            # print(f"#> Input: {batch_text[0]}, \t\t {firstbg}, \t\t {bsize}")
+            print_message(f"#> Input: {batch_text[0]}, \t\t {bsize}")
+            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
+            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
+            # print()
+
+        if bsize:
+            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)
+            batches = _split_into_batches(ids, mask, bsize)
+            return batches, reverse_indices
+
+        return ids, mask
+
+
+# In [1]: from colbert.modeling.tokenization import DocTokenizerXLMR
+
+# In [2]: t=DocTokenizerXLMR(50)
+
+# In [3]: t.tensorize(['Here is the answer.', 'Another longer answer is here.'])
+# (tensor([[     0,   9749,  11853,     83,     70,  35166,      5,      2,      1],
+#          [     0,   9749, 116267,  51713,  35166,     83,   3688,      5,      2]]),
+#  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
+#          [1, 1, 1, 1, 1, 1, 1, 1, 1]]))
+
+
+# In [4]:  t.D_marker_token_id
+# Out[4]: 9749
+
+
+# In [5]: t.tok.decode(range(6))
+# Out[6]: '<s><pad></s><unk>,.'
+
```

## primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-import torch
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert import HF_ColBERT
-from primeqa.ir.dense.colbert_top.colbert.infra import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import batch
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-class QueryTokenizer():
-    #     def __init__(self, config: ColBERTConfig):
-    def __init__(self, query_maxlen, model_type, attend_to_mask_tokens ):
-        # self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
-        # assert False
-
-        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(model_type)
-
-        # self.config = config
-        # self.query_maxlen = config.query_maxlen
-        self.query_maxlen = query_maxlen
-        self.background_maxlen = 512 - self.query_maxlen + 1  # FIXME: Make this configurable
-        self.attend_to_mask_tokens = attend_to_mask_tokens
-
-        self.Q_marker_token, self.Q_marker_token_id = '[Q]', self.tok.convert_tokens_to_ids('[unused0]')
-        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
-        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
-        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id
-
-        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103
-        self.used = False
-
-    def tokenize(self, batch_text, add_special_tokens=False):
-        # assert False
-
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
-
-        if not add_special_tokens:
-            return tokens
-
-        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]
-        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst)+3)) for lst in tokens]
-
-        return tokens
-
-    def encode(self, batch_text, add_special_tokens=False):
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']
-
-        if not add_special_tokens:
-            return ids
-
-        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]
-        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst)+3)) for lst in ids]
-
-        return ids
-
-    def tensorize(self, batch_text, bsize=None, context=None):
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        # add placehold for the [Q] marker
-        batch_text = ['. ' + x for x in batch_text]
-
-        obj = self.tok(batch_text, padding='max_length', truncation=True,
-                       return_tensors='pt', max_length=self.query_maxlen)
-
-        ids, mask = obj['input_ids'], obj['attention_mask']
-
-        # postprocess for the [Q] marker and the [MASK] augmentation
-        ids[:, 1] = self.Q_marker_token_id
-        ids[ids == 0] = self.mask_token_id
-
-        if context is not None:
-            assert len(context) == len(batch_text), (len(context), len(batch_text))
-
-            obj_2 = self.tok(context, padding='longest', truncation=True,
-                            return_tensors='pt', max_length=self.background_maxlen)
-
-            ids_2, mask_2 = obj_2['input_ids'][:, 1:], obj_2['attention_mask'][:, 1:]  # Skip the first [SEP]
-
-            ids = torch.cat((ids, ids_2), dim=-1)
-            mask = torch.cat((mask, mask_2), dim=-1)
-
-        # if self.config.attend_to_mask_tokens:
-        if self.attend_to_mask_tokens:
-            mask[ids == self.mask_token_id] = 1
-            assert mask.sum().item() == mask.size(0) * mask.size(1), mask
-
-        if not self.used:
-            self.used = True
-            firstbg = (context is None) or context[0]
-            print_message("#> BERT QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
-            print_message(f"#> Input: {batch_text[0]}, \t\t {firstbg}, \t\t {bsize}")
-            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
-            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
-
-        if bsize:
-            batches = _split_into_batches(ids, mask, bsize)
-            return batches
-
-        return ids, mask
+import torch
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert import HF_ColBERT
+from primeqa.ir.dense.colbert_top.colbert.infra import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import batch
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+class QueryTokenizer():
+    #     def __init__(self, config: ColBERTConfig):
+    def __init__(self, query_maxlen, model_type, attend_to_mask_tokens ):
+        # self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(config.checkpoint)
+        # assert False
+
+        self.tok = HF_ColBERT.raw_tokenizer_from_pretrained(model_type)
+
+        # self.config = config
+        # self.query_maxlen = config.query_maxlen
+        self.query_maxlen = query_maxlen
+        self.background_maxlen = 512 - self.query_maxlen + 1  # FIXME: Make this configurable
+        self.attend_to_mask_tokens = attend_to_mask_tokens
+
+        self.Q_marker_token, self.Q_marker_token_id = '[Q]', self.tok.convert_tokens_to_ids('[unused0]')
+        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id
+        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id
+        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id
+
+        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103
+        self.used = False
+
+    def tokenize(self, batch_text, add_special_tokens=False):
+        # assert False
+
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]
+
+        if not add_special_tokens:
+            return tokens
+
+        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]
+        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst)+3)) for lst in tokens]
+
+        return tokens
+
+    def encode(self, batch_text, add_special_tokens=False):
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']
+
+        if not add_special_tokens:
+            return ids
+
+        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]
+        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst)+3)) for lst in ids]
+
+        return ids
+
+    def tensorize(self, batch_text, bsize=None, context=None):
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        # add placehold for the [Q] marker
+        batch_text = ['. ' + x for x in batch_text]
+
+        obj = self.tok(batch_text, padding='max_length', truncation=True,
+                       return_tensors='pt', max_length=self.query_maxlen)
+
+        ids, mask = obj['input_ids'], obj['attention_mask']
+
+        # postprocess for the [Q] marker and the [MASK] augmentation
+        ids[:, 1] = self.Q_marker_token_id
+        ids[ids == 0] = self.mask_token_id
+
+        if context is not None:
+            assert len(context) == len(batch_text), (len(context), len(batch_text))
+
+            obj_2 = self.tok(context, padding='longest', truncation=True,
+                            return_tensors='pt', max_length=self.background_maxlen)
+
+            ids_2, mask_2 = obj_2['input_ids'][:, 1:], obj_2['attention_mask'][:, 1:]  # Skip the first [SEP]
+
+            ids = torch.cat((ids, ids_2), dim=-1)
+            mask = torch.cat((mask, mask_2), dim=-1)
+
+        # if self.config.attend_to_mask_tokens:
+        if self.attend_to_mask_tokens:
+            mask[ids == self.mask_token_id] = 1
+            assert mask.sum().item() == mask.size(0) * mask.size(1), mask
+
+        if not self.used:
+            self.used = True
+            firstbg = (context is None) or context[0]
+            print_message("#> BERT QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
+            print_message(f"#> Input: {batch_text[0]}, \t\t {firstbg}, \t\t {bsize}")
+            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
+            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
+
+        if bsize:
+            batches = _split_into_batches(ids, mask, bsize)
+            return batches
+
+        return ids, mask
```

## primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization_xlmr.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-import torch
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert_xlmr import HF_ColBERT_XLMR
-from transformers import XLMRobertaTokenizer # there's no Fast version
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-# only the following official escape sequences are available
-# 0            0     <s>
-# 1            1   <pad>
-# 2            2    </s>
-# 3            3   <unk>
-# 250001  250001  <mask>
-#
-# we will use the following unofficial escape sequences:
-# 246260,246260,?,9748 '\u2614' Umbrella with Rain Drops
-# 245281,245281,?,9749 '\u2615' Hot Beverage
-
-
-class QueryTokenizerXLMR():
-    def __init__(self, query_maxlen, model_type):
-        # self.tok = XLMRobertaTokenizer.from_pretrained(model_type)
-        self.tok = HF_ColBERT_XLMR.raw_tokenizer_from_pretrained(model_type)
-        self.query_maxlen = query_maxlen
-
-        self.Q_marker_token, self.Q_marker_token_id = '?', 9748  # Umbrellawith Rain Drops
-        self.mask_token, self.mask_token_id = self.tok.pad_token, self.tok.pad_token_id
-
-#        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103
-        self.used = False
-
-    # tokenizer is not used colbert code base, but is implemented in QueryTokenizer
-    def tokenize(self, batch_text, add_special_tokens=False):
-        raise NotImplementedError()
-
-    # encode is not used colbert code base, but is implemented in QueryTokenizer
-    def encode(self, batch_text, add_special_tokens=False):
-        raise NotImplementedError()
-
-    def tensorize(self, batch_text, bsize=None, context=None):
-        assert type(batch_text) in [list, tuple], (type(batch_text))
-
-        # add placehold for the [Q] marker
-        # strangely, prefixing with '. ' introduces _two_ extra tokens [5,6]
-        # it seems that 6 is the empty string
-        # into the output - I don't understand why ...
-        batch_text = ['$ ' + x for x in batch_text]
-
-        obj = self.tok(batch_text, padding='max_length', truncation=True,
-                       return_tensors='pt', max_length=self.query_maxlen)
-
-        ids, mask = obj['input_ids'], obj['attention_mask']
-        # postprocess for the [Q] marker and the [MASK] augmentation
-        ids[:, 1] = self.Q_marker_token_id
-        #
-        # roberta tokenizer has pad_token_id=1, <s>=0, so the following statement must be omitted
-        #        ids[ids == 0] = self.mask_token_id
-        # I'm keeping commented-out code here in case of comparison with QueryTokenizer.py (bert)
-
-        if context is not None:
-            print_message(f"#> length of context: {len(context)}")
-
-        if not self.used:
-            self.used = True
-            firstbg = (context is None) or context[0]
-
-            print_message("#> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
-            print_message(f"#> Input: {batch_text[0]}, \t\t {firstbg}, \t\t {bsize}")
-            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
-            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
-
-        if bsize:
-            batches = _split_into_batches(ids, mask, bsize)
-            return batches
-
-        return ids, mask
-
-
-
-# In [1]: from colbert.modeling.tokenization import QueryTokenizerXLMR
-
-# In [2]: t=QueryTokenizerXLMR(50)
-
-# In [3]: t.tensorize(['what is the answer?', 'is that not completely ridiculously false?'])
-# (tensor([[     0,   9748,   2367,     83,     70,  35166,     32,      2,      1,
-#                1,      1,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1],
-#          [     0,   9748,     83,    450,    959,  64557, 236873,    538,  98320,
-#               32,      2,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1,      1,      1,      1,      1,
-#                1,      1,      1,      1,      1]]),
-#  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
-#           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
-#           0, 0],
-#          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
-#           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
-#           0, 0]]))
-
-# In [4]: t.Q_marker_token_id
-# Out[4]: 9748
-
-# In [5]: t.mask_token_id
-# Out[6]: 1
-
-# In [6]: t.tok.decode(range(5))
-# Out[7]: '<s><pad></s><unk>,'
-
+import torch
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.hf_colbert_xlmr import HF_ColBERT_XLMR
+from transformers import XLMRobertaTokenizer # there's no Fast version
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization.utils import _split_into_batches
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+# only the following official escape sequences are available
+# 0            0     <s>
+# 1            1   <pad>
+# 2            2    </s>
+# 3            3   <unk>
+# 250001  250001  <mask>
+#
+# we will use the following unofficial escape sequences:
+# 246260,246260,?,9748 '\u2614' Umbrella with Rain Drops
+# 245281,245281,?,9749 '\u2615' Hot Beverage
+
+
+class QueryTokenizerXLMR():
+    def __init__(self, query_maxlen, model_type):
+        # self.tok = XLMRobertaTokenizer.from_pretrained(model_type)
+        self.tok = HF_ColBERT_XLMR.raw_tokenizer_from_pretrained(model_type)
+        self.query_maxlen = query_maxlen
+
+        self.Q_marker_token, self.Q_marker_token_id = '?', 9748  # Umbrellawith Rain Drops
+        self.mask_token, self.mask_token_id = self.tok.pad_token, self.tok.pad_token_id
+
+#        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103
+        self.used = False
+
+    # tokenizer is not used colbert code base, but is implemented in QueryTokenizer
+    def tokenize(self, batch_text, add_special_tokens=False):
+        raise NotImplementedError()
+
+    # encode is not used colbert code base, but is implemented in QueryTokenizer
+    def encode(self, batch_text, add_special_tokens=False):
+        raise NotImplementedError()
+
+    def tensorize(self, batch_text, bsize=None, context=None):
+        assert type(batch_text) in [list, tuple], (type(batch_text))
+
+        # add placehold for the [Q] marker
+        # strangely, prefixing with '. ' introduces _two_ extra tokens [5,6]
+        # it seems that 6 is the empty string
+        # into the output - I don't understand why ...
+        batch_text = ['$ ' + x for x in batch_text]
+
+        obj = self.tok(batch_text, padding='max_length', truncation=True,
+                       return_tensors='pt', max_length=self.query_maxlen)
+
+        ids, mask = obj['input_ids'], obj['attention_mask']
+        # postprocess for the [Q] marker and the [MASK] augmentation
+        ids[:, 1] = self.Q_marker_token_id
+        #
+        # roberta tokenizer has pad_token_id=1, <s>=0, so the following statement must be omitted
+        #        ids[ids == 0] = self.mask_token_id
+        # I'm keeping commented-out code here in case of comparison with QueryTokenizer.py (bert)
+
+        if context is not None:
+            print_message(f"#> length of context: {len(context)}")
+
+        if not self.used:
+            self.used = True
+            firstbg = (context is None) or context[0]
+
+            print_message("#> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==")
+            print_message(f"#> Input: {batch_text[0]}, \t\t {firstbg}, \t\t {bsize}")
+            print_message(f"#> Output IDs: {ids[0].size()}, {ids[0]}")
+            print_message(f"#> Output Mask: {mask[0].size()}, {mask[0]}")
+
+        if bsize:
+            batches = _split_into_batches(ids, mask, bsize)
+            return batches
+
+        return ids, mask
+
+
+
+# In [1]: from colbert.modeling.tokenization import QueryTokenizerXLMR
+
+# In [2]: t=QueryTokenizerXLMR(50)
+
+# In [3]: t.tensorize(['what is the answer?', 'is that not completely ridiculously false?'])
+# (tensor([[     0,   9748,   2367,     83,     70,  35166,     32,      2,      1,
+#                1,      1,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1],
+#          [     0,   9748,     83,    450,    959,  64557, 236873,    538,  98320,
+#               32,      2,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1,      1,      1,      1,      1,
+#                1,      1,      1,      1,      1]]),
+#  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+#           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+#           0, 0],
+#          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+#           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+#           0, 0]]))
+
+# In [4]: t.Q_marker_token_id
+# Out[4]: 9748
+
+# In [5]: t.mask_token_id
+# Out[6]: 1
+
+# In [6]: t.tok.decode(range(5))
+# Out[7]: '<s><pad></s><unk>,'
+
```

## primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/utils.py

 * *Ordering differences only*

```diff
@@ -1,63 +1,63 @@
-import torch
-
-
-def tensorize_triples(query_tokenizer, doc_tokenizer, queries, passages, scores, bsize, nway):
-    # assert len(passages) == len(scores) == bsize * nway
-    # assert bsize is None or len(queries) % bsize == 0
-
-    # N = len(queries)
-    Q_ids, Q_mask = query_tokenizer.tensorize(queries)
-    D_ids, D_mask = doc_tokenizer.tensorize(passages)
-    # D_ids, D_mask = D_ids.view(2, N, -1), D_mask.view(2, N, -1)
-
-    # # Compute max among {length of i^th positive, length of i^th negative} for i \in N
-    # maxlens = D_mask.sum(-1).max(0).values
-
-    # # Sort by maxlens
-    # indices = maxlens.sort().indices
-    # Q_ids, Q_mask = Q_ids[indices], Q_mask[indices]
-    # D_ids, D_mask = D_ids[:, indices], D_mask[:, indices]
-
-    # (positive_ids, negative_ids), (positive_mask, negative_mask) = D_ids, D_mask
-
-    query_batches = _split_into_batches(Q_ids, Q_mask, bsize)
-    doc_batches = _split_into_batches(D_ids, D_mask, bsize * nway)
-    # positive_batches = _split_into_batches(positive_ids, positive_mask, bsize)
-    # negative_batches = _split_into_batches(negative_ids, negative_mask, bsize)
-
-    if len(scores):
-        score_batches = _split_into_batches2(scores, bsize * nway)
-    else:
-        score_batches = [[] for _ in doc_batches]
-
-    batches = []
-    for Q, D, S in zip(query_batches, doc_batches, score_batches):
-        batches.append((Q, D, S))
-
-    return batches
-
-
-def _sort_by_length(ids, mask, bsize):
-    if ids.size(0) <= bsize:
-        return ids, mask, torch.arange(ids.size(0))
-
-    indices = mask.sum(-1).sort().indices
-    reverse_indices = indices.sort().indices
-
-    return ids[indices], mask[indices], reverse_indices
-
-
-def _split_into_batches(ids, mask, bsize):
-    batches = []
-    for offset in range(0, ids.size(0), bsize):
-        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))
-
-    return batches
-
-
-def _split_into_batches2(scores, bsize):
-    batches = []
-    for offset in range(0, len(scores), bsize):
-        batches.append(scores[offset:offset+bsize])
-
-    return batches
+import torch
+
+
+def tensorize_triples(query_tokenizer, doc_tokenizer, queries, passages, scores, bsize, nway):
+    # assert len(passages) == len(scores) == bsize * nway
+    # assert bsize is None or len(queries) % bsize == 0
+
+    # N = len(queries)
+    Q_ids, Q_mask = query_tokenizer.tensorize(queries)
+    D_ids, D_mask = doc_tokenizer.tensorize(passages)
+    # D_ids, D_mask = D_ids.view(2, N, -1), D_mask.view(2, N, -1)
+
+    # # Compute max among {length of i^th positive, length of i^th negative} for i \in N
+    # maxlens = D_mask.sum(-1).max(0).values
+
+    # # Sort by maxlens
+    # indices = maxlens.sort().indices
+    # Q_ids, Q_mask = Q_ids[indices], Q_mask[indices]
+    # D_ids, D_mask = D_ids[:, indices], D_mask[:, indices]
+
+    # (positive_ids, negative_ids), (positive_mask, negative_mask) = D_ids, D_mask
+
+    query_batches = _split_into_batches(Q_ids, Q_mask, bsize)
+    doc_batches = _split_into_batches(D_ids, D_mask, bsize * nway)
+    # positive_batches = _split_into_batches(positive_ids, positive_mask, bsize)
+    # negative_batches = _split_into_batches(negative_ids, negative_mask, bsize)
+
+    if len(scores):
+        score_batches = _split_into_batches2(scores, bsize * nway)
+    else:
+        score_batches = [[] for _ in doc_batches]
+
+    batches = []
+    for Q, D, S in zip(query_batches, doc_batches, score_batches):
+        batches.append((Q, D, S))
+
+    return batches
+
+
+def _sort_by_length(ids, mask, bsize):
+    if ids.size(0) <= bsize:
+        return ids, mask, torch.arange(ids.size(0))
+
+    indices = mask.sum(-1).sort().indices
+    reverse_indices = indices.sort().indices
+
+    return ids[indices], mask[indices], reverse_indices
+
+
+def _split_into_batches(ids, mask, bsize):
+    batches = []
+    for offset in range(0, ids.size(0), bsize):
+        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))
+
+    return batches
+
+
+def _split_into_batches2(scores, bsize):
+    batches = []
+    for offset in range(0, len(scores), bsize):
+        batches.append(scores[offset:offset+bsize])
+
+    return batches
```

## primeqa/ir/dense/colbert_top/colbert/search/candidate_generation.py

 * *Ordering differences only*

```diff
@@ -1,63 +1,63 @@
-import torch
-
-from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
-from .strided_tensor_core import _create_mask, _create_view
-
-class CandidateGeneration:
-
-    def __init__(self, use_gpu=True):
-        self.use_gpu = use_gpu
-
-    def get_cells(self, Q, ncells):
-        scores = (self.codec.centroids @ Q.T)
-        if ncells == 1:
-            cells = scores.argmax(dim=0, keepdim=True).permute(1, 0)
-        else:
-            cells = scores.topk(ncells, dim=0, sorted=False).indices.permute(1, 0)  # (32, ncells)
-        cells = cells.flatten().contiguous()  # (32 * ncells,)
-        cells = cells.unique(sorted=False)
-        return cells, scores
-
-    def generate_candidate_eids(self, Q, ncells):
-        cells, scores = self.get_cells(Q, ncells)
-
-        eids, cell_lengths = self.ivf.lookup(cells)  # eids = (packedlen,)  lengths = (32 * ncells,)
-        eids = eids.long()
-        if self.use_gpu:
-            eids = eids.cuda()
-        return eids, scores
-
-    def generate_candidate_pids(self, Q, ncells):
-        cells, scores = self.get_cells(Q, ncells)
-
-        pids, cell_lengths = self.ivf.lookup(cells)
-        if self.use_gpu:
-            pids = pids.cuda()
-        return pids, scores
-
-    def generate_candidate_scores(self, Q, eids):
-        E = self.lookup_eids(eids)
-        if self.use_gpu:
-            E = E.cuda()
-        return (Q.unsqueeze(0) @ E.unsqueeze(2)).squeeze(-1).T
-
-    def generate_candidates(self, config, Q):
-        ncells = config.ncells
-
-        assert isinstance(self.ivf, StridedTensor)
-
-        Q = Q.squeeze(0)
-        if self.use_gpu:
-            Q = Q.cuda().half()
-        assert Q.dim() == 2
-
-        pids, centroid_scores = self.generate_candidate_pids(Q, ncells)
-
-        sorter = pids.sort()
-        pids = sorter.values
-
-        pids, pids_counts = torch.unique_consecutive(pids, return_counts=True)
-        if self.use_gpu:
-            pids, pids_counts = pids.cuda(), pids_counts.cuda()
-
-        return pids, centroid_scores
+import torch
+
+from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
+from .strided_tensor_core import _create_mask, _create_view
+
+class CandidateGeneration:
+
+    def __init__(self, use_gpu=True):
+        self.use_gpu = use_gpu
+
+    def get_cells(self, Q, ncells):
+        scores = (self.codec.centroids @ Q.T)
+        if ncells == 1:
+            cells = scores.argmax(dim=0, keepdim=True).permute(1, 0)
+        else:
+            cells = scores.topk(ncells, dim=0, sorted=False).indices.permute(1, 0)  # (32, ncells)
+        cells = cells.flatten().contiguous()  # (32 * ncells,)
+        cells = cells.unique(sorted=False)
+        return cells, scores
+
+    def generate_candidate_eids(self, Q, ncells):
+        cells, scores = self.get_cells(Q, ncells)
+
+        eids, cell_lengths = self.ivf.lookup(cells)  # eids = (packedlen,)  lengths = (32 * ncells,)
+        eids = eids.long()
+        if self.use_gpu:
+            eids = eids.cuda()
+        return eids, scores
+
+    def generate_candidate_pids(self, Q, ncells):
+        cells, scores = self.get_cells(Q, ncells)
+
+        pids, cell_lengths = self.ivf.lookup(cells)
+        if self.use_gpu:
+            pids = pids.cuda()
+        return pids, scores
+
+    def generate_candidate_scores(self, Q, eids):
+        E = self.lookup_eids(eids)
+        if self.use_gpu:
+            E = E.cuda()
+        return (Q.unsqueeze(0) @ E.unsqueeze(2)).squeeze(-1).T
+
+    def generate_candidates(self, config, Q):
+        ncells = config.ncells
+
+        assert isinstance(self.ivf, StridedTensor)
+
+        Q = Q.squeeze(0)
+        if self.use_gpu:
+            Q = Q.cuda().half()
+        assert Q.dim() == 2
+
+        pids, centroid_scores = self.generate_candidate_pids(Q, ncells)
+
+        sorter = pids.sort()
+        pids = sorter.values
+
+        pids, pids_counts = torch.unique_consecutive(pids, return_counts=True)
+        if self.use_gpu:
+            pids, pids_counts = pids.cuda(), pids_counts.cuda()
+
+        return pids, centroid_scores
```

## primeqa/ir/dense/colbert_top/colbert/search/index_loader.py

 * *Ordering differences only*

```diff
@@ -1,78 +1,78 @@
-import os
-import ujson
-import torch
-import numpy as np
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import lengths2offsets, print_message, dotdict, flatten
-from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual import ResidualCodec
-from primeqa.ir.dense.colbert_top.colbert.indexing.utils import optimize_ivf
-from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
-
-
-class IndexLoader:
-    def __init__(self, index_path, use_gpu=torch.cuda.is_available()):
-        self.index_path = index_path
-        self.use_gpu = use_gpu
-
-        self._load_codec()
-        self._load_ivf()
-
-        self._load_doclens()
-        self._load_embeddings()
-
-    def _load_codec(self):
-        print_message(f"#> Loading codec...")
-        self.codec = ResidualCodec.load(self.index_path)
-
-    def _load_ivf(self):
-        print_message(f"#> Loading IVF...")
-
-        if os.path.exists(os.path.join(self.index_path, "ivf.pid.pt")):
-            ivf, ivf_lengths = torch.load(os.path.join(self.index_path, "ivf.pid.pt"), map_location='cpu')
-        else:
-            assert os.path.exists(os.path.join(self.index_path, "ivf.pt")), f"ivf.pt not found in {self.index_path}"
-            ivf, ivf_lengths = torch.load(os.path.join(self.index_path, "ivf.pt"), map_location='cpu')
-            ivf, ivf_lengths = optimize_ivf(ivf, ivf_lengths, self.index_path)
-
-        ivf = StridedTensor(ivf, ivf_lengths, use_gpu=self.use_gpu)
-
-        self.ivf = ivf
-
-    def _load_doclens(self):
-        doclens = []
-
-        for chunk_idx in range(self.num_chunks):
-            with open(os.path.join(self.index_path, f'doclens.{chunk_idx}.json')) as f:
-                chunk_doclens = ujson.load(f)
-                doclens.extend(chunk_doclens)
-
-        self.doclens = torch.tensor(doclens)
-
-    def _load_embeddings(self):
-        self.embeddings = ResidualCodec.Embeddings.load_chunks(self.index_path, range(self.num_chunks),
-                                                               self.num_embeddings)
-
-    @property
-    def metadata(self):
-        try:
-            self._metadata
-        except:
-            with open(os.path.join(self.index_path, 'metadata.json')) as f:
-                self._metadata = ujson.load(f)
-
-        return self._metadata
-
-    @property
-    def config(self):
-        raise NotImplementedError()  # load from dict at metadata['config']
-
-    @property
-    def num_chunks(self):
-        # EVENTUALLY: If num_chunks doesn't exist (i.e., old index), fall back to counting doclens.*.json files.
-        return self.metadata['num_chunks']
-
-    @property
-    def num_embeddings(self):
-        # EVENTUALLY: If num_embeddings doesn't exist (i.e., old index), sum the values in doclens.*.json files.
-        return self.metadata['num_embeddings']
-
+import os
+import ujson
+import torch
+import numpy as np
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import lengths2offsets, print_message, dotdict, flatten
+from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual import ResidualCodec
+from primeqa.ir.dense.colbert_top.colbert.indexing.utils import optimize_ivf
+from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
+
+
+class IndexLoader:
+    def __init__(self, index_path, use_gpu=torch.cuda.is_available()):
+        self.index_path = index_path
+        self.use_gpu = use_gpu
+
+        self._load_codec()
+        self._load_ivf()
+
+        self._load_doclens()
+        self._load_embeddings()
+
+    def _load_codec(self):
+        print_message(f"#> Loading codec...")
+        self.codec = ResidualCodec.load(self.index_path)
+
+    def _load_ivf(self):
+        print_message(f"#> Loading IVF...")
+
+        if os.path.exists(os.path.join(self.index_path, "ivf.pid.pt")):
+            ivf, ivf_lengths = torch.load(os.path.join(self.index_path, "ivf.pid.pt"), map_location='cpu')
+        else:
+            assert os.path.exists(os.path.join(self.index_path, "ivf.pt")), f"ivf.pt not found in {self.index_path}"
+            ivf, ivf_lengths = torch.load(os.path.join(self.index_path, "ivf.pt"), map_location='cpu')
+            ivf, ivf_lengths = optimize_ivf(ivf, ivf_lengths, self.index_path)
+
+        ivf = StridedTensor(ivf, ivf_lengths, use_gpu=self.use_gpu)
+
+        self.ivf = ivf
+
+    def _load_doclens(self):
+        doclens = []
+
+        for chunk_idx in range(self.num_chunks):
+            with open(os.path.join(self.index_path, f'doclens.{chunk_idx}.json')) as f:
+                chunk_doclens = ujson.load(f)
+                doclens.extend(chunk_doclens)
+
+        self.doclens = torch.tensor(doclens)
+
+    def _load_embeddings(self):
+        self.embeddings = ResidualCodec.Embeddings.load_chunks(self.index_path, range(self.num_chunks),
+                                                               self.num_embeddings)
+
+    @property
+    def metadata(self):
+        try:
+            self._metadata
+        except:
+            with open(os.path.join(self.index_path, 'metadata.json')) as f:
+                self._metadata = ujson.load(f)
+
+        return self._metadata
+
+    @property
+    def config(self):
+        raise NotImplementedError()  # load from dict at metadata['config']
+
+    @property
+    def num_chunks(self):
+        # EVENTUALLY: If num_chunks doesn't exist (i.e., old index), fall back to counting doclens.*.json files.
+        return self.metadata['num_chunks']
+
+    @property
+    def num_embeddings(self):
+        # EVENTUALLY: If num_embeddings doesn't exist (i.e., old index), sum the values in doclens.*.json files.
+        return self.metadata['num_embeddings']
+
```

## primeqa/ir/dense/colbert_top/colbert/search/index_storage.py

 * *Ordering differences only*

```diff
@@ -1,188 +1,188 @@
-import torch
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message, print_torch_extension_error_message
-
-
-from primeqa.ir.dense.colbert_top.colbert.indexing.loaders import load_doclens
-from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings_strided import ResidualEmbeddingsStrided
-
-from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
-from primeqa.ir.dense.colbert_top.colbert.search.candidate_generation import CandidateGeneration
-
-from .index_loader import IndexLoader
-from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import colbert_score, colbert_score_packed, colbert_score_reduce
-
-from math import ceil
-
-import os
-import pathlib
-from torch.utils.cpp_extension import load
-import sys
-
-class IndexScorer(IndexLoader, CandidateGeneration):
-    def __init__(self, index_path, use_gpu):
-        super().__init__(index_path, use_gpu=use_gpu)
-
-        IndexScorer.try_load_torch_extensions(use_gpu)
-
-        self.embeddings_strided = ResidualEmbeddingsStrided(self.codec, self.embeddings, self.doclens)
-
-    @classmethod
-    def try_load_torch_extensions(cls, use_gpu):
-        if hasattr(cls, "loaded_extensions") or use_gpu:
-            return
-
-        verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True"
-
-        print_message(f"Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
-        try:
-            filter_pids_cpp = load(
-                name="filter_pids_cpp",
-                sources=[
-                    os.path.join(
-                        pathlib.Path(__file__).parent.resolve(), "filter_pids.cpp"
-                    ),
-                ],
-                extra_cflags=["-O3"],
-                verbose=verbose,
-            )
-        except (RuntimeError, KeyboardInterrupt) as e:
-            if not verbose:
-                import traceback
-                traceback.print_exc()
-            print_torch_extension_error_message()
-            sys.exit(1)
-        cls.filter_pids = filter_pids_cpp.filter_pids_cpp
-
-        print_message(f"Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
-        try:
-            decompress_residuals_cpp = load(
-                name="decompress_residuals_cpp",
-                sources=[
-                    os.path.join(
-                        pathlib.Path(__file__).parent.resolve(), "decompress_residuals.cpp"
-                    ),
-                ],
-                extra_cflags=["-O3"],
-                verbose=verbose,
-            )
-        except (RuntimeError, KeyboardInterrupt) as e:
-            if not verbose:
-                import traceback
-                traceback.print_exc()
-            print_torch_extension_error_message()
-            sys.exit(1)
-        cls.decompress_residuals = decompress_residuals_cpp.decompress_residuals_cpp
-
-        cls.loaded_extensions = True
-
-
-    def lookup_eids(self, embedding_ids, codes=None, out_device='cuda'):
-        return self.embeddings_strided.lookup_eids(embedding_ids, codes=codes, out_device=out_device)
-
-    def lookup_pids(self, passage_ids, out_device='cuda', return_mask=False):
-        return self.embeddings_strided.lookup_pids(passage_ids, out_device)
-
-    def retrieve(self, config, Q):
-        Q = Q[:, :config.query_maxlen]   # NOTE: Candidate generation uses only the query tokens
-        embedding_ids, centroid_scores = self.generate_candidates(config, Q)
-
-        return embedding_ids, centroid_scores
-
-    def embedding_ids_to_pids(self, embedding_ids):
-        all_pids = torch.unique(self.emb2pid[embedding_ids.long()].cuda(), sorted=False)
-        return all_pids
-
-    def rank(self, config, Q, k):
-        with torch.inference_mode():
-            pids, centroid_scores = self.retrieve(config, Q)
-            scores, pids = self.score_pids(config, Q, pids, centroid_scores)
-
-            scores_sorter = scores.sort(descending=True)
-            pids, scores = pids[scores_sorter.indices].tolist(), scores_sorter.values.tolist()
-
-            return pids, scores
-
-    def score_pids(self, config, Q, pids, centroid_scores):
-        """
-            Always supply a flat list or tensor for `pids`.
-
-            Supply sizes Q = (1 | num_docs, *, dim) and D = (num_docs, *, dim).
-            If Q.size(0) is 1, the matrix will be compared with all passages.
-            Otherwise, each query matrix will be compared against the *aligned* passage.
-        """
-
-        # TODO: Remove batching?
-        batch_size = 2 ** 20
-
-        if self.use_gpu:
-            centroid_scores = centroid_scores.cuda()
-
-        idx = centroid_scores.max(-1).values >= config.centroid_score_threshold
-
-        if self.use_gpu:
-            approx_scores = []
-
-            # Filter docs using pruned centroid scores
-            for i in range(0, ceil(len(pids) / batch_size)):
-                pids_ = pids[i * batch_size : (i+1) * batch_size]
-                codes_packed, codes_lengths = self.embeddings_strided.lookup_codes(pids_)
-                idx_ = idx[codes_packed.long()]
-                pruned_codes_strided = StridedTensor(idx_, codes_lengths, use_gpu=self.use_gpu)
-                pruned_codes_padded, pruned_codes_mask = pruned_codes_strided.as_padded_tensor()
-                pruned_codes_lengths = (pruned_codes_padded * pruned_codes_mask).sum(dim=1)
-                codes_packed_ = codes_packed[idx_]
-                approx_scores_ = centroid_scores[codes_packed_.long()]
-                if approx_scores_.shape[0] == 0:
-                    approx_scores.append(torch.zeros((len(pids_),), dtype=approx_scores_.dtype))
-                    continue
-                approx_scores_strided = StridedTensor(approx_scores_, pruned_codes_lengths, use_gpu=self.use_gpu)
-                approx_scores_padded, approx_scores_mask = approx_scores_strided.as_padded_tensor()
-                approx_scores_ = colbert_score_reduce(approx_scores_padded, approx_scores_mask, config)
-                approx_scores.append(approx_scores_)
-            approx_scores = torch.cat(approx_scores, dim=0)
-            if config.ndocs < len(approx_scores):
-                pids = pids[torch.topk(approx_scores, k=config.ndocs).indices]
-
-            # Filter docs using full centroid scores
-            codes_packed, codes_lengths = self.embeddings_strided.lookup_codes(pids)
-            approx_scores = centroid_scores[codes_packed.long()]
-            approx_scores_strided = StridedTensor(approx_scores, codes_lengths, use_gpu=self.use_gpu)
-            approx_scores_padded, approx_scores_mask = approx_scores_strided.as_padded_tensor()
-            approx_scores = colbert_score_reduce(approx_scores_padded, approx_scores_mask, config)
-            if config.ndocs // 4 < len(approx_scores):
-                pids = pids[torch.topk(approx_scores, k=(config.ndocs // 4)).indices]
-        else:
-            pids = IndexScorer.filter_pids(
-                    pids, centroid_scores, self.embeddings.codes, self.doclens,
-                    self.embeddings_strided.codes_strided.offsets, idx, config.ndocs
-                )
-
-        # Rank final list of docs using full approximate embeddings (including residuals)
-        if self.use_gpu:
-            D_packed, D_mask = self.lookup_pids(pids)
-        else:
-            D_packed = IndexScorer.decompress_residuals(
-                    pids,
-                    self.doclens,
-                    self.embeddings_strided.codes_strided.offsets,
-                    self.codec.bucket_weights,
-                    self.codec.reversed_bit_map,
-                    self.codec.decompression_lookup_table,
-                    self.embeddings.residuals,
-                    self.embeddings.codes,
-                    self.codec.centroids,
-                    self.codec.dim,
-                    self.codec.nbits
-                )
-            D_packed = torch.nn.functional.normalize(D_packed.to(torch.float32), p=2, dim=-1)
-            D_mask = self.doclens[pids.long()]
-
-        if Q.size(0) == 1:
-            return colbert_score_packed(Q, D_packed, D_mask, config), pids
-
-        D_strided = StridedTensor(D_packed, D_mask, use_gpu=self.use_gpu)
-        D_padded, D_lengths = D_strided.as_padded_tensor()
-
-        return colbert_score(Q, D_padded, D_lengths, config), pids
-
+import torch
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message, print_torch_extension_error_message
+
+
+from primeqa.ir.dense.colbert_top.colbert.indexing.loaders import load_doclens
+from primeqa.ir.dense.colbert_top.colbert.indexing.codecs.residual_embeddings_strided import ResidualEmbeddingsStrided
+
+from primeqa.ir.dense.colbert_top.colbert.search.strided_tensor import StridedTensor
+from primeqa.ir.dense.colbert_top.colbert.search.candidate_generation import CandidateGeneration
+
+from .index_loader import IndexLoader
+from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import colbert_score, colbert_score_packed, colbert_score_reduce
+
+from math import ceil
+
+import os
+import pathlib
+from torch.utils.cpp_extension import load
+import sys
+
+class IndexScorer(IndexLoader, CandidateGeneration):
+    def __init__(self, index_path, use_gpu):
+        super().__init__(index_path, use_gpu=use_gpu)
+
+        IndexScorer.try_load_torch_extensions(use_gpu)
+
+        self.embeddings_strided = ResidualEmbeddingsStrided(self.codec, self.embeddings, self.doclens)
+
+    @classmethod
+    def try_load_torch_extensions(cls, use_gpu):
+        if hasattr(cls, "loaded_extensions") or use_gpu:
+            return
+
+        verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True"
+
+        print_message(f"Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
+        try:
+            filter_pids_cpp = load(
+                name="filter_pids_cpp",
+                sources=[
+                    os.path.join(
+                        pathlib.Path(__file__).parent.resolve(), "filter_pids.cpp"
+                    ),
+                ],
+                extra_cflags=["-O3"],
+                verbose=verbose,
+            )
+        except (RuntimeError, KeyboardInterrupt) as e:
+            if not verbose:
+                import traceback
+                traceback.print_exc()
+            print_torch_extension_error_message()
+            sys.exit(1)
+        cls.filter_pids = filter_pids_cpp.filter_pids_cpp
+
+        print_message(f"Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
+        try:
+            decompress_residuals_cpp = load(
+                name="decompress_residuals_cpp",
+                sources=[
+                    os.path.join(
+                        pathlib.Path(__file__).parent.resolve(), "decompress_residuals.cpp"
+                    ),
+                ],
+                extra_cflags=["-O3"],
+                verbose=verbose,
+            )
+        except (RuntimeError, KeyboardInterrupt) as e:
+            if not verbose:
+                import traceback
+                traceback.print_exc()
+            print_torch_extension_error_message()
+            sys.exit(1)
+        cls.decompress_residuals = decompress_residuals_cpp.decompress_residuals_cpp
+
+        cls.loaded_extensions = True
+
+
+    def lookup_eids(self, embedding_ids, codes=None, out_device='cuda'):
+        return self.embeddings_strided.lookup_eids(embedding_ids, codes=codes, out_device=out_device)
+
+    def lookup_pids(self, passage_ids, out_device='cuda', return_mask=False):
+        return self.embeddings_strided.lookup_pids(passage_ids, out_device)
+
+    def retrieve(self, config, Q):
+        Q = Q[:, :config.query_maxlen]   # NOTE: Candidate generation uses only the query tokens
+        embedding_ids, centroid_scores = self.generate_candidates(config, Q)
+
+        return embedding_ids, centroid_scores
+
+    def embedding_ids_to_pids(self, embedding_ids):
+        all_pids = torch.unique(self.emb2pid[embedding_ids.long()].cuda(), sorted=False)
+        return all_pids
+
+    def rank(self, config, Q, k):
+        with torch.inference_mode():
+            pids, centroid_scores = self.retrieve(config, Q)
+            scores, pids = self.score_pids(config, Q, pids, centroid_scores)
+
+            scores_sorter = scores.sort(descending=True)
+            pids, scores = pids[scores_sorter.indices].tolist(), scores_sorter.values.tolist()
+
+            return pids, scores
+
+    def score_pids(self, config, Q, pids, centroid_scores):
+        """
+            Always supply a flat list or tensor for `pids`.
+
+            Supply sizes Q = (1 | num_docs, *, dim) and D = (num_docs, *, dim).
+            If Q.size(0) is 1, the matrix will be compared with all passages.
+            Otherwise, each query matrix will be compared against the *aligned* passage.
+        """
+
+        # TODO: Remove batching?
+        batch_size = 2 ** 20
+
+        if self.use_gpu:
+            centroid_scores = centroid_scores.cuda()
+
+        idx = centroid_scores.max(-1).values >= config.centroid_score_threshold
+
+        if self.use_gpu:
+            approx_scores = []
+
+            # Filter docs using pruned centroid scores
+            for i in range(0, ceil(len(pids) / batch_size)):
+                pids_ = pids[i * batch_size : (i+1) * batch_size]
+                codes_packed, codes_lengths = self.embeddings_strided.lookup_codes(pids_)
+                idx_ = idx[codes_packed.long()]
+                pruned_codes_strided = StridedTensor(idx_, codes_lengths, use_gpu=self.use_gpu)
+                pruned_codes_padded, pruned_codes_mask = pruned_codes_strided.as_padded_tensor()
+                pruned_codes_lengths = (pruned_codes_padded * pruned_codes_mask).sum(dim=1)
+                codes_packed_ = codes_packed[idx_]
+                approx_scores_ = centroid_scores[codes_packed_.long()]
+                if approx_scores_.shape[0] == 0:
+                    approx_scores.append(torch.zeros((len(pids_),), dtype=approx_scores_.dtype))
+                    continue
+                approx_scores_strided = StridedTensor(approx_scores_, pruned_codes_lengths, use_gpu=self.use_gpu)
+                approx_scores_padded, approx_scores_mask = approx_scores_strided.as_padded_tensor()
+                approx_scores_ = colbert_score_reduce(approx_scores_padded, approx_scores_mask, config)
+                approx_scores.append(approx_scores_)
+            approx_scores = torch.cat(approx_scores, dim=0)
+            if config.ndocs < len(approx_scores):
+                pids = pids[torch.topk(approx_scores, k=config.ndocs).indices]
+
+            # Filter docs using full centroid scores
+            codes_packed, codes_lengths = self.embeddings_strided.lookup_codes(pids)
+            approx_scores = centroid_scores[codes_packed.long()]
+            approx_scores_strided = StridedTensor(approx_scores, codes_lengths, use_gpu=self.use_gpu)
+            approx_scores_padded, approx_scores_mask = approx_scores_strided.as_padded_tensor()
+            approx_scores = colbert_score_reduce(approx_scores_padded, approx_scores_mask, config)
+            if config.ndocs // 4 < len(approx_scores):
+                pids = pids[torch.topk(approx_scores, k=(config.ndocs // 4)).indices]
+        else:
+            pids = IndexScorer.filter_pids(
+                    pids, centroid_scores, self.embeddings.codes, self.doclens,
+                    self.embeddings_strided.codes_strided.offsets, idx, config.ndocs
+                )
+
+        # Rank final list of docs using full approximate embeddings (including residuals)
+        if self.use_gpu:
+            D_packed, D_mask = self.lookup_pids(pids)
+        else:
+            D_packed = IndexScorer.decompress_residuals(
+                    pids,
+                    self.doclens,
+                    self.embeddings_strided.codes_strided.offsets,
+                    self.codec.bucket_weights,
+                    self.codec.reversed_bit_map,
+                    self.codec.decompression_lookup_table,
+                    self.embeddings.residuals,
+                    self.embeddings.codes,
+                    self.codec.centroids,
+                    self.codec.dim,
+                    self.codec.nbits
+                )
+            D_packed = torch.nn.functional.normalize(D_packed.to(torch.float32), p=2, dim=-1)
+            D_mask = self.doclens[pids.long()]
+
+        if Q.size(0) == 1:
+            return colbert_score_packed(Q, D_packed, D_mask, config), pids
+
+        D_strided = StridedTensor(D_packed, D_mask, use_gpu=self.use_gpu)
+        D_padded, D_lengths = D_strided.as_padded_tensor()
+
+        return colbert_score(Q, D_padded, D_lengths, config), pids
+
```

## primeqa/ir/dense/colbert_top/colbert/search/strided_tensor.py

 * *Ordering differences only*

```diff
@@ -1,222 +1,222 @@
-from struct import pack
-import torch
-from torch._C import device
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message
-
-from .strided_tensor_core import StridedTensorCore, _create_mask, _create_view
-import os
-import pathlib
-from torch.utils.cpp_extension import load
-
-
-class StridedTensor(StridedTensorCore):
-    def __init__(self, packed_tensor, lengths, dim=None, use_gpu=torch.cuda.is_available()):
-        super().__init__(packed_tensor, lengths, dim=dim, use_gpu=use_gpu)
-
-        StridedTensor.try_load_torch_extensions(use_gpu)
-
-    @classmethod
-    def try_load_torch_extensions(cls, use_gpu):
-        if hasattr(cls, "loaded_extensions") or use_gpu:
-            return
-
-        print_message(f"Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
-        segmented_lookup_cpp = load(
-            name="segmented_lookup_cpp",
-            sources=[
-                os.path.join(
-                    pathlib.Path(__file__).parent.resolve(), "segmented_lookup.cpp"
-                ),
-            ],
-            extra_cflags=["-O3"],
-            verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True",
-        )
-        cls.segmented_lookup = segmented_lookup_cpp.segmented_lookup_cpp
-
-        cls.loaded_extensions = True
-
-    @classmethod
-    def pad_packed(cls, packed_tensor, lengths):
-        assert False, "This seems to be incorrect but I can't see why. Is it the inner_dims in the views?"
-
-        packed_tensor, lengths = packed_tensor.cuda().contiguous(), lengths.cuda()
-
-        inner_dims = packed_tensor.size()[1:]
-        stride = lengths.max().item()
-        offsets = torch.cumsum(lengths, dim=0) - lengths[0]
-
-        padding = torch.zeros(stride, *inner_dims, device=packed_tensor.device, dtype=packed_tensor.dtype)
-        packed_tensor = torch.cat((packed_tensor, padding))
-
-        view = _create_view(packed_tensor, stride, inner_dims)[offsets]
-        mask = _create_mask(lengths, stride, like=view)
-
-        return view, mask
-
-    def _prepare_lookup(self, pids):
-        if isinstance(pids, list):
-            pids = torch.tensor(pids)
-
-        assert pids.dim() == 1
-
-        if self.use_gpu:
-            pids = pids.cuda()
-        pids = pids.long()
-        lengths = self.lengths[pids]
-        if self.use_gpu:
-            lengths = lengths.cuda()
-        offsets = self.offsets[pids]
-
-        return pids, lengths, offsets
-
-    def lookup(self, pids, output='packed'):
-        pids, lengths, offsets = self._prepare_lookup(pids)
-
-        if self.use_gpu:
-            stride = lengths.max().item()
-            stride = next(s for s in self.strides if stride <= s)
-
-            tensor = self.views[stride][offsets]
-            if self.use_gpu:
-                tensor = tensor.cuda()
-
-            mask = _create_mask(lengths, stride)
-
-            if output == 'padded':
-                return tensor, mask
-
-            assert output == 'packed'
-
-            tensor = tensor[mask]
-        else:
-            tensor = StridedTensor.segmented_lookup(self.tensor, pids, lengths, offsets)
-
-        return tensor, lengths
-
-    def lookup_staggered(self, pids, output='packed'):
-        permute_idxs, unordered_tensors, unordered_lengths, unordered_masks = self.lookup_packed_unordered(pids)
-
-        output_tensor = torch.empty(permute_idxs.size(0), self.max_stride, *self.inner_dims,
-                                    dtype=unordered_tensors[0].dtype, device=unordered_tensors[0].device)
-
-        output_mask = torch.zeros(permute_idxs.size(0), self.max_stride,
-                                  dtype=unordered_masks[0].dtype, device=unordered_masks[0].device)
-
-        offset = 0
-        for tensor, mask in zip(unordered_tensors, unordered_masks):
-            endpos = offset + tensor.size(0)
-            output_tensor[offset:endpos, :tensor.size(1)] = tensor
-            output_mask[offset:endpos, :mask.size(1)] = mask
-            offset = endpos
-
-        output_mask = output_mask[permute_idxs]
-        output_tensor = output_tensor[permute_idxs]
-
-        if output == 'padded':
-            return output_tensor, output_mask
-
-        assert output == 'packed'
-
-        output_tensor = output_tensor[output_mask]
-
-        return output_tensor, unordered_lengths[permute_idxs]
-
-    def lookup_packed_unordered(self, pids):
-        pids, lengths, offsets = self._prepare_lookup(pids)
-
-        lengths2 = lengths.clone()
-        sentinel = self.strides[-1] + 1
-        order = torch.arange(pids.size(0), device='cuda' if self.use_gpu else 'cpu')
-
-        all_orders = []
-        all_tensors = []
-        all_lengths = []
-        all_masks = []
-
-        for stride in self.strides:
-            is_shorter = lengths2 <= stride
-
-            if is_shorter.sum() == 0:
-                continue
-
-            order_ = order[is_shorter]
-            tensor_, lengths_, mask_ = self._lookup_with_stride(stride, lengths[is_shorter], offsets[is_shorter])
-
-            all_orders.append(order_)
-            all_tensors.append(tensor_)
-            all_lengths.append(lengths_)
-            all_masks.append(mask_)
-
-            lengths2[is_shorter] = sentinel
-
-        assert lengths2.allclose(torch.tensor([sentinel], device='cuda' if self.use_gpu else 'cpu'))
-
-        all_orders = torch.cat(all_orders)
-        permute_idxs = torch.sort(all_orders).indices
-
-        return permute_idxs, all_tensors, torch.cat(all_lengths), all_masks
-
-    def _lookup_with_stride(self, stride, lengths, offsets):
-        tensor = self.views[stride][offsets]
-        if self.use_gpu:
-            tensor = tensor.cuda()
-
-        mask = _create_mask(lengths, stride, use_gpu=self.use_gpu)
-        # tensor = tensor[mask]
-
-        return tensor, lengths, mask
-
-
-if __name__ == '__main__':
-    # lst = []
-    # for _ in range(10):
-    #     lst.append(list(range(random.randint(0, 10))))
-
-    # print(lst)
-
-    # t = StridedTensor.from_nested_list(lst)
-    # print(t.lookup([9]))
-
-    import os
-    import pickle
-
-    index_path = '/future/u/okhattab/root/unit/indexes/2021/08/residual.NQ-micro'
-    with open(os.path.join(index_path, "centroid_idx_to_embedding_ids.pickle"), "rb") as f:
-        ivf_list = pickle.load(f)
-
-    assert len(ivf_list) == max(ivf_list.keys()) + 1
-    ivf_list = [ivf_list[i] for i in range(len(ivf_list))]
-
-    for x in ivf_list:
-        assert type(x) is list
-        assert type(x[0]) is int
-
-    ncentroids = len(ivf_list)
-
-    ivf = StridedTensor.from_nested_list(ivf_list)
-
-    import time
-
-    torch.cuda.synchronize()
-    t = time.time()
-
-    N = 100
-    for _ in range(N):
-        probed_centroids = torch.randint(0, ncentroids, size=(32, 8)).flatten()
-        emb_ids, emb_ids_lengths = ivf.lookup(probed_centroids).as_packed_tensor()
-
-    torch.cuda.synchronize()
-    print((time.time() - t) * 1000 / N, 'ms')
-
-    print(emb_ids_lengths)
-
-    slow_result = flatten([ivf_list[idx] for idx in probed_centroids.flatten().tolist()])
-    print(emb_ids.size(), len(slow_result))
-
-    for a, b in zip(slow_result, emb_ids.flatten().tolist()):
-        assert a == b, (a, b)
-
-    print("#> Done!")
-
-    print(ivf.lookup(probed_centroids).as_padded_tensor()[0].size())
+from struct import pack
+import torch
+from torch._C import device
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message
+
+from .strided_tensor_core import StridedTensorCore, _create_mask, _create_view
+import os
+import pathlib
+from torch.utils.cpp_extension import load
+
+
+class StridedTensor(StridedTensorCore):
+    def __init__(self, packed_tensor, lengths, dim=None, use_gpu=torch.cuda.is_available()):
+        super().__init__(packed_tensor, lengths, dim=dim, use_gpu=use_gpu)
+
+        StridedTensor.try_load_torch_extensions(use_gpu)
+
+    @classmethod
+    def try_load_torch_extensions(cls, use_gpu):
+        if hasattr(cls, "loaded_extensions") or use_gpu:
+            return
+
+        print_message(f"Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...")
+        segmented_lookup_cpp = load(
+            name="segmented_lookup_cpp",
+            sources=[
+                os.path.join(
+                    pathlib.Path(__file__).parent.resolve(), "segmented_lookup.cpp"
+                ),
+            ],
+            extra_cflags=["-O3"],
+            verbose=os.getenv("COLBERT_LOAD_TORCH_EXTENSION_VERBOSE", "False") == "True",
+        )
+        cls.segmented_lookup = segmented_lookup_cpp.segmented_lookup_cpp
+
+        cls.loaded_extensions = True
+
+    @classmethod
+    def pad_packed(cls, packed_tensor, lengths):
+        assert False, "This seems to be incorrect but I can't see why. Is it the inner_dims in the views?"
+
+        packed_tensor, lengths = packed_tensor.cuda().contiguous(), lengths.cuda()
+
+        inner_dims = packed_tensor.size()[1:]
+        stride = lengths.max().item()
+        offsets = torch.cumsum(lengths, dim=0) - lengths[0]
+
+        padding = torch.zeros(stride, *inner_dims, device=packed_tensor.device, dtype=packed_tensor.dtype)
+        packed_tensor = torch.cat((packed_tensor, padding))
+
+        view = _create_view(packed_tensor, stride, inner_dims)[offsets]
+        mask = _create_mask(lengths, stride, like=view)
+
+        return view, mask
+
+    def _prepare_lookup(self, pids):
+        if isinstance(pids, list):
+            pids = torch.tensor(pids)
+
+        assert pids.dim() == 1
+
+        if self.use_gpu:
+            pids = pids.cuda()
+        pids = pids.long()
+        lengths = self.lengths[pids]
+        if self.use_gpu:
+            lengths = lengths.cuda()
+        offsets = self.offsets[pids]
+
+        return pids, lengths, offsets
+
+    def lookup(self, pids, output='packed'):
+        pids, lengths, offsets = self._prepare_lookup(pids)
+
+        if self.use_gpu:
+            stride = lengths.max().item()
+            stride = next(s for s in self.strides if stride <= s)
+
+            tensor = self.views[stride][offsets]
+            if self.use_gpu:
+                tensor = tensor.cuda()
+
+            mask = _create_mask(lengths, stride)
+
+            if output == 'padded':
+                return tensor, mask
+
+            assert output == 'packed'
+
+            tensor = tensor[mask]
+        else:
+            tensor = StridedTensor.segmented_lookup(self.tensor, pids, lengths, offsets)
+
+        return tensor, lengths
+
+    def lookup_staggered(self, pids, output='packed'):
+        permute_idxs, unordered_tensors, unordered_lengths, unordered_masks = self.lookup_packed_unordered(pids)
+
+        output_tensor = torch.empty(permute_idxs.size(0), self.max_stride, *self.inner_dims,
+                                    dtype=unordered_tensors[0].dtype, device=unordered_tensors[0].device)
+
+        output_mask = torch.zeros(permute_idxs.size(0), self.max_stride,
+                                  dtype=unordered_masks[0].dtype, device=unordered_masks[0].device)
+
+        offset = 0
+        for tensor, mask in zip(unordered_tensors, unordered_masks):
+            endpos = offset + tensor.size(0)
+            output_tensor[offset:endpos, :tensor.size(1)] = tensor
+            output_mask[offset:endpos, :mask.size(1)] = mask
+            offset = endpos
+
+        output_mask = output_mask[permute_idxs]
+        output_tensor = output_tensor[permute_idxs]
+
+        if output == 'padded':
+            return output_tensor, output_mask
+
+        assert output == 'packed'
+
+        output_tensor = output_tensor[output_mask]
+
+        return output_tensor, unordered_lengths[permute_idxs]
+
+    def lookup_packed_unordered(self, pids):
+        pids, lengths, offsets = self._prepare_lookup(pids)
+
+        lengths2 = lengths.clone()
+        sentinel = self.strides[-1] + 1
+        order = torch.arange(pids.size(0), device='cuda' if self.use_gpu else 'cpu')
+
+        all_orders = []
+        all_tensors = []
+        all_lengths = []
+        all_masks = []
+
+        for stride in self.strides:
+            is_shorter = lengths2 <= stride
+
+            if is_shorter.sum() == 0:
+                continue
+
+            order_ = order[is_shorter]
+            tensor_, lengths_, mask_ = self._lookup_with_stride(stride, lengths[is_shorter], offsets[is_shorter])
+
+            all_orders.append(order_)
+            all_tensors.append(tensor_)
+            all_lengths.append(lengths_)
+            all_masks.append(mask_)
+
+            lengths2[is_shorter] = sentinel
+
+        assert lengths2.allclose(torch.tensor([sentinel], device='cuda' if self.use_gpu else 'cpu'))
+
+        all_orders = torch.cat(all_orders)
+        permute_idxs = torch.sort(all_orders).indices
+
+        return permute_idxs, all_tensors, torch.cat(all_lengths), all_masks
+
+    def _lookup_with_stride(self, stride, lengths, offsets):
+        tensor = self.views[stride][offsets]
+        if self.use_gpu:
+            tensor = tensor.cuda()
+
+        mask = _create_mask(lengths, stride, use_gpu=self.use_gpu)
+        # tensor = tensor[mask]
+
+        return tensor, lengths, mask
+
+
+if __name__ == '__main__':
+    # lst = []
+    # for _ in range(10):
+    #     lst.append(list(range(random.randint(0, 10))))
+
+    # print(lst)
+
+    # t = StridedTensor.from_nested_list(lst)
+    # print(t.lookup([9]))
+
+    import os
+    import pickle
+
+    index_path = '/future/u/okhattab/root/unit/indexes/2021/08/residual.NQ-micro'
+    with open(os.path.join(index_path, "centroid_idx_to_embedding_ids.pickle"), "rb") as f:
+        ivf_list = pickle.load(f)
+
+    assert len(ivf_list) == max(ivf_list.keys()) + 1
+    ivf_list = [ivf_list[i] for i in range(len(ivf_list))]
+
+    for x in ivf_list:
+        assert type(x) is list
+        assert type(x[0]) is int
+
+    ncentroids = len(ivf_list)
+
+    ivf = StridedTensor.from_nested_list(ivf_list)
+
+    import time
+
+    torch.cuda.synchronize()
+    t = time.time()
+
+    N = 100
+    for _ in range(N):
+        probed_centroids = torch.randint(0, ncentroids, size=(32, 8)).flatten()
+        emb_ids, emb_ids_lengths = ivf.lookup(probed_centroids).as_packed_tensor()
+
+    torch.cuda.synchronize()
+    print((time.time() - t) * 1000 / N, 'ms')
+
+    print(emb_ids_lengths)
+
+    slow_result = flatten([ivf_list[idx] for idx in probed_centroids.flatten().tolist()])
+    print(emb_ids.size(), len(slow_result))
+
+    for a, b in zip(slow_result, emb_ids.flatten().tolist()):
+        assert a == b, (a, b)
+
+    print("#> Done!")
+
+    print(ivf.lookup(probed_centroids).as_padded_tensor()[0].size())
```

## primeqa/ir/dense/colbert_top/colbert/search/strided_tensor_core.py

 * *Ordering differences only*

```diff
@@ -1,117 +1,117 @@
-import torch
-import random
-
-import numpy as np
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten
-
-
-class StridedTensorCore:
-    def __init__(self, packed_tensor, lengths, dim=None, use_gpu=torch.cuda.is_available()):
-        self.dim = dim
-        self.tensor = packed_tensor
-        self.inner_dims = self.tensor.size()[1:]
-        self.use_gpu = use_gpu
-
-        self.lengths = lengths.long() if torch.is_tensor(lengths) else torch.LongTensor(lengths)
-
-        self.strides = _select_strides(self.lengths, [.5, .75, .9, .95]) + [self.lengths.max().item()]
-        self.max_stride = self.strides[-1]
-
-        zero = torch.zeros(1, dtype=torch.long, device=self.lengths.device)
-        self.offsets = torch.cat((zero, torch.cumsum(self.lengths, dim=0)))
-
-        if self.offsets[-2] + self.max_stride > self.tensor.size(0):
-            # if self.tensor.size(0) > 10_000_000:
-            #     print("#> WARNING: StridedTensor has to add padding, internally, to a large tensor.")
-            #     print("#> WARNING: Consider doing this padding in advance to save memory!")
-
-            padding = torch.zeros(self.max_stride, *self.inner_dims, dtype=self.tensor.dtype, device=self.tensor.device)
-            self.tensor = torch.cat((self.tensor, padding))
-
-        self.views = {stride: _create_view(self.tensor, stride, self.inner_dims) for stride in self.strides}
-
-    @classmethod
-    def from_packed_tensor(cls, tensor, lengths):
-        return cls(tensor, lengths)
-
-    @classmethod
-    def from_padded_tensor(cls, tensor, mask):
-        pass
-
-    @classmethod
-    def from_nested_list(cls, lst):
-        flat_lst = flatten(lst)
-
-        tensor = torch.Tensor(flat_lst)
-        lengths = [len(sublst) for sublst in lst]
-
-        return cls(tensor, lengths, dim=0)
-
-    @classmethod
-    def from_tensors_list(cls, tensors):
-        # torch.cat(tensors)
-        # lengths.
-        # cls(tensor, lengths)
-        raise NotImplementedError()
-
-    def as_packed_tensor(self, return_offsets=False):
-        unpadded_packed_tensor = self.tensor  # [:self.offsets[-1]]
-
-        return_vals = [unpadded_packed_tensor, self.lengths]
-
-        if return_offsets:
-            return_vals.append(self.offsets)
-
-        return tuple(return_vals)
-
-    def as_padded_tensor(self):
-        if self.use_gpu:
-            view = _create_view(self.tensor.cuda(), self.max_stride, self.inner_dims)[self.offsets[:-1]]
-            mask = _create_mask(self.lengths.cuda(), self.max_stride, like=view, use_gpu=self.use_gpu)
-        else:
-            view = _create_view(self.tensor, self.max_stride, self.inner_dims)[self.offsets[:-1]]
-            mask = _create_mask(self.lengths, self.max_stride, like=view, use_gpu=self.use_gpu)
-
-        return view, mask
-
-    def as_tensors_list(self):
-        raise NotImplementedError()
-
-
-
-def _select_strides(lengths, quantiles):
-    if lengths.size(0) < 5_000:
-        return _get_quantiles(lengths, quantiles)
-
-    sample = torch.randint(0, lengths.size(0), size=(2_000,))
-
-    return _get_quantiles(lengths[sample], quantiles)
-
-def _get_quantiles(lengths, quantiles):
-    return torch.quantile(lengths.float(), torch.tensor(quantiles, device=lengths.device)).int().tolist()
-
-
-def _create_view(tensor, stride, inner_dims):
-    outdim = tensor.size(0) - stride + 1
-    size = (outdim, stride, *inner_dims)
-
-    inner_dim_prod = int(np.prod(inner_dims))
-    multidim_stride = [inner_dim_prod, inner_dim_prod] + [1] * len(inner_dims)
-
-    return torch.as_strided(tensor, size=size, stride=multidim_stride)
-
-
-def _create_mask(lengths, stride, like=None, use_gpu=torch.cuda.is_available()):
-    if use_gpu:
-        mask = torch.arange(stride).cuda() + 1
-        mask = mask.unsqueeze(0) <= lengths.cuda().unsqueeze(-1)
-    else:
-        mask = torch.arange(stride) + 1
-        mask = mask.unsqueeze(0) <= lengths.unsqueeze(-1)
-
-    if like is not None:
-        for _ in range(like.dim() - mask.dim()):
-            mask = mask.unsqueeze(-1)
-
-    return mask
+import torch
+import random
+
+import numpy as np
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten
+
+
+class StridedTensorCore:
+    def __init__(self, packed_tensor, lengths, dim=None, use_gpu=torch.cuda.is_available()):
+        self.dim = dim
+        self.tensor = packed_tensor
+        self.inner_dims = self.tensor.size()[1:]
+        self.use_gpu = use_gpu
+
+        self.lengths = lengths.long() if torch.is_tensor(lengths) else torch.LongTensor(lengths)
+
+        self.strides = _select_strides(self.lengths, [.5, .75, .9, .95]) + [self.lengths.max().item()]
+        self.max_stride = self.strides[-1]
+
+        zero = torch.zeros(1, dtype=torch.long, device=self.lengths.device)
+        self.offsets = torch.cat((zero, torch.cumsum(self.lengths, dim=0)))
+
+        if self.offsets[-2] + self.max_stride > self.tensor.size(0):
+            # if self.tensor.size(0) > 10_000_000:
+            #     print("#> WARNING: StridedTensor has to add padding, internally, to a large tensor.")
+            #     print("#> WARNING: Consider doing this padding in advance to save memory!")
+
+            padding = torch.zeros(self.max_stride, *self.inner_dims, dtype=self.tensor.dtype, device=self.tensor.device)
+            self.tensor = torch.cat((self.tensor, padding))
+
+        self.views = {stride: _create_view(self.tensor, stride, self.inner_dims) for stride in self.strides}
+
+    @classmethod
+    def from_packed_tensor(cls, tensor, lengths):
+        return cls(tensor, lengths)
+
+    @classmethod
+    def from_padded_tensor(cls, tensor, mask):
+        pass
+
+    @classmethod
+    def from_nested_list(cls, lst):
+        flat_lst = flatten(lst)
+
+        tensor = torch.Tensor(flat_lst)
+        lengths = [len(sublst) for sublst in lst]
+
+        return cls(tensor, lengths, dim=0)
+
+    @classmethod
+    def from_tensors_list(cls, tensors):
+        # torch.cat(tensors)
+        # lengths.
+        # cls(tensor, lengths)
+        raise NotImplementedError()
+
+    def as_packed_tensor(self, return_offsets=False):
+        unpadded_packed_tensor = self.tensor  # [:self.offsets[-1]]
+
+        return_vals = [unpadded_packed_tensor, self.lengths]
+
+        if return_offsets:
+            return_vals.append(self.offsets)
+
+        return tuple(return_vals)
+
+    def as_padded_tensor(self):
+        if self.use_gpu:
+            view = _create_view(self.tensor.cuda(), self.max_stride, self.inner_dims)[self.offsets[:-1]]
+            mask = _create_mask(self.lengths.cuda(), self.max_stride, like=view, use_gpu=self.use_gpu)
+        else:
+            view = _create_view(self.tensor, self.max_stride, self.inner_dims)[self.offsets[:-1]]
+            mask = _create_mask(self.lengths, self.max_stride, like=view, use_gpu=self.use_gpu)
+
+        return view, mask
+
+    def as_tensors_list(self):
+        raise NotImplementedError()
+
+
+
+def _select_strides(lengths, quantiles):
+    if lengths.size(0) < 5_000:
+        return _get_quantiles(lengths, quantiles)
+
+    sample = torch.randint(0, lengths.size(0), size=(2_000,))
+
+    return _get_quantiles(lengths[sample], quantiles)
+
+def _get_quantiles(lengths, quantiles):
+    return torch.quantile(lengths.float(), torch.tensor(quantiles, device=lengths.device)).int().tolist()
+
+
+def _create_view(tensor, stride, inner_dims):
+    outdim = tensor.size(0) - stride + 1
+    size = (outdim, stride, *inner_dims)
+
+    inner_dim_prod = int(np.prod(inner_dims))
+    multidim_stride = [inner_dim_prod, inner_dim_prod] + [1] * len(inner_dims)
+
+    return torch.as_strided(tensor, size=size, stride=multidim_stride)
+
+
+def _create_mask(lengths, stride, like=None, use_gpu=torch.cuda.is_available()):
+    if use_gpu:
+        mask = torch.arange(stride).cuda() + 1
+        mask = mask.unsqueeze(0) <= lengths.cuda().unsqueeze(-1)
+    else:
+        mask = torch.arange(stride) + 1
+        mask = mask.unsqueeze(0) <= lengths.unsqueeze(-1)
+
+    if like is not None:
+        for _ in range(like.dim() - mask.dim()):
+            mask = mask.unsqueeze(-1)
+
+    return mask
```

## primeqa/ir/dense/colbert_top/colbert/training/eager_batcher_v2.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-
-import random
-import os
-import ujson
-
-from functools import partial
-from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, zipstar, remove_first_and_last_quote
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import tensorize_triples
-from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
-from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_collection
-
-from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
-from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
-from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
-
-
-
-class EagerBatcher():
-    def __init__(self, config: ColBERTConfig, triples, rank=0, nranks=1, is_teacher=False):
-        self.bsize, self.accumsteps = config.bsize, config.accumsteps
-        self.rank, self.nranks = rank, nranks
-        self.nway = config.nway
-
-        # self.query_tokenizer = QueryTokenizer(config)
-        # self.doc_tokenizer = DocTokenizer(config)
-        #self.query_tokenizer = get_query_tokenizer(config.model_type, config.query_maxlen, config.attend_to_mask_tokens)
-        #self.doc_tokenizer = get_doc_tokenizer(config.model_type, config.doc_maxlen)
-        self.query_tokenizer = get_query_tokenizer(config.model_type if (not is_teacher or config.teacher_model_type is None) else config.teacher_model_type, config.query_maxlen, config.attend_to_mask_tokens)
-        self.doc_tokenizer = get_doc_tokenizer(config.model_type if (not is_teacher or config.teacher_model_type is None) else config.teacher_model_type, config.doc_maxlen if not is_teacher else config.teacher_doc_maxlen)
-
-        self.tensorize_triples = partial(tensorize_triples, self.query_tokenizer, self.doc_tokenizer)
-        self.position = 0
-
-        self.triples = self._load_triples(triples, rank, nranks)
-        self.reader = open(triples, mode='r', encoding="utf-8")
-        self.length = len(self.reader.readlines())
-
-    def shuffle(self):
-        print_message("#> Shuffling triples...")
-        random.shuffle(self.triples)
-
-    def _load_triples(self, path, rank, nranks):
-        """
-        NOTE: For distributed sampling, this isn't equivalent to perfectly uniform sampling.
-        In particular, each subset is perfectly represented in every batch! However, since we never
-        repeat passes over the data, we never repeat any particular triple, and the split across
-        nodes is random (since the underlying file is pre-shuffled), there's no concern here.
-        """
-        print_message("#> Loading triples...")
-
-        triples = []
-
-        with open(path) as f:
-            for line_idx, line in enumerate(f):
-                if line_idx % nranks == rank:
-                    query, pos, neg = line.strip().split('\t')
-
-                    # triples.append((remove_first_and_last_quote(query), remove_first_and_last_quote(pos), remove_first_and_last_quote(neg)))
-                    triples.append((query, pos, neg))
-
-        return triples
-
-    def __iter__(self):
-        return self
-
-    def __len__(self):
-        return self.length
-
-    def __next__(self):
-        queries, positives, negatives = [], [], []
-        passages = []
-        scores = []
-
-        for line_idx in range(self.bsize * self.nranks):
-            if (self.position + line_idx) % self.nranks != self.rank:
-                continue
-
-            real_line_idx = (self.position + line_idx) % len(self.triples)
-            query, pos, neg = self.triples[real_line_idx]
-            pas = [ pos, neg ]
-            sco = []
-
-            queries.append(query)
-            passages.extend(pas)
-            scores.extend(sco)
-
-        self.position += line_idx + 1
-
-        return self.collate(queries, passages, scores)
-
-    def collate(self, queries, passages, scores):
-        assert len(queries) == self.bsize
-        assert len(passages) == self.nway * self.bsize
-
-        return self.tensorize_triples(queries, passages, scores, self.bsize // self.accumsteps, self.nway)
-
-    # adding for training loop logic
-    def skip_to_batch(self, batch_idx, intended_batch_size):
-        print_message(f'Skipping to batch #{batch_idx} (with intended_batch_size = {intended_batch_size}) for training.')
+
+import random
+import os
+import ujson
+
+from functools import partial
+from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, zipstar, remove_first_and_last_quote
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import tensorize_triples
+from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
+from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_collection
+
+from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
+from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
+from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
+
+
+
+class EagerBatcher():
+    def __init__(self, config: ColBERTConfig, triples, rank=0, nranks=1, is_teacher=False):
+        self.bsize, self.accumsteps = config.bsize, config.accumsteps
+        self.rank, self.nranks = rank, nranks
+        self.nway = config.nway
+
+        # self.query_tokenizer = QueryTokenizer(config)
+        # self.doc_tokenizer = DocTokenizer(config)
+        #self.query_tokenizer = get_query_tokenizer(config.model_type, config.query_maxlen, config.attend_to_mask_tokens)
+        #self.doc_tokenizer = get_doc_tokenizer(config.model_type, config.doc_maxlen)
+        self.query_tokenizer = get_query_tokenizer(config.model_type if (not is_teacher or config.teacher_model_type is None) else config.teacher_model_type, config.query_maxlen, config.attend_to_mask_tokens)
+        self.doc_tokenizer = get_doc_tokenizer(config.model_type if (not is_teacher or config.teacher_model_type is None) else config.teacher_model_type, config.doc_maxlen if not is_teacher else config.teacher_doc_maxlen)
+
+        self.tensorize_triples = partial(tensorize_triples, self.query_tokenizer, self.doc_tokenizer)
+        self.position = 0
+
+        self.triples = self._load_triples(triples, rank, nranks)
+        self.reader = open(triples, mode='r', encoding="utf-8")
+        self.length = len(self.reader.readlines())
+
+    def shuffle(self):
+        print_message("#> Shuffling triples...")
+        random.shuffle(self.triples)
+
+    def _load_triples(self, path, rank, nranks):
+        """
+        NOTE: For distributed sampling, this isn't equivalent to perfectly uniform sampling.
+        In particular, each subset is perfectly represented in every batch! However, since we never
+        repeat passes over the data, we never repeat any particular triple, and the split across
+        nodes is random (since the underlying file is pre-shuffled), there's no concern here.
+        """
+        print_message("#> Loading triples...")
+
+        triples = []
+
+        with open(path) as f:
+            for line_idx, line in enumerate(f):
+                if line_idx % nranks == rank:
+                    query, pos, neg = line.strip().split('\t')
+
+                    # triples.append((remove_first_and_last_quote(query), remove_first_and_last_quote(pos), remove_first_and_last_quote(neg)))
+                    triples.append((query, pos, neg))
+
+        return triples
+
+    def __iter__(self):
+        return self
+
+    def __len__(self):
+        return self.length
+
+    def __next__(self):
+        queries, positives, negatives = [], [], []
+        passages = []
+        scores = []
+
+        for line_idx in range(self.bsize * self.nranks):
+            if (self.position + line_idx) % self.nranks != self.rank:
+                continue
+
+            real_line_idx = (self.position + line_idx) % len(self.triples)
+            query, pos, neg = self.triples[real_line_idx]
+            pas = [ pos, neg ]
+            sco = []
+
+            queries.append(query)
+            passages.extend(pas)
+            scores.extend(sco)
+
+        self.position += line_idx + 1
+
+        return self.collate(queries, passages, scores)
+
+    def collate(self, queries, passages, scores):
+        assert len(queries) == self.bsize
+        assert len(passages) == self.nway * self.bsize
+
+        return self.tensorize_triples(queries, passages, scores, self.bsize // self.accumsteps, self.nway)
+
+    # adding for training loop logic
+    def skip_to_batch(self, batch_idx, intended_batch_size):
+        print_message(f'Skipping to batch #{batch_idx} (with intended_batch_size = {intended_batch_size}) for training.')
         self.position = intended_batch_size * batch_idx
```

## primeqa/ir/dense/colbert_top/colbert/training/lazy_batcher.py

 * *Ordering differences only*

```diff
@@ -1,89 +1,89 @@
-import random
-import os
-import ujson
-
-from functools import partial
-from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, zipstar
-from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import tensorize_triples
-from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
-from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_collection
-
-from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
-from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
-from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
-
-# from colbert.utils.runs import Run
-
-
-
-class LazyBatcher():
-    def __init__(self, config: ColBERTConfig, triples, queries, collection, rank=0, nranks=1):
-        self.bsize, self.accumsteps = config.bsize, config.accumsteps
-        self.nway = config.nway
-
-        # self.query_tokenizer = QueryTokenizer(config)
-        # self.doc_tokenizer = DocTokenizer(config)
-        self.query_tokenizer = get_query_tokenizer(config.model_type, config.query_maxlen, config.attend_to_mask_tokens)
-        self.doc_tokenizer = get_doc_tokenizer(config.model_type, config.doc_maxlen)
-
-        self.tensorize_triples = partial(tensorize_triples, self.query_tokenizer, self.doc_tokenizer)
-        self.position = 0
-
-        self.triples = Examples.cast(triples, nway=self.nway).tolist(rank, nranks)
-        self.queries = Queries.cast(queries)
-        self.collection = Collection.cast(collection)
-
-    def __iter__(self):
-        return self
-
-    def __len__(self):
-        return len(self.triples)
-
-    def __next__(self):
-        offset, endpos = self.position, min(self.position + self.bsize, len(self.triples))
-        self.position = endpos
-
-        if offset + self.bsize > len(self.triples):
-            raise StopIteration
-
-        all_queries, all_passages, all_scores = [], [], []
-
-        for position in range(offset, endpos):
-            query, *pids = self.triples[position]
-            pids = pids[:self.nway]
-
-            query = self.queries[query]
-
-            try:
-                pids, scores = zipstar(pids)
-            except:
-                scores = []
-
-            passages = [self.collection[pid] for pid in pids]
-
-
-
-            all_queries.append(query)
-            all_passages.extend(passages)
-            all_scores.extend(scores)
-        
-        assert len(all_scores) in [0, len(all_passages)], len(all_scores)
-
-        return self.collate(all_queries, all_passages, all_scores)
-
-    def collate(self, queries, passages, scores):
-        assert len(queries) == self.bsize
-        assert len(passages) == self.nway * self.bsize
-
-        return self.tensorize_triples(queries, passages, scores, self.bsize // self.accumsteps, self.nway)
-
-    # adding shuffle
-    def shuffle(self):
-        print_message("#> Shuffling triples...")
-        random.shuffle(self.triples)
-
-    # adding for training loop logic
-    def skip_to_batch(self, batch_idx, intended_batch_size):
-        print_message(f'Skipping to batch #{batch_idx} (with intended_batch_size = {intended_batch_size}) for training.')
-        self.position = intended_batch_size * batch_idx
+import random
+import os
+import ujson
+
+from functools import partial
+from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, zipstar
+from primeqa.ir.dense.colbert_top.colbert.modeling.tokenization import tensorize_triples
+from primeqa.ir.dense.colbert_top.colbert.modeling.factory import get_query_tokenizer, get_doc_tokenizer
+from primeqa.ir.dense.colbert_top.colbert.evaluation.loaders import load_collection
+
+from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
+from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
+from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
+
+# from colbert.utils.runs import Run
+
+
+
+class LazyBatcher():
+    def __init__(self, config: ColBERTConfig, triples, queries, collection, rank=0, nranks=1):
+        self.bsize, self.accumsteps = config.bsize, config.accumsteps
+        self.nway = config.nway
+
+        # self.query_tokenizer = QueryTokenizer(config)
+        # self.doc_tokenizer = DocTokenizer(config)
+        self.query_tokenizer = get_query_tokenizer(config.model_type, config.query_maxlen, config.attend_to_mask_tokens)
+        self.doc_tokenizer = get_doc_tokenizer(config.model_type, config.doc_maxlen)
+
+        self.tensorize_triples = partial(tensorize_triples, self.query_tokenizer, self.doc_tokenizer)
+        self.position = 0
+
+        self.triples = Examples.cast(triples, nway=self.nway).tolist(rank, nranks)
+        self.queries = Queries.cast(queries)
+        self.collection = Collection.cast(collection)
+
+    def __iter__(self):
+        return self
+
+    def __len__(self):
+        return len(self.triples)
+
+    def __next__(self):
+        offset, endpos = self.position, min(self.position + self.bsize, len(self.triples))
+        self.position = endpos
+
+        if offset + self.bsize > len(self.triples):
+            raise StopIteration
+
+        all_queries, all_passages, all_scores = [], [], []
+
+        for position in range(offset, endpos):
+            query, *pids = self.triples[position]
+            pids = pids[:self.nway]
+
+            query = self.queries[query]
+
+            try:
+                pids, scores = zipstar(pids)
+            except:
+                scores = []
+
+            passages = [self.collection[pid] for pid in pids]
+
+
+
+            all_queries.append(query)
+            all_passages.extend(passages)
+            all_scores.extend(scores)
+        
+        assert len(all_scores) in [0, len(all_passages)], len(all_scores)
+
+        return self.collate(all_queries, all_passages, all_scores)
+
+    def collate(self, queries, passages, scores):
+        assert len(queries) == self.bsize
+        assert len(passages) == self.nway * self.bsize
+
+        return self.tensorize_triples(queries, passages, scores, self.bsize // self.accumsteps, self.nway)
+
+    # adding shuffle
+    def shuffle(self):
+        print_message("#> Shuffling triples...")
+        random.shuffle(self.triples)
+
+    # adding for training loop logic
+    def skip_to_batch(self, batch_idx, intended_batch_size):
+        print_message(f'Skipping to batch #{batch_idx} (with intended_batch_size = {intended_batch_size}) for training.')
+        self.position = intended_batch_size * batch_idx
```

## primeqa/ir/dense/colbert_top/colbert/training/rerank_batcher.py

 * *Ordering differences only*

```diff
@@ -1,75 +1,75 @@
-import os
-import ujson
-
-from functools import partial
-from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message, zipstar
-from primeqa.ir.dense.colbert_top.colbert.modeling.reranker.tokenizer import RerankerTokenizer
-
-from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
-from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
-from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
-
-# from colbert.utils.runs import Run
-
-
-class RerankBatcher():
-    def __init__(self, config: ColBERTConfig, triples, queries, collection, rank=0, nranks=1):
-        self.bsize, self.accumsteps = config.bsize, config.accumsteps
-        self.nway = config.nway
-        
-        assert self.accumsteps == 1, "The tensorizer doesn't support larger accumsteps yet --- but it's easy to add."
-
-        self.tokenizer = RerankerTokenizer(total_maxlen=config.doc_maxlen, base=config.checkpoint)
-        self.position = 0
-
-        self.triples = Examples.cast(triples, nway=self.nway).tolist(rank, nranks)
-        self.queries = Queries.cast(queries)
-        self.collection = Collection.cast(collection)
-
-    def __iter__(self):
-        return self
-
-    def __len__(self):
-        return len(self.triples)
-
-    def __next__(self):
-        offset, endpos = self.position, min(self.position + self.bsize, len(self.triples))
-        self.position = endpos
-
-        if offset + self.bsize > len(self.triples):
-            raise StopIteration
-
-        all_queries, all_passages, all_scores = [], [], []
-
-        for position in range(offset, endpos):
-            query, *pids = self.triples[position]
-            pids = pids[:self.nway]
-
-            query = self.queries[query]
-
-            try:
-                pids, scores = zipstar(pids)
-            except:
-                scores = []
-
-            passages = [self.collection[pid] for pid in pids]
-
-            all_queries.append(query)
-            all_passages.extend(passages)
-            all_scores.extend(scores)
-        
-        assert len(all_scores) in [0, len(all_passages)], len(all_scores)
-
-        return self.collate(all_queries, all_passages, all_scores)
-
-    def collate(self, queries, passages, scores):
-        assert len(queries) == self.bsize
-        assert len(passages) == self.nway * self.bsize
-
-        queries = flatten([[query] * self.nway for query in queries])
-        return [(self.tokenizer.tensorize(queries, passages), scores)]
-
-    # def skip_to_batch(self, batch_idx, intended_batch_size):
-    #     Run.warn(f'Skipping to batch #{batch_idx} (with intended_batch_size = {intended_batch_size}) for training.')
-    #     self.position = intended_batch_size * batch_idx
+import os
+import ujson
+
+from functools import partial
+from primeqa.ir.dense.colbert_top.colbert.infra.config.config import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import flatten, print_message, zipstar
+from primeqa.ir.dense.colbert_top.colbert.modeling.reranker.tokenizer import RerankerTokenizer
+
+from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
+from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
+from primeqa.ir.dense.colbert_top.colbert.data.examples import Examples
+
+# from colbert.utils.runs import Run
+
+
+class RerankBatcher():
+    def __init__(self, config: ColBERTConfig, triples, queries, collection, rank=0, nranks=1):
+        self.bsize, self.accumsteps = config.bsize, config.accumsteps
+        self.nway = config.nway
+        
+        assert self.accumsteps == 1, "The tensorizer doesn't support larger accumsteps yet --- but it's easy to add."
+
+        self.tokenizer = RerankerTokenizer(total_maxlen=config.doc_maxlen, base=config.checkpoint)
+        self.position = 0
+
+        self.triples = Examples.cast(triples, nway=self.nway).tolist(rank, nranks)
+        self.queries = Queries.cast(queries)
+        self.collection = Collection.cast(collection)
+
+    def __iter__(self):
+        return self
+
+    def __len__(self):
+        return len(self.triples)
+
+    def __next__(self):
+        offset, endpos = self.position, min(self.position + self.bsize, len(self.triples))
+        self.position = endpos
+
+        if offset + self.bsize > len(self.triples):
+            raise StopIteration
+
+        all_queries, all_passages, all_scores = [], [], []
+
+        for position in range(offset, endpos):
+            query, *pids = self.triples[position]
+            pids = pids[:self.nway]
+
+            query = self.queries[query]
+
+            try:
+                pids, scores = zipstar(pids)
+            except:
+                scores = []
+
+            passages = [self.collection[pid] for pid in pids]
+
+            all_queries.append(query)
+            all_passages.extend(passages)
+            all_scores.extend(scores)
+        
+        assert len(all_scores) in [0, len(all_passages)], len(all_scores)
+
+        return self.collate(all_queries, all_passages, all_scores)
+
+    def collate(self, queries, passages, scores):
+        assert len(queries) == self.bsize
+        assert len(passages) == self.nway * self.bsize
+
+        queries = flatten([[query] * self.nway for query in queries])
+        return [(self.tokenizer.tensorize(queries, passages), scores)]
+
+    # def skip_to_batch(self, batch_idx, intended_batch_size):
+    #     Run.warn(f'Skipping to batch #{batch_idx} (with intended_batch_size = {intended_batch_size}) for training.')
+    #     self.position = intended_batch_size * batch_idx
```

## primeqa/ir/dense/colbert_top/colbert/training/training.py

 * *Ordering differences only*

```diff
@@ -1,493 +1,493 @@
-import os
-import errno
-import time
-import torch
-import math
-import random
-import torch.nn as nn
-import numpy as np
-import glob
-import sys
-import re
-import copy
-from collections import OrderedDict
-
-from queue import Empty
-
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-
-from transformers import AdamW, get_linear_schedule_with_warmup
-from primeqa.ir.dense.colbert_top.colbert.infra import ColBERTConfig
-from primeqa.ir.dense.colbert_top.colbert.training.rerank_batcher import RerankBatcher
-from primeqa.ir.dense.colbert_top.colbert.training.eager_batcher_v2 import EagerBatcher  # support text input
-
-from primeqa.ir.dense.colbert_top.colbert.utils.amp import MixedPrecisionManager
-from primeqa.ir.dense.colbert_top.colbert.training.lazy_batcher import LazyBatcher
-from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
-
-from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
-from primeqa.ir.dense.colbert_top.colbert.modeling.reranker.electra import ElectraReranker
-
-from primeqa.ir.dense.colbert_top.colbert.utils import signals
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, save_checkpoint
-from primeqa.ir.dense.colbert_top.colbert.training.utils import print_progress, manage_checkpoints_consumed_all_triples, manage_checkpoints_with_path_save
-
-
-def calculate_distance(student_out, teacher_out):
-    '''calculate the distance between student output tokens and teacher output tokens'''
-    # start = time.time()
-    prod = teacher_out.matmul(student_out.transpose(1, 2))
-    student_out_norm = torch.norm(student_out, p=2, dim=-1)
-    teacher_out_norm = torch.norm(teacher_out, p=2, dim=-1)
-    m = teacher_out_norm.unsqueeze(2) * student_out_norm.unsqueeze(1)
-    esp = torch.ones_like(m) * 10**-8
-    distance = torch.ones_like(m) - prod /(m + esp)
-    # end = time.time()
-    # print("time to calculation distance matrix: ", end - start)
-    return distance
-
-def align(maxlen, student_out, teacher_out, teacher_queries):
-    '''re-order teacher output tokens so that it aligns with
-    student output tokens with greedy search'''
-    batch_distance_array=calculate_distance(student_out, teacher_out)
-    batch_distance_array = batch_distance_array.cpu().detach().numpy()
-    for idx, distance_array in enumerate(batch_distance_array):
-        swaps = []
-        for i in range(maxlen):
-            minValue =np.amin(distance_array)
-            indexs = np.where(distance_array == np.amin(minValue))
-            #get the index of the first min value
-            i, j = indexs[0][0], indexs[1][0]
-            #swap arrary row i and row j
-            distance_array[[i, j]] = distance_array[[j, i]]
-            distance_array[j, :] = 10  #anything larger than 1 to avoid double count
-            distance_array[:, j] = 10
-            swaps.append((i,j))
-        for swap in swaps:
-            teacher_out[idx][[swap[0], swap[1]]] = teacher_out[idx][[swap[1], swap[0]]]
-            teacher_queries[0][idx][[swap[0], swap[1]]] = teacher_queries[0][idx][[swap[1], swap[0]]]
-
-def train(config: ColBERTConfig, triples, queries=None, collection=None):
-
-    if config.rank < 1:
-        config.help()
-
-    assert not ( config.use_ib_negatives and config.distill_query_passage_separately ) , f" Simultaneous use of --use_ib_negatives and --distill_query_passage_separately options is not supported (yet)"
-
-    # When checkpoint specified, we need to get model_type from previous run if necessary or as a model type
-    if config.checkpoint is not None:
-        if config.checkpoint.endswith('.dnn') or config.checkpoint.endswith('.model'):
-            # adding "or config.checkpoint.endswith('.model')" to be compatible with V1
-            checkpoint = torch_load_dnn(config.checkpoint)
-            # if checkpoint['model_type'] is not None:
-            assert 'model_type' in checkpoint and checkpoint['model_type'] is not None, f"missing or invalid  checkpoint type in {config.checkpoint}"
-            config.model_type = checkpoint['model_type']
-
-        # Use checkpoint as a model type
-        elif config.checkpoint == 'bert-base-uncased' or config.checkpoint =='bert-large-uncased' \
-                or config.checkpoint == 'xlm-roberta-base' or config.checkpoint == 'xlm-roberta-large':
-            config.model_type = config.checkpoint
-        else:
-            print_message(f"unsupported checkpoint type or format: {config.checkpoint}")
-            raise NotImplementedError
-
-    print_message(f"model type: {config.model_type}")
-
-    random.seed(config.rng_seed)
-    np.random.seed(config.rng_seed)
-    torch.manual_seed(config.rng_seed)
-    torch.cuda.manual_seed_all(config.rng_seed)
-
-    assert config.bsize % config.nranks == 0, (config.bsize, config.nranks)
-    config.bsize = config.bsize // config.nranks
-
-    print_message("Using config.bsize =", config.bsize, "(per process) and config.accumsteps =", config.accumsteps)
-
-    # the reader , the proper tokenizer is based on model type
-    if collection is not None:
-        if config.reranker:
-            reader = RerankBatcher(config, triples, queries, collection, (0 if config.rank == -1 else config.rank), config.nranks)
-        else:
-            reader = LazyBatcher(config, triples, queries, collection, (0 if config.rank == -1 else config.rank), config.nranks)
-        assert config.teacher_checkpoint is None, "Student/Teacher training is not supported for numerical triples (yet)"
-    else:
-        # support text input
-        reader = EagerBatcher(config, triples, (0 if config.rank == -1 else config.rank), config.nranks)
-        if config.teacher_checkpoint is not None:
-            teacher_reader = EagerBatcher(config, config.teacher_triples, (0 if config.rank == -1 else config.rank), config.nranks)
-
-    if not config.reranker:
-        colbert = ColBERT(name=config.model_type, colbert_config=config)
-
-        # add support pre-trained representation
-        if config.init_from_lm is not None and config.checkpoint is None:
-            # checkpoint should override init_from_lm since it continues an already init'd run
-            print_message(f"#> Load init from lm {config.init_from_lm}")
-            if DEVICE == torch.device("cuda"):
-                lmweights = torch.load(config.init_from_lm)
-            else:    # expect path to pytorch_model.bin
-                lmweights = torch.load(config.init_from_lm, map_location=torch.device('cpu'))  # expect path to pytorch_model.bin
-
-
-            lmweights['model.linear.weight'] = colbert.linear.weight
-            # we don't need the keys in the lm head
-            keys_to_drop = ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight',
-                            'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias']
-            if config.model_type == 'xlm-roberta-base':
-                # TODO other model types may have a few extra keys to handle also ...
-
-                # resolve conflict between bert and roberta
-                lmweights_new = OrderedDict([(re.sub(r'^roberta\.', 'model.bert.', key), value) for key, value in lmweights.items()])
-
-                lmweights_new['model.bert.pooler.dense.weight'] = colbert.bert.pooler.dense.weight
-                lmweights_new['model.bert.pooler.dense.bias'] = colbert.bert.pooler.dense.bias
-
-                # I don't know what roberta.embeddings.position_ids is but it doesn't seem to be part of the model ...
-                # keys_to_drop += ['roberta.embeddings.position_ids']
-            elif config.model_type == 'tinybert':
-                keys_to_drop = ["cls.predictions.bias", "cls.predictions.transform.dense.weight",
-                                "cls.predictions.transform.dense.bias", "cls.predictions.transform.LayerNorm.weight",
-                                "cls.predictions.transform.LayerNorm.bias", "cls.predictions.decoder.weight",
-                                "cls.seq_relationship.weight", "cls.seq_relationship.bias", "fit_denses.0.weight",
-                                "fit_denses.0.bias", "fit_denses.1.weight", "fit_denses.1.bias", "fit_denses.2.weight",
-                                "fit_denses.2.bias", "fit_denses.3.weight", "fit_denses.3.bias", "fit_denses.4.weight",
-                                "fit_denses.4.bias"]
-
-            for k in keys_to_drop:
-                lmweights_new.pop(k)
-
-            colbert.load_state_dict(lmweights_new,False)
-
-        # load from checkpoint if checkpoint is an actual model
-        if config.checkpoint is not None:
-            if config.checkpoint.endswith('.dnn') or config.checkpoint.endswith('.model'):
-                print_message(f"#> Starting from checkpoint {config.checkpoint}")
-                checkpoint = torch.load(config.checkpoint, map_location='cpu')
-
-                try:
-                    colbert.load_state_dict(checkpoint['model_state_dict'])
-                except:
-                    print_message("[WARNING] Loading checkpoint with strict=False")
-                    colbert.load_state_dict(checkpoint['model_state_dict'], strict=False)
-
-        if config.teacher_checkpoint is not None:
-            teacher_colbert = ColBERT(name=config.teacher_model_type, colbert_config=config)
-
-            if config.teacher_checkpoint.endswith('.dnn') or config.teacher_checkpoint.endswith('.model'):
-                print_message(f"#> Loading teacher checkpoint {config.teacher_checkpoint}")
-                teacher_checkpoint = torch.load(config.teacher_checkpoint, map_location='cpu')
-
-                try:
-                    teacher_colbert.load_state_dict(teacher_checkpoint['model_state_dict'])
-                except:
-                    print_message("[WARNING] Loading checkpoint with strict=False")
-                    teacher_colbert.load_state_dict(teacher_checkpoint['model_state_dict'], strict=False)
-    else:
-        colbert = ElectraReranker.from_pretrained(config.checkpoint)
-
-
-    colbert = colbert.to(DEVICE)
-    colbert.train()
-
-    if config.teacher_checkpoint is not None:
-        teacher_colbert = teacher_colbert.to(DEVICE)
-        if config.distill_query_passage_separately:
-            #assert False, "distill_query_passage_separately functionality is not supported (yet)"
-            print_message("distill_query_passage_separately functionality is not supported (yet)")
-            if config.loss_function == 'MSE':
-                student_teacher_loss_fct = torch.nn.MSELoss()
-            else:
-                student_teacher_loss_fct = torch.nn.L1Loss()
-        else:
-            student_teacher_loss_fct = torch.nn.KLDivLoss(reduction="batchmean")
-
-    if DEVICE == torch.device("cuda"):
-        colbert = torch.nn.parallel.DistributedDataParallel(colbert, device_ids=[config.rank],
-                                                        output_device=config.rank,
-                                                        find_unused_parameters=True)
-        if config.teacher_checkpoint is not None:
-            teacher_colbert = torch.nn.parallel.DistributedDataParallel(teacher_colbert, device_ids=[config.rank],
-                                                output_device=config.rank,
-                                                find_unused_parameters=True)
-
-
-    optimizer = AdamW(filter(lambda p: p.requires_grad, colbert.parameters()), lr=config.lr, eps=1e-8)
-    optimizer.zero_grad()
-
-    if config.resume_optimizer:
-        print_message(f"#> Resuming optimizer from checkpoint {config.checkpoint}")
-        torch.set_rng_state(checkpoint['torch_rng_state'].to(torch.get_rng_state().device))
-        if torch.cuda.is_available():
-            torch.cuda.set_rng_state_all([ state.to(torch.cuda.get_rng_state_all()[pos].device) for pos, state in enumerate(checkpoint['torch_cuda_rng_states']) ] )
-        np.random.set_state(checkpoint['np_rng_state'])
-        random.setstate(checkpoint['python_rng_state'])
-
-        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
-
-    scheduler = None
-    if config.warmup is not None:
-        print_message(f"#> LR will use {config.warmup} warmup steps and linear decay over {config.maxsteps} steps.")
-        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup,
-                                                    num_training_steps=config.maxsteps)
-    
-    warmup_bert = config.warmup_bert
-    if warmup_bert is not None:
-        set_bert_grad(colbert, False)
-
-    amp = MixedPrecisionManager(config.amp)
-    if config.resume_optimizer and config.amp:
-        amp.scaler.load_state_dict(checkpoint['scaler_state_dict'])
-
-    labels = torch.zeros(config.bsize, dtype=torch.long, device=DEVICE)
-
-    start_time = time.time()
-    train_loss = None
-    train_loss_mu = 0.999
-
-    start_batch_idx = 0
-
-    if config.resume:
-         assert config.checkpoint is not None
-         start_batch_idx = checkpoint['batch']
-         train_loss = checkpoint['train_loss']
-
-         # reader.skip_to_batch(start_batch_idx, checkpoint['arguments']['bsize'])
-         reader.skip_to_batch(start_batch_idx, config.bsize)
-         if config.teacher_checkpoint is not None:
-            teacher_reader.skip_to_batch(start_batch_idx, config.bsize)
-
-    maxsteps = min(config.maxsteps, math.ceil((config.epochs * len(reader)) / (config.bsize * config.nranks)))
-
-    path = os.path.join(Run().path_, 'checkpoints')
-    if not os.path.exists(path):
-        os.makedirs(path)
-
-    name = os.path.join(path, "colbert-EXIT.dnn")
-    # arguments = config.input_arguments.__dict__
-    exit_queue = signals.checkpoint_on_exit(config.rank)
-
-    print_message(f"maxsteps: {config.maxsteps}")
-    print_message(f"{config.epochs} epochs of {len(reader)} examples")
-    print_message(f"batch size: {config.bsize}")
-    print_message(f"maxsteps set to {maxsteps}")
-
-    print_message(f"start batch idx: {start_batch_idx}")
-
-    # TODO: unify the student/teacher and student-only cases
-    if config.teacher_checkpoint is not None:
-        for batch_idx, BatchSteps, teacher_BatchSteps in zip(range(start_batch_idx, maxsteps), reader, teacher_reader):
-            if (warmup_bert is not None) and warmup_bert <= batch_idx:
-                set_bert_grad(colbert, True)
-                warmup_bert = None
-
-            # support shuffle_every_epoch option
-            n_instances = batch_idx * config.bsize * config.nranks
-            if (n_instances + 1) % len(reader) < config.bsize * config.nranks:
-                print_message("#> ====== Epoch {}...".format((n_instances+1) // len(reader)))
-                # AttributeError: 'ColBERTConfig' object has no attribute 'shuffle_every_epoch'
-                if config.shuffle_every_epoch:
-                    print_message("[WARNING] Data shuffling is not supported (yet) for Student/Teacher training")
-                else:
-                    print_message("#> Shuffling not specified.")
-
-            this_batch_loss = 0.0
-
-            for queries_passages, teacher_queries_passages in zip(BatchSteps, teacher_BatchSteps):
-                assert(config.teacher_model_type is not None or torch.equal(queries_passages[1][0], teacher_queries_passages[1][0]))
-
-                with amp.context():
-                    if config.distill_query_passage_separately :
-                        if config.query_only:
-                            assert False, "Training with --query-only option is not supported (yet)."
-                        else:
-                            queries, passages, target_scores = queries_passages
-                            encoding = [queries, passages]
-                            scores, student_output_q, student_output_p = colbert(*encoding)
-
-                            with torch.no_grad():
-                                teacher_queries, teacher_passages, teacher_target_scores = teacher_queries_passages
-                                teacher_encoding = [teacher_queries, teacher_passages]
-                                teacher_scores, teacher_output_q, teacher_output_p  = teacher_colbert(*teacher_encoding)
-
-                            teacher_queries_toks_masks = (teacher_queries_passages[0][0].repeat_interleave(config.nway, dim=0).contiguous(), teacher_queries_passages[0][1].repeat_interleave(config.nway, dim=0).contiguous())
-                            teacher_queries = copy.deepcopy(teacher_queries_toks_masks)
-                            maxlen = config.query_maxlen
-                            align(maxlen, student_output_q, teacher_output_q, teacher_queries)
-                            loss = config.query_weight * student_teacher_loss_fct(student_output_q, teacher_output_q) + (1 - config.query_weight)*student_teacher_loss_fct(student_output_p, teacher_output_p)
-                    else:
-                        try:
-                            queries, passages, target_scores = queries_passages
-                            encoding = [queries, passages]
-                        except:
-                            encoding, target_scores = queries_passages
-                            encoding = [encoding.to(DEVICE)]
-
-                        scores = colbert(*encoding)
-
-                        if config.use_ib_negatives:
-                            scores, ib_loss = scores
-
-                        scores = scores.view(-1, config.nway)
-
-                        with torch.no_grad():
-                            try:
-                                teacher_queries, teacher_passages, teacher_target_scores = teacher_queries_passages
-                                teacher_encoding = [teacher_queries, teacher_passages]
-                            except:
-                                teacher_encoding, teacher_target_scores = teacher_queries_passages
-                                teacher_encoding = [teacher_encoding.to(DEVICE)]
-
-                            teacher_scores = teacher_colbert(*teacher_encoding)
-
-                            if config.use_ib_negatives:
-                                teacher_scores, teacher_ib_loss = teacher_scores
-
-                            teacher_scores = teacher_scores.view(-1, config.nway)
-
-                        loss = student_teacher_loss_fct(
-                                    torch.nn.functional.log_softmax(scores / config.student_teacher_temperature, dim=-1),
-                                    torch.nn.functional.softmax(teacher_scores / config.student_teacher_temperature, dim=-1),
-                                ) * (config.student_teacher_temperature ** 2)
-
-                    loss = loss / config.accumsteps
-
-                if config.rank < 1:
-                    print_progress(scores.view(-1,2) if config.distill_query_passage_separately else scores)
-
-                amp.backward(loss)
-
-                this_batch_loss += loss.item()
-
-            train_loss = this_batch_loss if train_loss is None else train_loss
-            train_loss = train_loss_mu * train_loss + (1 - train_loss_mu) * this_batch_loss
-
-            amp.step(colbert, optimizer, scheduler)
-
-            if config.rank < 1:
-                print_message(batch_idx, train_loss)
-
-                num_per_epoch = len(reader)
-                epoch_idx = ((batch_idx + 1) * config.bsize * config.nranks) // num_per_epoch - 1
-                try:
-                    exit_queue.get_nowait()
-                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, arguments)
-                    save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type)
-                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type, arguments)
-                    sys.exit(0)
-                except Empty:
-                    # manage_checkpoints(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
-                    manage_checkpoints_with_path_save(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
-
-    else:
-        for batch_idx, BatchSteps in zip(range(start_batch_idx, config.maxsteps), reader):
-            if (warmup_bert is not None) and warmup_bert <= batch_idx:
-                set_bert_grad(colbert, True)
-                warmup_bert = None
-
-            # support shuffle_every_epoch option
-            n_instances = batch_idx * config.bsize * config.nranks
-            if (n_instances + 1) % len(reader) < config.bsize * config.nranks:
-                print_message("#> ====== Epoch {}...".format((n_instances+1) // len(reader)))
-                # AttributeError: 'ColBERTConfig' object has no attribute 'shuffle_every_epoch'
-                if config.shuffle_every_epoch:
-                    print_message("#> Shuffling ...")
-                    reader.shuffle()
-                else:
-                    print_message("#> Shuffling not specified.")
-
-            this_batch_loss = 0.0
-
-            for batch in BatchSteps:
-                with amp.context():
-                    try:
-                        queries, passages, target_scores = batch
-                        encoding = [queries, passages]
-                    except:
-                        encoding, target_scores = batch
-                        encoding = [encoding.to(DEVICE)]
-
-                    scores = colbert(*encoding)
-
-                    if config.use_ib_negatives:
-                        scores, ib_loss = scores
-
-                    scores = scores.view(-1, config.nway)
-
-                    if len(target_scores) and not config.ignore_scores:
-                        target_scores = torch.tensor(target_scores).view(-1, config.nway).to(DEVICE)
-                        target_scores = target_scores * config.distillation_alpha
-                        target_scores = torch.nn.functional.log_softmax(target_scores, dim=-1)
-
-                        log_scores = torch.nn.functional.log_softmax(scores, dim=-1)
-                        loss = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)(log_scores, target_scores)
-                    else:
-                        loss = nn.CrossEntropyLoss()(scores, labels[:scores.size(0)])
-
-                    if config.use_ib_negatives:
-                        if config.rank < 1:
-                            print('\t\t\t\t', loss.item(), ib_loss.item())
-
-                        loss += ib_loss
-
-                    loss = loss / config.accumsteps
-
-                if config.rank < 1:
-                    print_progress(scores)
-
-                amp.backward(loss)
-
-                this_batch_loss += loss.item()
-
-            train_loss = this_batch_loss if train_loss is None else train_loss
-            train_loss = train_loss_mu * train_loss + (1 - train_loss_mu) * this_batch_loss
-
-            amp.step(colbert, optimizer, scheduler)
-
-            if config.rank < 1:
-                print_message(batch_idx, train_loss)
-
-                num_per_epoch = len(reader)
-                epoch_idx = ((batch_idx + 1) * config.bsize * config.nranks) // num_per_epoch - 1
-                try:
-                    exit_queue.get_nowait()
-                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, arguments)
-                    save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type)
-                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type, arguments)
-                    sys.exit(0)
-                except Empty:
-                    # manage_checkpoints(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
-                    manage_checkpoints_with_path_save(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
-
-    # save last model
-    name = os.path.join(path, "colbert-LAST.dnn")
-    print_message('name:' + name)
-    list_of_files = glob.glob(f'{path}/*.model')  # * means all if need specific format then *.csv
-    latest_file = max(list_of_files, key=os.path.getctime)
-    # Run.info(f"Make a sym link of {latest_file} to {name}")
-    print_message(f"Make a sym link of {latest_file} to {name}")
-    try:
-        os.symlink(latest_file, name)
-    except OSError as e:
-        if e.errno == errno.EEXIST:
-            os.remove(name)
-            os.symlink(latest_file, name)
-        else:
-            raise
-
-    if config.rank < 1:
-        print_message("#> Done with all triples!")
-        ckpt_path = manage_checkpoints_consumed_all_triples(config, colbert, optimizer, batch_idx+1, savepath=None, consumed_all_triples=True)
-        return ckpt_path  # TODO: This should validate and return the best checkpoint, not just the last one.
-
-    # just return latest file
-    return latest_file
-
-
-def set_bert_grad(colbert, value):
-    try:
-        for p in colbert.bert.parameters():
-            assert p.requires_grad is (not value)
-            p.requires_grad = value
-    except AttributeError:
-        set_bert_grad(colbert.module, value)
+import os
+import errno
+import time
+import torch
+import math
+import random
+import torch.nn as nn
+import numpy as np
+import glob
+import sys
+import re
+import copy
+from collections import OrderedDict
+
+from queue import Empty
+
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+
+from transformers import AdamW, get_linear_schedule_with_warmup
+from primeqa.ir.dense.colbert_top.colbert.infra import ColBERTConfig
+from primeqa.ir.dense.colbert_top.colbert.training.rerank_batcher import RerankBatcher
+from primeqa.ir.dense.colbert_top.colbert.training.eager_batcher_v2 import EagerBatcher  # support text input
+
+from primeqa.ir.dense.colbert_top.colbert.utils.amp import MixedPrecisionManager
+from primeqa.ir.dense.colbert_top.colbert.training.lazy_batcher import LazyBatcher
+from primeqa.ir.dense.colbert_top.colbert.parameters import DEVICE
+
+from primeqa.ir.dense.colbert_top.colbert.modeling.colbert import ColBERT
+from primeqa.ir.dense.colbert_top.colbert.modeling.reranker.electra import ElectraReranker
+
+from primeqa.ir.dense.colbert_top.colbert.utils import signals
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import torch_load_dnn
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, save_checkpoint
+from primeqa.ir.dense.colbert_top.colbert.training.utils import print_progress, manage_checkpoints_consumed_all_triples, manage_checkpoints_with_path_save
+
+
+def calculate_distance(student_out, teacher_out):
+    '''calculate the distance between student output tokens and teacher output tokens'''
+    # start = time.time()
+    prod = teacher_out.matmul(student_out.transpose(1, 2))
+    student_out_norm = torch.norm(student_out, p=2, dim=-1)
+    teacher_out_norm = torch.norm(teacher_out, p=2, dim=-1)
+    m = teacher_out_norm.unsqueeze(2) * student_out_norm.unsqueeze(1)
+    esp = torch.ones_like(m) * 10**-8
+    distance = torch.ones_like(m) - prod /(m + esp)
+    # end = time.time()
+    # print("time to calculation distance matrix: ", end - start)
+    return distance
+
+def align(maxlen, student_out, teacher_out, teacher_queries):
+    '''re-order teacher output tokens so that it aligns with
+    student output tokens with greedy search'''
+    batch_distance_array=calculate_distance(student_out, teacher_out)
+    batch_distance_array = batch_distance_array.cpu().detach().numpy()
+    for idx, distance_array in enumerate(batch_distance_array):
+        swaps = []
+        for i in range(maxlen):
+            minValue =np.amin(distance_array)
+            indexs = np.where(distance_array == np.amin(minValue))
+            #get the index of the first min value
+            i, j = indexs[0][0], indexs[1][0]
+            #swap arrary row i and row j
+            distance_array[[i, j]] = distance_array[[j, i]]
+            distance_array[j, :] = 10  #anything larger than 1 to avoid double count
+            distance_array[:, j] = 10
+            swaps.append((i,j))
+        for swap in swaps:
+            teacher_out[idx][[swap[0], swap[1]]] = teacher_out[idx][[swap[1], swap[0]]]
+            teacher_queries[0][idx][[swap[0], swap[1]]] = teacher_queries[0][idx][[swap[1], swap[0]]]
+
+def train(config: ColBERTConfig, triples, queries=None, collection=None):
+
+    if config.rank < 1:
+        config.help()
+
+    assert not ( config.use_ib_negatives and config.distill_query_passage_separately ) , f" Simultaneous use of --use_ib_negatives and --distill_query_passage_separately options is not supported (yet)"
+
+    # When checkpoint specified, we need to get model_type from previous run if necessary or as a model type
+    if config.checkpoint is not None:
+        if config.checkpoint.endswith('.dnn') or config.checkpoint.endswith('.model'):
+            # adding "or config.checkpoint.endswith('.model')" to be compatible with V1
+            checkpoint = torch_load_dnn(config.checkpoint)
+            # if checkpoint['model_type'] is not None:
+            assert 'model_type' in checkpoint and checkpoint['model_type'] is not None, f"missing or invalid  checkpoint type in {config.checkpoint}"
+            config.model_type = checkpoint['model_type']
+
+        # Use checkpoint as a model type
+        elif config.checkpoint == 'bert-base-uncased' or config.checkpoint =='bert-large-uncased' \
+                or config.checkpoint == 'xlm-roberta-base' or config.checkpoint == 'xlm-roberta-large':
+            config.model_type = config.checkpoint
+        else:
+            print_message(f"unsupported checkpoint type or format: {config.checkpoint}")
+            raise NotImplementedError
+
+    print_message(f"model type: {config.model_type}")
+
+    random.seed(config.rng_seed)
+    np.random.seed(config.rng_seed)
+    torch.manual_seed(config.rng_seed)
+    torch.cuda.manual_seed_all(config.rng_seed)
+
+    assert config.bsize % config.nranks == 0, (config.bsize, config.nranks)
+    config.bsize = config.bsize // config.nranks
+
+    print_message("Using config.bsize =", config.bsize, "(per process) and config.accumsteps =", config.accumsteps)
+
+    # the reader , the proper tokenizer is based on model type
+    if collection is not None:
+        if config.reranker:
+            reader = RerankBatcher(config, triples, queries, collection, (0 if config.rank == -1 else config.rank), config.nranks)
+        else:
+            reader = LazyBatcher(config, triples, queries, collection, (0 if config.rank == -1 else config.rank), config.nranks)
+        assert config.teacher_checkpoint is None, "Student/Teacher training is not supported for numerical triples (yet)"
+    else:
+        # support text input
+        reader = EagerBatcher(config, triples, (0 if config.rank == -1 else config.rank), config.nranks)
+        if config.teacher_checkpoint is not None:
+            teacher_reader = EagerBatcher(config, config.teacher_triples, (0 if config.rank == -1 else config.rank), config.nranks)
+
+    if not config.reranker:
+        colbert = ColBERT(name=config.model_type, colbert_config=config)
+
+        # add support pre-trained representation
+        if config.init_from_lm is not None and config.checkpoint is None:
+            # checkpoint should override init_from_lm since it continues an already init'd run
+            print_message(f"#> Load init from lm {config.init_from_lm}")
+            if DEVICE == torch.device("cuda"):
+                lmweights = torch.load(config.init_from_lm)
+            else:    # expect path to pytorch_model.bin
+                lmweights = torch.load(config.init_from_lm, map_location=torch.device('cpu'))  # expect path to pytorch_model.bin
+
+
+            lmweights['model.linear.weight'] = colbert.linear.weight
+            # we don't need the keys in the lm head
+            keys_to_drop = ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight',
+                            'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias']
+            if config.model_type == 'xlm-roberta-base':
+                # TODO other model types may have a few extra keys to handle also ...
+
+                # resolve conflict between bert and roberta
+                lmweights_new = OrderedDict([(re.sub(r'^roberta\.', 'model.bert.', key), value) for key, value in lmweights.items()])
+
+                lmweights_new['model.bert.pooler.dense.weight'] = colbert.bert.pooler.dense.weight
+                lmweights_new['model.bert.pooler.dense.bias'] = colbert.bert.pooler.dense.bias
+
+                # I don't know what roberta.embeddings.position_ids is but it doesn't seem to be part of the model ...
+                # keys_to_drop += ['roberta.embeddings.position_ids']
+            elif config.model_type == 'tinybert':
+                keys_to_drop = ["cls.predictions.bias", "cls.predictions.transform.dense.weight",
+                                "cls.predictions.transform.dense.bias", "cls.predictions.transform.LayerNorm.weight",
+                                "cls.predictions.transform.LayerNorm.bias", "cls.predictions.decoder.weight",
+                                "cls.seq_relationship.weight", "cls.seq_relationship.bias", "fit_denses.0.weight",
+                                "fit_denses.0.bias", "fit_denses.1.weight", "fit_denses.1.bias", "fit_denses.2.weight",
+                                "fit_denses.2.bias", "fit_denses.3.weight", "fit_denses.3.bias", "fit_denses.4.weight",
+                                "fit_denses.4.bias"]
+
+            for k in keys_to_drop:
+                lmweights_new.pop(k)
+
+            colbert.load_state_dict(lmweights_new,False)
+
+        # load from checkpoint if checkpoint is an actual model
+        if config.checkpoint is not None:
+            if config.checkpoint.endswith('.dnn') or config.checkpoint.endswith('.model'):
+                print_message(f"#> Starting from checkpoint {config.checkpoint}")
+                checkpoint = torch.load(config.checkpoint, map_location='cpu')
+
+                try:
+                    colbert.load_state_dict(checkpoint['model_state_dict'])
+                except:
+                    print_message("[WARNING] Loading checkpoint with strict=False")
+                    colbert.load_state_dict(checkpoint['model_state_dict'], strict=False)
+
+        if config.teacher_checkpoint is not None:
+            teacher_colbert = ColBERT(name=config.teacher_model_type, colbert_config=config)
+
+            if config.teacher_checkpoint.endswith('.dnn') or config.teacher_checkpoint.endswith('.model'):
+                print_message(f"#> Loading teacher checkpoint {config.teacher_checkpoint}")
+                teacher_checkpoint = torch.load(config.teacher_checkpoint, map_location='cpu')
+
+                try:
+                    teacher_colbert.load_state_dict(teacher_checkpoint['model_state_dict'])
+                except:
+                    print_message("[WARNING] Loading checkpoint with strict=False")
+                    teacher_colbert.load_state_dict(teacher_checkpoint['model_state_dict'], strict=False)
+    else:
+        colbert = ElectraReranker.from_pretrained(config.checkpoint)
+
+
+    colbert = colbert.to(DEVICE)
+    colbert.train()
+
+    if config.teacher_checkpoint is not None:
+        teacher_colbert = teacher_colbert.to(DEVICE)
+        if config.distill_query_passage_separately:
+            #assert False, "distill_query_passage_separately functionality is not supported (yet)"
+            print_message("distill_query_passage_separately functionality is not supported (yet)")
+            if config.loss_function == 'MSE':
+                student_teacher_loss_fct = torch.nn.MSELoss()
+            else:
+                student_teacher_loss_fct = torch.nn.L1Loss()
+        else:
+            student_teacher_loss_fct = torch.nn.KLDivLoss(reduction="batchmean")
+
+    if DEVICE == torch.device("cuda"):
+        colbert = torch.nn.parallel.DistributedDataParallel(colbert, device_ids=[config.rank],
+                                                        output_device=config.rank,
+                                                        find_unused_parameters=True)
+        if config.teacher_checkpoint is not None:
+            teacher_colbert = torch.nn.parallel.DistributedDataParallel(teacher_colbert, device_ids=[config.rank],
+                                                output_device=config.rank,
+                                                find_unused_parameters=True)
+
+
+    optimizer = AdamW(filter(lambda p: p.requires_grad, colbert.parameters()), lr=config.lr, eps=1e-8)
+    optimizer.zero_grad()
+
+    if config.resume_optimizer:
+        print_message(f"#> Resuming optimizer from checkpoint {config.checkpoint}")
+        torch.set_rng_state(checkpoint['torch_rng_state'].to(torch.get_rng_state().device))
+        if torch.cuda.is_available():
+            torch.cuda.set_rng_state_all([ state.to(torch.cuda.get_rng_state_all()[pos].device) for pos, state in enumerate(checkpoint['torch_cuda_rng_states']) ] )
+        np.random.set_state(checkpoint['np_rng_state'])
+        random.setstate(checkpoint['python_rng_state'])
+
+        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
+
+    scheduler = None
+    if config.warmup is not None:
+        print_message(f"#> LR will use {config.warmup} warmup steps and linear decay over {config.maxsteps} steps.")
+        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup,
+                                                    num_training_steps=config.maxsteps)
+    
+    warmup_bert = config.warmup_bert
+    if warmup_bert is not None:
+        set_bert_grad(colbert, False)
+
+    amp = MixedPrecisionManager(config.amp)
+    if config.resume_optimizer and config.amp:
+        amp.scaler.load_state_dict(checkpoint['scaler_state_dict'])
+
+    labels = torch.zeros(config.bsize, dtype=torch.long, device=DEVICE)
+
+    start_time = time.time()
+    train_loss = None
+    train_loss_mu = 0.999
+
+    start_batch_idx = 0
+
+    if config.resume:
+         assert config.checkpoint is not None
+         start_batch_idx = checkpoint['batch']
+         train_loss = checkpoint['train_loss']
+
+         # reader.skip_to_batch(start_batch_idx, checkpoint['arguments']['bsize'])
+         reader.skip_to_batch(start_batch_idx, config.bsize)
+         if config.teacher_checkpoint is not None:
+            teacher_reader.skip_to_batch(start_batch_idx, config.bsize)
+
+    maxsteps = min(config.maxsteps, math.ceil((config.epochs * len(reader)) / (config.bsize * config.nranks)))
+
+    path = os.path.join(Run().path_, 'checkpoints')
+    if not os.path.exists(path):
+        os.makedirs(path)
+
+    name = os.path.join(path, "colbert-EXIT.dnn")
+    # arguments = config.input_arguments.__dict__
+    exit_queue = signals.checkpoint_on_exit(config.rank)
+
+    print_message(f"maxsteps: {config.maxsteps}")
+    print_message(f"{config.epochs} epochs of {len(reader)} examples")
+    print_message(f"batch size: {config.bsize}")
+    print_message(f"maxsteps set to {maxsteps}")
+
+    print_message(f"start batch idx: {start_batch_idx}")
+
+    # TODO: unify the student/teacher and student-only cases
+    if config.teacher_checkpoint is not None:
+        for batch_idx, BatchSteps, teacher_BatchSteps in zip(range(start_batch_idx, maxsteps), reader, teacher_reader):
+            if (warmup_bert is not None) and warmup_bert <= batch_idx:
+                set_bert_grad(colbert, True)
+                warmup_bert = None
+
+            # support shuffle_every_epoch option
+            n_instances = batch_idx * config.bsize * config.nranks
+            if (n_instances + 1) % len(reader) < config.bsize * config.nranks:
+                print_message("#> ====== Epoch {}...".format((n_instances+1) // len(reader)))
+                # AttributeError: 'ColBERTConfig' object has no attribute 'shuffle_every_epoch'
+                if config.shuffle_every_epoch:
+                    print_message("[WARNING] Data shuffling is not supported (yet) for Student/Teacher training")
+                else:
+                    print_message("#> Shuffling not specified.")
+
+            this_batch_loss = 0.0
+
+            for queries_passages, teacher_queries_passages in zip(BatchSteps, teacher_BatchSteps):
+                assert(config.teacher_model_type is not None or torch.equal(queries_passages[1][0], teacher_queries_passages[1][0]))
+
+                with amp.context():
+                    if config.distill_query_passage_separately :
+                        if config.query_only:
+                            assert False, "Training with --query-only option is not supported (yet)."
+                        else:
+                            queries, passages, target_scores = queries_passages
+                            encoding = [queries, passages]
+                            scores, student_output_q, student_output_p = colbert(*encoding)
+
+                            with torch.no_grad():
+                                teacher_queries, teacher_passages, teacher_target_scores = teacher_queries_passages
+                                teacher_encoding = [teacher_queries, teacher_passages]
+                                teacher_scores, teacher_output_q, teacher_output_p  = teacher_colbert(*teacher_encoding)
+
+                            teacher_queries_toks_masks = (teacher_queries_passages[0][0].repeat_interleave(config.nway, dim=0).contiguous(), teacher_queries_passages[0][1].repeat_interleave(config.nway, dim=0).contiguous())
+                            teacher_queries = copy.deepcopy(teacher_queries_toks_masks)
+                            maxlen = config.query_maxlen
+                            align(maxlen, student_output_q, teacher_output_q, teacher_queries)
+                            loss = config.query_weight * student_teacher_loss_fct(student_output_q, teacher_output_q) + (1 - config.query_weight)*student_teacher_loss_fct(student_output_p, teacher_output_p)
+                    else:
+                        try:
+                            queries, passages, target_scores = queries_passages
+                            encoding = [queries, passages]
+                        except:
+                            encoding, target_scores = queries_passages
+                            encoding = [encoding.to(DEVICE)]
+
+                        scores = colbert(*encoding)
+
+                        if config.use_ib_negatives:
+                            scores, ib_loss = scores
+
+                        scores = scores.view(-1, config.nway)
+
+                        with torch.no_grad():
+                            try:
+                                teacher_queries, teacher_passages, teacher_target_scores = teacher_queries_passages
+                                teacher_encoding = [teacher_queries, teacher_passages]
+                            except:
+                                teacher_encoding, teacher_target_scores = teacher_queries_passages
+                                teacher_encoding = [teacher_encoding.to(DEVICE)]
+
+                            teacher_scores = teacher_colbert(*teacher_encoding)
+
+                            if config.use_ib_negatives:
+                                teacher_scores, teacher_ib_loss = teacher_scores
+
+                            teacher_scores = teacher_scores.view(-1, config.nway)
+
+                        loss = student_teacher_loss_fct(
+                                    torch.nn.functional.log_softmax(scores / config.student_teacher_temperature, dim=-1),
+                                    torch.nn.functional.softmax(teacher_scores / config.student_teacher_temperature, dim=-1),
+                                ) * (config.student_teacher_temperature ** 2)
+
+                    loss = loss / config.accumsteps
+
+                if config.rank < 1:
+                    print_progress(scores.view(-1,2) if config.distill_query_passage_separately else scores)
+
+                amp.backward(loss)
+
+                this_batch_loss += loss.item()
+
+            train_loss = this_batch_loss if train_loss is None else train_loss
+            train_loss = train_loss_mu * train_loss + (1 - train_loss_mu) * this_batch_loss
+
+            amp.step(colbert, optimizer, scheduler)
+
+            if config.rank < 1:
+                print_message(batch_idx, train_loss)
+
+                num_per_epoch = len(reader)
+                epoch_idx = ((batch_idx + 1) * config.bsize * config.nranks) // num_per_epoch - 1
+                try:
+                    exit_queue.get_nowait()
+                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, arguments)
+                    save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type)
+                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type, arguments)
+                    sys.exit(0)
+                except Empty:
+                    # manage_checkpoints(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
+                    manage_checkpoints_with_path_save(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
+
+    else:
+        for batch_idx, BatchSteps in zip(range(start_batch_idx, config.maxsteps), reader):
+            if (warmup_bert is not None) and warmup_bert <= batch_idx:
+                set_bert_grad(colbert, True)
+                warmup_bert = None
+
+            # support shuffle_every_epoch option
+            n_instances = batch_idx * config.bsize * config.nranks
+            if (n_instances + 1) % len(reader) < config.bsize * config.nranks:
+                print_message("#> ====== Epoch {}...".format((n_instances+1) // len(reader)))
+                # AttributeError: 'ColBERTConfig' object has no attribute 'shuffle_every_epoch'
+                if config.shuffle_every_epoch:
+                    print_message("#> Shuffling ...")
+                    reader.shuffle()
+                else:
+                    print_message("#> Shuffling not specified.")
+
+            this_batch_loss = 0.0
+
+            for batch in BatchSteps:
+                with amp.context():
+                    try:
+                        queries, passages, target_scores = batch
+                        encoding = [queries, passages]
+                    except:
+                        encoding, target_scores = batch
+                        encoding = [encoding.to(DEVICE)]
+
+                    scores = colbert(*encoding)
+
+                    if config.use_ib_negatives:
+                        scores, ib_loss = scores
+
+                    scores = scores.view(-1, config.nway)
+
+                    if len(target_scores) and not config.ignore_scores:
+                        target_scores = torch.tensor(target_scores).view(-1, config.nway).to(DEVICE)
+                        target_scores = target_scores * config.distillation_alpha
+                        target_scores = torch.nn.functional.log_softmax(target_scores, dim=-1)
+
+                        log_scores = torch.nn.functional.log_softmax(scores, dim=-1)
+                        loss = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)(log_scores, target_scores)
+                    else:
+                        loss = nn.CrossEntropyLoss()(scores, labels[:scores.size(0)])
+
+                    if config.use_ib_negatives:
+                        if config.rank < 1:
+                            print('\t\t\t\t', loss.item(), ib_loss.item())
+
+                        loss += ib_loss
+
+                    loss = loss / config.accumsteps
+
+                if config.rank < 1:
+                    print_progress(scores)
+
+                amp.backward(loss)
+
+                this_batch_loss += loss.item()
+
+            train_loss = this_batch_loss if train_loss is None else train_loss
+            train_loss = train_loss_mu * train_loss + (1 - train_loss_mu) * this_batch_loss
+
+            amp.step(colbert, optimizer, scheduler)
+
+            if config.rank < 1:
+                print_message(batch_idx, train_loss)
+
+                num_per_epoch = len(reader)
+                epoch_idx = ((batch_idx + 1) * config.bsize * config.nranks) // num_per_epoch - 1
+                try:
+                    exit_queue.get_nowait()
+                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, arguments)
+                    save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type)
+                    # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, config.model_type, arguments)
+                    sys.exit(0)
+                except Empty:
+                    # manage_checkpoints(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
+                    manage_checkpoints_with_path_save(config, colbert, optimizer, amp, batch_idx + 1, num_per_epoch, epoch_idx, train_loss)
+
+    # save last model
+    name = os.path.join(path, "colbert-LAST.dnn")
+    print_message('name:' + name)
+    list_of_files = glob.glob(f'{path}/*.model')  # * means all if need specific format then *.csv
+    latest_file = max(list_of_files, key=os.path.getctime)
+    # Run.info(f"Make a sym link of {latest_file} to {name}")
+    print_message(f"Make a sym link of {latest_file} to {name}")
+    try:
+        os.symlink(latest_file, name)
+    except OSError as e:
+        if e.errno == errno.EEXIST:
+            os.remove(name)
+            os.symlink(latest_file, name)
+        else:
+            raise
+
+    if config.rank < 1:
+        print_message("#> Done with all triples!")
+        ckpt_path = manage_checkpoints_consumed_all_triples(config, colbert, optimizer, batch_idx+1, savepath=None, consumed_all_triples=True)
+        return ckpt_path  # TODO: This should validate and return the best checkpoint, not just the last one.
+
+    # just return latest file
+    return latest_file
+
+
+def set_bert_grad(colbert, value):
+    try:
+        for p in colbert.bert.parameters():
+            assert p.requires_grad is (not value)
+            p.requires_grad = value
+    except AttributeError:
+        set_bert_grad(colbert.module, value)
```

## primeqa/ir/dense/colbert_top/colbert/training/utils.py

 * *Ordering differences only*

```diff
@@ -1,130 +1,130 @@
-import os
-import torch
-
-# from colbert.utils.runs import Run
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, save_checkpoint
-from primeqa.ir.dense.colbert_top.colbert.parameters import SAVED_CHECKPOINTS, SAVED_STEPS_PROGRESS
-from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
-
-
-def print_progress(scores):
-    positive_avg, negative_avg = round(scores[:, 0].mean().item(), 2), round(scores[:, 1].mean().item(), 2)
-    print("#>>>   ", positive_avg, negative_avg, '\t\t|\t\t', positive_avg - negative_avg)
-
-# change the "manage_checkpoints" to "manage_checkpoints_consumed_all_triples" as we use it aftere consumed akk triples
-def manage_checkpoints_consumed_all_triples(args, colbert, optimizer, batch_idx, savepath=None, consumed_all_triples=False):
-    # arguments = dict(args)
-    # arguments = args.input_arguments.__dict__
-
-    # TODO: Call provenance() on the values that support it??
-
-    checkpoints_path = savepath or os.path.join(Run().path_, 'checkpoints')
-    name = None
-
-    try:
-        save = colbert.save
-    except:
-        save = colbert.module.save
-
-    if not os.path.exists(checkpoints_path):
-        os.makedirs(checkpoints_path)
-    
-    path_save = None
-
-    if consumed_all_triples or (batch_idx % 2000 == 0):
-        # name = os.path.join(path, "colbert.dnn")
-        # save_checkpoint(name, 0, batch_idx, colbert, optimizer, arguments)
-        path_save = os.path.join(checkpoints_path, "colbert")
-
-    if batch_idx in SAVED_CHECKPOINTS:
-        # name = os.path.join(path, "colbert-{}.dnn".format(batch_idx))
-        # save_checkpoint(name, 0, batch_idx, colbert, optimizer, arguments)
-        path_save = os.path.join(checkpoints_path, f"colbert-{batch_idx}")
-
-    if path_save:
-        print(f"#> Saving a checkpoint to {path_save} ..")
-
-        # checkpoint = {}
-        # checkpoint['batch'] = batch_idx
-        # checkpoint['epoch'] = 0
-        # checkpoint['model_state_dict'] = model.state_dict()
-        # checkpoint['optimizer_state_dict'] = optimizer.state_dict()
-        # checkpoint['arguments'] = arguments
-
-        save(path_save)
-
-    return path_save
-
-def manage_checkpoints_with_path_save(args, colbert, optimizer, amp, batch_idx, num_per_epoch, epoch_idx=0, train_loss=0):
-    # arguments = args.input_arguments.__dict__
-
-    saved_name = ""
-    # NOTE: V2 uses "from colbert.infra.run import Run"  vs. V1 uses "from colbert.utils.runs import Run"
-    # path = os.path.join(Run.path, 'checkpoints')
-    path = os.path.join(Run().path_, 'checkpoints')
-
-    if not os.path.exists(path):
-        os.makedirs(path)
-    prefix = os.path.join(path, "colbert.dnn")
-
-    path_save = None
-
-    # saving the last checkpoint, to be rewritten every args.save_steps
-    if batch_idx % SAVED_STEPS_PROGRESS == 0:
-        saved_name = prefix + f".progress.model"
-        save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
-
-    if args.save_epochs == -1:
-        if batch_idx % args.save_steps == 0:
-            saved_name = prefix + f".batch_{batch_idx}.model"
-            save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
-            # save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
-
-            path_save = os.path.join(path, f"colbert-batch_{batch_idx}")
-    else:
-        if batch_idx * args.bsize * args.nranks % int(args.save_epochs * num_per_epoch) < args.bsize * args.nranks:
-            if args.save_epochs.is_integer():
-                saved_name = prefix + f".epoch_{epoch_idx}.model"
-                path_save = os.path.join(path, f"colbert-epoch_{epoch_idx}")
-            else:
-                saved_name = prefix + f".epoch_{epoch_idx}_batch_{batch_idx}.model"
-                path_save = os.path.join(path, f"colbert-epoch_{epoch_idx}_batch_{batch_idx}")
-
-            save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
-            # save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
-
-
-    if batch_idx in SAVED_CHECKPOINTS or batch_idx == args.maxsteps:
-        name = prefix + f".batch_{batch_idx}.model"
-        if not name == saved_name:
-            save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
-            # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
-            path_save = os.path.join(path, f"colbert-batch_{batch_idx}")
-
-    if (batch_idx * args.bsize * args.nranks) % (args.epochs * num_per_epoch) < args.bsize * args.nranks:
-        name = prefix + f".epoch_{args.epochs - 1}.model"
-        if not name == saved_name:
-            save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
-            # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
-            path_save = os.path.join(path, f"colbert-epoch_{epoch_idx}_batch_{batch_idx}")
-    try:
-        save = colbert.save
-    except:
-        save = colbert.module.save
-
-    if path_save:
-        print(f"#> Saving a checkpoint to {path_save} ..")
-
-        # NOTE: none of the following would be reflected in the output path even if specified
-        # checkpoint = {}
-        # checkpoint['batch'] = batch_idx
-        # checkpoint['epoch'] = 0
-        # checkpoint['model_state_dict'] = model.state_dict()
-        # checkpoint['optimizer_state_dict'] = optimizer.state_dict()
-        #checkpoint['arguments'] = arguments
-
-        if not os.path.exists(path_save):
-            os.makedirs(path_save)
-        save(path_save)
-
-    return path_save
+import os
+import torch
+
+# from colbert.utils.runs import Run
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, save_checkpoint
+from primeqa.ir.dense.colbert_top.colbert.parameters import SAVED_CHECKPOINTS, SAVED_STEPS_PROGRESS
+from primeqa.ir.dense.colbert_top.colbert.infra.run import Run
+
+
+def print_progress(scores):
+    positive_avg, negative_avg = round(scores[:, 0].mean().item(), 2), round(scores[:, 1].mean().item(), 2)
+    print("#>>>   ", positive_avg, negative_avg, '\t\t|\t\t', positive_avg - negative_avg)
+
+# change the "manage_checkpoints" to "manage_checkpoints_consumed_all_triples" as we use it aftere consumed akk triples
+def manage_checkpoints_consumed_all_triples(args, colbert, optimizer, batch_idx, savepath=None, consumed_all_triples=False):
+    # arguments = dict(args)
+    # arguments = args.input_arguments.__dict__
+
+    # TODO: Call provenance() on the values that support it??
+
+    checkpoints_path = savepath or os.path.join(Run().path_, 'checkpoints')
+    name = None
+
+    try:
+        save = colbert.save
+    except:
+        save = colbert.module.save
+
+    if not os.path.exists(checkpoints_path):
+        os.makedirs(checkpoints_path)
+    
+    path_save = None
+
+    if consumed_all_triples or (batch_idx % 2000 == 0):
+        # name = os.path.join(path, "colbert.dnn")
+        # save_checkpoint(name, 0, batch_idx, colbert, optimizer, arguments)
+        path_save = os.path.join(checkpoints_path, "colbert")
+
+    if batch_idx in SAVED_CHECKPOINTS:
+        # name = os.path.join(path, "colbert-{}.dnn".format(batch_idx))
+        # save_checkpoint(name, 0, batch_idx, colbert, optimizer, arguments)
+        path_save = os.path.join(checkpoints_path, f"colbert-{batch_idx}")
+
+    if path_save:
+        print(f"#> Saving a checkpoint to {path_save} ..")
+
+        # checkpoint = {}
+        # checkpoint['batch'] = batch_idx
+        # checkpoint['epoch'] = 0
+        # checkpoint['model_state_dict'] = model.state_dict()
+        # checkpoint['optimizer_state_dict'] = optimizer.state_dict()
+        # checkpoint['arguments'] = arguments
+
+        save(path_save)
+
+    return path_save
+
+def manage_checkpoints_with_path_save(args, colbert, optimizer, amp, batch_idx, num_per_epoch, epoch_idx=0, train_loss=0):
+    # arguments = args.input_arguments.__dict__
+
+    saved_name = ""
+    # NOTE: V2 uses "from colbert.infra.run import Run"  vs. V1 uses "from colbert.utils.runs import Run"
+    # path = os.path.join(Run.path, 'checkpoints')
+    path = os.path.join(Run().path_, 'checkpoints')
+
+    if not os.path.exists(path):
+        os.makedirs(path)
+    prefix = os.path.join(path, "colbert.dnn")
+
+    path_save = None
+
+    # saving the last checkpoint, to be rewritten every args.save_steps
+    if batch_idx % SAVED_STEPS_PROGRESS == 0:
+        saved_name = prefix + f".progress.model"
+        save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
+
+    if args.save_epochs == -1:
+        if batch_idx % args.save_steps == 0:
+            saved_name = prefix + f".batch_{batch_idx}.model"
+            save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
+            # save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
+
+            path_save = os.path.join(path, f"colbert-batch_{batch_idx}")
+    else:
+        if batch_idx * args.bsize * args.nranks % int(args.save_epochs * num_per_epoch) < args.bsize * args.nranks:
+            if args.save_epochs.is_integer():
+                saved_name = prefix + f".epoch_{epoch_idx}.model"
+                path_save = os.path.join(path, f"colbert-epoch_{epoch_idx}")
+            else:
+                saved_name = prefix + f".epoch_{epoch_idx}_batch_{batch_idx}.model"
+                path_save = os.path.join(path, f"colbert-epoch_{epoch_idx}_batch_{batch_idx}")
+
+            save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
+            # save_checkpoint(saved_name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
+
+
+    if batch_idx in SAVED_CHECKPOINTS or batch_idx == args.maxsteps:
+        name = prefix + f".batch_{batch_idx}.model"
+        if not name == saved_name:
+            save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
+            # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
+            path_save = os.path.join(path, f"colbert-batch_{batch_idx}")
+
+    if (batch_idx * args.bsize * args.nranks) % (args.epochs * num_per_epoch) < args.bsize * args.nranks:
+        name = prefix + f".epoch_{args.epochs - 1}.model"
+        if not name == saved_name:
+            save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type)
+            # save_checkpoint(name, epoch_idx, batch_idx, colbert, optimizer, amp, train_loss, args.model_type, arguments)
+            path_save = os.path.join(path, f"colbert-epoch_{epoch_idx}_batch_{batch_idx}")
+    try:
+        save = colbert.save
+    except:
+        save = colbert.module.save
+
+    if path_save:
+        print(f"#> Saving a checkpoint to {path_save} ..")
+
+        # NOTE: none of the following would be reflected in the output path even if specified
+        # checkpoint = {}
+        # checkpoint['batch'] = batch_idx
+        # checkpoint['epoch'] = 0
+        # checkpoint['model_state_dict'] = model.state_dict()
+        # checkpoint['optimizer_state_dict'] = optimizer.state_dict()
+        #checkpoint['arguments'] = arguments
+
+        if not os.path.exists(path_save):
+            os.makedirs(path_save)
+        save(path_save)
+
+    return path_save
```

## primeqa/ir/dense/colbert_top/colbert/utilities/minicorpus.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-import os
-import random
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory
-
-from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
-from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
-from primeqa.ir.dense.colbert_top.colbert.data.ranking import Ranking
-
-
-def sample_minicorpus(name, factor, topk=30, maxdev=3000):
-    """
-    Factor:
-        * nano=1
-        * micro=10
-        * mini=100
-        * small=100 with topk=100
-        * medium=150 with topk=300
-    """
-
-    random.seed(12345)
-
-    # Load collection
-    collection = Collection(path='/dfs/scratch0/okhattab/OpenQA/collection.tsv')
-
-    # Load train and dev queries
-    qas_train = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/qas.json').qas()
-    qas_dev = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/qas.json').qas()
-
-    # Load train and dev C3 rankings
-    ranking_train = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/rankings/C3.tsv.annotated').todict()
-    ranking_dev = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/rankings/C3.tsv.annotated').todict()
-
-    # Sample NT and ND queries from each, keep only the top-k passages for those
-    sample_train = random.sample(list(qas_train.keys()), min(len(qas_train.keys()), 300*factor))
-    sample_dev = random.sample(list(qas_dev.keys()), min(len(qas_dev.keys()), maxdev, 30*factor))
-
-    train_pids = [pid for qid in sample_train for qpids in ranking_train[qid][:topk] for pid in qpids]
-    dev_pids = [pid for qid in sample_dev for qpids in ranking_dev[qid][:topk] for pid in qpids]
-
-    sample_pids = sorted(list(set(train_pids + dev_pids)))
-    print(f'len(sample_pids) = {len(sample_pids)}')
-
-    # Save the new query sets: train and dev
-    ROOT = f'/future/u/okhattab/root/unit/data/NQ-{name}'
-
-    create_directory(os.path.join(ROOT, 'train'))
-    create_directory(os.path.join(ROOT, 'dev'))
-
-    new_train = Queries(data={qid: qas_train[qid] for qid in sample_train})
-    new_train.save(os.path.join(ROOT, 'train/questions.tsv'))
-    new_train.save_qas(os.path.join(ROOT, 'train/qas.json'))
-
-    new_dev = Queries(data={qid: qas_dev[qid] for qid in sample_dev})
-    new_dev.save(os.path.join(ROOT, 'dev/questions.tsv'))
-    new_dev.save_qas(os.path.join(ROOT, 'dev/qas.json'))
-
-    # Save the new collection
-    print(f"Saving to {os.path.join(ROOT, 'collection.tsv')}")
-    Collection(data=[collection[pid] for pid in sample_pids]).save(os.path.join(ROOT, 'collection.tsv'))
-
-    print('#> Done!')
-
-
-if __name__ == '__main__':
-    sample_minicorpus('medium', 150, topk=300)
+import os
+import random
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import create_directory
+
+from primeqa.ir.dense.colbert_top.colbert.data.collection import Collection
+from primeqa.ir.dense.colbert_top.colbert.data.queries import Queries
+from primeqa.ir.dense.colbert_top.colbert.data.ranking import Ranking
+
+
+def sample_minicorpus(name, factor, topk=30, maxdev=3000):
+    """
+    Factor:
+        * nano=1
+        * micro=10
+        * mini=100
+        * small=100 with topk=100
+        * medium=150 with topk=300
+    """
+
+    random.seed(12345)
+
+    # Load collection
+    collection = Collection(path='/dfs/scratch0/okhattab/OpenQA/collection.tsv')
+
+    # Load train and dev queries
+    qas_train = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/qas.json').qas()
+    qas_dev = Queries(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/qas.json').qas()
+
+    # Load train and dev C3 rankings
+    ranking_train = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/train/rankings/C3.tsv.annotated').todict()
+    ranking_dev = Ranking(path='/dfs/scratch0/okhattab/OpenQA/NQ/dev/rankings/C3.tsv.annotated').todict()
+
+    # Sample NT and ND queries from each, keep only the top-k passages for those
+    sample_train = random.sample(list(qas_train.keys()), min(len(qas_train.keys()), 300*factor))
+    sample_dev = random.sample(list(qas_dev.keys()), min(len(qas_dev.keys()), maxdev, 30*factor))
+
+    train_pids = [pid for qid in sample_train for qpids in ranking_train[qid][:topk] for pid in qpids]
+    dev_pids = [pid for qid in sample_dev for qpids in ranking_dev[qid][:topk] for pid in qpids]
+
+    sample_pids = sorted(list(set(train_pids + dev_pids)))
+    print(f'len(sample_pids) = {len(sample_pids)}')
+
+    # Save the new query sets: train and dev
+    ROOT = f'/future/u/okhattab/root/unit/data/NQ-{name}'
+
+    create_directory(os.path.join(ROOT, 'train'))
+    create_directory(os.path.join(ROOT, 'dev'))
+
+    new_train = Queries(data={qid: qas_train[qid] for qid in sample_train})
+    new_train.save(os.path.join(ROOT, 'train/questions.tsv'))
+    new_train.save_qas(os.path.join(ROOT, 'train/qas.json'))
+
+    new_dev = Queries(data={qid: qas_dev[qid] for qid in sample_dev})
+    new_dev.save(os.path.join(ROOT, 'dev/questions.tsv'))
+    new_dev.save_qas(os.path.join(ROOT, 'dev/qas.json'))
+
+    # Save the new collection
+    print(f"Saving to {os.path.join(ROOT, 'collection.tsv')}")
+    Collection(data=[collection[pid] for pid in sample_pids]).save(os.path.join(ROOT, 'collection.tsv'))
+
+    print('#> Done!')
+
+
+if __name__ == '__main__':
+    sample_minicorpus('medium', 150, topk=300)
```

## primeqa/ir/dense/colbert_top/colbert/utils/amp.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-import torch
-
-from contextlib import contextmanager
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import NullContextManager
-
-
-class MixedPrecisionManager():
-    def __init__(self, activated):
-        self.activated = activated
-
-        if self.activated:
-            self.scaler = torch.cuda.amp.GradScaler()
-
-    def context(self):
-        return torch.cuda.amp.autocast() if self.activated else NullContextManager()
-
-    def backward(self, loss):
-        if self.activated:
-            self.scaler.scale(loss).backward()
-        else:
-            loss.backward()
-
-    def step(self, colbert, optimizer, scheduler=None):
-        if self.activated:
-            self.scaler.unscale_(optimizer)
-            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0, error_if_nonfinite=False)
-
-            self.scaler.step(optimizer)
-            self.scaler.update()
-        else:
-            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)
-            optimizer.step()
-        
-        if scheduler is not None:
-            scheduler.step()
-
-        optimizer.zero_grad()
+import torch
+
+from contextlib import contextmanager
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import NullContextManager
+
+
+class MixedPrecisionManager():
+    def __init__(self, activated):
+        self.activated = activated
+
+        if self.activated:
+            self.scaler = torch.cuda.amp.GradScaler()
+
+    def context(self):
+        return torch.cuda.amp.autocast() if self.activated else NullContextManager()
+
+    def backward(self, loss):
+        if self.activated:
+            self.scaler.scale(loss).backward()
+        else:
+            loss.backward()
+
+    def step(self, colbert, optimizer, scheduler=None):
+        if self.activated:
+            self.scaler.unscale_(optimizer)
+            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0, error_if_nonfinite=False)
+
+            self.scaler.step(optimizer)
+            self.scaler.update()
+        else:
+            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)
+            optimizer.step()
+        
+        if scheduler is not None:
+            scheduler.step()
+
+        optimizer.zero_grad()
```

## primeqa/ir/dense/colbert_top/colbert/utils/distributed.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-import os
-import random
-import torch
-import numpy as np
-
-ALREADY_INITALIZED = False
-
-# TODO: Consider torch.distributed.is_initialized() instead
-
-
-def init(rank):
-    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])
-    nranks = max(1, nranks)
-    is_distributed = (nranks > 1) or ('WORLD_SIZE' in os.environ)
-
-    global ALREADY_INITALIZED
-    if ALREADY_INITALIZED:
-        return nranks, is_distributed
-
-    ALREADY_INITALIZED = True
-
-    if is_distributed and torch.cuda.is_available():
-        num_gpus = torch.cuda.device_count()
-        print(f'nranks = {nranks} \t num_gpus = {num_gpus} \t device={rank % num_gpus}')
-
-        torch.cuda.set_device(rank % num_gpus)
-        torch.distributed.init_process_group(backend='nccl', init_method='env://')
-
-    return nranks, is_distributed
-
-
-def barrier(rank):
-    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])
-    nranks = max(1, nranks)
-
-    if rank >= 0 and nranks > 1:
-        torch.distributed.barrier(device_ids=[rank % torch.cuda.device_count()])
+import os
+import random
+import torch
+import numpy as np
+
+ALREADY_INITALIZED = False
+
+# TODO: Consider torch.distributed.is_initialized() instead
+
+
+def init(rank):
+    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])
+    nranks = max(1, nranks)
+    is_distributed = (nranks > 1) or ('WORLD_SIZE' in os.environ)
+
+    global ALREADY_INITALIZED
+    if ALREADY_INITALIZED:
+        return nranks, is_distributed
+
+    ALREADY_INITALIZED = True
+
+    if is_distributed and torch.cuda.is_available():
+        num_gpus = torch.cuda.device_count()
+        print(f'nranks = {nranks} \t num_gpus = {num_gpus} \t device={rank % num_gpus}')
+
+        torch.cuda.set_device(rank % num_gpus)
+        torch.distributed.init_process_group(backend='nccl', init_method='env://')
+
+    return nranks, is_distributed
+
+
+def barrier(rank):
+    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])
+    nranks = max(1, nranks)
+
+    if rank >= 0 and nranks > 1:
+        torch.distributed.barrier(device_ids=[rank % torch.cuda.device_count()])
```

## primeqa/ir/dense/colbert_top/colbert/utils/logging.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-import os
-import sys
-import ujson
-# import mlflow
-import traceback
-
-# from torch.utils.tensorboard import SummaryWriter
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, create_directory
-
-
-class Logger():
-    def __init__(self, rank, run):
-        self.rank = rank
-        self.is_main = self.rank in [-1, 0]
-        self.run = run
-        self.logs_path = os.path.join(self.run.path, "logs/")
-
-        if self.is_main:
-            # self._init_mlflow()
-            # self.initialized_tensorboard = False
-            create_directory(self.logs_path)
-
-    # def _init_mlflow(self):
-    #     mlflow.set_tracking_uri('file://' + os.path.join(self.run.experiments_root, "logs/mlruns/"))
-    #     mlflow.set_experiment('/'.join([self.run.experiment, self.run.script]))
-        
-    #     mlflow.set_tag('experiment', self.run.experiment)
-    #     mlflow.set_tag('name', self.run.name)
-    #     mlflow.set_tag('path', self.run.path)
-
-    # def _init_tensorboard(self):
-    #     root = os.path.join(self.run.experiments_root, "logs/tensorboard/")
-    #     logdir = '__'.join([self.run.experiment, self.run.script, self.run.name])
-    #     logdir = os.path.join(root, logdir)
-
-    #     self.writer = SummaryWriter(log_dir=logdir)
-    #     self.initialized_tensorboard = True
-
-    def _log_exception(self, etype, value, tb):
-        if not self.is_main:
-            return
-
-        output_path = os.path.join(self.logs_path, 'exception.txt')
-        trace = ''.join(traceback.format_exception(etype, value, tb)) + '\n'
-        print_message(trace, '\n\n')
-
-        self.log_new_artifact(output_path, trace)
-
-    def _log_all_artifacts(self):
-        if not self.is_main:
-            return
-
-        # mlflow.log_artifacts(self.logs_path)
-
-    def _log_args(self, args):
-        if not self.is_main:
-            return
-
-        # for key in vars(args):
-        #     value = getattr(args, key)
-        #     if type(value) in [int, float, str, bool]:
-        #         mlflow.log_param(key, value)
-
-        # with open(os.path.join(self.logs_path, 'args.json'), 'w') as output_metadata:
-        #     # TODO: Call provenance() on the values that support it
-        #     ujson.dump(args.input_arguments.__dict__, output_metadata, indent=4)
-        #     output_metadata.write('\n')
-
-        with open(os.path.join(self.logs_path, 'args.txt'), 'w') as output_metadata:
-            output_metadata.write(' '.join(sys.argv) + '\n')
-
-    def log_metric(self, name, value, step, log_to_mlflow=True):
-        if not self.is_main:
-            return
-
-        # if not self.initialized_tensorboard:
-        #     self._init_tensorboard()
-
-        # if log_to_mlflow:
-        #     mlflow.log_metric(name, value, step=step)
-        # self.writer.add_scalar(name, value, step)
-
-    def log_new_artifact(self, path, content):
-        with open(path, 'w') as f:
-            f.write(content)
-
-        # mlflow.log_artifact(path)
-
-    def warn(self, *args):
-        msg = print_message('[WARNING]', '\t', *args)
-
-        with open(os.path.join(self.logs_path, 'warnings.txt'), 'a') as output_metadata:
-            output_metadata.write(msg + '\n\n\n')
-
-    def info_all(self, *args):
-        print_message('[' + str(self.rank) + ']', '\t', *args)
-
-    def info(self, *args):
-        if self.is_main:
-            print_message(*args)
+import os
+import sys
+import ujson
+# import mlflow
+import traceback
+
+# from torch.utils.tensorboard import SummaryWriter
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, create_directory
+
+
+class Logger():
+    def __init__(self, rank, run):
+        self.rank = rank
+        self.is_main = self.rank in [-1, 0]
+        self.run = run
+        self.logs_path = os.path.join(self.run.path, "logs/")
+
+        if self.is_main:
+            # self._init_mlflow()
+            # self.initialized_tensorboard = False
+            create_directory(self.logs_path)
+
+    # def _init_mlflow(self):
+    #     mlflow.set_tracking_uri('file://' + os.path.join(self.run.experiments_root, "logs/mlruns/"))
+    #     mlflow.set_experiment('/'.join([self.run.experiment, self.run.script]))
+        
+    #     mlflow.set_tag('experiment', self.run.experiment)
+    #     mlflow.set_tag('name', self.run.name)
+    #     mlflow.set_tag('path', self.run.path)
+
+    # def _init_tensorboard(self):
+    #     root = os.path.join(self.run.experiments_root, "logs/tensorboard/")
+    #     logdir = '__'.join([self.run.experiment, self.run.script, self.run.name])
+    #     logdir = os.path.join(root, logdir)
+
+    #     self.writer = SummaryWriter(log_dir=logdir)
+    #     self.initialized_tensorboard = True
+
+    def _log_exception(self, etype, value, tb):
+        if not self.is_main:
+            return
+
+        output_path = os.path.join(self.logs_path, 'exception.txt')
+        trace = ''.join(traceback.format_exception(etype, value, tb)) + '\n'
+        print_message(trace, '\n\n')
+
+        self.log_new_artifact(output_path, trace)
+
+    def _log_all_artifacts(self):
+        if not self.is_main:
+            return
+
+        # mlflow.log_artifacts(self.logs_path)
+
+    def _log_args(self, args):
+        if not self.is_main:
+            return
+
+        # for key in vars(args):
+        #     value = getattr(args, key)
+        #     if type(value) in [int, float, str, bool]:
+        #         mlflow.log_param(key, value)
+
+        # with open(os.path.join(self.logs_path, 'args.json'), 'w') as output_metadata:
+        #     # TODO: Call provenance() on the values that support it
+        #     ujson.dump(args.input_arguments.__dict__, output_metadata, indent=4)
+        #     output_metadata.write('\n')
+
+        with open(os.path.join(self.logs_path, 'args.txt'), 'w') as output_metadata:
+            output_metadata.write(' '.join(sys.argv) + '\n')
+
+    def log_metric(self, name, value, step, log_to_mlflow=True):
+        if not self.is_main:
+            return
+
+        # if not self.initialized_tensorboard:
+        #     self._init_tensorboard()
+
+        # if log_to_mlflow:
+        #     mlflow.log_metric(name, value, step=step)
+        # self.writer.add_scalar(name, value, step)
+
+    def log_new_artifact(self, path, content):
+        with open(path, 'w') as f:
+            f.write(content)
+
+        # mlflow.log_artifact(path)
+
+    def warn(self, *args):
+        msg = print_message('[WARNING]', '\t', *args)
+
+        with open(os.path.join(self.logs_path, 'warnings.txt'), 'a') as output_metadata:
+            output_metadata.write(msg + '\n\n\n')
+
+    def info_all(self, *args):
+        print_message('[' + str(self.rank) + ']', '\t', *args)
+
+    def info(self, *args):
+        if self.is_main:
+            print_message(*args)
```

## primeqa/ir/dense/colbert_top/colbert/utils/parser.py

```diff
@@ -1,163 +1,167 @@
-import os
-import copy
-import faiss
-
-from argparse import ArgumentParser
-
-import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
-from primeqa.ir.dense.colbert_top.colbert.utils.runs import Run
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, timestamp, create_directory
-
-
-class Arguments():
-    def __init__(self, description):
-        self.parser = ArgumentParser(description=description)
-        self.checks = []
-
-        self.add_argument('--root', dest='root', default='experiments')
-        self.add_argument('--experiment', dest='experiment', default='dirty')
-        self.add_argument('--run', dest='run', default=Run.name)
-
-        self.add_argument('--local_rank', dest='rank', default=-1, type=int)
-        self.add_argument('--rng_seed', dest='rng_seed', default=12345, type=int)
-
-    def add_model_parameters(self):
-        # Core Arguments
-        self.add_argument('--similarity', dest='similarity', default='cosine', choices=['cosine', 'l2'])
-        self.add_argument('--dim', dest='dim', default=128, type=int)
-        self.add_argument('--query_maxlen', dest='query_maxlen', default=32, type=int)
-        self.add_argument('--doc_maxlen', dest='doc_maxlen', default=180, type=int)
-
-        # Filtering-related Arguments
-        self.add_argument('--mask-punctuation', dest='mask_punctuation', action='store_true')
-        self.add_argument('--no-mask-punctuation', dest='mask_punctuation', action='store_false')
-        self.parser.set_defaults(mask_punctuation=True)
-
-        # for handling models in local repository
-        self.add_argument('--local_models_repository', dest='local_models_repository', default=None, required=False)
-
-    def add_model_training_parameters(self):
-        # NOTE: Providing a checkpoint is one thing, --resume is another, --resume_optimizer is yet another.
-        self.add_argument('--resume', dest='resume', default=False, action='store_true')
-        self.add_argument('--resume_optimizer', dest='resume_optimizer', default=False, action='store_true')
-        self.add_argument('--checkpoint', dest='checkpoint', default=None, required=False)
-
-        self.add_argument('--init_from_lm', dest='init_from_lm', default=None, required=False)
-        self.add_argument('--model_type', dest='model_type', default='bert-base-uncased', choices=['bert-base-uncased', 'bert-large-uncased','xlm-roberta-base','xlm-roberta-large', 'tinybert'], required=False)
-
-        self.add_argument('--lr', dest='lr', default=3e-06, type=float)
-        self.add_argument('--maxsteps', dest='maxsteps', default=400000, type=int)
-        self.add_argument('--bsize', dest='bsize', default=32, type=int)
-        self.add_argument('--accumsteps', dest='accumsteps', default=1, type=int)
-        self.add_argument('--amp', dest='amp', default=False, action='store_true')
-        # adding shuffle option
-        self.add_argument('--shuffle_every_epoch', dest='shuffle_every_epoch', default=False, action='store_true')
-        # support checkpoint
-        self.add_argument('--save_every', dest='save_every', default=None, type=int)
-        # TODO: deprecate save_steps and save_epochs
-        self.add_argument('--save_steps', dest='save_steps', default=2000, type=int)
-        #                  help="Training will save checkpoint at the specified steps. "
-        #                       "Overridden by save_epochs.")
-        self.add_argument('--save_epochs', dest='save_epochs', default=-1, type=int) # ,
-        #                  help="Training will save checkpoint at the specified epochs. Overrides save_steps.")
-        self.add_argument('--epochs', dest='epochs', default=10, type=int) #,
-        #                  help="Training will end at the earlier of the specified epochs or maxsteps.")
-
-        # used in distillation (Student/Teacher) training
-        self.add_argument('--teacher_checkpoint', dest='teacher_checkpoint', default=None, required=False)
-        self.add_argument('--student_teacher_temperature', dest='student_teacher_temperature', default=1.0, type=float)
-        self.add_argument('--student_teacher_top_loss_weight', dest='student_teacher_top_loss_weight', default=0.5, type=float)
-        self.add_argument('--teacher_model_type', dest='teacher_model_type', choices=['bert-base-uncased','bert-large-uncased','roberta-base','roberta-large', 'xlm-roberta-base','xlm-roberta-large','bert-base-multilingual-cased','bert-base-multilingual-uncased'], default=None, required=False )
-        self.add_argument('--teacher_doc_maxlen', dest='teacher_doc_maxlen', default=180, type=int)
-        self.add_argument('--distill_query_passage_separately', dest='distill_query_passage_separately', default=False, required=False, type=bool)
-        self.add_argument('--query_only', dest='query_only', default=False, required=False, type=bool)
-        self.add_argument('--loss_function', dest='loss_function', required=False)
-        self.add_argument('--query_weight', dest='query_weight', default=0.5, type=float)
-        self.add_argument('--use_ib_negatives', dest='use_ib_negatives', default=False, action='store_true')
-
-    def add_model_inference_parameters(self):
-        self.add_argument('--checkpoint', dest='checkpoint', required=True)
-        self.add_argument('--bsize', dest='bsize', default=128, type=int)
-        self.add_argument('--amp', dest='amp', default=False, action='store_true')
-
-    def add_training_input(self):
-        self.add_argument('--triples', dest='triples', required=True)
-        self.add_argument('--queries', dest='queries', default=None)
-        self.add_argument('--collection', dest='collection', default=None)
-        # used in distillation (Student/Teacher) training
-        self.add_argument('--teacher_triples', dest='teacher_triples', default=None)
-
-        def check_training_input(args):
-            assert (args.collection is None) == (args.queries is None), \
-                "For training, both (or neither) --collection and --queries must be supplied." \
-                "If neither is supplied, the --triples file must contain texts (not PIDs)."
-
-        self.checks.append(check_training_input)
-
-    def add_ranking_input(self):
-        self.add_argument('--queries', dest='queries', default=None)
-        self.add_argument('--collection', dest='collection', default=None)
-        self.add_argument('--qrels', dest='qrels', default=None)
-        self.add_argument('--ranks_fn', dest='ranks_fn', required=True)
-        self.add_argument('--topK', dest='topK', default=100, type=int)
-
-    def add_reranking_input(self):
-        self.add_ranking_input()
-        self.add_argument('--topk', dest='topK', required=True)
-        self.add_argument('--shortcircuit', dest='shortcircuit', default=False, action='store_true')
-
-    def add_indexing_input(self):
-        self.add_argument('--collection', dest='collection', required=True)
-        self.add_argument('--index_root', dest='index_root', required=True)
-        self.add_argument('--index_name', dest='index_name', required=True)
-
-    def add_compressed_index_input(self):
-        self.add_argument('--nbits', dest='nbits', choices=[1, 2, 4], type=int, default=1)
-        self.add_argument('--kmeans_niters', type=int, default=4)
-        self.add_argument('--num_partitions_max', type=int, default=10000000)
-
-    def add_index_use_input(self):
-        self.add_argument('--index_root', dest='index_root', required=True)
-        self.add_argument('--index_name', dest='index_name', required=True)
-        self.add_argument('--partitions', dest='partitions', default=None, type=int, required=False)
-        self.add_argument('--index_path', dest='index_path', default=None, type=str)
-
-
-    def add_retrieval_input(self):
-        self.add_index_use_input()
-        self.add_argument('--ncells', dest='ncells', default=None, type=int)
-        self.add_argument('--centroid_score_threshold', dest='centroid_score_threshold', default=None, type=float)
-        self.add_argument('--ndocs', dest='ndocs', default=None, type=int)
-        self.add_argument('--retrieve_only', dest='retrieve_only', default=False, action='store_true')
-
-    def add_argument(self, *args, **kw_args):
-        return self.parser.add_argument(*args, **kw_args)
-
-    def check_arguments(self, args):
-        for check in self.checks:
-            check(args)
-
-    def parse(self):
-        (args, remaining_args) = self.parser.parse_known_args()
-        if len(remaining_args):
-            print_message(f'arguments not used by ColBERT engine: {remaining_args}')
-
-        self.check_arguments(args)
-
-        args.input_arguments = copy.deepcopy(args)
-
-        args.nranks, args.distributed = distributed.init(args.rank)
-
-        args.nthreads = int(max(os.cpu_count(), faiss.omp_get_max_threads()) * 0.8)
-        args.nthreads = max(1, args.nthreads // args.nranks)
-
-        if args.nranks > 1:
-            print_message(f"#> Restricting number of threads for FAISS to {args.nthreads} per process",
-                          condition=(args.rank == 0))
-            faiss.omp_set_num_threads(args.nthreads)
-
-        Run.init(args.rank, args.root, args.experiment, args.run)
-        Run._log_args(args)
-        Run.info(args.input_arguments.__dict__, '\n')
-
-        return args
+import os
+import copy
+import faiss
+
+from argparse import ArgumentParser
+
+import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
+from primeqa.ir.dense.colbert_top.colbert.utils.runs import Run
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, timestamp, create_directory
+
+
+class Arguments():
+    def __init__(self, description):
+        self.parser = ArgumentParser(description=description)
+        self.checks = []
+
+        self.add_argument('--root', dest='root', default='experiments')
+        self.add_argument('--experiment', dest='experiment', default='default_experiment_name')
+        self.add_argument('--run', dest='run', default=Run.name)
+
+        self.add_argument('--local_rank', dest='rank', default=-1, type=int)
+        self.add_argument('--rng_seed', dest='rng_seed', default=12345, type=int)
+
+    def add_model_parameters(self):
+        # Core Arguments
+        self.add_argument('--similarity', dest='similarity', default='cosine', choices=['cosine'])
+        self.add_argument('--dim', dest='dim', default=128, type=int)
+        self.add_argument('--query_maxlen', dest='query_maxlen', default=32, type=int)
+        self.add_argument('--doc_maxlen', dest='doc_maxlen', default=180, type=int)
+
+        # Filtering-related Arguments
+        self.add_argument('--mask-punctuation', dest='mask_punctuation', action='store_true')
+        self.add_argument('--no-mask-punctuation', dest='mask_punctuation', action='store_false')
+        self.parser.set_defaults(mask_punctuation=True)
+
+        # for handling models in local repository
+        self.add_argument('--local_models_repository', dest='local_models_repository', default=None, required=False)
+
+    def add_model_training_parameters(self):
+        # NOTE: Providing a checkpoint is one thing, --resume is another, --resume_optimizer is yet another.
+        self.add_argument('--resume', dest='resume', default=False, action='store_true')
+        self.add_argument('--resume_optimizer', dest='resume_optimizer', default=False, action='store_true')
+        self.add_argument('--checkpoint', dest='checkpoint', default=None, required=False)
+
+        self.add_argument('--init_from_lm', dest='init_from_lm', default=None, required=False)
+        self.add_argument('--model_type', dest='model_type', default='bert-base-uncased', choices=['bert-base-uncased', 'bert-large-uncased','xlm-roberta-base','xlm-roberta-large', 'tinybert'], required=False)
+
+        self.add_argument('--lr', dest='lr', default=3e-06, type=float)
+        self.add_argument('--maxsteps', dest='maxsteps', default=400000, type=int)
+        self.add_argument('--bsize', dest='bsize', default=32, type=int)
+        self.add_argument('--accumsteps', dest='accumsteps', default=1, type=int)
+        self.add_argument('--amp', dest='amp', default=True, action='store_true')
+        self.add_argument('--no_amp', dest='amp', default=False, action='store_false')
+        self.parser.set_defaults(amp=True)
+
+        # adding shuffle option
+        self.add_argument('--shuffle_every_epoch', dest='shuffle_every_epoch', default=False, action='store_true')
+        # support checkpoint
+        self.add_argument('--save_every', dest='save_every', default=None, type=int)
+        # TODO: deprecate save_steps and save_epochs
+        self.add_argument('--save_steps', dest='save_steps', default=2000, type=int)
+        #                  help="Training will save checkpoint at the specified steps. "
+        #                       "Overridden by save_epochs.")
+        self.add_argument('--save_epochs', dest='save_epochs', default=-1, type=int) # ,
+        #                  help="Training will save checkpoint at the specified epochs. Overrides save_steps.")
+        self.add_argument('--epochs', dest='epochs', default=10, type=int) #,
+        #                  help="Training will end at the earlier of the specified epochs or maxsteps.")
+
+        # used in distillation (Student/Teacher) training
+        self.add_argument('--teacher_checkpoint', dest='teacher_checkpoint', default=None, required=False)
+        self.add_argument('--student_teacher_temperature', dest='student_teacher_temperature', default=1.0, type=float)
+        self.add_argument('--student_teacher_top_loss_weight', dest='student_teacher_top_loss_weight', default=0.5, type=float)
+        self.add_argument('--teacher_model_type', dest='teacher_model_type', choices=['bert-base-uncased','bert-large-uncased','roberta-base','roberta-large', 'xlm-roberta-base','xlm-roberta-large','bert-base-multilingual-cased','bert-base-multilingual-uncased'], default=None, required=False )
+        self.add_argument('--teacher_doc_maxlen', dest='teacher_doc_maxlen', default=180, type=int)
+        self.add_argument('--distill_query_passage_separately', dest='distill_query_passage_separately', default=False, required=False, type=bool)
+        self.add_argument('--query_only', dest='query_only', default=False, required=False, type=bool)
+        self.add_argument('--loss_function', dest='loss_function', required=False)
+        self.add_argument('--query_weight', dest='query_weight', default=0.5, type=float)
+        self.add_argument('--use_ib_negatives', dest='use_ib_negatives', default=False, action='store_true')
+
+    def add_model_inference_parameters(self):
+        self.add_argument('--model_name_or_path', dest='checkpoint', required=True)
+        self.add_argument('--bsize', dest='bsize', default=128, type=int)
+        self.add_argument('--amp', dest='amp', default=True, action='store_true')
+        self.add_argument('--no_amp', dest='amp', default=False, action='store_false')
+        self.parser.set_defaults(amp=True)
+
+    def add_training_input(self):
+        self.add_argument('--triples', dest='triples', required=True)
+        self.add_argument('--queries', dest='queries', default=None)
+        self.add_argument('--collection', dest='collection', default=None)
+        # used in distillation (Student/Teacher) training
+        self.add_argument('--teacher_triples', dest='teacher_triples', default=None)
+
+        def check_training_input(args):
+            assert (args.collection is None) == (args.queries is None), \
+                "For training, both (or neither) --collection and --queries must be supplied." \
+                "If neither is supplied, the --triples file must contain texts (not PIDs)."
+
+        self.checks.append(check_training_input)
+
+    def add_ranking_input(self):
+        self.add_argument('--queries', dest='queries', default=None)
+        self.add_argument('--collection', dest='collection', default=None)
+        self.add_argument('--qrels', dest='qrels', default=None)
+        self.add_argument('--output_dir', dest='output_dir', required=True)
+        self.add_argument('--top_k', dest='topK', default=100, type=int)
+
+    def add_reranking_input(self):
+        self.add_ranking_input()
+        self.add_argument('--topk', dest='topK', required=True)
+        self.add_argument('--shortcircuit', dest='shortcircuit', default=False, action='store_true')
+
+    def add_indexing_input(self):
+        self.add_argument('--collection', dest='collection', required=True)
+        self.add_argument('--index_root', dest='index_root', default=None)
+        self.add_argument('--index_name', dest='index_name', required=True)
+
+    def add_compressed_index_input(self):
+        self.add_argument('--nbits', dest='nbits', choices=[1, 2, 4], type=int, default=1)
+        self.add_argument('--kmeans_niters', type=int, default=4)
+        self.add_argument('--num_partitions_max', type=int, default=10000000)
+
+    def add_index_use_input(self):
+        self.add_argument('--index_root', dest='index_root', default=None)
+        self.add_argument('--index_name', dest='index_name', required=False)
+        self.add_argument('--partitions', dest='partitions', default=None, type=int, required=False)
+        self.add_argument('--index_path', dest='index_path', default=None, type=str)
+        self.add_argument('--index_location', dest='index_location', default=None, type=str)
+
+    def add_retrieval_input(self):
+        self.add_index_use_input()
+        self.add_argument('--ncells', dest='ncells', default=None, type=int)
+        self.add_argument('--centroid_score_threshold', dest='centroid_score_threshold', default=None, type=float)
+        self.add_argument('--ndocs', dest='ndocs', default=None, type=int)
+
+    def add_argument(self, *args, **kw_args):
+        return self.parser.add_argument(*args, **kw_args)
+
+    def check_arguments(self, args):
+        for check in self.checks:
+            check(args)
+
+    def parse(self):
+        (args, remaining_args) = self.parser.parse_known_args()
+        if len(remaining_args):
+            print_message(f'arguments not used by ColBERT engine: {remaining_args}')
+
+        self.check_arguments(args)
+
+        args.input_arguments = copy.deepcopy(args)
+
+        args.nranks, args.distributed = distributed.init(args.rank)
+
+        args.nthreads = int(max(os.cpu_count(), faiss.omp_get_max_threads()) * 0.8)
+        args.nthreads = max(1, args.nthreads // args.nranks)
+
+        if args.nranks > 1:
+            print_message(f"#> Restricting number of threads for FAISS to {args.nthreads} per process",
+                          condition=(args.rank == 0))
+            faiss.omp_set_num_threads(args.nthreads)
+
+        Run.init(args.rank, args.root, args.experiment, args.run)
+        Run._log_args(args)
+        Run.info(args.input_arguments.__dict__, '\n')
+
+        return args
```

## primeqa/ir/dense/colbert_top/colbert/utils/runs.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-import os
-import sys
-import time
-import __main__
-import traceback
-# import mlflow
-
-import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
-
-from contextlib import contextmanager
-from primeqa.ir.dense.colbert_top.colbert.utils.logging import Logger
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp, create_directory, print_message
-
-
-class _RunManager():
-    def __init__(self):
-        self.experiments_root = None
-        self.experiment = None
-        self.path = None
-        self.script = self._get_script_name()
-        self.name = self._generate_default_run_name()
-        self.original_name = self.name
-        self.exit_status = 'FINISHED'
-
-        self._logger = None
-        self.start_time = time.time()
-
-    def init(self, rank, root, experiment, name):
-        assert '/' not in experiment, experiment
-        assert '/' not in name, name
-
-        self.experiments_root = os.path.abspath(root)
-        self.experiment = experiment
-        self.name = name
-        self.path = os.path.join(self.experiments_root, self.experiment, self.script, self.name)
-
-        if rank < 1:
-            if os.path.exists(self.path):
-                print('\n\n')
-                print_message("It seems that ", self.path, " already exists.")
-                print_message("Do you want to overwrite it? \t yes/no \n")
-
-                # TODO: This should timeout and exit (i.e., fail) given no response for 60 seconds.
-
-                # response = input()
-                response = 'yes'
-                if response.strip() != 'yes':
-                    assert not os.path.exists(self.path), self.path
-            else:
-                create_directory(self.path)
-
-        distributed.barrier(rank)
-
-        self._logger = Logger(rank, self)
-        self._log_args = self._logger._log_args
-        self.warn = self._logger.warn
-        self.info = self._logger.info
-        self.info_all = self._logger.info_all
-        self.log_metric = self._logger.log_metric
-        self.log_new_artifact = self._logger.log_new_artifact
-
-    def _generate_default_run_name(self):
-        return timestamp()
-
-    def _get_script_name(self):
-        return os.path.basename(__main__.__file__) if '__file__' in dir(__main__) else 'none'
-
-    @contextmanager
-    def context(self, consider_failed_if_interrupted=True):
-        try:
-            yield
-
-        except KeyboardInterrupt as ex:
-            print('\n\nInterrupted\n\n')
-            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)
-            self._logger._log_all_artifacts()
-
-            if consider_failed_if_interrupted:
-                self.exit_status = 'KILLED'  # mlflow.entities.RunStatus.KILLED
-
-            sys.exit(128 + 2)
-
-        except Exception as ex:
-            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)
-            self._logger._log_all_artifacts()
-
-            self.exit_status = 'FAILED'  # mlflow.entities.RunStatus.FAILED
-
-            raise ex
-
-        finally:
-            total_seconds = str(time.time() - self.start_time) + '\n'
-            original_name = str(self.original_name)
-            name = str(self.name)
-
-            self.log_new_artifact(os.path.join(self._logger.logs_path, 'elapsed.txt'), total_seconds)
-            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.original.txt'), original_name)
-            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.txt'), name)
-
-            self._logger._log_all_artifacts()
-
-            # mlflow.end_run(status=self.exit_status)
-
-
-Run = _RunManager()
+import os
+import sys
+import time
+import __main__
+import traceback
+# import mlflow
+
+import primeqa.ir.dense.colbert_top.colbert.utils.distributed as distributed
+
+from contextlib import contextmanager
+from primeqa.ir.dense.colbert_top.colbert.utils.logging import Logger
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import timestamp, create_directory, print_message
+
+
+class _RunManager():
+    def __init__(self):
+        self.experiments_root = None
+        self.experiment = None
+        self.path = None
+        self.script = self._get_script_name()
+        self.name = self._generate_default_run_name()
+        self.original_name = self.name
+        self.exit_status = 'FINISHED'
+
+        self._logger = None
+        self.start_time = time.time()
+
+    def init(self, rank, root, experiment, name):
+        assert '/' not in experiment, experiment
+        assert '/' not in name, name
+
+        self.experiments_root = os.path.abspath(root)
+        self.experiment = experiment
+        self.name = name
+        self.path = os.path.join(self.experiments_root, self.experiment, self.script, self.name)
+
+        if rank < 1:
+            if os.path.exists(self.path):
+                print('\n\n')
+                print_message("It seems that ", self.path, " already exists.")
+                print_message("Do you want to overwrite it? \t yes/no \n")
+
+                # TODO: This should timeout and exit (i.e., fail) given no response for 60 seconds.
+
+                # response = input()
+                response = 'yes'
+                if response.strip() != 'yes':
+                    assert not os.path.exists(self.path), self.path
+            else:
+                create_directory(self.path)
+
+        distributed.barrier(rank)
+
+        self._logger = Logger(rank, self)
+        self._log_args = self._logger._log_args
+        self.warn = self._logger.warn
+        self.info = self._logger.info
+        self.info_all = self._logger.info_all
+        self.log_metric = self._logger.log_metric
+        self.log_new_artifact = self._logger.log_new_artifact
+
+    def _generate_default_run_name(self):
+        return timestamp()
+
+    def _get_script_name(self):
+        return os.path.basename(__main__.__file__) if '__file__' in dir(__main__) else 'none'
+
+    @contextmanager
+    def context(self, consider_failed_if_interrupted=True):
+        try:
+            yield
+
+        except KeyboardInterrupt as ex:
+            print('\n\nInterrupted\n\n')
+            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)
+            self._logger._log_all_artifacts()
+
+            if consider_failed_if_interrupted:
+                self.exit_status = 'KILLED'  # mlflow.entities.RunStatus.KILLED
+
+            sys.exit(128 + 2)
+
+        except Exception as ex:
+            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)
+            self._logger._log_all_artifacts()
+
+            self.exit_status = 'FAILED'  # mlflow.entities.RunStatus.FAILED
+
+            raise ex
+
+        finally:
+            total_seconds = str(time.time() - self.start_time) + '\n'
+            original_name = str(self.original_name)
+            name = str(self.name)
+
+            self.log_new_artifact(os.path.join(self._logger.logs_path, 'elapsed.txt'), total_seconds)
+            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.original.txt'), original_name)
+            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.txt'), name)
+
+            self._logger._log_all_artifacts()
+
+            # mlflow.end_run(status=self.exit_status)
+
+
+Run = _RunManager()
```

## primeqa/ir/dense/colbert_top/colbert/utils/signals.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-"""
-BEGIN_COPYRIGHT
-
-IBM Confidential
-OCO Source Materials
-
-5727-I17
-(C) Copyright IBM Corp. 2021 All Rights Reserved.
- 
-The source code for this program is not published or otherwise
-divested of its trade secrets, irrespective of what has been
-deposited with the U.S. Copyright Office.
-
-END_COPYRIGHT
-"""
-import signal
-import sys
-from functools import partial
-from queue import Queue
-from types import FrameType
-from typing import Optional
-
-SENTINEL = None
-
-
-def checkpoint_on_exit(rank: int):
-    if rank < 1:
-        queue = Queue()
-        handler = partial(_trigger_checkpoint, queue=queue)
-    else:
-        queue = None
-        handler = _exit
-
-    signal.signal(signal.SIGTERM, handler)
-    return queue
-
-
-def _trigger_checkpoint(signo: int, frame: Optional[FrameType], queue: Queue):
-    queue.put(SENTINEL)
-
-
-def _exit(signo: int, frame: Optional[FrameType]):
-    sys.exit(0)
+"""
+BEGIN_COPYRIGHT
+
+IBM Confidential
+OCO Source Materials
+
+5727-I17
+(C) Copyright IBM Corp. 2021 All Rights Reserved.
+ 
+The source code for this program is not published or otherwise
+divested of its trade secrets, irrespective of what has been
+deposited with the U.S. Copyright Office.
+
+END_COPYRIGHT
+"""
+import signal
+import sys
+from functools import partial
+from queue import Queue
+from types import FrameType
+from typing import Optional
+
+SENTINEL = None
+
+
+def checkpoint_on_exit(rank: int):
+    if rank < 1:
+        queue = Queue()
+        handler = partial(_trigger_checkpoint, queue=queue)
+    else:
+        queue = None
+        handler = _exit
+
+    signal.signal(signal.SIGTERM, handler)
+    return queue
+
+
+def _trigger_checkpoint(signo: int, frame: Optional[FrameType], queue: Queue):
+    queue.put(SENTINEL)
+
+
+def _exit(signo: int, frame: Optional[FrameType]):
+    sys.exit(0)
```

## primeqa/ir/dense/colbert_top/colbert/utils/utils.py

 * *Ordering differences only*

```diff
@@ -1,343 +1,343 @@
-import os
-import tqdm
-import torch
-import datetime
-import itertools
-
-from multiprocessing import Pool
-from collections import OrderedDict, defaultdict
-
-
-# removing (") if it is at both the begining and the end of a string (field)
-def remove_first_and_last_quote(a_str):
-    if a_str.startswith('"') and a_str.endswith('"'):
-        a_str = a_str[1:-1]
-        a_str=a_str.replace('""','"')
-    return a_str
-
-
-def print_message(*s, condition=True, pad=False):
-    s = ' '.join([str(x) for x in s])
-    msg = "[{}] {}".format(datetime.datetime.now().strftime("%b %d, %H:%M:%S"), s)
-
-    if condition:
-        msg = msg if not pad else f'\n{msg}\n'
-        print(msg, flush=True)
-
-
-    return msg
-
-def print_torch_extension_error_message():
-    msg = """Troubleshooting possible causes for failed PyTorch extension compilation:
-
-    - PyTorch is using the system CUDA installation instead of environment CUDA (possible fix: set CUDA_PATH environment variable)
-    - Incompatible gcc and nvcc compiler versions (possible fix: manually install a different gcc/gxx version, e.g. 9.4.0)
-    - Compilation hangs indefinitely (possible fix: remove /path/to/.cache/torch_extensions directory)
-    """
-
-    return print_message(msg, pad=True)
-
-def timestamp(daydir=False):
-    format_str = f"%Y-%m{'/' if daydir else '-'}%d{'/' if daydir else '_'}%H.%M.%S"
-    result = datetime.datetime.now().strftime(format_str)
-    return result
-
-
-def file_tqdm(file):
-    print(f"#> Reading {file.name}")
-
-    with tqdm.tqdm(total=os.path.getsize(file.name) / 1024.0 / 1024.0, unit="MiB") as pbar:
-        for line in file:
-            yield line
-            pbar.update(len(line) / 1024.0 / 1024.0)
-
-        pbar.close()
-
-
-def torch_load_dnn(path):
-    if path.startswith("http:") or path.startswith("https:"):
-        dnn = torch.hub.load_state_dict_from_url(path, map_location='cpu')
-    else:
-        dnn = torch.load(path, map_location='cpu')
-
-    return dnn
-
-# def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, amp, train_loss, arguments=None):
-# It makes sense to use model type instead of input arguments
-def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, amp, train_loss, model_type):
-# def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, amp, train_loss, model_type, arguments=None):
-    print(f"#> Saving a checkpoint to {path} ..")
-
-    if hasattr(model, 'module'):
-        model = model.module  # extract model from a distributed/data-parallel wrapper
-
-    checkpoint = {}
-    checkpoint['epoch'] = epoch_idx
-    checkpoint['batch'] = mb_idx
-    checkpoint['train_loss'] = train_loss
-    checkpoint['model_state_dict'] = model.state_dict()
-    checkpoint['optimizer_state_dict'] = optimizer.state_dict()
-    # checkpoint['arguments'] = arguments
-    checkpoint['model_type'] = model_type
-
-    checkpoint['scaler_state_dict'] = amp.scaler.state_dict() if amp.activated else None
-
-    checkpoint['torch_rng_state'] = torch.get_rng_state()
-    checkpoint['torch_cuda_rng_states'] = torch.cuda.get_rng_state_all()
-
-    import numpy as np
-    checkpoint['np_rng_state'] = np.random.get_state()
-
-    import random
-    checkpoint['python_rng_state'] = random.getstate()
-
-    torch.save(checkpoint, path)
-
-
-def load_checkpoint(path, model, checkpoint=None, optimizer=None, do_print=True):
-    if do_print:
-        print_message("#> Loading checkpoint", path, "..")
-
-    if checkpoint is None:
-        checkpoint = load_checkpoint_raw(path)
-
-    try:
-        model.load_state_dict(checkpoint['model_state_dict'])
-    except:
-        print_message("[WARNING] Loading checkpoint with strict=False")
-        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
-
-    if optimizer:
-        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
-
-    if do_print:
-        print_message("#> checkpoint['epoch'] =", checkpoint['epoch'])
-        print_message("#> checkpoint['batch'] =", checkpoint['batch'])
-
-    return checkpoint
-
-
-def load_checkpoint_raw(path):
-    if path.startswith("http:") or path.startswith("https:"):
-        checkpoint = torch.hub.load_state_dict_from_url(path, map_location='cpu')
-    else:
-        checkpoint = torch.load(path, map_location='cpu')
-
-    state_dict = checkpoint['model_state_dict']
-    new_state_dict = OrderedDict()
-    for k, v in state_dict.items():
-        name = k
-        if k[:7] == 'module.':
-            name = k[7:]
-        new_state_dict[name] = v
-
-    checkpoint['model_state_dict'] = new_state_dict
-
-    return checkpoint
-
-
-def create_directory(path):
-    if os.path.exists(path):
-        print('\n')
-        print_message("#> Note: Output directory", path, 'already exists\n\n')
-    else:
-        print('\n')
-        print_message("#> Creating directory", path, '\n\n')
-        os.makedirs(path)
-
-# def batch(file, bsize):
-#     while True:
-#         L = [ujson.loads(file.readline()) for _ in range(bsize)]
-#         yield L
-#     return
-
-
-def f7(seq):
-    """
-    Source: https://stackoverflow.com/a/480227/1493011
-    """
-
-    seen = set()
-    return [x for x in seq if not (x in seen or seen.add(x))]
-
-
-def batch(group, bsize, provide_offset=False):
-    offset = 0
-    while offset < len(group):
-        L = group[offset: offset + bsize]
-        yield ((offset, L) if provide_offset else L)
-        offset += len(L)
-    return
-
-
-class dotdict(dict):
-    """
-    dot.notation access to dictionary attributes
-    Credit: derek73 @ https://stackoverflow.com/questions/2352181
-    """
-    __getattr__ = dict.__getitem__
-    __setattr__ = dict.__setitem__
-    __delattr__ = dict.__delitem__
-
-
-class dotdict_lax(dict):
-    __getattr__ = dict.get
-    __setattr__ = dict.__setitem__
-    __delattr__ = dict.__delitem__
-
-
-def flatten(L):
-    # return [x for y in L for x in y]
-
-    result = []
-    for _list in L:
-        result += _list
-
-    return result
-
-
-def zipstar(L, lazy=False):
-    """
-    A much faster A, B, C = zip(*[(a, b, c), (a, b, c), ...])
-    May return lists or tuples.
-    """
-
-    if len(L) == 0:
-        return L
-
-    width = len(L[0])
-
-    if width < 100:
-        return [[elem[idx] for elem in L] for idx in range(width)]
-
-    L = zip(*L)
-
-    return L if lazy else list(L)
-
-
-def zip_first(L1, L2):
-    length = len(L1) if type(L1) in [tuple, list] else None
-
-    L3 = list(zip(L1, L2))
-
-    assert length in [None, len(L3)], "zip_first() failure: length differs!"
-
-    return L3
-
-
-def int_or_float(val):
-    if '.' in val:
-        return float(val)
-
-    return int(val)
-
-def load_ranking(path, types=None, lazy=False):
-    print_message(f"#> Loading the ranked lists from {path} ..")
-
-    try:
-        lists = torch.load(path)
-        lists = zipstar([l.tolist() for l in tqdm.tqdm(lists)], lazy=lazy)
-    except:
-        if types is None:
-            types = itertools.cycle([int_or_float])
-
-        with open(path) as f:
-            lists = [[typ(x) for typ, x in zip_first(types, line.strip().split('\t'))]
-                     for line in file_tqdm(f)]
-
-    return lists
-
-
-def save_ranking(ranking, path):
-    lists = zipstar(ranking)
-    lists = [torch.tensor(l) for l in lists]
-
-    torch.save(lists, path)
-
-    return lists
-
-
-def groupby_first_item(lst):
-    groups = defaultdict(list)
-
-    for first, *rest in lst:
-        rest = rest[0] if len(rest) == 1 else rest
-        groups[first].append(rest)
-
-    return groups
-
-
-def process_grouped_by_first_item(lst):
-    """
-        Requires items in list to already be grouped by first item.
-    """
-
-    groups = defaultdict(list)
-
-    started = False
-    last_group = None
-
-    for first, *rest in lst:
-        rest = rest[0] if len(rest) == 1 else rest
-
-        if started and first != last_group:
-            yield (last_group, groups[last_group])
-            assert first not in groups, f"{first} seen earlier --- violates precondition."
-
-        groups[first].append(rest)
-
-        last_group = first
-        started = True
-
-    return groups
-
-
-def grouper(iterable, n, fillvalue=None):
-    """
-    Collect data into fixed-length chunks or blocks
-        Example: grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx"
-        Source: https://docs.python.org/3/library/itertools.html#itertools-recipes
-    """
-
-    args = [iter(iterable)] * n
-    return itertools.zip_longest(*args, fillvalue=fillvalue)
-
-
-def lengths2offsets(lengths):
-    offset = 0
-
-    for length in lengths:
-        yield (offset, offset + length)
-        offset += length
-
-    return
-
-
-# see https://stackoverflow.com/a/45187287
-class NullContextManager(object):
-    def __init__(self, dummy_resource=None):
-        self.dummy_resource = dummy_resource
-    def __enter__(self):
-        return self.dummy_resource
-    def __exit__(self, *args):
-        pass
-
-
-def load_batch_backgrounds(args, qids):
-    if args.qid2backgrounds is None:
-        return None
-
-    qbackgrounds = []
-
-    for qid in qids:
-        back = args.qid2backgrounds[qid]
-
-        if len(back) and type(back[0]) == int:
-            x = [args.collection[pid] for pid in back]
-        else:
-            x = [args.collectionX.get(pid, '') for pid in back]
-
-        x = ' [SEP] '.join(x)
-        qbackgrounds.append(x)
-
-    return qbackgrounds
+import os
+import tqdm
+import torch
+import datetime
+import itertools
+
+from multiprocessing import Pool
+from collections import OrderedDict, defaultdict
+
+
+# removing (") if it is at both the begining and the end of a string (field)
+def remove_first_and_last_quote(a_str):
+    if a_str.startswith('"') and a_str.endswith('"'):
+        a_str = a_str[1:-1]
+        a_str=a_str.replace('""','"')
+    return a_str
+
+
+def print_message(*s, condition=True, pad=False):
+    s = ' '.join([str(x) for x in s])
+    msg = "[{}] {}".format(datetime.datetime.now().strftime("%b %d, %H:%M:%S"), s)
+
+    if condition:
+        msg = msg if not pad else f'\n{msg}\n'
+        print(msg, flush=True)
+
+
+    return msg
+
+def print_torch_extension_error_message():
+    msg = """Troubleshooting possible causes for failed PyTorch extension compilation:
+
+    - PyTorch is using the system CUDA installation instead of environment CUDA (possible fix: set CUDA_PATH environment variable)
+    - Incompatible gcc and nvcc compiler versions (possible fix: manually install a different gcc/gxx version, e.g. 9.4.0)
+    - Compilation hangs indefinitely (possible fix: remove /path/to/.cache/torch_extensions directory)
+    """
+
+    return print_message(msg, pad=True)
+
+def timestamp(daydir=False):
+    format_str = f"%Y-%m{'/' if daydir else '-'}%d{'/' if daydir else '_'}%H.%M.%S"
+    result = datetime.datetime.now().strftime(format_str)
+    return result
+
+
+def file_tqdm(file):
+    print(f"#> Reading {file.name}")
+
+    with tqdm.tqdm(total=os.path.getsize(file.name) / 1024.0 / 1024.0, unit="MiB") as pbar:
+        for line in file:
+            yield line
+            pbar.update(len(line) / 1024.0 / 1024.0)
+
+        pbar.close()
+
+
+def torch_load_dnn(path):
+    if path.startswith("http:") or path.startswith("https:"):
+        dnn = torch.hub.load_state_dict_from_url(path, map_location='cpu')
+    else:
+        dnn = torch.load(path, map_location='cpu')
+
+    return dnn
+
+# def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, amp, train_loss, arguments=None):
+# It makes sense to use model type instead of input arguments
+def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, amp, train_loss, model_type):
+# def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, amp, train_loss, model_type, arguments=None):
+    print(f"#> Saving a checkpoint to {path} ..")
+
+    if hasattr(model, 'module'):
+        model = model.module  # extract model from a distributed/data-parallel wrapper
+
+    checkpoint = {}
+    checkpoint['epoch'] = epoch_idx
+    checkpoint['batch'] = mb_idx
+    checkpoint['train_loss'] = train_loss
+    checkpoint['model_state_dict'] = model.state_dict()
+    checkpoint['optimizer_state_dict'] = optimizer.state_dict()
+    # checkpoint['arguments'] = arguments
+    checkpoint['model_type'] = model_type
+
+    checkpoint['scaler_state_dict'] = amp.scaler.state_dict() if amp.activated else None
+
+    checkpoint['torch_rng_state'] = torch.get_rng_state()
+    checkpoint['torch_cuda_rng_states'] = torch.cuda.get_rng_state_all()
+
+    import numpy as np
+    checkpoint['np_rng_state'] = np.random.get_state()
+
+    import random
+    checkpoint['python_rng_state'] = random.getstate()
+
+    torch.save(checkpoint, path)
+
+
+def load_checkpoint(path, model, checkpoint=None, optimizer=None, do_print=True):
+    if do_print:
+        print_message("#> Loading checkpoint", path, "..")
+
+    if checkpoint is None:
+        checkpoint = load_checkpoint_raw(path)
+
+    try:
+        model.load_state_dict(checkpoint['model_state_dict'])
+    except:
+        print_message("[WARNING] Loading checkpoint with strict=False")
+        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
+
+    if optimizer:
+        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
+
+    if do_print:
+        print_message("#> checkpoint['epoch'] =", checkpoint['epoch'])
+        print_message("#> checkpoint['batch'] =", checkpoint['batch'])
+
+    return checkpoint
+
+
+def load_checkpoint_raw(path):
+    if path.startswith("http:") or path.startswith("https:"):
+        checkpoint = torch.hub.load_state_dict_from_url(path, map_location='cpu')
+    else:
+        checkpoint = torch.load(path, map_location='cpu')
+
+    state_dict = checkpoint['model_state_dict']
+    new_state_dict = OrderedDict()
+    for k, v in state_dict.items():
+        name = k
+        if k[:7] == 'module.':
+            name = k[7:]
+        new_state_dict[name] = v
+
+    checkpoint['model_state_dict'] = new_state_dict
+
+    return checkpoint
+
+
+def create_directory(path):
+    if os.path.exists(path):
+        print('\n')
+        print_message("#> Note: Output directory", path, 'already exists\n\n')
+    else:
+        print('\n')
+        print_message("#> Creating directory", path, '\n\n')
+        os.makedirs(path)
+
+# def batch(file, bsize):
+#     while True:
+#         L = [ujson.loads(file.readline()) for _ in range(bsize)]
+#         yield L
+#     return
+
+
+def f7(seq):
+    """
+    Source: https://stackoverflow.com/a/480227/1493011
+    """
+
+    seen = set()
+    return [x for x in seq if not (x in seen or seen.add(x))]
+
+
+def batch(group, bsize, provide_offset=False):
+    offset = 0
+    while offset < len(group):
+        L = group[offset: offset + bsize]
+        yield ((offset, L) if provide_offset else L)
+        offset += len(L)
+    return
+
+
+class dotdict(dict):
+    """
+    dot.notation access to dictionary attributes
+    Credit: derek73 @ https://stackoverflow.com/questions/2352181
+    """
+    __getattr__ = dict.__getitem__
+    __setattr__ = dict.__setitem__
+    __delattr__ = dict.__delitem__
+
+
+class dotdict_lax(dict):
+    __getattr__ = dict.get
+    __setattr__ = dict.__setitem__
+    __delattr__ = dict.__delitem__
+
+
+def flatten(L):
+    # return [x for y in L for x in y]
+
+    result = []
+    for _list in L:
+        result += _list
+
+    return result
+
+
+def zipstar(L, lazy=False):
+    """
+    A much faster A, B, C = zip(*[(a, b, c), (a, b, c), ...])
+    May return lists or tuples.
+    """
+
+    if len(L) == 0:
+        return L
+
+    width = len(L[0])
+
+    if width < 100:
+        return [[elem[idx] for elem in L] for idx in range(width)]
+
+    L = zip(*L)
+
+    return L if lazy else list(L)
+
+
+def zip_first(L1, L2):
+    length = len(L1) if type(L1) in [tuple, list] else None
+
+    L3 = list(zip(L1, L2))
+
+    assert length in [None, len(L3)], "zip_first() failure: length differs!"
+
+    return L3
+
+
+def int_or_float(val):
+    if '.' in val:
+        return float(val)
+
+    return int(val)
+
+def load_ranking(path, types=None, lazy=False):
+    print_message(f"#> Loading the ranked lists from {path} ..")
+
+    try:
+        lists = torch.load(path)
+        lists = zipstar([l.tolist() for l in tqdm.tqdm(lists)], lazy=lazy)
+    except:
+        if types is None:
+            types = itertools.cycle([int_or_float])
+
+        with open(path) as f:
+            lists = [[typ(x) for typ, x in zip_first(types, line.strip().split('\t'))]
+                     for line in file_tqdm(f)]
+
+    return lists
+
+
+def save_ranking(ranking, path):
+    lists = zipstar(ranking)
+    lists = [torch.tensor(l) for l in lists]
+
+    torch.save(lists, path)
+
+    return lists
+
+
+def groupby_first_item(lst):
+    groups = defaultdict(list)
+
+    for first, *rest in lst:
+        rest = rest[0] if len(rest) == 1 else rest
+        groups[first].append(rest)
+
+    return groups
+
+
+def process_grouped_by_first_item(lst):
+    """
+        Requires items in list to already be grouped by first item.
+    """
+
+    groups = defaultdict(list)
+
+    started = False
+    last_group = None
+
+    for first, *rest in lst:
+        rest = rest[0] if len(rest) == 1 else rest
+
+        if started and first != last_group:
+            yield (last_group, groups[last_group])
+            assert first not in groups, f"{first} seen earlier --- violates precondition."
+
+        groups[first].append(rest)
+
+        last_group = first
+        started = True
+
+    return groups
+
+
+def grouper(iterable, n, fillvalue=None):
+    """
+    Collect data into fixed-length chunks or blocks
+        Example: grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx"
+        Source: https://docs.python.org/3/library/itertools.html#itertools-recipes
+    """
+
+    args = [iter(iterable)] * n
+    return itertools.zip_longest(*args, fillvalue=fillvalue)
+
+
+def lengths2offsets(lengths):
+    offset = 0
+
+    for length in lengths:
+        yield (offset, offset + length)
+        offset += length
+
+    return
+
+
+# see https://stackoverflow.com/a/45187287
+class NullContextManager(object):
+    def __init__(self, dummy_resource=None):
+        self.dummy_resource = dummy_resource
+    def __enter__(self):
+        return self.dummy_resource
+    def __exit__(self, *args):
+        pass
+
+
+def load_batch_backgrounds(args, qids):
+    if args.qid2backgrounds is None:
+        return None
+
+    qbackgrounds = []
+
+    for qid in qids:
+        back = args.qid2backgrounds[qid]
+
+        if len(back) and type(back[0]) == int:
+            x = [args.collection[pid] for pid in back]
+        else:
+            x = [args.collectionX.get(pid, '') for pid in back]
+
+        x = ' [SEP] '.join(x)
+        qbackgrounds.append(x)
+
+    return qbackgrounds
```

## primeqa/ir/dense/colbert_top/utility/preprocess/docs2passages.py

 * *Ordering differences only*

```diff
@@ -1,154 +1,154 @@
-"""
-    Divide a document collection into N-word/token passage spans (with wrap-around for last passage).
-"""
-
-import os
-import math
-import ujson
-import random
-
-from multiprocessing import Pool
-from argparse import ArgumentParser
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
-
-Format1 = 'docid,text'  # MS MARCO Passages
-Format2 = 'docid,text,title'   # DPR Wikipedia
-Format3 = 'docid,url,title,text'  # MS MARCO Documents
-
-
-def process_page(inp):
-    """
-        Wraps around if we split: make sure last passage isn't too short.
-        This is meant to be similar to the DPR preprocessing.
-    """
-
-    (nwords, overlap, tokenizer), (title_idx, docid, title, url, content) = inp
-
-    if tokenizer is None:
-        words = content.split()
-    else:
-        words = tokenizer.tokenize(content)
-
-    words_ = (words + words) if len(words) > nwords else words
-    passages = [words_[offset:offset + nwords] for offset in range(0, len(words) - overlap, nwords - overlap)]
-
-    assert all(len(psg) in [len(words), nwords] for psg in passages), (list(map(len, passages)), len(words))
-
-    if tokenizer is None:
-        passages = [' '.join(psg) for psg in passages]
-    else:
-        passages = [' '.join(psg).replace(' ##', '') for psg in passages]
-
-    if title_idx % 100000 == 0:
-        print("#> ", title_idx, '\t\t\t', title)
-
-        for p in passages:
-            print("$$$ ", '\t\t', p)
-            print()
-
-        print()
-        print()
-        print()
-
-    return (docid, title, url, passages)
-
-
-def main(args):
-    random.seed(12345)
-    print_message("#> Starting...")
-
-    letter = 'w' if not args.use_wordpiece else 't'
-
-    output_path = f'{args.input}.{letter}{args.nwords}_{args.overlap}'
-    if args.output_path is not None:
-        output_path = os.path.join(args.output_path, os.path.basename(output_path))
-
-    assert not os.path.exists(output_path)
-
-    RawCollection = []
-    Collection = []
-
-    NumIllFormattedLines = 0
-
-    with open(args.input) as f:
-        for line_idx, line in enumerate(f):
-            if line_idx % (100*1000) == 0:
-                print(line_idx, end=' ')
-
-            title, url = None, None
-
-            try:
-                line = line.strip().split('\t')
-
-                if args.format == Format1:
-                    docid, doc = line
-                elif args.format == Format2:
-                    docid, doc, title = line
-                elif args.format == Format3:
-                    docid, url, title, doc = line
-
-                RawCollection.append((line_idx, docid, title, url, doc))
-            except:
-                NumIllFormattedLines += 1
-
-                if NumIllFormattedLines % 1000 == 0:
-                    print(f'\n[{line_idx}] NumIllFormattedLines = {NumIllFormattedLines}\n')
-
-    print()
-    print_message("# of documents is", len(RawCollection), '\n')
-
-    p = Pool(args.nthreads)
-
-    print_message("#> Starting parallel processing...")
-
-    tokenizer = None
-    if args.use_wordpiece:
-        from transformers import BertTokenizerFast
-        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
-
-    process_page_params = [(args.nwords, args.overlap, tokenizer)] * len(RawCollection)
-    Collection = p.map(process_page, zip(process_page_params, RawCollection))
-
-    print_message(f"#> Writing to {output_path} ...")
-    with open(output_path, 'w') as f:
-        line_idx = 1
-
-        if args.format == Format1:
-            f.write('\t'.join(['id', 'text']) + '\n')
-        elif args.format == Format2:
-            f.write('\t'.join(['id', 'text', 'title']) + '\n')
-        elif args.format == Format3:
-            f.write('\t'.join(['id', 'text', 'title', 'docid']) + '\n')
-
-        for docid, title, url, passages in Collection:
-            for passage in passages:
-                if args.format == Format1:
-                    f.write('\t'.join([str(line_idx), passage]) + '\n')
-                elif args.format == Format2:
-                    f.write('\t'.join([str(line_idx), passage, title]) + '\n')
-                elif args.format == Format3:
-                    f.write('\t'.join([str(line_idx), passage, title, docid]) + '\n')
-
-                line_idx += 1
-
-
-if __name__ == "__main__":
-    parser = ArgumentParser(description="docs2passages.")
-
-    # Input Arguments.
-    parser.add_argument('--input', dest='input', required=True)
-    parser.add_argument('--format', dest='format', required=True, choices=[Format1, Format2, Format3])
-
-    # Output Arguments.
-    parser.add_argument('--use-wordpiece', dest='use_wordpiece', default=False, action='store_true')
-    parser.add_argument('--nwords', dest='nwords', default=100, type=int)
-    parser.add_argument('--overlap', dest='overlap', default=0, type=int)
-    parser.add_argument('--output_path', dest='output_path', default=None)
-
-    # Other Arguments.
-    parser.add_argument('--nthreads', dest='nthreads', default=28, type=int)
-
-    args = parser.parse_args()
-    assert args.nwords in range(50, 500)
-
-    main(args)
+"""
+    Divide a document collection into N-word/token passage spans (with wrap-around for last passage).
+"""
+
+import os
+import math
+import ujson
+import random
+
+from multiprocessing import Pool
+from argparse import ArgumentParser
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message
+
+Format1 = 'docid,text'  # MS MARCO Passages
+Format2 = 'docid,text,title'   # DPR Wikipedia
+Format3 = 'docid,url,title,text'  # MS MARCO Documents
+
+
+def process_page(inp):
+    """
+        Wraps around if we split: make sure last passage isn't too short.
+        This is meant to be similar to the DPR preprocessing.
+    """
+
+    (nwords, overlap, tokenizer), (title_idx, docid, title, url, content) = inp
+
+    if tokenizer is None:
+        words = content.split()
+    else:
+        words = tokenizer.tokenize(content)
+
+    words_ = (words + words) if len(words) > nwords else words
+    passages = [words_[offset:offset + nwords] for offset in range(0, len(words) - overlap, nwords - overlap)]
+
+    assert all(len(psg) in [len(words), nwords] for psg in passages), (list(map(len, passages)), len(words))
+
+    if tokenizer is None:
+        passages = [' '.join(psg) for psg in passages]
+    else:
+        passages = [' '.join(psg).replace(' ##', '') for psg in passages]
+
+    if title_idx % 100000 == 0:
+        print("#> ", title_idx, '\t\t\t', title)
+
+        for p in passages:
+            print("$$$ ", '\t\t', p)
+            print()
+
+        print()
+        print()
+        print()
+
+    return (docid, title, url, passages)
+
+
+def main(args):
+    random.seed(12345)
+    print_message("#> Starting...")
+
+    letter = 'w' if not args.use_wordpiece else 't'
+
+    output_path = f'{args.input}.{letter}{args.nwords}_{args.overlap}'
+    if args.output_path is not None:
+        output_path = os.path.join(args.output_path, os.path.basename(output_path))
+
+    assert not os.path.exists(output_path)
+
+    RawCollection = []
+    Collection = []
+
+    NumIllFormattedLines = 0
+
+    with open(args.input) as f:
+        for line_idx, line in enumerate(f):
+            if line_idx % (100*1000) == 0:
+                print(line_idx, end=' ')
+
+            title, url = None, None
+
+            try:
+                line = line.strip().split('\t')
+
+                if args.format == Format1:
+                    docid, doc = line
+                elif args.format == Format2:
+                    docid, doc, title = line
+                elif args.format == Format3:
+                    docid, url, title, doc = line
+
+                RawCollection.append((line_idx, docid, title, url, doc))
+            except:
+                NumIllFormattedLines += 1
+
+                if NumIllFormattedLines % 1000 == 0:
+                    print(f'\n[{line_idx}] NumIllFormattedLines = {NumIllFormattedLines}\n')
+
+    print()
+    print_message("# of documents is", len(RawCollection), '\n')
+
+    p = Pool(args.nthreads)
+
+    print_message("#> Starting parallel processing...")
+
+    tokenizer = None
+    if args.use_wordpiece:
+        from transformers import BertTokenizerFast
+        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
+
+    process_page_params = [(args.nwords, args.overlap, tokenizer)] * len(RawCollection)
+    Collection = p.map(process_page, zip(process_page_params, RawCollection))
+
+    print_message(f"#> Writing to {output_path} ...")
+    with open(output_path, 'w') as f:
+        line_idx = 1
+
+        if args.format == Format1:
+            f.write('\t'.join(['id', 'text']) + '\n')
+        elif args.format == Format2:
+            f.write('\t'.join(['id', 'text', 'title']) + '\n')
+        elif args.format == Format3:
+            f.write('\t'.join(['id', 'text', 'title', 'docid']) + '\n')
+
+        for docid, title, url, passages in Collection:
+            for passage in passages:
+                if args.format == Format1:
+                    f.write('\t'.join([str(line_idx), passage]) + '\n')
+                elif args.format == Format2:
+                    f.write('\t'.join([str(line_idx), passage, title]) + '\n')
+                elif args.format == Format3:
+                    f.write('\t'.join([str(line_idx), passage, title, docid]) + '\n')
+
+                line_idx += 1
+
+
+if __name__ == "__main__":
+    parser = ArgumentParser(description="docs2passages.")
+
+    # Input Arguments.
+    parser.add_argument('--input', dest='input', required=True)
+    parser.add_argument('--format', dest='format', required=True, choices=[Format1, Format2, Format3])
+
+    # Output Arguments.
+    parser.add_argument('--use-wordpiece', dest='use_wordpiece', default=False, action='store_true')
+    parser.add_argument('--nwords', dest='nwords', default=100, type=int)
+    parser.add_argument('--overlap', dest='overlap', default=0, type=int)
+    parser.add_argument('--output_path', dest='output_path', default=None)
+
+    # Other Arguments.
+    parser.add_argument('--nthreads', dest='nthreads', default=28, type=int)
+
+    args = parser.parse_args()
+    assert args.nwords in range(50, 500)
+
+    main(args)
```

## primeqa/ir/dense/colbert_top/utility/rankings/dev_subsample.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-import os
-import ujson
-import random
-
-from argparse import ArgumentParser
-
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, create_directory, load_ranking, groupby_first_item
-from primeqa.ir.dense.colbert_top.utility.utils.qa_loaders import load_qas_
-
-
-def main(args):
-    print_message("#> Loading all..")
-    qas = load_qas_(args.qas)
-    rankings = load_ranking(args.ranking)
-    qid2rankings = groupby_first_item(rankings)
-
-    print_message("#> Subsampling all..")
-    qas_sample = random.sample(qas, args.sample)
-
-    with open(args.output, 'w') as f:
-        for qid, *_ in qas_sample:
-            for items in qid2rankings[qid]:
-                items = [qid] + items
-                line = '\t'.join(map(str, items)) + '\n'
-                f.write(line)
-
-    print('\n\n')
-    print(args.output)
-    print("#> Done.")
-
-
-if __name__ == "__main__":
-    random.seed(12345)
-
-    parser = ArgumentParser(description='Subsample the dev set.')
-    parser.add_argument('--qas', dest='qas', required=True, type=str)
-    parser.add_argument('--ranking', dest='ranking', required=True)
-    parser.add_argument('--output', dest='output', required=True)
-
-    parser.add_argument('--sample', dest='sample', default=1500, type=int)
-
-    args = parser.parse_args()
-
-    assert not os.path.exists(args.output), args.output
-    create_directory(os.path.dirname(args.output))
-
-    main(args)
+import os
+import ujson
+import random
+
+from argparse import ArgumentParser
+
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, create_directory, load_ranking, groupby_first_item
+from primeqa.ir.dense.colbert_top.utility.utils.qa_loaders import load_qas_
+
+
+def main(args):
+    print_message("#> Loading all..")
+    qas = load_qas_(args.qas)
+    rankings = load_ranking(args.ranking)
+    qid2rankings = groupby_first_item(rankings)
+
+    print_message("#> Subsampling all..")
+    qas_sample = random.sample(qas, args.sample)
+
+    with open(args.output, 'w') as f:
+        for qid, *_ in qas_sample:
+            for items in qid2rankings[qid]:
+                items = [qid] + items
+                line = '\t'.join(map(str, items)) + '\n'
+                f.write(line)
+
+    print('\n\n')
+    print(args.output)
+    print("#> Done.")
+
+
+if __name__ == "__main__":
+    random.seed(12345)
+
+    parser = ArgumentParser(description='Subsample the dev set.')
+    parser.add_argument('--qas', dest='qas', required=True, type=str)
+    parser.add_argument('--ranking', dest='ranking', required=True)
+    parser.add_argument('--output', dest='output', required=True)
+
+    parser.add_argument('--sample', dest='sample', default=1500, type=int)
+
+    args = parser.parse_args()
+
+    assert not os.path.exists(args.output), args.output
+    create_directory(os.path.dirname(args.output))
+
+    main(args)
```

## primeqa/ir/dense/colbert_top/utility/rankings/merge.py

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-"""
-    Divide two or more ranking files, by score.
-"""
-
-import os
-import tqdm
-
-from argparse import ArgumentParser
-from collections import defaultdict
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, file_tqdm
-
-
-def main(args):
-    Rankings = defaultdict(list)
-
-    for path in args.input:
-        print_message(f"#> Loading the rankings in {path} ..")
-
-        with open(path) as f:
-            for line in file_tqdm(f):
-                qid, pid, rank, score = line.strip().split('\t')
-                qid, pid, rank = map(int, [qid, pid, rank])
-                score = float(score)
-
-                Rankings[qid].append((score, rank, pid))
-
-    with open(args.output, 'w') as f:
-        print_message(f"#> Writing the output rankings to {args.output} ..")
-
-        for qid in tqdm.tqdm(Rankings):
-            ranking = sorted(Rankings[qid], reverse=True)
-
-            for rank, (score, original_rank, pid) in enumerate(ranking):
-                rank = rank + 1  # 1-indexed
-
-                if (args.depth > 0) and (rank > args.depth):
-                    break
-
-                line = [qid, pid, rank, score]
-                line = '\t'.join(map(str, line)) + '\n'
-                f.write(line)
-
-
-if __name__ == "__main__":
-    parser = ArgumentParser(description="merge_rankings.")
-
-    # Input Arguments.
-    parser.add_argument('--input', dest='input', required=True, nargs='+')
-    parser.add_argument('--output', dest='output', required=True, type=str)
-
-    parser.add_argument('--depth', dest='depth', required=True, type=int)
-
-    args = parser.parse_args()
-
-    assert not os.path.exists(args.output), args.output
-
-    main(args)
+"""
+    Divide two or more ranking files, by score.
+"""
+
+import os
+import tqdm
+
+from argparse import ArgumentParser
+from collections import defaultdict
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, file_tqdm
+
+
+def main(args):
+    Rankings = defaultdict(list)
+
+    for path in args.input:
+        print_message(f"#> Loading the rankings in {path} ..")
+
+        with open(path) as f:
+            for line in file_tqdm(f):
+                qid, pid, rank, score = line.strip().split('\t')
+                qid, pid, rank = map(int, [qid, pid, rank])
+                score = float(score)
+
+                Rankings[qid].append((score, rank, pid))
+
+    with open(args.output, 'w') as f:
+        print_message(f"#> Writing the output rankings to {args.output} ..")
+
+        for qid in tqdm.tqdm(Rankings):
+            ranking = sorted(Rankings[qid], reverse=True)
+
+            for rank, (score, original_rank, pid) in enumerate(ranking):
+                rank = rank + 1  # 1-indexed
+
+                if (args.depth > 0) and (rank > args.depth):
+                    break
+
+                line = [qid, pid, rank, score]
+                line = '\t'.join(map(str, line)) + '\n'
+                f.write(line)
+
+
+if __name__ == "__main__":
+    parser = ArgumentParser(description="merge_rankings.")
+
+    # Input Arguments.
+    parser.add_argument('--input', dest='input', required=True, nargs='+')
+    parser.add_argument('--output', dest='output', required=True, type=str)
+
+    parser.add_argument('--depth', dest='depth', required=True, type=int)
+
+    args = parser.parse_args()
+
+    assert not os.path.exists(args.output), args.output
+
+    main(args)
```

## primeqa/ir/dense/colbert_top/utility/rankings/split_by_offset.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-"""
-Split the ranked lists after retrieval with a merged query set.
-"""
-
-import os
-import random
-
-from argparse import ArgumentParser
-
-
-def main(args):
-    output_paths = ['{}.{}'.format(args.ranking, split) for split in args.names]
-    assert all(not os.path.exists(path) for path in output_paths), output_paths
-
-    output_files = [open(path, 'w') for path in output_paths]
-
-    with open(args.ranking) as f:
-        for line in f:
-            qid, pid, rank, *other = line.strip().split('\t')
-            qid = int(qid)
-            split_output_path = output_files[qid // args.gap - 1]
-            qid = qid % args.gap
-
-            split_output_path.write('\t'.join([str(x) for x in [qid, pid, rank, *other]]) + '\n')
-        
-        print(f.name)
-    
-    _ = [f.close() for f in output_files]
-    
-    print("#> Done!")
-
-
-if __name__ == "__main__":
-    random.seed(12345)
-
-    parser = ArgumentParser(description='Subsample the dev set.')
-    parser.add_argument('--ranking', dest='ranking', required=True)
-
-    parser.add_argument('--names', dest='names', required=False, default=['train', 'dev', 'test'], type=str, nargs='+')  # order matters!
-    parser.add_argument('--gap', dest='gap', required=False, default=1_000_000_000, type=int)  # larger than any individual query set
-
-    args = parser.parse_args()
-
-    main(args)
+"""
+Split the ranked lists after retrieval with a merged query set.
+"""
+
+import os
+import random
+
+from argparse import ArgumentParser
+
+
+def main(args):
+    output_paths = ['{}.{}'.format(args.ranking, split) for split in args.names]
+    assert all(not os.path.exists(path) for path in output_paths), output_paths
+
+    output_files = [open(path, 'w') for path in output_paths]
+
+    with open(args.ranking) as f:
+        for line in f:
+            qid, pid, rank, *other = line.strip().split('\t')
+            qid = int(qid)
+            split_output_path = output_files[qid // args.gap - 1]
+            qid = qid % args.gap
+
+            split_output_path.write('\t'.join([str(x) for x in [qid, pid, rank, *other]]) + '\n')
+        
+        print(f.name)
+    
+    _ = [f.close() for f in output_files]
+    
+    print("#> Done!")
+
+
+if __name__ == "__main__":
+    random.seed(12345)
+
+    parser = ArgumentParser(description='Subsample the dev set.')
+    parser.add_argument('--ranking', dest='ranking', required=True)
+
+    parser.add_argument('--names', dest='names', required=False, default=['train', 'dev', 'test'], type=str, nargs='+')  # order matters!
+    parser.add_argument('--gap', dest='gap', required=False, default=1_000_000_000, type=int)  # larger than any individual query set
+
+    args = parser.parse_args()
+
+    main(args)
```

## primeqa/ir/dense/colbert_top/utility/rankings/split_by_queries.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-import os
-import sys
-import tqdm
-import ujson
-import random
-
-from argparse import ArgumentParser
-from collections import OrderedDict
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, file_tqdm
-
-
-def main(args):
-    qid_to_file_idx = {}
-
-    for qrels_idx, qrels in enumerate(args.all_queries):
-        with open(qrels) as f:
-            for line in f:
-                qid, *_ = line.strip().split('\t')
-                qid = int(qid)
-
-                assert qid_to_file_idx.get(qid, qrels_idx) == qrels_idx, (qid, qrels_idx)
-                qid_to_file_idx[qid] = qrels_idx
-
-    all_outputs_paths = [f'{args.ranking}.{idx}' for idx in range(len(args.all_queries))]
-
-    assert all(not os.path.exists(path) for path in all_outputs_paths)
-
-    all_outputs = [open(path, 'w') for path in all_outputs_paths]
-
-    with open(args.ranking) as f:
-        print_message(f"#> Loading ranked lists from {f.name} ..")
-
-        last_file_idx = -1
-
-        for line in file_tqdm(f):
-            qid, *_ = line.strip().split('\t')
-
-            file_idx = qid_to_file_idx[int(qid)]
-
-            if file_idx != last_file_idx:
-                print_message(f"#> Switched to file #{file_idx} at {all_outputs[file_idx].name}")
-
-            last_file_idx = file_idx
-
-            all_outputs[file_idx].write(line)
-
-    print()
-
-    for f in all_outputs:
-        print(f.name)
-        f.close()
-
-    print("#> Done!")
-
-
-if __name__ == "__main__":
-    random.seed(12345)
-
-    parser = ArgumentParser(description='.')
-
-    # Input Arguments
-    parser.add_argument('--ranking', dest='ranking', required=True, type=str)
-    parser.add_argument('--all-queries', dest='all_queries', required=True, type=str, nargs='+')
-
-    args = parser.parse_args()
-
-    main(args)
+import os
+import sys
+import tqdm
+import ujson
+import random
+
+from argparse import ArgumentParser
+from collections import OrderedDict
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, file_tqdm
+
+
+def main(args):
+    qid_to_file_idx = {}
+
+    for qrels_idx, qrels in enumerate(args.all_queries):
+        with open(qrels) as f:
+            for line in f:
+                qid, *_ = line.strip().split('\t')
+                qid = int(qid)
+
+                assert qid_to_file_idx.get(qid, qrels_idx) == qrels_idx, (qid, qrels_idx)
+                qid_to_file_idx[qid] = qrels_idx
+
+    all_outputs_paths = [f'{args.ranking}.{idx}' for idx in range(len(args.all_queries))]
+
+    assert all(not os.path.exists(path) for path in all_outputs_paths)
+
+    all_outputs = [open(path, 'w') for path in all_outputs_paths]
+
+    with open(args.ranking) as f:
+        print_message(f"#> Loading ranked lists from {f.name} ..")
+
+        last_file_idx = -1
+
+        for line in file_tqdm(f):
+            qid, *_ = line.strip().split('\t')
+
+            file_idx = qid_to_file_idx[int(qid)]
+
+            if file_idx != last_file_idx:
+                print_message(f"#> Switched to file #{file_idx} at {all_outputs[file_idx].name}")
+
+            last_file_idx = file_idx
+
+            all_outputs[file_idx].write(line)
+
+    print()
+
+    for f in all_outputs:
+        print(f.name)
+        f.close()
+
+    print("#> Done!")
+
+
+if __name__ == "__main__":
+    random.seed(12345)
+
+    parser = ArgumentParser(description='.')
+
+    # Input Arguments
+    parser.add_argument('--ranking', dest='ranking', required=True, type=str)
+    parser.add_argument('--all-queries', dest='all_queries', required=True, type=str, nargs='+')
+
+    args = parser.parse_args()
+
+    main(args)
```

## primeqa/ir/dense/colbert_top/utility/rankings/tune.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-import os
-import ujson
-import random
-
-from argparse import ArgumentParser
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, create_directory
-from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import save_metadata
-
-
-def main(args):
-    AllMetrics = {}
-    Scores = {}
-
-    for path in args.paths:
-        with open(path) as f:
-            metric = ujson.load(f)
-            AllMetrics[path] = metric
-
-            for k in args.metric:
-                metric = metric[k]
-
-            assert type(metric) is float
-            Scores[path] = metric
-    
-    MaxKey = max(Scores, key=Scores.get)
-
-    MaxCKPT = int(MaxKey.split('/')[-2].split('.')[-1])
-    MaxARGS = os.path.join(os.path.dirname(MaxKey), 'logs', 'args.json')
-
-    with open(MaxARGS) as f:
-        logs = ujson.load(f)
-        MaxCHECKPOINT = logs['checkpoint']
-
-        assert MaxCHECKPOINT.endswith(f'colbert-{MaxCKPT}.dnn'), (MaxCHECKPOINT, MaxCKPT)
-
-    with open(args.output, 'w') as f:
-        f.write(MaxCHECKPOINT)
-
-    args.Scores = Scores
-    args.AllMetrics = AllMetrics
-
-    save_metadata(f'{args.output}.meta', args)
-
-    print('\n\n', args, '\n\n')
-    print(args.output)
-    print_message("#> Done.")
-
-
-if __name__ == "__main__":
-    random.seed(12345)
-
-    parser = ArgumentParser(description='.')
-
-    # Input / Output Arguments
-    parser.add_argument('--metric', dest='metric', required=True, type=str)  # e.g., success.20
-    parser.add_argument('--paths', dest='paths', required=True, type=str, nargs='+')
-    parser.add_argument('--output', dest='output', required=True, type=str)
-
-    args = parser.parse_args()
-
-    args.metric = args.metric.split('.')
-
-    assert not os.path.exists(args.output), args.output
-    create_directory(os.path.dirname(args.output))
-
-    main(args)
+import os
+import ujson
+import random
+
+from argparse import ArgumentParser
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, create_directory
+from primeqa.ir.dense.colbert_top.utility.utils.save_metadata import save_metadata
+
+
+def main(args):
+    AllMetrics = {}
+    Scores = {}
+
+    for path in args.paths:
+        with open(path) as f:
+            metric = ujson.load(f)
+            AllMetrics[path] = metric
+
+            for k in args.metric:
+                metric = metric[k]
+
+            assert type(metric) is float
+            Scores[path] = metric
+    
+    MaxKey = max(Scores, key=Scores.get)
+
+    MaxCKPT = int(MaxKey.split('/')[-2].split('.')[-1])
+    MaxARGS = os.path.join(os.path.dirname(MaxKey), 'logs', 'args.json')
+
+    with open(MaxARGS) as f:
+        logs = ujson.load(f)
+        MaxCHECKPOINT = logs['checkpoint']
+
+        assert MaxCHECKPOINT.endswith(f'colbert-{MaxCKPT}.dnn'), (MaxCHECKPOINT, MaxCKPT)
+
+    with open(args.output, 'w') as f:
+        f.write(MaxCHECKPOINT)
+
+    args.Scores = Scores
+    args.AllMetrics = AllMetrics
+
+    save_metadata(f'{args.output}.meta', args)
+
+    print('\n\n', args, '\n\n')
+    print(args.output)
+    print_message("#> Done.")
+
+
+if __name__ == "__main__":
+    random.seed(12345)
+
+    parser = ArgumentParser(description='.')
+
+    # Input / Output Arguments
+    parser.add_argument('--metric', dest='metric', required=True, type=str)  # e.g., success.20
+    parser.add_argument('--paths', dest='paths', required=True, type=str, nargs='+')
+    parser.add_argument('--output', dest='output', required=True, type=str)
+
+    args = parser.parse_args()
+
+    args.metric = args.metric.split('.')
+
+    assert not os.path.exists(args.output), args.output
+    create_directory(os.path.dirname(args.output))
+
+    main(args)
```

## primeqa/ir/dense/colbert_top/utility/utils/qa_loaders.py

 * *Ordering differences only*

```diff
@@ -1,33 +1,33 @@
-import os
-import ujson
-
-from collections import defaultdict
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, file_tqdm
-
-
-def load_collection_(path, retain_titles):
-    with open(path) as f:
-        collection = []
-
-        for line in file_tqdm(f):
-            _, passage, title = line.strip().split('\t')
-
-            if retain_titles:
-                passage = title + ' | ' + passage
-
-            collection.append(passage)
-
-    return collection
-
-
-def load_qas_(path):
-    print_message("#> Loading the reference QAs from", path)
-
-    triples = []
-
-    with open(path) as f:
-        for line in f:
-            qa = ujson.loads(line)
-            triples.append((qa['qid'], qa['question'], qa['answers']))
-
-    return triples
+import os
+import ujson
+
+from collections import defaultdict
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import print_message, file_tqdm
+
+
+def load_collection_(path, retain_titles):
+    with open(path) as f:
+        collection = []
+
+        for line in file_tqdm(f):
+            _, passage, title = line.strip().split('\t')
+
+            if retain_titles:
+                passage = title + ' | ' + passage
+
+            collection.append(passage)
+
+    return collection
+
+
+def load_qas_(path):
+    print_message("#> Loading the reference QAs from", path)
+
+    triples = []
+
+    with open(path) as f:
+        for line in f:
+            qa = ujson.loads(line)
+            triples.append((qa['qid'], qa['question'], qa['answers']))
+
+    return triples
```

## primeqa/ir/dense/colbert_top/utility/utils/save_metadata.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-from primeqa.ir.dense.colbert_top.colbert.utils.utils import dotdict
-import os
-import sys
-import git
-import time
-import copy
-import ujson
-import socket
-
-
-def get_metadata_only():
-    args = dotdict()
-
-    args.hostname = socket.gethostname()
-    import os
-
-    try:
-        args.git_branch = git.Repo(search_parent_directories=True).active_branch.name
-        args.git_hash = git.Repo(search_parent_directories=True).head.object.hexsha
-        args.git_commit_datetime = str(git.Repo(search_parent_directories=True).head.object.committed_datetime)
-    except:
-        args.git_branch = None
-        args.git_hash = None
-        args.git_commit_datetime = None
-        cwd = os.getcwd()
-        print(f">> WARNING: CWD: {cwd} not in git, git parameters not stored")
-
-    args.current_datetime = time.strftime('%b %d, %Y ; %l:%M%p %Z (%z)')
-    args.cmd = ' '.join(sys.argv)
-
-    return args
-
-
-def get_metadata(args):
-    args = copy.deepcopy(args)
-
-    args.hostname = socket.gethostname()
-    args.git_branch = git.Repo(search_parent_directories=True).active_branch.name
-    args.git_hash = git.Repo(search_parent_directories=True).head.object.hexsha
-    args.git_commit_datetime = str(git.Repo(search_parent_directories=True).head.object.committed_datetime)
-    args.current_datetime = time.strftime('%b %d, %Y ; %l:%M%p %Z (%z)')
-    args.cmd = ' '.join(sys.argv)
-
-    try:
-        args.input_arguments = copy.deepcopy(args.input_arguments.__dict__)
-    except:
-        args.input_arguments = None
-
-    return dict(args.__dict__)
-
-# TODO:  No reason for deepcopy. But: (a) Call provenance() on objects that can, (b) Only save simple, small objects. No massive lists or models or weird stuff!
-# With that, I think we don't even need (necessarily) to restrict things to input_arguments.
-
-def format_metadata(metadata):
-    assert type(metadata) == dict
-
-    return ujson.dumps(metadata, indent=4)
-
-
-def save_metadata(path, args):
-    assert not os.path.exists(path), path
-
-    with open(path, 'w') as output_metadata:
-        data = get_metadata(args)
-        output_metadata.write(format_metadata(data) + '\n')
-
-    return data
+from primeqa.ir.dense.colbert_top.colbert.utils.utils import dotdict
+import os
+import sys
+import git
+import time
+import copy
+import ujson
+import socket
+
+
+def get_metadata_only():
+    args = dotdict()
+
+    args.hostname = socket.gethostname()
+    import os
+
+    try:
+        args.git_branch = git.Repo(search_parent_directories=True).active_branch.name
+        args.git_hash = git.Repo(search_parent_directories=True).head.object.hexsha
+        args.git_commit_datetime = str(git.Repo(search_parent_directories=True).head.object.committed_datetime)
+    except:
+        args.git_branch = None
+        args.git_hash = None
+        args.git_commit_datetime = None
+        cwd = os.getcwd()
+        print(f">> WARNING: CWD: {cwd} not in git, git parameters not stored")
+
+    args.current_datetime = time.strftime('%b %d, %Y ; %l:%M%p %Z (%z)')
+    args.cmd = ' '.join(sys.argv)
+
+    return args
+
+
+def get_metadata(args):
+    args = copy.deepcopy(args)
+
+    args.hostname = socket.gethostname()
+    args.git_branch = git.Repo(search_parent_directories=True).active_branch.name
+    args.git_hash = git.Repo(search_parent_directories=True).head.object.hexsha
+    args.git_commit_datetime = str(git.Repo(search_parent_directories=True).head.object.committed_datetime)
+    args.current_datetime = time.strftime('%b %d, %Y ; %l:%M%p %Z (%z)')
+    args.cmd = ' '.join(sys.argv)
+
+    try:
+        args.input_arguments = copy.deepcopy(args.input_arguments.__dict__)
+    except:
+        args.input_arguments = None
+
+    return dict(args.__dict__)
+
+# TODO:  No reason for deepcopy. But: (a) Call provenance() on objects that can, (b) Only save simple, small objects. No massive lists or models or weird stuff!
+# With that, I think we don't even need (necessarily) to restrict things to input_arguments.
+
+def format_metadata(metadata):
+    assert type(metadata) == dict
+
+    return ujson.dumps(metadata, indent=4)
+
+
+def save_metadata(path, args):
+    assert not os.path.exists(path), path
+
+    with open(path, 'w') as output_metadata:
+        data = get_metadata(args)
+        output_metadata.write(format_metadata(data) + '\n')
+
+    return data
```

## primeqa/ir/sparse/bm25_engine.py

```diff
@@ -1,39 +1,39 @@
-import os
-import logging
-
-from primeqa.ir.sparse.retriever import PyseriniRetriever
-from primeqa.ir.sparse.indexer import PyseriniIndexer
-from primeqa.ir.sparse.utils import load_queries, write_colbert_ranking_tsv
-from primeqa.ir.sparse.config import BM25Config
-
-logger = logging.getLogger(__name__)
-
-class BM25Engine:
-    def __init__(self, config: BM25Config):
-        self.config = config
-        logger.info(f"Running BM25")
-        
-    def do_index(self):
-        logger.info("Running BM25 indexing")
-        indexer = PyseriniIndexer()
-        rc = indexer.index_collection(self.config.corpus_path, self.config.index_path, 
-                    self.config.fieldnames, self.config.overwrite, 
-                    self.config.threads, self.config.additional_indexing_args )
-        logger.info(f"BM25 Indexing finished with rc: {rc}")
-
-    def do_search(self):
-            logger.info("Running BM25 search")
-            queries = load_queries(self.config.queries_path)
-            logger.info(f"Loaded queries num {len(queries)}")
-            logger.info(f"Loaded index from {self.config.index_path}")
-            searcher = PyseriniRetriever(self.config.index_path,use_bm25=self.config.use_bm25,k1=self.config.k1,b=self.config.b)
-            logger.info(f"Running search num queries: {len(queries)} top_k: {self.config.nhits} threads: {self.config.threads}")
-            search_results = searcher.batch_retrieve(list(queries.values()),list(queries.keys()),
-                        top_k=self.config.nhits,threads=self.config.threads)
-
-            if self.config.output_dir != None:
-                logger.info(f"Writing ranked results to {self.config.output_dir}")
-                if not os.path.exists(self.config.output_dir):
-                    os.makedirs(self.config.output_dir)
-                write_colbert_ranking_tsv(self.config.output_dir, search_results)
+import os
+import logging
+
+from primeqa.ir.sparse.retriever import PyseriniRetriever
+from primeqa.ir.sparse.indexer import PyseriniIndexer
+from primeqa.ir.sparse.utils import load_queries, write_colbert_ranking_tsv
+from primeqa.ir.sparse.config import BM25Config
+
+logger = logging.getLogger(__name__)
+
+class BM25Engine:
+    def __init__(self, config: BM25Config):
+        self.config = config
+        logger.info(f"Running BM25")
+        
+    def do_index(self):
+        logger.info("Running BM25 indexing")
+        indexer = PyseriniIndexer()
+        rc = indexer.index_collection(self.config.corpus_path, self.config.index_location, 
+                    self.config.fieldnames, self.config.overwrite, 
+                    self.config.threads, self.config.additional_indexing_args )
+        logger.info(f"BM25 Indexing finished with rc: {rc}")
+
+    def do_search(self):
+            logger.info("Running BM25 search with uniform parameters")
+            queries = load_queries(self.config.queries)
+            logger.info(f"Loaded queries num {len(queries)}")
+            logger.info(f"Loaded index from {self.config.index_location}")
+            searcher = PyseriniRetriever(self.config.index_location,use_bm25=self.config.use_bm25,k1=self.config.k1,b=self.config.b)
+            logger.info(f"Running search num queries: {len(queries)} topK: {self.config.topK} threads: {self.config.threads}")
+            search_results = searcher.batch_retrieve(list(queries.values()),list(queries.keys()),
+                        topK=self.config.topK,threads=self.config.threads)
+
+            if self.config.output_dir != None:
+                logger.info(f"Writing ranked results to {self.config.output_dir}")
+                if not os.path.exists(self.config.output_dir):
+                    os.makedirs(self.config.output_dir)
+                write_colbert_ranking_tsv(self.config.output_dir, search_results)
             logger.info("BM25 Search finished")
```

## primeqa/ir/sparse/config.py

```diff
@@ -1,38 +1,38 @@
-from dataclasses import dataclass, field
-import dataclasses
-
-@dataclass
-class IndexingArguments():
-
-    index_path: str = field(default=None, metadata={"help":"Path to the index directory location"})
-
-    overwrite: bool = field(default=False, metadata={"help": "Overwrite existing directory"})
-
-    corpus_path: str = field(default=None, metadata={"help":"Path to a corpus tsv or json file or directory"})
-
-    fieldnames: list = field(default=None, metadata={"help":"fields names to use to identify document_id, title, text if corpus tsv has no headings"})
-
-    additional_indexing_args: str = field(default='--storePositions --storeDocvectors --storeRaw', metadata={"help":'pyserini index options'})
-
-    threads: int = field(default=1, metadata={"help":'num threads'})
-
-
-@dataclass
-class SearchArguments():
-    index_path: str = field(default=None, metadata={"help":"Path to the index directory location"})
-
-    queries_path: str = field(default=None, metadata={"help":"Path to the tsv file where each line is in format 'id\tquery'"})
-
-    nhits: int = field(default=10, metadata={"help":"Number of hits to return"})
-
-    use_bm25: bool = field(default=True, metadata={"help":"Use bm25 scoring"})
-    
-    k1: float = field(default=0.8, metadata={"help":"bm25 parameter to tune impact of term frequency"})
-    
-    b: float = field(default=0.4, metadata={"help":"bm25 constant to fine tune the effect of document length"})
-
-    output_dir: str = field(default=None, metadata={"help":"Output directory to write out search results"})
-
-@dataclass
-class BM25Config(SearchArguments, IndexingArguments):
-    pass
+from dataclasses import dataclass, field
+import dataclasses
+
+@dataclass
+class IndexingArguments():
+
+    index_path: str = field(default=None, metadata={"help":"Path to the index directory location"})
+
+    overwrite: bool = field(default=False, metadata={"help": "Overwrite existing directory"})
+
+    corpus_path: str = field(default=None, metadata={"help":"Path to a corpus tsv or json file or directory"})
+
+    fieldnames: list = field(default=None, metadata={"help":"fields names to use to identify document_id, title, text if corpus tsv has no headings"})
+
+    additional_indexing_args: str = field(default='--storePositions --storeDocvectors --storeRaw', metadata={"help":'pyserini index options'})
+
+    threads: int = field(default=1, metadata={"help":'num threads'})
+
+
+@dataclass
+class SearchArguments():
+    index_location: str = field(default=None, metadata={"help":"Path to the index directory location"})
+
+    queries: str = field(default=None, metadata={"help":"Path to the tsv file where each line is in format 'id\tquery'"})
+
+    topK: int = field(default=10, metadata={"help":"Number of hits to return"})
+
+    use_bm25: bool = field(default=True, metadata={"help":"Use bm25 scoring"})
+    
+    k1: float = field(default=0.8, metadata={"help":"bm25 parameter to tune impact of term frequency"})
+    
+    b: float = field(default=0.4, metadata={"help":"bm25 constant to fine tune the effect of document length"})
+
+    output_dir: str = field(default=None, metadata={"help":"Output directory to write out search results"})
+
+@dataclass
+class BM25Config(SearchArguments, IndexingArguments):
+    pass
```

## primeqa/ir/sparse/indexer.py

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-
-import logging
-import tempfile
-from primeqa.ir.util.corpus_reader import corpus_reader
-import tempfile
-import os
-import shutil
-from tqdm import tqdm
-import json
-from pyserini.search import LuceneSearcher
-import subprocess
-
-logger = logging.getLogger(__name__)
-
-class PyseriniIndexer:
-    """ 
-        A class to handle indexing a collection of documents in Pyserini
-    """
-
-    def __init__(self):
-        pass
-
-    def _clean_text(self,text: str):
-        return text.replace('\t',' ')
-
-    def _run_command(self, cmd):
-        logger.info(cmd)
-        process = subprocess.Popen(cmd.split())
-        rc = process.wait()
-        return rc
-
-    def _preprocess_corpus(self, corpus_path, tmpdirname, fieldnames=None):
-        reader = corpus_reader(corpus_path, fieldnames=fieldnames)
-        outf = open( os.path.join(tmpdirname,"corpus_pyserini_fmt.jsonl"), 'w' )
-        num_docs = 0
-        for passage in tqdm(reader):
-            json_string = json.dumps({
-                'id': passage.pid,
-                'contents': f'{self._clean_text(passage.title)}\t{self._clean_text(passage.text)}'
-            })
-            outf.write(f'{json_string}\n')
-            num_docs += 1
-        return num_docs
-
-    """
-
-        Index the corpus of documents.
-        - First convert the input corpus to the json format requiered by Pyserini 'DefaultLuceneDocumentGenerator'. 
-        - This will write to a temporary directory within the directory specified by the 'index_path' argument
-        - Second run the indexing command.  This launches a subprocess and runs  'python -m pyserini.index.lucene <args>'
-        - Validate the index is usable by opening the index and checking the the number of documents 
-        is equal to the intput corpus.
-
-
-        Args:
-            corpus_path (str) : path to file or directory of documents in tsv or jsonl format.
-            index_path (str) : output directory path where the index is written
-            fieldnames ( List, Optional): column headers to be assigned to tsv without headers
-            overwrite (bool, Optional): overwrite an existing directory, defaults to false
-            threads (int): num threads to be used when indexing
-            additional_index_cmd_args (str, Optional): indexing arguments, defaults to '--storePositions --storeDocvectors --storeRaw'
-
-        Returns:
-
-
-        """
-    def index_collection(self, corpus_path: str, index_path: str, fieldnames=None, overwrite=False, 
-            threads=1, additional_index_cmd_args='--storePositions --storeDocvectors --storeRaw' ):
-        if not overwrite and os.path.exists(index_path) and os.listdir(index_path) :
-            raise ValueError(f"Index path not empty '{index_path}' and overwrite not specified")
-        if not os.path.exists(index_path):
-            os.makedirs(index_path)
-        # create temporary subdirectory for the corpus
-        with tempfile.TemporaryDirectory(prefix='tmp',dir=index_path) as tmpdirname:
-            # convert corpus documents to pyserini jsonl
-            num_docs = self._preprocess_corpus(corpus_path, tmpdirname, fieldnames=fieldnames)
-            # build index command
-            cmd1 = f'python -m pyserini.index.lucene -collection JsonCollection ' + \
-                f'-generator DefaultLuceneDocumentGenerator ' + \
-                f'-threads {threads}  {additional_index_cmd_args} ' \
-                f'-input {tmpdirname} -index {index_path}'
-            # run the command
-            rc = self._run_command(cmd1)
-            # cleanup temporary corpus directory
-            shutil.rmtree(tmpdirname)
-        assert(rc == 0)
-
-        logger.info(f"Indexing completed at index location {index_path}. validating document count" )
-        searcher = LuceneSearcher(index_path)
-        logger.info(f"Index {index_path} contains {searcher.num_docs} documents")
-        assert(searcher.num_docs == num_docs)
-        logging.info(f"Index available at {index_path}")
-        return rc
-
+
+import logging
+import tempfile
+from primeqa.ir.util.corpus_reader import corpus_reader
+import tempfile
+import os
+import shutil
+from tqdm import tqdm
+import json
+from pyserini.search import LuceneSearcher
+import subprocess
+
+logger = logging.getLogger(__name__)
+
+class PyseriniIndexer:
+    """ 
+        A class to handle indexing a collection of documents in Pyserini
+    """
+
+    def __init__(self):
+        pass
+
+    def _clean_text(self,text: str):
+        return text.replace('\t',' ')
+
+    def _run_command(self, cmd):
+        logger.info(cmd)
+        process = subprocess.Popen(cmd.split())
+        rc = process.wait()
+        return rc
+
+    def _preprocess_corpus(self, corpus_path, tmpdirname, fieldnames=None):
+        reader = corpus_reader(corpus_path, fieldnames=fieldnames)
+        outf = open( os.path.join(tmpdirname,"corpus_pyserini_fmt.jsonl"), 'w' )
+        num_docs = 0
+        for passage in tqdm(reader):
+            json_string = json.dumps({
+                'id': passage.pid,
+                'contents': f'{self._clean_text(passage.title)}\t{self._clean_text(passage.text)}'
+            })
+            outf.write(f'{json_string}\n')
+            num_docs += 1
+        return num_docs
+
+    """
+
+        Index the corpus of documents.
+        - First convert the input corpus to the json format requiered by Pyserini 'DefaultLuceneDocumentGenerator'. 
+        - This will write to a temporary directory within the directory specified by the 'index_path' argument
+        - Second run the indexing command.  This launches a subprocess and runs  'python -m pyserini.index.lucene <args>'
+        - Validate the index is usable by opening the index and checking the the number of documents 
+        is equal to the intput corpus.
+
+
+        Args:
+            corpus_path (str) : path to file or directory of documents in tsv or jsonl format.
+            index_path (str) : output directory path where the index is written
+            fieldnames ( List, Optional): column headers to be assigned to tsv without headers
+            overwrite (bool, Optional): overwrite an existing directory, defaults to false
+            threads (int): num threads to be used when indexing
+            additional_index_cmd_args (str, Optional): indexing arguments, defaults to '--storePositions --storeDocvectors --storeRaw'
+
+        Returns:
+
+
+        """
+    def index_collection(self, corpus_path: str, index_path: str, fieldnames=None, overwrite=False, 
+            threads=1, additional_index_cmd_args='--storePositions --storeDocvectors --storeRaw' ):
+        if not overwrite and os.path.exists(index_path) and os.listdir(index_path) :
+            raise ValueError(f"Index path not empty '{index_path}' and overwrite not specified")
+        if not os.path.exists(index_path):
+            os.makedirs(index_path)
+        # create temporary subdirectory for the corpus
+        with tempfile.TemporaryDirectory(prefix='tmp',dir=index_path) as tmpdirname:
+            # convert corpus documents to pyserini jsonl
+            num_docs = self._preprocess_corpus(corpus_path, tmpdirname, fieldnames=fieldnames)
+            # build index command
+            cmd1 = f'python -m pyserini.index.lucene -collection JsonCollection ' + \
+                f'-generator DefaultLuceneDocumentGenerator ' + \
+                f'-threads {threads}  {additional_index_cmd_args} ' \
+                f'-input {tmpdirname} -index {index_path}'
+            # run the command
+            rc = self._run_command(cmd1)
+            # cleanup temporary corpus directory
+            shutil.rmtree(tmpdirname)
+        assert(rc == 0)
+
+        logger.info(f"Indexing completed at index location {index_path}. validating document count" )
+        searcher = LuceneSearcher(index_path)
+        logger.info(f"Index {index_path} contains {searcher.num_docs} documents")
+        assert(searcher.num_docs == num_docs)
+        logging.info(f"Index available at {index_path}")
+        return rc
+
```

## primeqa/ir/sparse/retriever.py

```diff
@@ -1,146 +1,146 @@
-from pyserini.search import LuceneSearcher
-from typing import Optional, List
-import logging
-import json
-from abc import ABCMeta, abstractmethod
-
-logger = logging.getLogger(__name__)
-
-class BaseRetriever(metaclass=ABCMeta):
-    """ 
-        Base class for Retriever
-    """
-
-    @abstractmethod
-    def retrieve(self, query: str, top_k: Optional[int] = 10):
-        """
-
-        Run queries against the index to retrieve ranked list of documents
-        Return documents that are most relevant to the query.
-
-        Args:
-             query: search
-             top_k: number of hits to return, defaults to 10
-
-
-        Returns:
-             List of hits, each hit is a dict containing :
-             {
-                "rank": i,
-                "score": hit.score,
-                "doc_id": docid,
-                "title": title,
-                "text": text 
-            }
-                
-
-        """
-        pass
-
-    @abstractmethod
-    def batch_retrieve(self,  queries: List[str], qids: List[str], top_k: int = 10, threads: int = 1):
-        """
-           Run a batch of queries 
-
-           Args:
-                queries:  list of query strings
-                qids:     list of qid strings corresponding to queries
-                top_k:    number of hits to return, defaults to 10
-                threads:  maximum number of threads to use
-                
-            Returns:
-                Dict of qid to hits
-                
-        """
-        pass
-
-class PyseriniRetriever(BaseRetriever):
-    def __init__(self, index_path: str, use_bm25: bool = True, k1: float = float(0.9), b: float = float(0.4)):
-        """
-        Initialize Pyserini retriever
-
-        Args:
-            index_path (str): Path to a Pyserini index
-            use_bm25 (bool, optional): set BM25 as the scoring function. Defaults to True.
-            k1 (float, optional): bm25 parameter to tune impact of term frequency Defaults to float(0.9).
-            b (float, optional): bm25 constant to fine tune the effect of document length   Defaults to float(0.4).
-        """
-        self.index_path = index_path
-        self.searcher = LuceneSearcher(index_path)
-        self.searcher.set_bm25()
-        if use_bm25:
-            self.searcher.set_bm25(k1=k1,b=b)
-        self.top_k = 10
-        logger.info(f'Initialized LuceneSearcher index_dir: {self.searcher.index_dir}  num_docs: {self.searcher.num_docs} use_bm25: {use_bm25} k1: {k1} b: {b}')
-
-    def retrieve(self, query: str, top_k: Optional[int] = 10):
-        """
-
-        Run queries against the index to retrieve ranked list of documents
-        Return documents that are most relevant to the query.
-
-        Args:
-             query: search
-             top_k: number of hits to return, defaults to 10
-
-
-        Returns:
-             List of hits, each hit is a dict containing :
-             {
-                "rank": i,
-                "score": hit.score,
-                "doc_id": docid,
-                "title": title,
-                "text": text 
-            }
-                
-
-        """
-
-        hits = self.searcher.search(query, top_k)
-        search_results = self._collect_hits(hits)
-        return search_results
-
-
-    def batch_retrieve(self,  queries: List[str], qids: List[str], top_k: int = 10, threads: int = 1):
-
-        """
-           Run a batch of queries 
-
-           Args:
-                queries:  list of query strings
-                qids:     list of qid strings corresponding to queries
-                top_k:    number of hits to return, defaults to 10
-                threads:  maximum number of threads to use
-                
-            Returns:
-                Dict of qid to hits
-
-                
-        """
-
-        hits = self.searcher.batch_search(queries, qids, k=top_k, threads=threads)
-        query_to_hits = {}
-        for q, hits in hits.items():
-            query_to_hits[q] = self._collect_hits(hits)
-        return query_to_hits
-
-
-    def _collect_hits(self, hits: List):
-        search_results = []
-        for i, hit in enumerate(hits):
-            title, text = json.loads(hit.raw)['contents'].split("\t")
-            title = title.replace('\n',' ')
-            text = text.replace('\n',' ')
-            docid = hit.docid
-            search_result = {
-                "rank": i,
-                "score": hit.score,
-                "doc_id": docid,
-                "title": title,
-                "text": text 
-            }
-            search_results.append(search_result)
-        return search_results
-
-
+from pyserini.search import LuceneSearcher
+from typing import Optional, List
+import logging
+import json
+from abc import ABCMeta, abstractmethod
+
+logger = logging.getLogger(__name__)
+
+class BaseRetriever(metaclass=ABCMeta):
+    """ 
+        Base class for Retriever
+    """
+
+    @abstractmethod
+    def retrieve(self, query: str, topK: Optional[int] = 10):
+        """
+
+        Run queries against the index to retrieve ranked list of documents
+        Return documents that are most relevant to the query.
+
+        Args:
+             query: search
+             top_k: number of hits to return, defaults to 10
+
+
+        Returns:
+             List of hits, each hit is a dict containing :
+             {
+                "rank": i,
+                "score": hit.score,
+                "doc_id": docid,
+                "title": title,
+                "text": text 
+            }
+                
+
+        """
+        pass
+
+    @abstractmethod
+    def batch_retrieve(self,  queries: List[str], qids: List[str], topK: int = 10, threads: int = 1):
+        """
+           Run a batch of queries 
+
+           Args:
+                queries:  list of query strings
+                qids:     list of qid strings corresponding to queries
+                top_k:    number of hits to return, defaults to 10
+                threads:  maximum number of threads to use
+                
+            Returns:
+                Dict of qid to hits
+                
+        """
+        pass
+
+class PyseriniRetriever(BaseRetriever):
+    def __init__(self, index_location: str, use_bm25: bool = True, k1: float = float(0.9), b: float = float(0.4)):
+        """
+        Initialize Pyserini retriever
+
+        Args:
+            index_path (str): Path to a Pyserini index
+            use_bm25 (bool, optional): set BM25 as the scoring function. Defaults to True.
+            k1 (float, optional): bm25 parameter to tune impact of term frequency Defaults to float(0.9).
+            b (float, optional): bm25 constant to fine tune the effect of document length   Defaults to float(0.4).
+        """
+        self.index_location = index_location
+        self.searcher = LuceneSearcher(index_location)
+        self.searcher.set_bm25()
+        if use_bm25:
+            self.searcher.set_bm25(k1=k1,b=b)
+        self.topK = 10
+        logger.info(f'Initialized LuceneSearcher index_dir: {self.searcher.index_dir}  num_docs: {self.searcher.num_docs} use_bm25: {use_bm25} k1: {k1} b: {b}')
+
+    def retrieve(self, query: str, topK: Optional[int] = 10):
+        """
+
+        Run queries against the index to retrieve ranked list of documents
+        Return documents that are most relevant to the query.
+
+        Args:
+             query: search
+             top_k: number of hits to return, defaults to 10
+
+
+        Returns:
+             List of hits, each hit is a dict containing :
+             {
+                "rank": i,
+                "score": hit.score,
+                "doc_id": docid,
+                "title": title,
+                "text": text 
+            }
+                
+
+        """
+
+        hits = self.searcher.search(query, topK)
+        search_results = self._collect_hits(hits)
+        return search_results
+
+
+    def batch_retrieve(self,  queries: List[str], qids: List[str], topK: int = 10, threads: int = 1):
+
+        """
+           Run a batch of queries 
+
+           Args:
+                queries:  list of query strings
+                qids:     list of qid strings corresponding to queries
+                top_k:    number of hits to return, defaults to 10
+                threads:  maximum number of threads to use
+                
+            Returns:
+                Dict of qid to hits
+
+                
+        """
+
+        hits = self.searcher.batch_search(queries, qids, k=topK, threads=threads)
+        query_to_hits = {}
+        for q, hits in hits.items():
+            query_to_hits[q] = self._collect_hits(hits)
+        return query_to_hits
+
+
+    def _collect_hits(self, hits: List):
+        search_results = []
+        for i, hit in enumerate(hits):
+            title, text = json.loads(hit.raw)['contents'].split("\t")
+            title = title.replace('\n',' ')
+            text = text.replace('\n',' ')
+            docid = hit.docid
+            search_result = {
+                "rank": i,
+                "score": hit.score,
+                "doc_id": docid,
+                "title": title,
+                "text": text 
+            }
+            search_results.append(search_result)
+        return search_results
+
+
```

## primeqa/ir/sparse/utils.py

```diff
@@ -1,69 +1,69 @@
-import os
-import json
-import csv
-from csv import DictReader
-import logging
-from typing import List, Dict
-
-logger = logging.getLogger(__name__)
-
-def load_queries(queries_tsv_filepath):
-    queries = {}
-    with open(queries_tsv_filepath,'r') as f:
-        reader = DictReader(f,delimiter='\t',fieldnames=['id','text'])
-        for row in reader:
-            queries[row['id']] = row['text']
-    return queries
-
-def write_colbert_ranking_tsv(output_dir: str , id_to_hits: Dict):
-    output_file = os.path.join(output_dir,'ranking.tsv')
-    search_results = []
-    for id in id_to_hits:
-        for i, hit in enumerate(id_to_hits[id]):
-            result = {
-                "id": id,
-                "docid": hit['doc_id'],
-                "rank": i+1, 
-                "score": hit['score']
-            }
-            search_results.append(result)
-
-    with open(output_file,'w',encoding='utf-8') as f:
-        writer = csv.writer(f, delimiter='\t', lineterminator='\n')
-        for r in search_results:
-            writer.writerow(r.values())
-    logger.info(f"Wrote {output_file}")
-        
-def get_language_id(xorqa_data_file: str) -> Dict:
-    id_to_lang = {}
-    with open(xorqa_data_file,'r') as f:
-        for line in f:
-            data = json.loads(line.strip())
-            id_to_lang[data['id']] = data['lang']
-    return id_to_lang
-
-def write_xorqa_json(output_dir: str, id_to_hits: Dict, top_n: int =100, xorqa_data_file: str = None):
-    id_to_lang = None
-    if xorqa_data_file != None:
-        id_to_lang = get_language_id(xorqa_data_file)
-
-    output_file = os.path.join(output_dir,'ranking_xortydi_format.json')
-    search_results = []
-    for id in id_to_hits:
-        json_data = {
-            "id" : id,
-            "lang" : id_to_lang[id] if id_to_lang != None else "", 
-            "ctxs" : []
-        }
-        for hit in id_to_hits[id][:top_n]:
-            json_data["ctxs"].append(hit['text'])
-        search_results.append(json_data)
-    with open(output_file,'w',encoding='utf-8') as f:
-        json.dump(search_results,f, indent=4)
-    logger.info(f"Wrote {output_file}")
-
-def write_search_results(id_to_hits: Dict, output_dir: str, max_hits_output: int, xorqa_data_file: str =None):
-    if not os.path.exists(output_dir):
-        os.makedirs(output_dir)
-    write_xorqa_json(output_dir, id_to_hits, top_n=max_hits_output, xorqa_data_file=xorqa_data_file)
+import os
+import json
+import csv
+from csv import DictReader
+import logging
+from typing import List, Dict
+
+logger = logging.getLogger(__name__)
+
+def load_queries(queries_tsv_filepath):
+    queries = {}
+    with open(queries_tsv_filepath,'r') as f:
+        reader = DictReader(f,delimiter='\t',fieldnames=['id','text'])
+        for row in reader:
+            queries[row['id']] = row['text']
+    return queries
+
+def write_colbert_ranking_tsv(output_dir: str , id_to_hits: Dict):
+    output_file = os.path.join(output_dir,'ranked_passages.tsv')
+    search_results = []
+    for id in id_to_hits:
+        for i, hit in enumerate(id_to_hits[id]):
+            result = {
+                "id": id,
+                "docid": hit['doc_id'],
+                "rank": i+1, 
+                "score": hit['score']
+            }
+            search_results.append(result)
+
+    with open(output_file,'w',encoding='utf-8') as f:
+        writer = csv.writer(f, delimiter='\t', lineterminator='\n')
+        for r in search_results:
+            writer.writerow(r.values())
+    logger.info(f"Wrote {output_file}")
+        
+def get_language_id(xorqa_data_file: str) -> Dict:
+    id_to_lang = {}
+    with open(xorqa_data_file,'r') as f:
+        for line in f:
+            data = json.loads(line.strip())
+            id_to_lang[data['id']] = data['lang']
+    return id_to_lang
+
+def write_xorqa_json(output_dir: str, id_to_hits: Dict, top_n: int =100, xorqa_data_file: str = None):
+    id_to_lang = None
+    if xorqa_data_file != None:
+        id_to_lang = get_language_id(xorqa_data_file)
+
+    output_file = os.path.join(output_dir,'ranking_xortydi_format.json')
+    search_results = []
+    for id in id_to_hits:
+        json_data = {
+            "id" : id,
+            "lang" : id_to_lang[id] if id_to_lang != None else "", 
+            "ctxs" : []
+        }
+        for hit in id_to_hits[id][:top_n]:
+            json_data["ctxs"].append(hit['text'])
+        search_results.append(json_data)
+    with open(output_file,'w',encoding='utf-8') as f:
+        json.dump(search_results,f, indent=4)
+    logger.info(f"Wrote {output_file}")
+
+def write_search_results(id_to_hits: Dict, output_dir: str, max_hits_output: int, xorqa_data_file: str =None):
+    if not os.path.exists(output_dir):
+        os.makedirs(output_dir)
+    write_xorqa_json(output_dir, id_to_hits, top_n=max_hits_output, xorqa_data_file=xorqa_data_file)
     write_colbert_ranking_tsv(output_dir, id_to_hits)
```

## primeqa/mrc/run_mrc.py

```diff
@@ -1,541 +1,568 @@
-import logging
-import os
-import sys
-import traceback
-import json
-import torch
-import gc
-import glob
-from dataclasses import dataclass, field
-from importlib import import_module
-from operator import attrgetter
-from typing import Optional, Type
-
-import datasets
-import apache_beam as beam
-from transformers import HfArgumentParser, TrainingArguments, DataCollatorWithPadding, AutoConfig, AutoTokenizer
-from transformers.trainer_utils import get_last_checkpoint, set_seed
-
-from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
-from primeqa.mrc.metrics.tydi_f1.tydi_f1 import TyDiF1
-from primeqa.mrc.metrics.mlqa.mlqa import MLQA
-from primeqa.mrc.metrics.squad.squad import SQUAD
-from primeqa.mrc.metrics.nq_f1.nq_f1 import NQF1
-from primeqa.mrc.models.heads.extractive import EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD
-from primeqa.mrc.models.task_model import ModelForDownstreamTasks
-from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
-from primeqa.boolqa.processors.postprocessors.extractive import ExtractivePipelinePostProcessor
-from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers
-from primeqa.mrc.processors.preprocessors.tydiqa import TyDiQAPreprocessor
-from primeqa.mrc.processors.preprocessors.squad import SQUADPreprocessor
-from primeqa.mrc.processors.postprocessors.squad import SQUADPostProcessor
-from primeqa.mrc.processors.preprocessors.natural_questions import NaturalQuestionsPreProcessor
-from primeqa.mrc.processors.postprocessors.natural_questions import NaturalQuestionsPostProcessor
-from primeqa.mrc.processors.preprocessors.tydiqa_google import TyDiQAGooglePreprocessor
-from primeqa.mrc.trainers.mrc import MRCTrainer
-from primeqa.boolqa.run_boolqa_classifier import main as cls_main
-from primeqa.boolqa.run_score_normalizer import main as sn_main
-
-from primeqa.tableqa.run_tableqa import run_table_qa
-
-def object_reference(reference_as_str: str) -> object:
-    """
-    Given a fully qualified path to a class reference, return a pointer to the reference.
-    This will work with types, functions, methods, and other objects (e.g. dict).
-
-    Args:
-        reference_as_str: the fully qualified path (expects the fully qualified path in dot notation,
-                          e.g. primeqa.mrc.processors.postprocessors.extractive.ExtractivePostProcessor).
-
-    Returns:
-        reference to path given by input
-
-    Raises:
-        TypeError: Unable to resolve input path
-    """
-    def _split_into_class_and_module_name(class_path):
-        modules = class_path.split('.')
-        if len(modules) > 1:
-            return ".".join(modules[:-1]), modules[-1]
-        else:
-            return class_path, None
-
-    try:
-        module_name, object_name = _split_into_class_and_module_name(reference_as_str)
-        module_reference = import_module(module_name)
-        if object_name is None:
-            return module_reference
-        else:
-            return getattr(module_reference, object_name)
-    except Exception as ex:
-        traceback.print_exc()  # Shows additional traceback for why imports fail
-        raise TypeError(f"Unable to resolve the string {reference_as_str} to a fully qualified class path") from ex
-
-
-# modified from
-# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
-@dataclass
-class ModelArguments:
-    """
-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
-    """
-
-    model_name_or_path: str = field(
-        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
-    )
-    config_name: Optional[str] = field(
-        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
-    )
-    tokenizer_name: Optional[str] = field(
-        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
-    )
-    cache_dir: Optional[str] = field(
-        default=None,
-        metadata={"help": "Path to directory to store the pretrained models downloaded from huggingface.co"},
-    )
-    confidence_model_path: str = field(
-        default=None,
-        metadata={"help": "Path to the confidence calibration model"}
-    )
-
-
-# modified from
-# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
-@dataclass
-class DataTrainingArguments:
-    """
-    Arguments pertaining to what data we are going to input our model for training and eval.
-    """
-
-    tableqa_config_file: Optional[str] = field(
-        default=None, metadata={"help": "TableQA additional arguments"}
-    )
-    dataset_name: str = field(
-        default="tydiqa", metadata={"help": "The name of the dataset to use (via the datasets library)."}
-    )
-    train_file: Optional[str] = field(
-        default=None, metadata={"help": "local file(s) to train on."}
-    )
-    eval_file: Optional[str] = field(
-        default=None, metadata={"help": "local file(s) to test on."}
-    )
-    data_file_format: str = field(
-        default="json", metadata={"help": "the format of the local dataset files (json, csv, text, pandas)"}
-    )
-    dataset_config_name: str = field(
-        default="primary_task", metadata={
-            "help": "The configuration name of the dataset to use (via the datasets library)."
-        }
-    )
-    overwrite_cache: bool = field(
-        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
-    )
-    preprocessing_num_workers: Optional[int] = field(
-        default=None,
-        metadata={"help": "The number of processes to use for the preprocessing."},
-    )
-    max_seq_length: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "The maximum total input sequence length after tokenization. Sequences longer "
-                    "than this will be truncated, sequences shorter will be padded."
-        },
-    )
-    max_q_char_len: int = field(
-        default=128, metadata={"help": "Max length per question in characters"}
-    )
-    single_context_multiple_passages: bool = field(
-        default=False, metadata={
-            "help": "Allow multiple passages in the same input feature. "
-                    "For an example with question q and context c_{1..n} setting this to True"
-                    "will allow q|c_{i}c_{i+1}; whereas setting this to False enforces q|c_{i} q|c_{i+1}. "
-                    "Note that not all datasets/preprocessors support both values of this parameter. "
-                    "Some preprocessors may override this value."
-            },
-    )
-    max_contexts: Optional[int] = field(
-        default=None, metadata={"help": "Max contexts per consider"}
-    )
-    max_train_samples: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
-                    "value if set."
-        },
-    )
-    max_eval_samples: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
-                    "value if set."
-        },
-    )
-    doc_stride: int = field(
-        default=128,
-        metadata={"help": "When splitting up a long document into chunks, how much stride to take between chunks."},
-    )
-    n_best_size: int = field(
-        default=20,
-        metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
-    )
-    n_best_logits: int = field(
-        default=20,
-        metadata={"help": "The number of logits to consider when searching for start and end position of an answer"}
-    )
-    max_answer_length: int = field(
-        default=32,
-        metadata={
-            "help": "The maximum length of an answer that can be generated. This is needed because the start "
-                    "and end predictions are not conditioned on one another."
-        },
-    )
-    negative_sampling_prob_when_has_answer: float = field(
-        default=0.01,
-        metadata={
-            "help": "Only used when preparing training features, not for decoding. "
-                    "For an example with question q and context c_{1..n} where ∃ answer a ∈ c"
-                    "an input feature span q|c_{i} where a ∉ c_{i} will be kept with this probability."
-                    "Otherwise it will be discarded."
-        },
-    )
-    negative_sampling_prob_when_no_answer: float = field(
-        default=0.04,
-        metadata={
-            "help": "Only used when preparing training features, not for decoding. "
-                    "For an example with question q and context c_{1..n} where ∄ answer a ∈ c"
-                    "an input feature span q|c_{i} will be kept with this probability."
-                    "Otherwise it will be discarded."
-        },
-    )
-    beam_runner: str = field(
-        default=None,
-        metadata={"help": "The beam runner for loading large dataset.",
-                  "choices": ['DirectRunner'],
-                  }
-    )
-
-
-@dataclass
-class TaskArguments:
-    """
-    Task specific arguments.
-    """
-    modality: str = field(
-        default='text',
-        metadata={"help": "whether modality is table or text",
-        "choices": ["text", "table"]
-                  }
-    )
-
-    scorer_type: str = field(
-        default='weighted_sum_target_type_and_score_diff',
-        metadata={"help": "The name of the scorer to compute answer score.",
-                  "choices": SupportedSpanScorers.get_supported()
-                  }
-    )
-    task_heads: object_reference = field(
-        default=None,
-        metadata={"help": "The name of the task head to use.",
-                  "choices": [EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD]
-                  }
-    )
-    preprocessor: object_reference = field(
-        default=TyDiQAPreprocessor,
-        metadata={"help": "The name of the preprocessor to use.",
-                  "choices": [TyDiQAPreprocessor,SQUADPreprocessor,TyDiQAGooglePreprocessor,NaturalQuestionsPreProcessor]
-                  }
-    )
-    postprocessor: object_reference = field(
-        default=ExtractivePostProcessor,
-        metadata={"help": "The name of the postprocessor to use.",
-                  "choices": [ExtractivePostProcessor,ExtractivePipelinePostProcessor,SQUADPostProcessor, NaturalQuestionsPostProcessor]
-                  }
-    )
-    eval_metrics: str = field(
-        default="TyDiF1",
-        metadata={"help": "The name of the evaluation metric function implemented in primeqa (e.g. TyDiF1).",
-                  "choices": ["TyDiF1","SQUAD","MLQA","NQF1"]
-                 }
-    )
-    do_boolean: bool = field(
-        default=False, metadata={"help": "Enable processing of boolean questions.  If activated,"
-                                        "--do_eval will be forced also, and --postprocessor will be "
-                                        "defaulted to ExtractivePipelinePostProcessor unless overridden"
-                                        "by a postprocessor that subclasses ExtractivePipelinePostProcessor"}
-    )
-    boolean_config: str = field(
-        default=None, metadata={"help": "The configuration name file for the boolean task in json format"}
-    )    
-    passage_non_null_threshold: int = field(
-        default=2,
-        metadata={"help": "The passage level non-null threshold (number of annotators to indicate no answer). This should be set to 1 if there is only one annotation"}
-    )
-    span_non_null_threshold: int = field(
-        default=2,
-        metadata={"help": "The span level non-null threshold (number of annotators to indicate no answer). This should be set to 1 if there is only one annotation"}
-    )
-    verbose: bool = field(
-        default=False,
-        metadata={"help": "Prints logging info if true (including evaluation output)"}
-    )
-    output_dropout_rate: float = field(
-        default=0.25,
-        metadata={"help": "The dropout probability applied to LM output in "
-                          "order to generate confidence calibration features."
-                  },
-    )
-    decoding_times_with_dropout: int = field(
-        default=5,
-        metadata={"help": "The number of decoding times to generate confidence "
-                          "calibration features with dropout."
-                  },
-    )
-
-    def __post_init__(self):
-        if not self.task_heads:
-            self.task_heads = EXTRACTIVE_HEAD  # cannot directly set mutable value as default
-
-
-def main():
-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments, TaskArguments))
-    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
-        # If we pass only one argument to the script and it's the path to a json file,
-        # let's parse it to get our arguments.
-        model_args, data_args, training_args, task_args = \
-            parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
-    else:
-        model_args, data_args, training_args, task_args = parser.parse_args_into_dataclasses()
-
-    # if we are doing the boolean post-processing, require do_eval, because the id's (not included in HF
-    # dataset) might have changed
-    # we require ExtractivePipelinePostProcessor to populate certain fields for the boolqa classifiers,
-    # so force it here - this can't be done in a __post_init__ postprocess is in TaskArguments and
-    # do_eval is in TrainingArguments
-    if task_args.do_boolean:
-        training_args.do_eval = True
-        if not isinstance(task_args.postprocessor, ExtractivePipelinePostProcessor):
-            task_args.postprocessor = ExtractivePipelinePostProcessor
-
-
-    logger = logging.getLogger(__name__)
-    if task_args.verbose:
-        logging.basicConfig(level = logging.INFO)
-    scorer_type = task_args.scorer_type
-    set_seed(training_args.seed)
-
-    # Detecting last checkpoint.
-    last_checkpoint = None
-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
-        last_checkpoint = get_last_checkpoint(training_args.output_dir)
-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
-            raise ValueError(
-                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
-                "Use --overwrite_output_dir to overcome."
-            )
-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
-            logger.info(
-                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
-                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
-            )
-
-    # Run Table Question Answering        
-    if task_args.modality=="table":
-        run_table_qa(data_args,model_args,training_args)
-        sys.exit(0)
-
-    task_heads = task_args.task_heads
-    config = AutoConfig.from_pretrained(
-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
-        cache_dir=model_args.cache_dir,
-    )
-    tokenizer = AutoTokenizer.from_pretrained(
-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
-        cache_dir=model_args.cache_dir,
-        use_fast=True,
-        config=config,
-    )
-
-    config.sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)
-    config.output_dropout_rate = task_args.output_dropout_rate
-    config.decoding_times_with_dropout = task_args.decoding_times_with_dropout
-    model = ModelForDownstreamTasks.from_config(
-        config,
-        model_args.model_name_or_path,
-        task_heads=task_heads,
-        cache_dir=model_args.cache_dir,
-    )
-    model.set_task_head(next(iter(task_heads)))
-
-    # load data
-    logger.info('Loading dataset')
-    if data_args.train_file is not None or data_args.eval_file is not None:
-        data_files = {}
-
-        if data_args.train_file is not None: 
-            data_files['train'] = glob.glob(data_args.train_file)
-        if data_args.eval_file is not None: 
-            data_files['validation'] = glob.glob(data_args.eval_file)
-
-        raw_datasets = datasets.load_dataset(data_args.data_file_format, 
-            data_files=data_files,
-            cache_dir=model_args.cache_dir)
-    else:
-        if data_args.dataset_name == "natural_questions":
-            raw_datasets = datasets.load_dataset(
-                data_args.dataset_name,
-                data_args.dataset_config_name,
-                cache_dir=model_args.cache_dir,
-                beam_runner=data_args.beam_runner,
-                revision="main"
-            )
-        else:
-            raw_datasets = datasets.load_dataset(
-                data_args.dataset_name,
-                data_args.dataset_config_name,
-                cache_dir=model_args.cache_dir
-            )
-
-    # load preprocessor
-    preprocessor_class = task_args.preprocessor
-    preprocessor = preprocessor_class(
-        stride=data_args.doc_stride,
-        tokenizer=tokenizer,
-        negative_sampling_prob_when_has_answer=data_args.negative_sampling_prob_when_has_answer,
-        negative_sampling_prob_when_no_answer=data_args.negative_sampling_prob_when_no_answer,
-        load_from_cache_file=not data_args.overwrite_cache,
-        max_seq_len=data_args.max_seq_length,
-        num_workers=data_args.preprocessing_num_workers,
-        max_q_char_len=data_args.max_q_char_len,
-        single_context_multiple_passages=data_args.single_context_multiple_passages,
-        max_contexts=data_args.max_contexts,
-    )
-
-    # process train data
-    if training_args.do_train:
-        train_dataset = raw_datasets["train"]
-        max_train_samples = data_args.max_train_samples
-        if max_train_samples is not None:
-            # We will select sample from whole data if argument is specified
-            train_dataset = train_dataset.select(range(max_train_samples))
-        # Train Feature Creation
-        with training_args.main_process_first(desc="train dataset map pre-processing"):
-            _, train_dataset = preprocessor.process_train(train_dataset)
-
-    # process val data
-    if training_args.do_eval:
-        eval_examples = raw_datasets["validation"]
-        max_eval_samples = data_args.max_eval_samples
-        if max_eval_samples is not None:
-            # We will select sample from whole data if argument is specified
-            eval_examples = eval_examples.select(range(max_eval_samples))
-        # Validation Feature Creation
-        with training_args.main_process_first(desc="validation dataset map pre-processing"):
-            eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)
-
-    # If using mixed precision we pad for efficient hardware acceleration
-    using_mixed_precision = any(attrgetter('fp16', 'bf16')(training_args))
-    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=64 if using_mixed_precision else None)
-
-    postprocessor_class = task_args.postprocessor
-
-    # noinspection PyProtectedMember
-    postprocessor = postprocessor_class(
-        k=data_args.n_best_logits,
-        n_best_size=data_args.n_best_size,
-        max_answer_length=data_args.max_answer_length,
-        scorer_type=SupportedSpanScorers(scorer_type),
-        single_context_multiple_passages=preprocessor._single_context_multiple_passages,
-        confidence_model_path=model_args.confidence_model_path,
-        output_confidence_feature=True if task_args.task_heads == EXTRACTIVE_WITH_CONFIDENCE_HEAD else False,
-    )
-
-    eval_metrics = getattr(sys.modules[__name__], task_args.eval_metrics)()
-
-    def compute_metrics(p: EvalPredictionWithProcessing):
-        return eval_metrics.compute(predictions=p.processed_predictions, references=p.label_ids,
-            passage_non_null_threshold=task_args.passage_non_null_threshold, 
-            span_non_null_threshold=task_args.span_non_null_threshold,verbose=task_args.verbose,
-            dataset_config_name = eval_dataset.config_name)
-
-    trainer = MRCTrainer(
-        model=model,
-        args=training_args,
-        train_dataset=train_dataset if training_args.do_train else None,
-        eval_dataset=eval_dataset if training_args.do_eval else None,
-        eval_examples=eval_examples if training_args.do_eval else None,
-        tokenizer=tokenizer,
-        data_collator=data_collator,
-        post_process_function=postprocessor.process_references_and_predictions,  # see QATrainer in Huggingface
-        compute_metrics=compute_metrics,
-    )
-
-    checkpoint = None
-    if training_args.resume_from_checkpoint is not None:
-        checkpoint = training_args.resume_from_checkpoint
-    elif last_checkpoint is not None:
-        checkpoint = last_checkpoint
-
-    # training
-    if training_args.do_train:
-        train_result = trainer.train(resume_from_checkpoint=checkpoint)
-        trainer.save_model()  # Saves the tokenizer too for easy upload
-
-        metrics = train_result.metrics
-        max_train_samples = (
-            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
-        )
-        metrics["train_samples"] = min(max_train_samples, len(train_dataset))
-
-        trainer.log_metrics("train", metrics)
-        trainer.save_metrics("train", metrics)
-        trainer.save_state()
-
-    # validation
-    if training_args.do_eval:
-        logger.info("*** Evaluate ***")
-        metrics = trainer.evaluate()
-
-        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
-        metrics["eval_samples"] = min(max_eval_samples, len(eval_examples))
-
-        trainer.log_metrics("eval", metrics)
-        trainer.save_metrics("eval", metrics)
-
-    if task_args.do_boolean:
-        logger.info("Processing of boolean questions")
-        if not os.path.exists(os.path.join(training_args.output_dir,"eval_predictions.json")):
-            raise Exception(f"No MRC predictions were found at {training_args.output_dir}")
-        with open(task_args.boolean_config, 'r') as f:
-            boolean_config = json.load(f)
-
-        boolean_config['qtc']['output_dir'] = training_args.output_dir+"/qtc"
-        boolean_config['qtc']['test_file'] = training_args.output_dir + "/eval_predictions.json"
-        boolean_config['evc']['output_dir'] = training_args.output_dir+"/evc"
-        boolean_config['evc']['test_file'] = training_args.output_dir + "/qtc/eval_predictions.json"
-        boolean_config['sn']['output_dir'] = training_args.output_dir+"/sn"
-        boolean_config['sn']['test_file'] = training_args.output_dir + "/evc/eval_predictions.json"
-
-        if model: del model
-        gc.collect()
-        torch.cuda.empty_cache()
-        logger.info(f"torch memory allocated {torch.cuda.memory_allocated()} \
-            max memory {torch.cuda.max_memory_allocated()}")
-
-        cls_main([boolean_config['qtc']])
-        cls_main([boolean_config['evc']])
-        sn_main([boolean_config['sn']])
-
-        with open(os.path.join(boolean_config['sn']['output_dir'], 'eval_predictions_processed.json'), 'r') as f:
-            processed_predictions = json.load(f)
-            
-        references = postprocessor.prepare_examples_as_references(eval_examples)
-        boolean_eval_metric = eval_metrics.compute(predictions=processed_predictions, references=references)
-        boolean_eval_metric["eval_samples"] = min(max_eval_samples, len(eval_examples))
-        trainer.log_metrics("eval", boolean_eval_metric)
-        path = os.path.join(boolean_config['sn']['output_dir'], f"all_results.json")
-        with open(path, "w") as f:
-            json.dump(boolean_eval_metric, f, indent=4, sort_keys=True)        
-
-
-if __name__ == '__main__':
+import logging
+import os
+import sys
+import traceback
+import json
+import torch
+import gc
+import glob
+from dataclasses import dataclass, field
+from importlib import import_module
+from operator import attrgetter
+from typing import Optional, Type, List
+
+import datasets
+import apache_beam as beam
+from transformers import HfArgumentParser, TrainingArguments, DataCollatorWithPadding, AutoConfig, AutoTokenizer
+from transformers.trainer_utils import get_last_checkpoint, set_seed
+
+from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
+from primeqa.mrc.metrics.tydi_f1.tydi_f1 import TyDiF1
+from primeqa.mrc.metrics.mlqa.mlqa import MLQA
+from primeqa.mrc.metrics.squad.squad import SQUAD
+from primeqa.mrc.metrics.nq_f1.nq_f1 import NQF1
+from primeqa.mrc.models.heads.extractive import EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD
+from primeqa.mrc.models.task_model import ModelForDownstreamTasks
+from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
+from primeqa.boolqa.processors.postprocessors.extractive import ExtractivePipelinePostProcessor
+from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers
+from primeqa.mrc.processors.preprocessors.tydiqa import TyDiQAPreprocessor
+from primeqa.mrc.processors.preprocessors.squad import SQUADPreprocessor
+from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
+from primeqa.mrc.processors.postprocessors.squad import SQUADPostProcessor
+from primeqa.mrc.processors.preprocessors.natural_questions import NaturalQuestionsPreProcessor
+from primeqa.mrc.processors.postprocessors.natural_questions import NaturalQuestionsPostProcessor
+from primeqa.mrc.processors.preprocessors.tydiqa_google import TyDiQAGooglePreprocessor
+from primeqa.mrc.processors.preprocessors.mrqa import MRQAPreprocessor
+from primeqa.mrc.trainers.mrc import MRCTrainer
+from primeqa.boolqa.run_boolqa_classifier import main as cls_main
+from primeqa.boolqa.run_score_normalizer import main as sn_main
+
+from primeqa.tableqa.run_tableqa import run_table_qa
+
+def object_reference(reference_as_str: str) -> object:
+    """
+    Given a fully qualified path to a class reference, return a pointer to the reference.
+    This will work with types, functions, methods, and other objects (e.g. dict).
+
+    Args:
+        reference_as_str: the fully qualified path (expects the fully qualified path in dot notation,
+                          e.g. primeqa.mrc.processors.postprocessors.extractive.ExtractivePostProcessor).
+
+    Returns:
+        reference to path given by input
+
+    Raises:
+        TypeError: Unable to resolve input path
+    """
+    def _split_into_class_and_module_name(class_path):
+        modules = class_path.split('.')
+        if len(modules) > 1:
+            return ".".join(modules[:-1]), modules[-1]
+        else:
+            return class_path, None
+
+    try:
+        module_name, object_name = _split_into_class_and_module_name(reference_as_str)
+        module_reference = import_module(module_name)
+        if object_name is None:
+            return module_reference
+        else:
+            return getattr(module_reference, object_name)
+    except Exception as ex:
+        traceback.print_exc()  # Shows additional traceback for why imports fail
+        raise TypeError(f"Unable to resolve the string {reference_as_str} to a fully qualified class path") from ex
+
+
+# modified from
+# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
+@dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={"help": "Path to directory to store the pretrained models downloaded from huggingface.co"},
+    )
+    confidence_model_path: str = field(
+        default=None,
+        metadata={"help": "Path to the confidence calibration model"}
+    )
+
+
+# modified from
+# https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py
+@dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    tableqa_config_file: Optional[str] = field(
+        default=None, metadata={"help": "TableQA additional arguments"}
+    )
+    dataset_name: str = field(
+        default="tydiqa", metadata={"help": "The name of the dataset to use (via the datasets library)."}
+    )
+    train_file: Optional[str] = field(
+        default=None, metadata={"help": "local file(s) to train on."}
+    )
+    eval_file: Optional[str] = field(
+        default=None, metadata={"help": "local file(s) to test on."}
+    )
+    data_file_format: str = field(
+        default="json", metadata={"help": "the format of the local dataset files (json, jsonl, csv, text, pandas)"}
+    )
+    dataset_config_name: str = field(
+        default="primary_task", metadata={
+            "help": "The configuration name of the dataset to use (via the datasets library)."
+        }
+    )
+    dataset_filter_column_name: str = field(
+        default=None, metadata={
+            "help": "Dataset column name to filter on, e.g. 'subset'"
+        }
+    )
+    dataset_filter_column_values: List[str] = field(
+        default=None, metadata={
+            "help": "Dataset column values to match when filtering e.g. 'SQuAD HotpotQA'"
+        }
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
+    )
+    preprocessing_num_workers: Optional[int] = field(
+        default=None,
+        metadata={"help": "The number of processes to use for the preprocessing."},
+    )
+    max_seq_length: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "The maximum total input sequence length after tokenization. Sequences longer "
+                    "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    max_q_char_len: int = field(
+        default=128, metadata={"help": "Max length per question in characters"}
+    )
+    single_context_multiple_passages: bool = field(
+        default=False, metadata={
+            "help": "Allow multiple passages in the same input feature. "
+                    "For an example with question q and context c_{1..n} setting this to True"
+                    "will allow q|c_{i}c_{i+1}; whereas setting this to False enforces q|c_{i} q|c_{i+1}. "
+                    "Note that not all datasets/preprocessors support both values of this parameter. "
+                    "Some preprocessors may override this value."
+            },
+    )
+    max_contexts: Optional[int] = field(
+        default=None, metadata={"help": "Max contexts per consider"}
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+                    "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+                    "value if set."
+        },
+    )
+    doc_stride: int = field(
+        default=128,
+        metadata={"help": "When splitting up a long document into chunks, how much stride to take between chunks."},
+    )
+    n_best_size: int = field(
+        default=20,
+        metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
+    )
+    n_best_logits: int = field(
+        default=20,
+        metadata={"help": "The number of logits to consider when searching for start and end position of an answer"}
+    )
+    max_answer_length: int = field(
+        default=32,
+        metadata={
+            "help": "The maximum length of an answer that can be generated. This is needed because the start "
+                    "and end predictions are not conditioned on one another."
+        },
+    )
+    negative_sampling_prob_when_has_answer: float = field(
+        default=0.01,
+        metadata={
+            "help": "Only used when preparing training features, not for decoding. "
+                    "For an example with question q and context c_{1..n} where ∃ answer a ∈ c"
+                    "an input feature span q|c_{i} where a ∉ c_{i} will be kept with this probability."
+                    "Otherwise it will be discarded."
+        },
+    )
+    negative_sampling_prob_when_no_answer: float = field(
+        default=0.04,
+        metadata={
+            "help": "Only used when preparing training features, not for decoding. "
+                    "For an example with question q and context c_{1..n} where ∄ answer a ∈ c"
+                    "an input feature span q|c_{i} will be kept with this probability."
+                    "Otherwise it will be discarded."
+        },
+    )
+    beam_runner: str = field(
+        default=None,
+        metadata={"help": "The beam runner for loading large dataset.",
+                  "choices": ['DirectRunner'],
+                  }
+    )
+
+
+@dataclass
+class TaskArguments:
+    """
+    Task specific arguments.
+    """
+    modality: str = field(
+        default='text',
+        metadata={"help": "whether modality is table or text",
+        "choices": ["text", "table"]
+                  }
+    )
+
+    scorer_type: str = field(
+        default='weighted_sum_target_type_and_score_diff',
+        metadata={"help": "The name of the scorer to compute answer score.",
+                  "choices": SupportedSpanScorers.get_supported()
+                  }
+    )
+    task_heads: object_reference = field(
+        default=None,
+        metadata={"help": "The name of the task head to use.",
+                  "choices": [EXTRACTIVE_HEAD, EXTRACTIVE_WITH_CONFIDENCE_HEAD]
+                  }
+    )
+    preprocessor: object_reference = field(
+        default=TyDiQAPreprocessor,
+        metadata={"help": "The name of the preprocessor to use.",
+                  "choices": [MRQAPreprocessor, BasePreProcessor, TyDiQAPreprocessor,SQUADPreprocessor,TyDiQAGooglePreprocessor,NaturalQuestionsPreProcessor]
+                  }
+    )
+    postprocessor: object_reference = field(
+        default=ExtractivePostProcessor,
+        metadata={"help": "The name of the postprocessor to use.",
+                  "choices": [ExtractivePostProcessor,ExtractivePipelinePostProcessor,SQUADPostProcessor, NaturalQuestionsPostProcessor]
+                  }
+    )
+    eval_metrics: str = field(
+        default="TyDiF1",
+        metadata={"help": "The name of the evaluation metric function implemented in primeqa (e.g. TyDiF1).",
+                  "choices": ["TyDiF1","SQUAD","MLQA","NQF1"]
+                 }
+    )
+    do_boolean: bool = field(
+        default=False, metadata={"help": "Enable processing of boolean questions.  If activated,"
+                                        "--do_eval will be forced also, and --postprocessor will be "
+                                        "defaulted to ExtractivePipelinePostProcessor unless overridden"
+                                        "by a postprocessor that subclasses ExtractivePipelinePostProcessor"}
+    )
+    boolean_config: str = field(
+        default=None, metadata={"help": "The configuration name file for the boolean task in json format"}
+    )    
+    passage_non_null_threshold: int = field(
+        default=2,
+        metadata={"help": "The passage level non-null threshold (number of annotators to indicate no answer). This should be set to 1 if there is only one annotation"}
+    )
+    span_non_null_threshold: int = field(
+        default=2,
+        metadata={"help": "The span level non-null threshold (number of annotators to indicate no answer). This should be set to 1 if there is only one annotation"}
+    )
+    verbose: bool = field(
+        default=False,
+        metadata={"help": "Prints logging info if true (including evaluation output)"}
+    )
+    output_dropout_rate: float = field(
+        default=0.25,
+        metadata={"help": "The dropout probability applied to LM output in "
+                          "order to generate confidence calibration features."
+                  },
+    )
+    decoding_times_with_dropout: int = field(
+        default=5,
+        metadata={"help": "The number of decoding times to generate confidence "
+                          "calibration features with dropout."
+                  },
+    )
+
+    def __post_init__(self):
+        if not self.task_heads:
+            self.task_heads = EXTRACTIVE_HEAD  # cannot directly set mutable value as default
+
+
+def main():
+    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments, TaskArguments))
+    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
+        # If we pass only one argument to the script and it's the path to a json file,
+        # let's parse it to get our arguments.
+        model_args, data_args, training_args, task_args = \
+            parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
+    else:
+        model_args, data_args, training_args, task_args = parser.parse_args_into_dataclasses()
+
+    # if we are doing the boolean post-processing, require do_eval, because the id's (not included in HF
+    # dataset) might have changed
+    # we require ExtractivePipelinePostProcessor to populate certain fields for the boolqa classifiers,
+    # so force it here - this can't be done in a __post_init__ postprocess is in TaskArguments and
+    # do_eval is in TrainingArguments
+    if task_args.do_boolean:
+        training_args.do_eval = True
+        if not isinstance(task_args.postprocessor, ExtractivePipelinePostProcessor):
+            task_args.postprocessor = ExtractivePipelinePostProcessor
+
+
+    logger = logging.getLogger(__name__)
+    if task_args.verbose:
+        logging.basicConfig(level = logging.INFO)
+    scorer_type = task_args.scorer_type
+    set_seed(training_args.seed)
+
+    # Detecting last checkpoint.
+    last_checkpoint = None
+    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
+        last_checkpoint = get_last_checkpoint(training_args.output_dir)
+        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use --overwrite_output_dir to overcome."
+            )
+        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
+            logger.info(
+                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
+                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
+            )
+
+    # Run Table Question Answering        
+    if task_args.modality=="table":
+        run_table_qa(data_args,model_args,training_args)
+        sys.exit(0)
+
+    task_heads = task_args.task_heads
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
+        cache_dir=model_args.cache_dir,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
+        cache_dir=model_args.cache_dir,
+        use_fast=True,
+        config=config,
+    )
+
+    config.sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)
+    config.output_dropout_rate = task_args.output_dropout_rate
+    config.decoding_times_with_dropout = task_args.decoding_times_with_dropout
+    model = ModelForDownstreamTasks.from_config(
+        config,
+        model_args.model_name_or_path,
+        task_heads=task_heads,
+        cache_dir=model_args.cache_dir,
+    )
+    model.set_task_head(next(iter(task_heads)))
+
+    # load data
+    logger.info('Loading dataset')
+    if data_args.train_file is not None or data_args.eval_file is not None:
+        data_files = {}
+
+        if data_args.train_file is not None: 
+            data_files['train'] = glob.glob(data_args.train_file)
+        if data_args.eval_file is not None: 
+            data_files['validation'] = glob.glob(data_args.eval_file)
+
+        raw_datasets = datasets.load_dataset(data_args.data_file_format, 
+            data_files=data_files,
+            cache_dir=model_args.cache_dir)
+    else:
+        if data_args.dataset_name == "natural_questions":
+            raw_datasets = datasets.load_dataset(
+                data_args.dataset_name,
+                data_args.dataset_config_name,
+                cache_dir=model_args.cache_dir,
+                beam_runner=data_args.beam_runner,
+                revision="main"
+            )
+        else: 
+            raw_datasets = datasets.load_dataset(
+                data_args.dataset_name,
+                data_args.dataset_config_name,
+                cache_dir=model_args.cache_dir
+            )
+
+    # load preprocessor
+    preprocessor_class = task_args.preprocessor
+    preprocessor = preprocessor_class(
+        stride=data_args.doc_stride,
+        tokenizer=tokenizer,
+        negative_sampling_prob_when_has_answer=data_args.negative_sampling_prob_when_has_answer,
+        negative_sampling_prob_when_no_answer=data_args.negative_sampling_prob_when_no_answer,
+        load_from_cache_file=not data_args.overwrite_cache,
+        max_seq_len=data_args.max_seq_length,
+        num_workers=data_args.preprocessing_num_workers,
+        max_q_char_len=data_args.max_q_char_len,
+        single_context_multiple_passages=data_args.single_context_multiple_passages,
+        max_contexts=data_args.max_contexts,
+    )
+
+    # if filtering, check that both column name and column values are provided
+    if data_args.dataset_filter_column_values is not None:
+        if data_args.dataset_filter_column_name is None:
+            raise ValueError(f"Filtering on --dataset_filter_column_values ({data_args.dataset_filter_column_values}) "
+                      "requires --dataset_filter_column_name to be provided.")
+
+    # process train data
+    if training_args.do_train:
+        train_dataset = raw_datasets["train"]
+        if data_args.dataset_filter_column_values is not None:
+            logger.info(f"Filter TRAIN dataset {data_args.dataset_filter_column_name} {data_args.dataset_filter_column_values}")
+            train_dataset = train_dataset.filter(lambda example: example[data_args.dataset_filter_column_name] in (data_args.dataset_filter_column_values))
+            train_dataset = train_dataset.shuffle(seed=training_args.seed)
+            logger.info(f"Filtered TRAIN dataset size {train_dataset.num_rows}")
+        max_train_samples = data_args.max_train_samples
+        if max_train_samples is not None:
+            # We will select sample from whole data if argument is specified
+            train_dataset = train_dataset.select(range(max_train_samples))
+        # Train Feature Creation
+        with training_args.main_process_first(desc="train dataset map pre-processing"):
+            _, train_dataset = preprocessor.process_train(train_dataset)
+
+    # process val data
+    if training_args.do_eval:
+        eval_examples = raw_datasets["validation"]
+        if data_args.dataset_filter_column_values is not None:
+            logger.info(f"Filter EVAL dataset {data_args.dataset_filter_column_name} {data_args.dataset_filter_column_values}")
+            eval_examples = eval_examples.filter(lambda example: example[data_args.dataset_filter_column_name] in (data_args.dataset_filter_column_values))
+            logger.info(f"Filtered EVAL dataset size {eval_examples.num_rows}")
+        max_eval_samples = data_args.max_eval_samples
+        if max_eval_samples is not None:
+            # We will select sample from whole data if argument is specified
+            eval_examples = eval_examples.select(range(max_eval_samples))
+        # Validation Feature Creation
+        with training_args.main_process_first(desc="validation dataset map pre-processing"):
+            eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)
+
+    # If using mixed precision we pad for efficient hardware acceleration
+    using_mixed_precision = any(attrgetter('fp16', 'bf16')(training_args))
+    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=64 if using_mixed_precision else None)
+
+    postprocessor_class = task_args.postprocessor
+
+    # noinspection PyProtectedMember
+    postprocessor = postprocessor_class(
+        k=data_args.n_best_logits,
+        n_best_size=data_args.n_best_size,
+        max_answer_length=data_args.max_answer_length,
+        scorer_type=SupportedSpanScorers(scorer_type),
+        single_context_multiple_passages=preprocessor._single_context_multiple_passages,
+        confidence_model_path=model_args.confidence_model_path,
+        output_confidence_feature=True if task_args.task_heads == EXTRACTIVE_WITH_CONFIDENCE_HEAD else False,
+    )
+
+    eval_metrics = getattr(sys.modules[__name__], task_args.eval_metrics)()
+
+    def compute_metrics(p: EvalPredictionWithProcessing):
+        return eval_metrics.compute(predictions=p.processed_predictions, references=p.label_ids,
+            passage_non_null_threshold=task_args.passage_non_null_threshold, 
+            span_non_null_threshold=task_args.span_non_null_threshold,verbose=task_args.verbose,
+            dataset_config_name = eval_dataset.config_name)
+
+    trainer = MRCTrainer(
+        model=model,
+        args=training_args,
+        train_dataset=train_dataset if training_args.do_train else None,
+        eval_dataset=eval_dataset if training_args.do_eval else None,
+        eval_examples=eval_examples if training_args.do_eval else None,
+        tokenizer=tokenizer,
+        data_collator=data_collator,
+        post_process_function=postprocessor.process_references_and_predictions,  # see QATrainer in Huggingface
+        compute_metrics=compute_metrics,
+    )
+
+    checkpoint = None
+    if training_args.resume_from_checkpoint is not None:
+        checkpoint = training_args.resume_from_checkpoint
+    elif last_checkpoint is not None:
+        checkpoint = last_checkpoint
+
+    # training
+    if training_args.do_train:
+        train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        trainer.save_model()  # Saves the tokenizer too for easy upload
+
+        metrics = train_result.metrics
+        max_train_samples = (
+            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
+        )
+        metrics["train_samples"] = min(max_train_samples, len(train_dataset))
+
+        trainer.log_metrics("train", metrics)
+        trainer.save_metrics("train", metrics)
+        trainer.save_state()
+
+    # validation
+    if training_args.do_eval:
+        logger.info("*** Evaluate ***")
+        metrics = trainer.evaluate()
+
+        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
+        metrics["eval_samples"] = min(max_eval_samples, len(eval_examples))
+
+        trainer.log_metrics("eval", metrics)
+        trainer.save_metrics("eval", metrics)
+
+    if task_args.do_boolean:
+        logger.info("Processing of boolean questions")
+        if not os.path.exists(os.path.join(training_args.output_dir,"eval_predictions.json")):
+            raise Exception(f"No MRC predictions were found at {training_args.output_dir}")
+        with open(task_args.boolean_config, 'r') as f:
+            boolean_config = json.load(f)
+
+        boolean_config['qtc']['output_dir'] = training_args.output_dir+"/qtc"
+        boolean_config['qtc']['test_file'] = training_args.output_dir + "/eval_predictions.json"
+        boolean_config['evc']['output_dir'] = training_args.output_dir+"/evc"
+        boolean_config['evc']['test_file'] = training_args.output_dir + "/qtc/eval_predictions.json"
+        boolean_config['sn']['output_dir'] = training_args.output_dir+"/sn"
+        boolean_config['sn']['test_file'] = training_args.output_dir + "/evc/eval_predictions.json"
+
+        if model: del model
+        gc.collect()
+        torch.cuda.empty_cache()
+        logger.info(f"torch memory allocated {torch.cuda.memory_allocated()} \
+            max memory {torch.cuda.max_memory_allocated()}")
+
+        cls_main([boolean_config['qtc']])
+        cls_main([boolean_config['evc']])
+        sn_main([boolean_config['sn']])
+
+        with open(os.path.join(boolean_config['sn']['output_dir'], 'eval_predictions_processed.json'), 'r') as f:
+            processed_predictions = json.load(f)
+            
+        references = postprocessor.prepare_examples_as_references(eval_examples)
+        boolean_eval_metric = eval_metrics.compute(predictions=processed_predictions, references=references)
+        boolean_eval_metric["eval_samples"] = min(max_eval_samples, len(eval_examples))
+        trainer.log_metrics("eval", boolean_eval_metric)
+        path = os.path.join(boolean_config['sn']['output_dir'], f"all_results.json")
+        with open(path, "w") as f:
+            json.dump(boolean_eval_metric, f, indent=4, sort_keys=True)        
+
+
+if __name__ == '__main__':
     main()
```

## primeqa/mrc/data_models/eval_prediction_with_processing.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-from typing import NamedTuple, Tuple, Union
-
-import numpy as np
-
-
-class EvalPredictionWithProcessing(NamedTuple):
-    """
-    Evaluation output (always contains labels), to be used to compute metrics.
-
-    Args:
-        predictions: Predictions of the model.
-        label_ids: Targets to be matched.
-        processed_predictions: Predictions of the model processed for metric use.
-    """
-
-    predictions: Union[np.ndarray, Tuple[np.ndarray]]
-    label_ids: Union[np.ndarray, Tuple[np.ndarray]]
+from typing import NamedTuple, Tuple, Union
+
+import numpy as np
+
+
+class EvalPredictionWithProcessing(NamedTuple):
+    """
+    Evaluation output (always contains labels), to be used to compute metrics.
+
+    Args:
+        predictions: Predictions of the model.
+        label_ids: Targets to be matched.
+        processed_predictions: Predictions of the model processed for metric use.
+    """
+
+    predictions: Union[np.ndarray, Tuple[np.ndarray]]
+    label_ids: Union[np.ndarray, Tuple[np.ndarray]]
     processed_predictions: Union[np.ndarray, Tuple[np.ndarray]]
```

## primeqa/mrc/data_models/subsample_type.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-from enum import IntEnum, unique
-
-
-@unique
-class SubsampleType(IntEnum):
-    """
-    Enumeration type representing whether a feature is positive or negative (with/without an answer)
-    for use in subsampling.
-    """
-    POSITIVE = 0
-    NEGATIVE_HAS_ANSWER = 1
-    NEGATIVE_NO_ANSWER = 2
+from enum import IntEnum, unique
+
+
+@unique
+class SubsampleType(IntEnum):
+    """
+    Enumeration type representing whether a feature is positive or negative (with/without an answer)
+    for use in subsampling.
+    """
+    POSITIVE = 0
+    NEGATIVE_HAS_ANSWER = 1
+    NEGATIVE_NO_ANSWER = 2
```

## primeqa/mrc/data_models/target_type.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-from enum import IntEnum, unique
-
-
-@unique
-class TargetType(IntEnum):
-    """
-    Enumeration representing different types of answers to be used as a target in training.
-    """
-    NO_ANSWER = 0
-    SPAN_ANSWER = 1
-    PASSAGE_ANSWER = 2
-    YES = 3
-    NO = 4
-
-    @classmethod
-    def from_bool_label(cls, label: str) -> 'TargetType':
-        """
-        Alternate constructor from a boolean label string.
-
-        Args:
-            label: yes|no|none
-
-        Returns:
-            target type corresponding to label
-        """
-        label = label.upper()
-        if label == 'NONE':
-            label = 'NO_ANSWER'
-        return cls[label]
+from enum import IntEnum, unique
+
+
+@unique
+class TargetType(IntEnum):
+    """
+    Enumeration representing different types of answers to be used as a target in training.
+    """
+    NO_ANSWER = 0
+    SPAN_ANSWER = 1
+    PASSAGE_ANSWER = 2
+    YES = 3
+    NO = 4
+
+    @classmethod
+    def from_bool_label(cls, label: str) -> 'TargetType':
+        """
+        Alternate constructor from a boolean label string.
+
+        Args:
+            label: yes|no|none
+
+        Returns:
+            target type corresponding to label
+        """
+        label = label.upper()
+        if label == 'NONE':
+            label = 'NO_ANSWER'
+        return cls[label]
```

## primeqa/mrc/data_models/model_outputs/extractive.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-from dataclasses import dataclass
-from typing import Optional, Tuple
-
-import torch
-from transformers.file_utils import ModelOutput
-
-
-@dataclass
-class ExtractiveQAModelOutput(ModelOutput):
-    """
-    Extractive QA model outputs comprising:
-    (loss), start_logits, end_logits, target_type_logits, (hidden_states), (attentions).
-    """
-    loss: Optional[torch.FloatTensor] = None
-    start_logits: torch.FloatTensor = None
-    end_logits: torch.FloatTensor = None
-    target_type_logits: torch.FloatTensor = None
-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
-    attentions: Optional[Tuple[torch.FloatTensor]] = None
-
-@dataclass
-class ExtractiveQAWithConfidenceModelOutput(ModelOutput):
-    """
-    Extractive QA model outputs comprising:
-    (loss), start_logits, end_logits, target_type_logits,
-    start_stdev, end_stdev, query_passage_similarity,
-    (hidden_states), (attentions).
-    """
-    loss: Optional[torch.FloatTensor] = None
-    start_logits: torch.FloatTensor = None
-    end_logits: torch.FloatTensor = None
-    target_type_logits: torch.FloatTensor = None
-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
-    attentions: Optional[Tuple[torch.FloatTensor]] = None
-    start_stdev: Optional[torch.FloatTensor] = None
-    end_stdev: Optional[torch.FloatTensor] = None
-    query_passage_similarity: Optional[torch.FloatTensor] = None
+from dataclasses import dataclass
+from typing import Optional, Tuple
+
+import torch
+from transformers.file_utils import ModelOutput
+
+
+@dataclass
+class ExtractiveQAModelOutput(ModelOutput):
+    """
+    Extractive QA model outputs comprising:
+    (loss), start_logits, end_logits, target_type_logits, (hidden_states), (attentions).
+    """
+    loss: Optional[torch.FloatTensor] = None
+    start_logits: torch.FloatTensor = None
+    end_logits: torch.FloatTensor = None
+    target_type_logits: torch.FloatTensor = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
+
+@dataclass
+class ExtractiveQAWithConfidenceModelOutput(ModelOutput):
+    """
+    Extractive QA model outputs comprising:
+    (loss), start_logits, end_logits, target_type_logits,
+    start_stdev, end_stdev, query_passage_similarity,
+    (hidden_states), (attentions).
+    """
+    loss: Optional[torch.FloatTensor] = None
+    start_logits: torch.FloatTensor = None
+    end_logits: torch.FloatTensor = None
+    target_type_logits: torch.FloatTensor = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
+    start_stdev: Optional[torch.FloatTensor] = None
+    end_stdev: Optional[torch.FloatTensor] = None
+    query_passage_similarity: Optional[torch.FloatTensor] = None
```

## primeqa/mrc/metrics/nq_f1/eval_utils.py

 * *Ordering differences only*

```diff
@@ -1,414 +1,414 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import collections
-import functools
-import glob
-import json
-import logging
-import multiprocessing
-from gzip import GzipFile
-
-from functools import partial
-from typing import Dict, Set, Optional, List
-
-
-class InconsistentSpanError(ValueError):
-    pass
-
-
-@functools.total_ordering
-class NQSpan(object):
-    """A class for handling token and byte spans.
-       Taken from https://github.com/google-research-datasets/natural-questions/blob/master/eval_utils.py#L60
-      The logic is:
-      1) if both start_byte !=  -1 and end_byte != -1 then the span is defined
-         by byte offsets
-      2) else, if start_token != -1 and end_token != -1 then the span is define
-         by token offsets
-      3) else, this is a null span.
-      Null spans means that there is no (long or short) answers.
-      If your systems only care about token spans rather than byte spans, set all
-      byte spans to -1.
-    """
-    __slots__ = ['start_byte', 'end_byte', 'start_token', 'end_token', 'score', 'long_score']
-
-    def __init__(self, start_byte: int, end_byte: int, start_token: int, end_token: int,
-                 score: Optional[float] = None, long_score: Optional[float] = None, enforce_byte_consistency: bool = True,
-                 enforce_token_consistency: bool = True):
-
-        self.start_byte = start_byte
-        self.end_byte = end_byte
-        self.start_token = start_token
-        self.end_token = end_token
-        self.score = score
-        self.long_score = long_score
-
-        if enforce_byte_consistency:
-            if ((start_byte < 0 <= end_byte) or
-                    (start_byte >= 0 > end_byte)):
-                raise InconsistentSpanError('Inconsistent Null Spans (Byte): %s' % self)
-
-            if start_byte >= 0 and 0 <= end_byte < start_byte:
-                raise InconsistentSpanError('Invalid byte spans (start_byte > end_byte): %s' % self)
-
-        if enforce_token_consistency:
-            if ((start_token < 0 <= end_token) or
-                    (start_token >= 0 > end_token)):
-                raise InconsistentSpanError('Inconsistent Null Spans (Token): %s' % self)
-
-            if ((start_token >= 0 and end_token >= 0) and
-                    (start_token > end_token)):
-                raise InconsistentSpanError(
-                    'Invalid token spans (start_token_idx > end_token_idx): %s' % self)
-
-    def is_null_span(self):
-        """A span is a null span if the start and end are both -1."""
-
-        if (self.start_byte < 0 and self.end_byte < 0 and
-                    self.start_token < 0 and self.end_token < 0):
-            return True
-        return False
-
-    @staticmethod
-    def null_span():
-        return NQSpan(start_byte=-1, end_byte=-1, start_token=-1, end_token=-1)
-
-    def contains(self, other_span):
-
-        if other_span.start_byte > -1 and self.start_byte > -1 and other_span.end_byte > -1 and self.end_byte > -1:
-            # Compare using bytes
-            if other_span.start_byte >= self.start_byte and other_span.end_byte <= self.end_byte:
-                return True
-            else:
-                return False
-        elif other_span.start_token > -1 and self.start_token > -1 and other_span.end_token > -1 and self.end_token > -1:
-            # Compare using tokens
-            if other_span.start_token >= self.start_token and other_span.end_token <= self.end_token:
-                return True
-            else:
-                return False
-        else:
-            return False
-
-    def __str__(self):
-        byte_str = 'bytes: [' + str(self.start_byte) + ',' + str(self.end_byte) + ')'
-        tok_str = ('tokens: [' + str(self.start_token) + ',' + str(
-            self.end_token) + ')')
-        if self.score is not None:
-            score_str = ', score: %s' % self.score
-        else:
-            score_str = ''
-
-        return "Span(" + byte_str + ', ' + tok_str + score_str + ")"
-
-    def __repr__(self):
-        return self.__str__()
-
-    def __hash__(self):
-        return hash((self.start_byte, self.end_byte, self.start_token, self.end_token, self.score))
-
-    def __lt__(self, other):
-        if self.score < other.score:
-            return True
-        return False
-
-    def __eq__(self, o: object) -> bool:
-
-        if not isinstance(o, NQSpan):
-            return False
-
-        for attribute in self.__slots__:
-            if not hasattr(o, attribute):
-                return False
-            elif getattr(o, attribute) != getattr(self, attribute):
-                return False
-
-        return True
-
-
-# A data structure for storing prediction and annotation.
-# When a example has multiple annotations, multiple NQLabel will be used.
-NQLabel = collections.namedtuple(
-    'NQLabel',
-    [
-        'example_id',  # the unique id for each NQ example.
-        'long_answer_span',  # A Span object for long answer.
-        'short_answer_span_list',  # A list of Spans for short answer.
-        #   Note that In NQ, the short answers
-        #   do not need to be in a single span.
-        'yes_no_answer',  # Indicate if the short answer is an yes/no answer
-        #   The possible values are "yes", "no", "none".
-        #   (case insensitive)
-        #   If the field is "yes", short_answer_span_list
-        #   should be empty or only contain null spans.
-        'long_score',  # The prediction score for the long answer prediction.
-        'short_score'  # The prediction score for the short answer prediction.
-    ])
-
-
-def is_null_span_list(span_list):
-    """Returns true iff all spans in span_list are null or span_list is empty."""
-    if not span_list or all([span.is_null_span() for span in span_list]):
-        return True
-    return False
-
-
-def nonnull_span_equal(span_a: NQSpan, span_b: NQSpan):
-    """Given two spans, return if they are equal.
-
-    Args:
-      span_a: a Span object.
-      span_b: a Span object.  Only compare non-null spans. First, if the bytes are
-        not negative, compare byte offsets, Otherwise, compare token offsets.
-
-    Returns:
-      True or False
-    """
-    assert isinstance(span_a, NQSpan)
-    assert isinstance(span_b, NQSpan)
-    assert not span_a.is_null_span()
-    assert not span_b.is_null_span()
-
-    # if byte offsets are not negative, compare byte offsets
-    if ((span_a.start_byte >= 0 and span_a.end_byte >= 0) and
-            (span_b.start_byte >= 0 and span_b.end_byte >= 0)):
-
-        if ((span_a.start_byte == span_b.start_byte) and
-                (span_a.end_byte == span_b.end_byte)):
-            return True
-
-    # if token offsets are not negative, compare token offsets
-    if ((span_a.start_token >= 0 and span_a.end_token >= 0) and
-            (span_b.start_token >= 0 and span_b.end_token >= 0)):
-
-        if ((span_a.start_token == span_b.start_token) and
-                (span_a.end_token == span_b.end_token)):
-            return True
-
-    return False
-
-
-def span_set_equal(gold_span_list, pred_span_list):
-    """Make the spans are completely equal besides null spans."""
-
-    gold_span_list = [span for span in gold_span_list if not span.is_null_span()]
-    pred_span_list = [span for span in pred_span_list if not span.is_null_span()]
-
-    for pspan in pred_span_list:
-        # not finding pspan equal to any spans in gold_span_list
-        if not any([nonnull_span_equal(pspan, gspan) for gspan in gold_span_list]):
-            return False
-
-    for gspan in gold_span_list:
-        # not finding gspan equal to any spans in pred_span_list
-        if not any([nonnull_span_equal(pspan, gspan) for pspan in pred_span_list]):
-            return False
-
-    return True
-
-
-def gold_has_short_answer(gold_label_list: list, short_non_null_threshold: int = 2) -> bool:
-    """
-    Gets vote from multi-annotators for judging if there is a short answer.
-    :param gold_label_list: list of gold labels
-    :param short_non_null_threshold: Require this many non-null short answer annotations
-      to count gold as containing a short answer. Defaults to 2 like the original paper.
-    """
-
-    if not gold_label_list:
-        return False  # Empty list will not have gold short answer
-
-    # We consider if there is a short answer if there is an short answer span or
-    #  the yes/no answer is not none.
-    gold_has_answer = sum([
-        ((not is_null_span_list(label.short_answer_span_list)) or
-         (label.yes_no_answer != 'none')) for label in gold_label_list
-    ]) >= short_non_null_threshold
-
-    return gold_has_answer
-
-
-def gold_has_long_answer(gold_label_list: list, long_non_null_threshold: int = 2) -> bool:
-    """
-    Gets vote from multi-annotators for judging if there is a long answer.
-    :param gold_label_list: list of gold labels for judging
-    :param long_non_null_threshold: Require this many non-null long answer annotations
-      to count gold as containing a long answer. Defaults to 2 like the original paper.
-    """
-
-    if not gold_label_list:
-        return False  # Empty list will not have gold long answer
-
-    gold_has_answer = (sum([
-        not label.long_answer_span.is_null_span()  # long answer not null
-        for label in gold_label_list  # for each annotator
-    ]) >= long_non_null_threshold)
-
-    return gold_has_answer
-
-
-def read_prediction_json_from_file(predictions_path, examples_to_filter_for=None):
-    """Read the prediction json with scores.
-
-    Args:
-      predictions_path: the path for the prediction json.
-      examples_to_filter_for: a set of examples to filter for (can be None, in which
-       case all examples are kept)
-
-    Returns:
-      A dictionary with key = example_id, value = NQInstancePrediction.
-
-    """
-    logging.info('Reading predictions from file: %s', format(predictions_path))
-    with open(predictions_path, 'r') as f:
-        predictions = json.loads(f.read())
-
-    return parse_json_as_predictions(predictions, examples_to_filter_for)
-
-
-def parse_json_as_topk_predictions(
-        predictions: dict, examples_to_filter_for: Optional[Set] = None) -> \
-        Dict[int, List[NQLabel]]:
-
-    nq_pred_dict = {}
-    for topk_predictions_for_single_example in predictions["top_k_best_predictions"]:
-        example_id = None
-        topk_predictions = list()
-        for single_prediction in topk_predictions_for_single_example:
-            prediction = _parse_single_prediction_json(single_prediction)
-            if example_id is None:
-                example_id = prediction.example_id
-                if examples_to_filter_for is not None and example_id not in examples_to_filter_for:
-                    logging.debug('Skipping example %s' % example_id)
-                    break
-                else:
-                    example_id = single_prediction['example_id']
-
-            topk_predictions.append(prediction)
-
-        if topk_predictions:
-            nq_pred_dict[example_id] = topk_predictions
-
-    return nq_pred_dict
-
-
-def _parse_single_prediction_json(single_prediction: dict) -> NQLabel:
-    if 'long_answer' in single_prediction:
-        long_span = NQSpan(single_prediction['long_answer']['start_byte'],
-                           single_prediction['long_answer']['end_byte'],
-                           single_prediction['long_answer']['start_token'],
-                           single_prediction['long_answer']['end_token'])
-    else:
-        long_span = NQSpan.null_span()  # Span is null if not presented.
-
-    short_span_list = []
-    if 'short_answers' in single_prediction:
-        for short_item in single_prediction['short_answers']:
-            short_span_list.append(
-                NQSpan(short_item['start_byte'], short_item['end_byte'],
-                       short_item['start_token'], short_item['end_token']))
-
-    yes_no_answer = 'none'
-    if 'yes_no_answer' in single_prediction:
-        yes_no_answer = single_prediction['yes_no_answer'].lower()
-        if yes_no_answer not in ['yes', 'no', 'none']:
-            raise ValueError('Invalid yes_no_answer value in prediction')
-
-        if yes_no_answer != 'none' and not is_null_span_list(short_span_list):
-            raise ValueError('yes/no prediction and short answers cannot coexist.')
-
-    return NQLabel(
-        example_id=single_prediction['example_id'],
-        long_answer_span=long_span,
-        short_answer_span_list=short_span_list,
-        yes_no_answer=yes_no_answer,
-        long_score=float(single_prediction['long_answer_score']),
-        short_score=float(single_prediction['short_answers_score']))
-
-
-def parse_json_as_predictions(predictions: dict, examples_to_filter_for: Optional[Set] = None) -> \
-        Dict[int, NQLabel]:
-    nq_pred_dict = {}
-
-    for single_prediction in predictions['predictions']:
-        if examples_to_filter_for is None or \
-                        single_prediction['example_id'] in examples_to_filter_for:
-            nq_pred_dict[single_prediction['example_id']] = _parse_single_prediction_json(
-                single_prediction)
-
-    return nq_pred_dict
-
-
-def read_annotation_from_one_split(gzipped_input_file, example_ids_to_filter_by=None):
-    """Read annotation from one split of file."""
-    if example_ids_to_filter_by and not isinstance(example_ids_to_filter_by, set):
-        example_ids_to_filter_by = set(example_ids_to_filter_by)
-
-    if isinstance(gzipped_input_file, str):
-        gzipped_input_file = open(gzipped_input_file, mode='rb')
-    logging.info('parsing %s ..... ', gzipped_input_file.name)
-    annotation_dict = {}
-    with GzipFile(fileobj=gzipped_input_file) as input_file:
-        for line in input_file:
-            json_example = json.loads(line)
-            example_id = json_example['example_id']
-
-            if not example_ids_to_filter_by or example_id in example_ids_to_filter_by:
-
-                # There are multiple annotations for one nq example.
-                annotation_list = []
-
-                for annotation in json_example['annotations']:
-                    long_span_rec = annotation['long_answer']
-                    long_span = NQSpan(long_span_rec['start_byte'], long_span_rec['end_byte'],
-                                       long_span_rec['start_token'],
-                                       long_span_rec['end_token'])
-
-                    short_span_list = []
-                    for short_span_rec in annotation['short_answers']:
-                        short_span = NQSpan(
-                            short_span_rec['start_byte'], short_span_rec['end_byte'],
-                            short_span_rec['start_token'], short_span_rec['end_token'])
-                        short_span_list.append(short_span)
-
-                    gold_label = NQLabel(
-                        example_id=example_id,
-                        long_answer_span=long_span,
-                        short_answer_span_list=short_span_list,
-                        long_score=0,
-                        short_score=0,
-                        yes_no_answer=annotation['yes_no_answer'].lower())
-
-                    annotation_list.append(gold_label)
-                annotation_dict[example_id] = annotation_list
-
-    return annotation_dict
-
-
-def read_annotation(path_name, n_threads=10, example_ids_to_filter_by=None,
-                    read_from_split_fn=read_annotation_from_one_split):
-    """Read annotations with real multiple processes."""
-    input_paths = glob.glob(path_name)
-    pool = multiprocessing.Pool(n_threads)
-    logging.debug("Reading annotation from: {}".format(path_name))
-
-    if example_ids_to_filter_by is not None:
-        example_ids_to_filter_by = list(example_ids_to_filter_by)
-
-    annotation_reader = partial(read_from_split_fn,
-                                example_ids_to_filter_by=example_ids_to_filter_by)
-    try:
-        dict_list = pool.map(annotation_reader, input_paths)
-    finally:
-        pool.close()
-        pool.join()
-
-    final_dict = {}
-    for single_dict in dict_list:
-        final_dict.update(single_dict)
-
-    logging.debug("Read annotation (from {}): {}".format(path_name, final_dict))
-
-    return final_dict
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import glob
+import json
+import logging
+import multiprocessing
+from gzip import GzipFile
+
+from functools import partial
+from typing import Dict, Set, Optional, List
+
+
+class InconsistentSpanError(ValueError):
+    pass
+
+
+@functools.total_ordering
+class NQSpan(object):
+    """A class for handling token and byte spans.
+       Taken from https://github.com/google-research-datasets/natural-questions/blob/master/eval_utils.py#L60
+      The logic is:
+      1) if both start_byte !=  -1 and end_byte != -1 then the span is defined
+         by byte offsets
+      2) else, if start_token != -1 and end_token != -1 then the span is define
+         by token offsets
+      3) else, this is a null span.
+      Null spans means that there is no (long or short) answers.
+      If your systems only care about token spans rather than byte spans, set all
+      byte spans to -1.
+    """
+    __slots__ = ['start_byte', 'end_byte', 'start_token', 'end_token', 'score', 'long_score']
+
+    def __init__(self, start_byte: int, end_byte: int, start_token: int, end_token: int,
+                 score: Optional[float] = None, long_score: Optional[float] = None, enforce_byte_consistency: bool = True,
+                 enforce_token_consistency: bool = True):
+
+        self.start_byte = start_byte
+        self.end_byte = end_byte
+        self.start_token = start_token
+        self.end_token = end_token
+        self.score = score
+        self.long_score = long_score
+
+        if enforce_byte_consistency:
+            if ((start_byte < 0 <= end_byte) or
+                    (start_byte >= 0 > end_byte)):
+                raise InconsistentSpanError('Inconsistent Null Spans (Byte): %s' % self)
+
+            if start_byte >= 0 and 0 <= end_byte < start_byte:
+                raise InconsistentSpanError('Invalid byte spans (start_byte > end_byte): %s' % self)
+
+        if enforce_token_consistency:
+            if ((start_token < 0 <= end_token) or
+                    (start_token >= 0 > end_token)):
+                raise InconsistentSpanError('Inconsistent Null Spans (Token): %s' % self)
+
+            if ((start_token >= 0 and end_token >= 0) and
+                    (start_token > end_token)):
+                raise InconsistentSpanError(
+                    'Invalid token spans (start_token_idx > end_token_idx): %s' % self)
+
+    def is_null_span(self):
+        """A span is a null span if the start and end are both -1."""
+
+        if (self.start_byte < 0 and self.end_byte < 0 and
+                    self.start_token < 0 and self.end_token < 0):
+            return True
+        return False
+
+    @staticmethod
+    def null_span():
+        return NQSpan(start_byte=-1, end_byte=-1, start_token=-1, end_token=-1)
+
+    def contains(self, other_span):
+
+        if other_span.start_byte > -1 and self.start_byte > -1 and other_span.end_byte > -1 and self.end_byte > -1:
+            # Compare using bytes
+            if other_span.start_byte >= self.start_byte and other_span.end_byte <= self.end_byte:
+                return True
+            else:
+                return False
+        elif other_span.start_token > -1 and self.start_token > -1 and other_span.end_token > -1 and self.end_token > -1:
+            # Compare using tokens
+            if other_span.start_token >= self.start_token and other_span.end_token <= self.end_token:
+                return True
+            else:
+                return False
+        else:
+            return False
+
+    def __str__(self):
+        byte_str = 'bytes: [' + str(self.start_byte) + ',' + str(self.end_byte) + ')'
+        tok_str = ('tokens: [' + str(self.start_token) + ',' + str(
+            self.end_token) + ')')
+        if self.score is not None:
+            score_str = ', score: %s' % self.score
+        else:
+            score_str = ''
+
+        return "Span(" + byte_str + ', ' + tok_str + score_str + ")"
+
+    def __repr__(self):
+        return self.__str__()
+
+    def __hash__(self):
+        return hash((self.start_byte, self.end_byte, self.start_token, self.end_token, self.score))
+
+    def __lt__(self, other):
+        if self.score < other.score:
+            return True
+        return False
+
+    def __eq__(self, o: object) -> bool:
+
+        if not isinstance(o, NQSpan):
+            return False
+
+        for attribute in self.__slots__:
+            if not hasattr(o, attribute):
+                return False
+            elif getattr(o, attribute) != getattr(self, attribute):
+                return False
+
+        return True
+
+
+# A data structure for storing prediction and annotation.
+# When a example has multiple annotations, multiple NQLabel will be used.
+NQLabel = collections.namedtuple(
+    'NQLabel',
+    [
+        'example_id',  # the unique id for each NQ example.
+        'long_answer_span',  # A Span object for long answer.
+        'short_answer_span_list',  # A list of Spans for short answer.
+        #   Note that In NQ, the short answers
+        #   do not need to be in a single span.
+        'yes_no_answer',  # Indicate if the short answer is an yes/no answer
+        #   The possible values are "yes", "no", "none".
+        #   (case insensitive)
+        #   If the field is "yes", short_answer_span_list
+        #   should be empty or only contain null spans.
+        'long_score',  # The prediction score for the long answer prediction.
+        'short_score'  # The prediction score for the short answer prediction.
+    ])
+
+
+def is_null_span_list(span_list):
+    """Returns true iff all spans in span_list are null or span_list is empty."""
+    if not span_list or all([span.is_null_span() for span in span_list]):
+        return True
+    return False
+
+
+def nonnull_span_equal(span_a: NQSpan, span_b: NQSpan):
+    """Given two spans, return if they are equal.
+
+    Args:
+      span_a: a Span object.
+      span_b: a Span object.  Only compare non-null spans. First, if the bytes are
+        not negative, compare byte offsets, Otherwise, compare token offsets.
+
+    Returns:
+      True or False
+    """
+    assert isinstance(span_a, NQSpan)
+    assert isinstance(span_b, NQSpan)
+    assert not span_a.is_null_span()
+    assert not span_b.is_null_span()
+
+    # if byte offsets are not negative, compare byte offsets
+    if ((span_a.start_byte >= 0 and span_a.end_byte >= 0) and
+            (span_b.start_byte >= 0 and span_b.end_byte >= 0)):
+
+        if ((span_a.start_byte == span_b.start_byte) and
+                (span_a.end_byte == span_b.end_byte)):
+            return True
+
+    # if token offsets are not negative, compare token offsets
+    if ((span_a.start_token >= 0 and span_a.end_token >= 0) and
+            (span_b.start_token >= 0 and span_b.end_token >= 0)):
+
+        if ((span_a.start_token == span_b.start_token) and
+                (span_a.end_token == span_b.end_token)):
+            return True
+
+    return False
+
+
+def span_set_equal(gold_span_list, pred_span_list):
+    """Make the spans are completely equal besides null spans."""
+
+    gold_span_list = [span for span in gold_span_list if not span.is_null_span()]
+    pred_span_list = [span for span in pred_span_list if not span.is_null_span()]
+
+    for pspan in pred_span_list:
+        # not finding pspan equal to any spans in gold_span_list
+        if not any([nonnull_span_equal(pspan, gspan) for gspan in gold_span_list]):
+            return False
+
+    for gspan in gold_span_list:
+        # not finding gspan equal to any spans in pred_span_list
+        if not any([nonnull_span_equal(pspan, gspan) for pspan in pred_span_list]):
+            return False
+
+    return True
+
+
+def gold_has_short_answer(gold_label_list: list, short_non_null_threshold: int = 2) -> bool:
+    """
+    Gets vote from multi-annotators for judging if there is a short answer.
+    :param gold_label_list: list of gold labels
+    :param short_non_null_threshold: Require this many non-null short answer annotations
+      to count gold as containing a short answer. Defaults to 2 like the original paper.
+    """
+
+    if not gold_label_list:
+        return False  # Empty list will not have gold short answer
+
+    # We consider if there is a short answer if there is an short answer span or
+    #  the yes/no answer is not none.
+    gold_has_answer = sum([
+        ((not is_null_span_list(label.short_answer_span_list)) or
+         (label.yes_no_answer != 'none')) for label in gold_label_list
+    ]) >= short_non_null_threshold
+
+    return gold_has_answer
+
+
+def gold_has_long_answer(gold_label_list: list, long_non_null_threshold: int = 2) -> bool:
+    """
+    Gets vote from multi-annotators for judging if there is a long answer.
+    :param gold_label_list: list of gold labels for judging
+    :param long_non_null_threshold: Require this many non-null long answer annotations
+      to count gold as containing a long answer. Defaults to 2 like the original paper.
+    """
+
+    if not gold_label_list:
+        return False  # Empty list will not have gold long answer
+
+    gold_has_answer = (sum([
+        not label.long_answer_span.is_null_span()  # long answer not null
+        for label in gold_label_list  # for each annotator
+    ]) >= long_non_null_threshold)
+
+    return gold_has_answer
+
+
+def read_prediction_json_from_file(predictions_path, examples_to_filter_for=None):
+    """Read the prediction json with scores.
+
+    Args:
+      predictions_path: the path for the prediction json.
+      examples_to_filter_for: a set of examples to filter for (can be None, in which
+       case all examples are kept)
+
+    Returns:
+      A dictionary with key = example_id, value = NQInstancePrediction.
+
+    """
+    logging.info('Reading predictions from file: %s', format(predictions_path))
+    with open(predictions_path, 'r') as f:
+        predictions = json.loads(f.read())
+
+    return parse_json_as_predictions(predictions, examples_to_filter_for)
+
+
+def parse_json_as_topk_predictions(
+        predictions: dict, examples_to_filter_for: Optional[Set] = None) -> \
+        Dict[int, List[NQLabel]]:
+
+    nq_pred_dict = {}
+    for topk_predictions_for_single_example in predictions["top_k_best_predictions"]:
+        example_id = None
+        topk_predictions = list()
+        for single_prediction in topk_predictions_for_single_example:
+            prediction = _parse_single_prediction_json(single_prediction)
+            if example_id is None:
+                example_id = prediction.example_id
+                if examples_to_filter_for is not None and example_id not in examples_to_filter_for:
+                    logging.debug('Skipping example %s' % example_id)
+                    break
+                else:
+                    example_id = single_prediction['example_id']
+
+            topk_predictions.append(prediction)
+
+        if topk_predictions:
+            nq_pred_dict[example_id] = topk_predictions
+
+    return nq_pred_dict
+
+
+def _parse_single_prediction_json(single_prediction: dict) -> NQLabel:
+    if 'long_answer' in single_prediction:
+        long_span = NQSpan(single_prediction['long_answer']['start_byte'],
+                           single_prediction['long_answer']['end_byte'],
+                           single_prediction['long_answer']['start_token'],
+                           single_prediction['long_answer']['end_token'])
+    else:
+        long_span = NQSpan.null_span()  # Span is null if not presented.
+
+    short_span_list = []
+    if 'short_answers' in single_prediction:
+        for short_item in single_prediction['short_answers']:
+            short_span_list.append(
+                NQSpan(short_item['start_byte'], short_item['end_byte'],
+                       short_item['start_token'], short_item['end_token']))
+
+    yes_no_answer = 'none'
+    if 'yes_no_answer' in single_prediction:
+        yes_no_answer = single_prediction['yes_no_answer'].lower()
+        if yes_no_answer not in ['yes', 'no', 'none']:
+            raise ValueError('Invalid yes_no_answer value in prediction')
+
+        if yes_no_answer != 'none' and not is_null_span_list(short_span_list):
+            raise ValueError('yes/no prediction and short answers cannot coexist.')
+
+    return NQLabel(
+        example_id=single_prediction['example_id'],
+        long_answer_span=long_span,
+        short_answer_span_list=short_span_list,
+        yes_no_answer=yes_no_answer,
+        long_score=float(single_prediction['long_answer_score']),
+        short_score=float(single_prediction['short_answers_score']))
+
+
+def parse_json_as_predictions(predictions: dict, examples_to_filter_for: Optional[Set] = None) -> \
+        Dict[int, NQLabel]:
+    nq_pred_dict = {}
+
+    for single_prediction in predictions['predictions']:
+        if examples_to_filter_for is None or \
+                        single_prediction['example_id'] in examples_to_filter_for:
+            nq_pred_dict[single_prediction['example_id']] = _parse_single_prediction_json(
+                single_prediction)
+
+    return nq_pred_dict
+
+
+def read_annotation_from_one_split(gzipped_input_file, example_ids_to_filter_by=None):
+    """Read annotation from one split of file."""
+    if example_ids_to_filter_by and not isinstance(example_ids_to_filter_by, set):
+        example_ids_to_filter_by = set(example_ids_to_filter_by)
+
+    if isinstance(gzipped_input_file, str):
+        gzipped_input_file = open(gzipped_input_file, mode='rb')
+    logging.info('parsing %s ..... ', gzipped_input_file.name)
+    annotation_dict = {}
+    with GzipFile(fileobj=gzipped_input_file) as input_file:
+        for line in input_file:
+            json_example = json.loads(line)
+            example_id = json_example['example_id']
+
+            if not example_ids_to_filter_by or example_id in example_ids_to_filter_by:
+
+                # There are multiple annotations for one nq example.
+                annotation_list = []
+
+                for annotation in json_example['annotations']:
+                    long_span_rec = annotation['long_answer']
+                    long_span = NQSpan(long_span_rec['start_byte'], long_span_rec['end_byte'],
+                                       long_span_rec['start_token'],
+                                       long_span_rec['end_token'])
+
+                    short_span_list = []
+                    for short_span_rec in annotation['short_answers']:
+                        short_span = NQSpan(
+                            short_span_rec['start_byte'], short_span_rec['end_byte'],
+                            short_span_rec['start_token'], short_span_rec['end_token'])
+                        short_span_list.append(short_span)
+
+                    gold_label = NQLabel(
+                        example_id=example_id,
+                        long_answer_span=long_span,
+                        short_answer_span_list=short_span_list,
+                        long_score=0,
+                        short_score=0,
+                        yes_no_answer=annotation['yes_no_answer'].lower())
+
+                    annotation_list.append(gold_label)
+                annotation_dict[example_id] = annotation_list
+
+    return annotation_dict
+
+
+def read_annotation(path_name, n_threads=10, example_ids_to_filter_by=None,
+                    read_from_split_fn=read_annotation_from_one_split):
+    """Read annotations with real multiple processes."""
+    input_paths = glob.glob(path_name)
+    pool = multiprocessing.Pool(n_threads)
+    logging.debug("Reading annotation from: {}".format(path_name))
+
+    if example_ids_to_filter_by is not None:
+        example_ids_to_filter_by = list(example_ids_to_filter_by)
+
+    annotation_reader = partial(read_from_split_fn,
+                                example_ids_to_filter_by=example_ids_to_filter_by)
+    try:
+        dict_list = pool.map(annotation_reader, input_paths)
+    finally:
+        pool.close()
+        pool.join()
+
+    final_dict = {}
+    for single_dict in dict_list:
+        final_dict.update(single_dict)
+
+    logging.debug("Read annotation (from {}): {}".format(path_name, final_dict))
+
+    return final_dict
```

## primeqa/mrc/metrics/nq_f1/nq_eval.py

 * *Ordering differences only*

```diff
@@ -1,432 +1,432 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import argparse
-import json
-import logging
-import pickle
-from collections import OrderedDict
-
-# from iota.mrc.mnlp.io.cmd_arg_parser import positive_integer_type
-from os import path
-from typing import List, Union, Dict, Tuple, Optional
-
-from primeqa.mrc.metrics.nq_f1 import eval_utils as util
-
-
-def safe_divide(x, y):
-    """Compute x / y, but return 0 if y is zero."""
-    if y == 0:
-        return 0
-    else:
-        return x / y
-
-
-def score_long_answer(gold_label_list, pred_label, long_non_null_threshold=None):
-    """Scores a long answer as correct or not.
-
-    1) First decide if there is a gold long answer with LONG_NO_NULL_THRESHOLD.
-    2) The prediction will get a match if:
-       a. There is a gold long answer.
-       b. The prediction span match exactly with *one* of the non-null gold
-          long answer span.
-
-    Args:
-      gold_label_list: A list of NQLabel, could be None.
-      pred_label: A single NQLabel, could be None.
-      long_non_null_threshold: Min number of non null spans in the annotations to consider the
-       question as having a non null answer
-
-    Returns:
-      gold_has_answer, pred_has_answer, is_correct, score
-    """
-    gold_has_answer_kwargs = dict(gold_label_list=gold_label_list)
-    if long_non_null_threshold is not None:  # if not set omit kwarg to use default in util.gold_has_long_answer
-        gold_has_answer_kwargs['long_non_null_threshold'] = long_non_null_threshold
-    gold_has_answer = util.gold_has_long_answer(**gold_has_answer_kwargs)
-
-    pred_has_answer = pred_label and (
-        not pred_label.long_answer_span.is_null_span())
-
-    is_correct = False
-    score = pred_label.long_score
-
-    # Both sides are non-null spans.
-    if gold_has_answer and pred_has_answer:
-        for gold_label in gold_label_list:
-            # while the voting results indicate there is an long answer, each
-            # annotator might still say there is no long answer.
-            if gold_label.long_answer_span.is_null_span():
-                continue
-
-            if util.nonnull_span_equal(gold_label.long_answer_span,
-                                       pred_label.long_answer_span):
-                is_correct = True
-                break
-
-    return gold_has_answer, pred_has_answer, is_correct, score
-
-
-def score_short_answer(gold_label_list, pred_label, short_non_null_threshold=None):
-    """Scores a short answer as correct or not.
-
-    1) First decide if there is a gold short answer with SHORT_NO_NULL_THRESHOLD.
-    2) The prediction will get a match if:
-       a. There is a gold short answer.
-       b. The prediction span *set* match exactly with *one* of the non-null gold
-          short answer span *set*.
-
-    Args:
-      gold_label_list: A list of NQLabel.
-      pred_label: A single NQLabel.
-      short_non_null_threshold: Min number of non null annotations required before considering
-        the question as having a non null answer.  Optional.
-
-    Returns:
-      gold_has_answer, pred_has_answer, is_correct, score
-    """
-
-    # There is a gold short answer if gold_label_list not empty and non null
-    # answers is over the threshold (sum over annotators).
-    gold_has_answer_kwargs = dict(gold_label_list=gold_label_list)
-    if short_non_null_threshold is not None:  # if not set omit kwarg to use default in util.gold_has_short_answer
-        gold_has_answer_kwargs['short_non_null_threshold'] = short_non_null_threshold
-    gold_has_answer = util.gold_has_short_answer(**gold_has_answer_kwargs)
-
-    # There is a pred long answer if pred_label is not empty and short answer
-    # set is not empty.
-    pred_has_answer = pred_label and (
-        (not util.is_null_span_list(pred_label.short_answer_span_list)) or
-        pred_label.yes_no_answer != 'none')
-
-    is_correct = False
-    score = pred_label.short_score
-
-    # Both sides have short answers, which contains yes/no questions.
-    if gold_has_answer and pred_has_answer:
-        if pred_label.yes_no_answer != 'none':  # System thinks its y/n questions.
-            for gold_label in gold_label_list:
-                if pred_label.yes_no_answer == gold_label.yes_no_answer:
-                    is_correct = True
-                    break
-        else:
-            for gold_label in gold_label_list:
-                if util.span_set_equal(gold_label.short_answer_span_list,
-                                       pred_label.short_answer_span_list):
-                    is_correct = True
-                    break
-
-    return gold_has_answer, pred_has_answer, is_correct, score
-
-
-def score_answers(gold_annotation_dict, pred_dict, skip_missing_example_ids: bool = False,
-                  long_non_null_threshold: int = 2, short_non_null_threshold: int = 2):
-    """Scores all answers for all documents.
-
-    Args:
-      gold_annotation_dict: a dict from example id to list of NQLabels.
-      pred_dict: a dict from example id to list of NQLabels.
-      skip_missing_example_ids: True to only use example ids from intersection of gold and preds
-      long_non_null_threshold: Min number of non null spans in the annotation before considering
-        the question to be requiring a non null answer
-      short_non_null_threshold: Min number of non null spans in the annotation before considering
-        the question to be one with a non null answer
-
-    Returns:
-      long_answer_stats: List of scores for long answers.
-      short_answer_stats: List of scores for short answers.
-    """
-    gold_id_set = set(gold_annotation_dict.keys())
-    pred_id_set = set(pred_dict.keys())
-    sym_diff = gold_id_set.symmetric_difference(pred_id_set)
-
-    if (not skip_missing_example_ids) and sym_diff:
-        raise ValueError('ERROR: the example ids in gold annotations and example '
-                         'ids in the prediction are not equal.')
-    elif skip_missing_example_ids and sym_diff:
-        logging.warning("Skipping {} example ids that are only in either gold or preds".format(len(sym_diff)))
-
-    long_answer_stats = []
-    short_answer_stats = []
-    id_set = gold_id_set if not skip_missing_example_ids else gold_id_set.intersection(pred_id_set)
-
-    for example_id in id_set:
-        gold = gold_annotation_dict[example_id]
-        pred = pred_dict[example_id]
-
-        long_answer_stats.append(score_long_answer(gold_label_list=gold, pred_label=pred,
-                                                   long_non_null_threshold=long_non_null_threshold))
-        short_answer_stats.append(score_short_answer(gold_label_list=gold, pred_label=pred,
-                                                     short_non_null_threshold=short_non_null_threshold))
-
-    # use the 'score' column, which is last
-    long_answer_stats.sort(key=lambda x: x[-1], reverse=True)
-    short_answer_stats.sort(key=lambda x: x[-1], reverse=True)
-
-    return long_answer_stats, short_answer_stats
-
-
-def compute_f1(answer_stats, prefix=''):
-    """Computes F1, precision, recall for a list of answer scores.
-
-    Args:
-      answer_stats: List of per-example scores.
-      prefix (''): Prefix to prepend to score dictionary.
-
-    Returns:
-      Dictionary mapping string names to scores.
-    """
-
-    has_gold, has_pred, is_correct, _ = zip(*answer_stats)
-    precision = safe_divide(sum(is_correct), sum(has_pred))
-    recall = safe_divide(sum(is_correct), sum(has_gold))
-    f1 = safe_divide(2 * precision * recall, precision + recall)
-
-    return OrderedDict({
-        prefix + 'n': len(answer_stats),
-        prefix + 'f1': f1,
-        prefix + 'precision': precision,
-        prefix + 'recall': recall
-    })
-
-
-def extract_metrics_at_optimal_threshold(answer_stats: list) -> Tuple[float, float, float, float]:
-    """
-    :param answer_stats: one of the dictionaries returned from score_answers
-    :return: Tuple (f1, precision, recall, optimal_threshold)
-    """
-    opt_result, pr_table = compute_pr_curves(
-        answer_stats, targets=[0.5, 0.75, 0.9])
-    return opt_result # f1, precision, recall, optimal_threshold
-
-
-def compute_optimal_metrics(long_answer_stats: List[List[Union[bool, float]]],
-                            short_answer_stats: List[List[Union[bool, float]]]) -> Dict[str, float]:
-    """Computes overall metrics for long and short answers for their respective optimal thresholds
-    Arguments:
-       long_answer_stats: List of long answer scores.
-       short_answer_stats: List of short answer scores.
-    Returns:
-       Ordered Dictionary of name (string) -> score.
-    """
-    f1, precision, recall, optimal_threshold = extract_metrics_at_optimal_threshold(long_answer_stats)
-    prefix = 'long-answer-'
-    scores = OrderedDict({
-        prefix + 'n': len(long_answer_stats),
-        prefix + 'f1': f1,
-        prefix + 'precision': precision,
-        prefix + 'recall': recall,
-        prefix + 'optimal-threshold': optimal_threshold
-    })
-
-    f1, precision, recall, optimal_threshold = extract_metrics_at_optimal_threshold(short_answer_stats)
-    prefix = 'short-answer-'
-    scores.update(OrderedDict({
-        prefix + 'n': len(short_answer_stats),
-        prefix + 'f1': f1,
-        prefix + 'precision': precision,
-        prefix + 'recall': recall,
-        prefix + 'optimal-threshold': optimal_threshold
-    }))
-    return scores
-
-
-def compute_final_f1(long_answer_stats, short_answer_stats):
-    """Computes overall F1 given long and short answers, ignoring scores.
-
-    Note: this assumes that the answers have been thresholded.
-
-    Arguments:
-       long_answer_stats: List of long answer scores.
-       short_answer_stats: List of short answer scores.
-
-    Returns:
-       Dictionary of name (string) -> score.
-    """
-    scores = compute_f1(long_answer_stats, prefix='long-answer-')
-    scores.update(compute_f1(short_answer_stats, prefix='short-answer-'))
-    return scores
-
-
-def compute_pr_curves(answer_stats, targets: Optional[List]=None):
-    """Computes PR curve and returns R@P for specific targets.
-
-    The values are computed as follows: find the (precision, recall) point
-    with maximum recall and where precision > target.
-
-    Arguments:
-      answer_stats: List of statistic tuples from the answer scores.
-      targets (None): List of precision thresholds to target.
-
-    Returns:
-      List of table with rows: [target, r, p, score].
-    """
-    try:
-        total_correct = 0
-        total_has_pred = 0
-        total_has_gold = 0
-
-        # Count the number of gold annotations.
-        for has_gold, _, _, _ in answer_stats:
-            total_has_gold += has_gold
-
-        # Keep track of the point of maximum recall for each target.
-        max_recall = [0 for _ in targets]
-        max_precision = [0 for _ in targets]
-        max_scores = [None for _ in targets]
-
-        # Only keep track of unique thresholds in this dictionary.
-        scores_to_stats = OrderedDict()
-
-        # Loop through every possible threshold and compute precision + recall.
-        for has_gold, has_pred, is_correct, score in answer_stats:
-            total_correct += is_correct
-            total_has_pred += has_pred
-
-            precision = safe_divide(total_correct, total_has_pred)
-            recall = safe_divide(total_correct, total_has_gold)
-
-            # If there are any ties, this will be updated multiple times until the
-            # ties are all counted.
-            scores_to_stats[score] = [precision, recall]
-
-        best_f1 = 0.0
-        best_precision = 0.0
-        best_recall = 0.0
-        best_threshold = 0.0
-
-        for threshold, (precision, recall) in scores_to_stats.items():
-            # Match the thresholds to the find the closest precision above some target.
-            for t, target in enumerate(targets):
-                if precision >= target and recall > max_recall[t]:
-                    max_recall[t] = recall
-                    max_precision[t] = precision
-                    max_scores[t] = threshold
-
-            # Compute optimal threshold.
-            f1 = safe_divide(2 * precision * recall, precision + recall)
-            if f1 > best_f1:
-                best_f1 = f1
-                best_precision = precision
-                best_recall = recall
-                best_threshold = threshold
-
-        return ((best_f1, best_precision, best_recall, best_threshold),
-                zip(targets, max_recall, max_precision, max_scores))
-    except Exception as ex:
-        logging.error("Caught exception {} while computing p/r curve"
-                      " for answers: {} and targets: {}".format(ex, answer_stats, targets))
-        raise
-
-
-def print_r_at_p_table(answer_stats):
-    """Pretty prints the R@P table for default targets."""
-    opt_result, pr_table = compute_pr_curves(
-        answer_stats, targets=[0.5, 0.75, 0.9])
-    f1, precision, recall, optimal_threshold = opt_result
-    print('Optimal threshold: {:.5}'.format(optimal_threshold))
-    print(' F1     /  P      /  R')
-    print('{: >7.2%} / {: >7.2%} / {: >7.2%}'.format(f1, precision, recall))
-    for target, recall, precision, threshold in pr_table:
-        if threshold is not None:
-            print('R@P={}: {:.2%} (actual p={:.2%}, score threshold={:.4})'.format(
-                target, recall, precision, threshold))
-        else:
-            print('R@P={}: No possible threshold values satisfy this R@P'.format(target))
-
-
-def get_metrics_as_dict(gold_path, prediction_path, num_threads=10):
-    """Library version of the end-to-end evaluation.
-
-    Arguments:
-      gold_path: Path to the gzip JSON data. For multiple files, should be a glob
-        pattern (e.g. "/path/to/files-*")
-      prediction_path: Path to the JSON prediction data.
-      num_threads (10): Number of threads to use when parsing multiple files.
-
-    Returns:
-      metrics: A dictionary mapping string names to metric scores.
-    """
-
-    nq_gold_dict = util.read_annotation(gold_path, n_threads=num_threads)
-    nq_pred_dict = util.read_prediction_json_from_file(prediction_path)
-    logging.debug("Loaded gold {}: {}".format(gold_path, nq_gold_dict))
-    logging.debug("Loaded pred {}: {}".format(prediction_path, nq_pred_dict))
-
-    long_answer_stats, short_answer_stats = score_answers(nq_gold_dict, nq_pred_dict)
-
-    return get_metrics_with_answer_stats(long_answer_stats, short_answer_stats)
-
-
-def get_metrics_with_answer_stats(long_answer_stats, short_answer_stats):
-    """Generate metrics dict using long and short answer stats."""
-
-    def _get_metric_dict(answer_stats, prefix=''):
-        """Compute all metrics for a set of answer statistics."""
-        opt_result, pr_table = compute_pr_curves(
-            answer_stats, targets=[0.5, 0.75, 0.9])
-        f1, precision, recall, threshold = opt_result
-        metrics = OrderedDict({
-            'best-threshold-f1': f1,
-            'best-threshold-precision': precision,
-            'best-threshold-recall': recall,
-            'best-threshold': threshold,
-        })
-        for target, recall, precision, _ in pr_table:
-            metrics['recall-at-precision>={:.2}'.format(target)] = recall
-            metrics['precision-at-precision>={:.2}'.format(target)] = precision
-
-        # Add prefix before returning.
-        return dict([(prefix + k, v) for k, v in metrics.items()])
-
-    logging.debug("Computing p/r for long answer stats {}".format(long_answer_stats))
-    metrics = _get_metric_dict(long_answer_stats, 'long-')
-    logging.debug("Computing p/r for short answer stats {}".format(short_answer_stats))
-    metrics.update(_get_metric_dict(short_answer_stats, 'short-'))
-    return metrics
-
-
-def load_gt_lookup_as_dict(ground_truth_gzip_file_pattern, num_workers,
-                           read_from_split_fn=util.read_annotation_from_one_split):
-    previously_cached_gt_lookup = '{0}{1}_gt_lookup_cache.pickle'.format(
-        path.splitext(ground_truth_gzip_file_pattern)[0],
-        ('_' + read_from_split_fn.__name__) if read_from_split_fn != util.read_annotation_from_one_split else '')
-    if path.isfile(previously_cached_gt_lookup):
-        logging.info('Loading ground truth lookup from previously cached: %s' %
-                     previously_cached_gt_lookup)
-        with open(previously_cached_gt_lookup, 'rb') as infile:
-            nq_gold_dict = pickle.load(infile)
-    else:
-        logging.info('No previously cached lookup; so generating lookup from files matching'
-                     ' pattern: %s' % ground_truth_gzip_file_pattern)
-        nq_gold_dict = util.read_annotation(ground_truth_gzip_file_pattern, n_threads=num_workers,
-                                            read_from_split_fn=read_from_split_fn)
-        cache_to_pickle_file(nq_gold_dict, previously_cached_gt_lookup)
-
-    logging.info('Read in gt lookup for %d examples into memory' % len(nq_gold_dict))
-    return nq_gold_dict
-
-
-def pretty_print(long_answer_stats, short_answer_stats):
-    print('*' * 20)
-    print('LONG ANSWER R@P TABLE:')
-    logging.debug("Printing r@p table for long answer stats: {}".format(long_answer_stats))
-    print_r_at_p_table(long_answer_stats)
-    print('*' * 20)
-    print('SHORT ANSWER R@P TABLE:')
-    print_r_at_p_table(short_answer_stats)
-
-    scores = compute_final_f1(long_answer_stats, short_answer_stats)
-    print('*' * 20)
-    print('METRICS IGNORING SCORES (n={}):'.format(scores['long-answer-n']))
-    print('              F1     /  P      /  R')
-    print('Long answer  {: >7.2%} / {: >7.2%} / {: >7.2%}'.format(
-        scores['long-answer-f1'], scores['long-answer-precision'],
-        scores['long-answer-recall']))
-    print('Short answer {: >7.2%} / {: >7.2%} / {: >7.2%}'.format(
-        scores['short-answer-f1'], scores['short-answer-precision'],
-        scores['short-answer-recall']))
-
-    scores = {name.replace('-', '_'): value for name, value in scores.items()}
-    return scores
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import json
+import logging
+import pickle
+from collections import OrderedDict
+
+# from iota.mrc.mnlp.io.cmd_arg_parser import positive_integer_type
+from os import path
+from typing import List, Union, Dict, Tuple, Optional
+
+from primeqa.mrc.metrics.nq_f1 import eval_utils as util
+
+
+def safe_divide(x, y):
+    """Compute x / y, but return 0 if y is zero."""
+    if y == 0:
+        return 0
+    else:
+        return x / y
+
+
+def score_long_answer(gold_label_list, pred_label, long_non_null_threshold=None):
+    """Scores a long answer as correct or not.
+
+    1) First decide if there is a gold long answer with LONG_NO_NULL_THRESHOLD.
+    2) The prediction will get a match if:
+       a. There is a gold long answer.
+       b. The prediction span match exactly with *one* of the non-null gold
+          long answer span.
+
+    Args:
+      gold_label_list: A list of NQLabel, could be None.
+      pred_label: A single NQLabel, could be None.
+      long_non_null_threshold: Min number of non null spans in the annotations to consider the
+       question as having a non null answer
+
+    Returns:
+      gold_has_answer, pred_has_answer, is_correct, score
+    """
+    gold_has_answer_kwargs = dict(gold_label_list=gold_label_list)
+    if long_non_null_threshold is not None:  # if not set omit kwarg to use default in util.gold_has_long_answer
+        gold_has_answer_kwargs['long_non_null_threshold'] = long_non_null_threshold
+    gold_has_answer = util.gold_has_long_answer(**gold_has_answer_kwargs)
+
+    pred_has_answer = pred_label and (
+        not pred_label.long_answer_span.is_null_span())
+
+    is_correct = False
+    score = pred_label.long_score
+
+    # Both sides are non-null spans.
+    if gold_has_answer and pred_has_answer:
+        for gold_label in gold_label_list:
+            # while the voting results indicate there is an long answer, each
+            # annotator might still say there is no long answer.
+            if gold_label.long_answer_span.is_null_span():
+                continue
+
+            if util.nonnull_span_equal(gold_label.long_answer_span,
+                                       pred_label.long_answer_span):
+                is_correct = True
+                break
+
+    return gold_has_answer, pred_has_answer, is_correct, score
+
+
+def score_short_answer(gold_label_list, pred_label, short_non_null_threshold=None):
+    """Scores a short answer as correct or not.
+
+    1) First decide if there is a gold short answer with SHORT_NO_NULL_THRESHOLD.
+    2) The prediction will get a match if:
+       a. There is a gold short answer.
+       b. The prediction span *set* match exactly with *one* of the non-null gold
+          short answer span *set*.
+
+    Args:
+      gold_label_list: A list of NQLabel.
+      pred_label: A single NQLabel.
+      short_non_null_threshold: Min number of non null annotations required before considering
+        the question as having a non null answer.  Optional.
+
+    Returns:
+      gold_has_answer, pred_has_answer, is_correct, score
+    """
+
+    # There is a gold short answer if gold_label_list not empty and non null
+    # answers is over the threshold (sum over annotators).
+    gold_has_answer_kwargs = dict(gold_label_list=gold_label_list)
+    if short_non_null_threshold is not None:  # if not set omit kwarg to use default in util.gold_has_short_answer
+        gold_has_answer_kwargs['short_non_null_threshold'] = short_non_null_threshold
+    gold_has_answer = util.gold_has_short_answer(**gold_has_answer_kwargs)
+
+    # There is a pred long answer if pred_label is not empty and short answer
+    # set is not empty.
+    pred_has_answer = pred_label and (
+        (not util.is_null_span_list(pred_label.short_answer_span_list)) or
+        pred_label.yes_no_answer != 'none')
+
+    is_correct = False
+    score = pred_label.short_score
+
+    # Both sides have short answers, which contains yes/no questions.
+    if gold_has_answer and pred_has_answer:
+        if pred_label.yes_no_answer != 'none':  # System thinks its y/n questions.
+            for gold_label in gold_label_list:
+                if pred_label.yes_no_answer == gold_label.yes_no_answer:
+                    is_correct = True
+                    break
+        else:
+            for gold_label in gold_label_list:
+                if util.span_set_equal(gold_label.short_answer_span_list,
+                                       pred_label.short_answer_span_list):
+                    is_correct = True
+                    break
+
+    return gold_has_answer, pred_has_answer, is_correct, score
+
+
+def score_answers(gold_annotation_dict, pred_dict, skip_missing_example_ids: bool = False,
+                  long_non_null_threshold: int = 2, short_non_null_threshold: int = 2):
+    """Scores all answers for all documents.
+
+    Args:
+      gold_annotation_dict: a dict from example id to list of NQLabels.
+      pred_dict: a dict from example id to list of NQLabels.
+      skip_missing_example_ids: True to only use example ids from intersection of gold and preds
+      long_non_null_threshold: Min number of non null spans in the annotation before considering
+        the question to be requiring a non null answer
+      short_non_null_threshold: Min number of non null spans in the annotation before considering
+        the question to be one with a non null answer
+
+    Returns:
+      long_answer_stats: List of scores for long answers.
+      short_answer_stats: List of scores for short answers.
+    """
+    gold_id_set = set(gold_annotation_dict.keys())
+    pred_id_set = set(pred_dict.keys())
+    sym_diff = gold_id_set.symmetric_difference(pred_id_set)
+
+    if (not skip_missing_example_ids) and sym_diff:
+        raise ValueError('ERROR: the example ids in gold annotations and example '
+                         'ids in the prediction are not equal.')
+    elif skip_missing_example_ids and sym_diff:
+        logging.warning("Skipping {} example ids that are only in either gold or preds".format(len(sym_diff)))
+
+    long_answer_stats = []
+    short_answer_stats = []
+    id_set = gold_id_set if not skip_missing_example_ids else gold_id_set.intersection(pred_id_set)
+
+    for example_id in id_set:
+        gold = gold_annotation_dict[example_id]
+        pred = pred_dict[example_id]
+
+        long_answer_stats.append(score_long_answer(gold_label_list=gold, pred_label=pred,
+                                                   long_non_null_threshold=long_non_null_threshold))
+        short_answer_stats.append(score_short_answer(gold_label_list=gold, pred_label=pred,
+                                                     short_non_null_threshold=short_non_null_threshold))
+
+    # use the 'score' column, which is last
+    long_answer_stats.sort(key=lambda x: x[-1], reverse=True)
+    short_answer_stats.sort(key=lambda x: x[-1], reverse=True)
+
+    return long_answer_stats, short_answer_stats
+
+
+def compute_f1(answer_stats, prefix=''):
+    """Computes F1, precision, recall for a list of answer scores.
+
+    Args:
+      answer_stats: List of per-example scores.
+      prefix (''): Prefix to prepend to score dictionary.
+
+    Returns:
+      Dictionary mapping string names to scores.
+    """
+
+    has_gold, has_pred, is_correct, _ = zip(*answer_stats)
+    precision = safe_divide(sum(is_correct), sum(has_pred))
+    recall = safe_divide(sum(is_correct), sum(has_gold))
+    f1 = safe_divide(2 * precision * recall, precision + recall)
+
+    return OrderedDict({
+        prefix + 'n': len(answer_stats),
+        prefix + 'f1': f1,
+        prefix + 'precision': precision,
+        prefix + 'recall': recall
+    })
+
+
+def extract_metrics_at_optimal_threshold(answer_stats: list) -> Tuple[float, float, float, float]:
+    """
+    :param answer_stats: one of the dictionaries returned from score_answers
+    :return: Tuple (f1, precision, recall, optimal_threshold)
+    """
+    opt_result, pr_table = compute_pr_curves(
+        answer_stats, targets=[0.5, 0.75, 0.9])
+    return opt_result # f1, precision, recall, optimal_threshold
+
+
+def compute_optimal_metrics(long_answer_stats: List[List[Union[bool, float]]],
+                            short_answer_stats: List[List[Union[bool, float]]]) -> Dict[str, float]:
+    """Computes overall metrics for long and short answers for their respective optimal thresholds
+    Arguments:
+       long_answer_stats: List of long answer scores.
+       short_answer_stats: List of short answer scores.
+    Returns:
+       Ordered Dictionary of name (string) -> score.
+    """
+    f1, precision, recall, optimal_threshold = extract_metrics_at_optimal_threshold(long_answer_stats)
+    prefix = 'long-answer-'
+    scores = OrderedDict({
+        prefix + 'n': len(long_answer_stats),
+        prefix + 'f1': f1,
+        prefix + 'precision': precision,
+        prefix + 'recall': recall,
+        prefix + 'optimal-threshold': optimal_threshold
+    })
+
+    f1, precision, recall, optimal_threshold = extract_metrics_at_optimal_threshold(short_answer_stats)
+    prefix = 'short-answer-'
+    scores.update(OrderedDict({
+        prefix + 'n': len(short_answer_stats),
+        prefix + 'f1': f1,
+        prefix + 'precision': precision,
+        prefix + 'recall': recall,
+        prefix + 'optimal-threshold': optimal_threshold
+    }))
+    return scores
+
+
+def compute_final_f1(long_answer_stats, short_answer_stats):
+    """Computes overall F1 given long and short answers, ignoring scores.
+
+    Note: this assumes that the answers have been thresholded.
+
+    Arguments:
+       long_answer_stats: List of long answer scores.
+       short_answer_stats: List of short answer scores.
+
+    Returns:
+       Dictionary of name (string) -> score.
+    """
+    scores = compute_f1(long_answer_stats, prefix='long-answer-')
+    scores.update(compute_f1(short_answer_stats, prefix='short-answer-'))
+    return scores
+
+
+def compute_pr_curves(answer_stats, targets: Optional[List]=None):
+    """Computes PR curve and returns R@P for specific targets.
+
+    The values are computed as follows: find the (precision, recall) point
+    with maximum recall and where precision > target.
+
+    Arguments:
+      answer_stats: List of statistic tuples from the answer scores.
+      targets (None): List of precision thresholds to target.
+
+    Returns:
+      List of table with rows: [target, r, p, score].
+    """
+    try:
+        total_correct = 0
+        total_has_pred = 0
+        total_has_gold = 0
+
+        # Count the number of gold annotations.
+        for has_gold, _, _, _ in answer_stats:
+            total_has_gold += has_gold
+
+        # Keep track of the point of maximum recall for each target.
+        max_recall = [0 for _ in targets]
+        max_precision = [0 for _ in targets]
+        max_scores = [None for _ in targets]
+
+        # Only keep track of unique thresholds in this dictionary.
+        scores_to_stats = OrderedDict()
+
+        # Loop through every possible threshold and compute precision + recall.
+        for has_gold, has_pred, is_correct, score in answer_stats:
+            total_correct += is_correct
+            total_has_pred += has_pred
+
+            precision = safe_divide(total_correct, total_has_pred)
+            recall = safe_divide(total_correct, total_has_gold)
+
+            # If there are any ties, this will be updated multiple times until the
+            # ties are all counted.
+            scores_to_stats[score] = [precision, recall]
+
+        best_f1 = 0.0
+        best_precision = 0.0
+        best_recall = 0.0
+        best_threshold = 0.0
+
+        for threshold, (precision, recall) in scores_to_stats.items():
+            # Match the thresholds to the find the closest precision above some target.
+            for t, target in enumerate(targets):
+                if precision >= target and recall > max_recall[t]:
+                    max_recall[t] = recall
+                    max_precision[t] = precision
+                    max_scores[t] = threshold
+
+            # Compute optimal threshold.
+            f1 = safe_divide(2 * precision * recall, precision + recall)
+            if f1 > best_f1:
+                best_f1 = f1
+                best_precision = precision
+                best_recall = recall
+                best_threshold = threshold
+
+        return ((best_f1, best_precision, best_recall, best_threshold),
+                zip(targets, max_recall, max_precision, max_scores))
+    except Exception as ex:
+        logging.error("Caught exception {} while computing p/r curve"
+                      " for answers: {} and targets: {}".format(ex, answer_stats, targets))
+        raise
+
+
+def print_r_at_p_table(answer_stats):
+    """Pretty prints the R@P table for default targets."""
+    opt_result, pr_table = compute_pr_curves(
+        answer_stats, targets=[0.5, 0.75, 0.9])
+    f1, precision, recall, optimal_threshold = opt_result
+    print('Optimal threshold: {:.5}'.format(optimal_threshold))
+    print(' F1     /  P      /  R')
+    print('{: >7.2%} / {: >7.2%} / {: >7.2%}'.format(f1, precision, recall))
+    for target, recall, precision, threshold in pr_table:
+        if threshold is not None:
+            print('R@P={}: {:.2%} (actual p={:.2%}, score threshold={:.4})'.format(
+                target, recall, precision, threshold))
+        else:
+            print('R@P={}: No possible threshold values satisfy this R@P'.format(target))
+
+
+def get_metrics_as_dict(gold_path, prediction_path, num_threads=10):
+    """Library version of the end-to-end evaluation.
+
+    Arguments:
+      gold_path: Path to the gzip JSON data. For multiple files, should be a glob
+        pattern (e.g. "/path/to/files-*")
+      prediction_path: Path to the JSON prediction data.
+      num_threads (10): Number of threads to use when parsing multiple files.
+
+    Returns:
+      metrics: A dictionary mapping string names to metric scores.
+    """
+
+    nq_gold_dict = util.read_annotation(gold_path, n_threads=num_threads)
+    nq_pred_dict = util.read_prediction_json_from_file(prediction_path)
+    logging.debug("Loaded gold {}: {}".format(gold_path, nq_gold_dict))
+    logging.debug("Loaded pred {}: {}".format(prediction_path, nq_pred_dict))
+
+    long_answer_stats, short_answer_stats = score_answers(nq_gold_dict, nq_pred_dict)
+
+    return get_metrics_with_answer_stats(long_answer_stats, short_answer_stats)
+
+
+def get_metrics_with_answer_stats(long_answer_stats, short_answer_stats):
+    """Generate metrics dict using long and short answer stats."""
+
+    def _get_metric_dict(answer_stats, prefix=''):
+        """Compute all metrics for a set of answer statistics."""
+        opt_result, pr_table = compute_pr_curves(
+            answer_stats, targets=[0.5, 0.75, 0.9])
+        f1, precision, recall, threshold = opt_result
+        metrics = OrderedDict({
+            'best-threshold-f1': f1,
+            'best-threshold-precision': precision,
+            'best-threshold-recall': recall,
+            'best-threshold': threshold,
+        })
+        for target, recall, precision, _ in pr_table:
+            metrics['recall-at-precision>={:.2}'.format(target)] = recall
+            metrics['precision-at-precision>={:.2}'.format(target)] = precision
+
+        # Add prefix before returning.
+        return dict([(prefix + k, v) for k, v in metrics.items()])
+
+    logging.debug("Computing p/r for long answer stats {}".format(long_answer_stats))
+    metrics = _get_metric_dict(long_answer_stats, 'long-')
+    logging.debug("Computing p/r for short answer stats {}".format(short_answer_stats))
+    metrics.update(_get_metric_dict(short_answer_stats, 'short-'))
+    return metrics
+
+
+def load_gt_lookup_as_dict(ground_truth_gzip_file_pattern, num_workers,
+                           read_from_split_fn=util.read_annotation_from_one_split):
+    previously_cached_gt_lookup = '{0}{1}_gt_lookup_cache.pickle'.format(
+        path.splitext(ground_truth_gzip_file_pattern)[0],
+        ('_' + read_from_split_fn.__name__) if read_from_split_fn != util.read_annotation_from_one_split else '')
+    if path.isfile(previously_cached_gt_lookup):
+        logging.info('Loading ground truth lookup from previously cached: %s' %
+                     previously_cached_gt_lookup)
+        with open(previously_cached_gt_lookup, 'rb') as infile:
+            nq_gold_dict = pickle.load(infile)
+    else:
+        logging.info('No previously cached lookup; so generating lookup from files matching'
+                     ' pattern: %s' % ground_truth_gzip_file_pattern)
+        nq_gold_dict = util.read_annotation(ground_truth_gzip_file_pattern, n_threads=num_workers,
+                                            read_from_split_fn=read_from_split_fn)
+        cache_to_pickle_file(nq_gold_dict, previously_cached_gt_lookup)
+
+    logging.info('Read in gt lookup for %d examples into memory' % len(nq_gold_dict))
+    return nq_gold_dict
+
+
+def pretty_print(long_answer_stats, short_answer_stats):
+    print('*' * 20)
+    print('LONG ANSWER R@P TABLE:')
+    logging.debug("Printing r@p table for long answer stats: {}".format(long_answer_stats))
+    print_r_at_p_table(long_answer_stats)
+    print('*' * 20)
+    print('SHORT ANSWER R@P TABLE:')
+    print_r_at_p_table(short_answer_stats)
+
+    scores = compute_final_f1(long_answer_stats, short_answer_stats)
+    print('*' * 20)
+    print('METRICS IGNORING SCORES (n={}):'.format(scores['long-answer-n']))
+    print('              F1     /  P      /  R')
+    print('Long answer  {: >7.2%} / {: >7.2%} / {: >7.2%}'.format(
+        scores['long-answer-f1'], scores['long-answer-precision'],
+        scores['long-answer-recall']))
+    print('Short answer {: >7.2%} / {: >7.2%} / {: >7.2%}'.format(
+        scores['short-answer-f1'], scores['short-answer-precision'],
+        scores['short-answer-recall']))
+
+    scores = {name.replace('-', '_'): value for name, value in scores.items()}
+    return scores
```

## primeqa/mrc/metrics/nq_f1/nq_f1.py

 * *Ordering differences only*

```diff
@@ -1,166 +1,166 @@
-from typing import Dict, Any, Tuple, List
-
-import datasets
-
-from primeqa.mrc.metrics.nq_f1.eval_utils import NQLabel, NQSpan
-from primeqa.mrc.metrics.nq_f1.nq_eval import pretty_print, get_metrics_with_answer_stats, score_answers
-from primeqa.mrc.data_models.target_type import TargetType
-
-
-_DESCRIPTION = """
-The F1 score is the harmonic mean of the precision and recall. It can be computed with:
-F1 = 2 * (precision * recall) / (precision + recall).
-"""
-
-_KWARGS_DESCRIPTION = """
-Args:
-    predictions: Predicted labels.
-    references: Ground truth labels.
-
-Returns: metrics dict comprising:
-
-  * LONG ANSWER R@P TABLE.
-  * SHORT ANSWER R@P TABLE.
-"""
-
-_CITATION = """\
-@article{47761,
-title	= {Natural Questions: a Benchmark for Question Answering Research},
-author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
-year	= {2019},
-journal	= {Transactions of the Association of Computational Linguistics}
-}
-"""
-
-
-@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
-class NQF1(datasets.Metric):
-    _common_answer_schema = dict(
-        start_position=datasets.Value("int32"),
-        end_position=datasets.Value("int32"),
-        passage_index=datasets.Value("int32"),
-        yes_no_answer=datasets.Value("int32"),
-        example_id=datasets.Value("string"),
-    )
-    _pred_answer_schema = dict(
-        confidence_score=datasets.Value("float32"),
-    )
-    _ref_answer_schema = dict(
-        language=datasets.Value("string"),  # Kept for schema compatibility (unused in NQF1)
-    )
-
-    def _info(self):
-        return datasets.MetricInfo(
-            description=_DESCRIPTION,
-            citation=_CITATION,
-            inputs_description=_KWARGS_DESCRIPTION,
-            features=datasets.Features(
-                dict(
-                    predictions={**self._common_answer_schema, **self._pred_answer_schema},
-                    references=datasets.Sequence(feature={**self._common_answer_schema, **self._ref_answer_schema})
-                )),
-            reference_urls=["https://github.com/google-research-datasets/natural-questions/blob/master/nq_eval.py"],
-        )
-
-    def _compute(self, *, predictions=None, references=None, **kwargs) -> Dict[str, Any]:
-
-        if not predictions:
-            raise ValueError("No predictions provided")
-        elif not references:
-            raise ValueError("No references provided")
-
-        predictions = dict(map(self._convert_pred_to_entry, predictions))
-        references = dict(map(self._convert_ref_to_entry, references))
-
-        # TODO: parameterize
-        skip_missing_example_ids = False
-        long_non_null_threshold = 2
-        short_non_null_threshold = 2
-
-        long_answer_stats, short_answer_stats = score_answers(
-            gold_annotation_dict=references, pred_dict=predictions,
-            skip_missing_example_ids=skip_missing_example_ids,
-            long_non_null_threshold=long_non_null_threshold,
-            short_non_null_threshold=short_non_null_threshold)
-
-        metrics = pretty_print(long_answer_stats=long_answer_stats, short_answer_stats=short_answer_stats)
-        return metrics
-
-    def _convert_ref_to_entry(self, ref: dict) -> Tuple[str, List[NQLabel]]:
-        """
-        Converts a reference dict into an example_id, [labels] pair.
-        """
-        if not all(ref['example_id'][0] == ref['example_id'][i] for i in range(len(ref['example_id']))):
-            raise ValueError("Found mismatched examples")
-        elif not all(ref['language'][0] == ref['language'][i] for i in range(len(ref['language']))):
-            raise ValueError("Found mismatched languages")
-
-        key = ref['example_id'][0]
-        value = [
-            NQLabel(
-                example_id=ref['example_id'][i],
-                long_answer_span=self._passage_index_to_long_span(ref['passage_index'][i]),
-                short_answer_span_list=[NQSpan(
-                    start_byte=ref['start_position'][i],
-                    end_byte=ref['end_position'][i],
-                    start_token=-1,
-                    end_token=-1)]
-                ,
-                yes_no_answer=self._bool_target(
-                    TargetType(ref['yes_no_answer'][i])
-                ),
-                long_score=0,
-                short_score=0,
-            ) for i in range(len(ref['passage_index']))
-        ]
-        return key, value
-
-    def _convert_pred_to_entry(self, pred: dict) -> Tuple[str, NQLabel]:
-        """
-        Converts a prediction dict into an example_id, label pair.
-        """
-        key = pred['example_id']
-        value = NQLabel(
-            example_id=pred['example_id'],
-            long_answer_span=self._passage_index_to_long_span(pred['passage_index']),
-            short_answer_span_list=[NQSpan(
-                pred['start_position'],
-                pred['end_position'],
-                start_token=-1,
-                end_token=-1)]
-            ,
-            yes_no_answer=self._bool_target(
-                TargetType(pred['yes_no_answer'])
-            ),
-            long_score=pred['confidence_score'],
-            short_score=pred['confidence_score'],
-        )
-        return key, value
-
-    @staticmethod
-    def _bool_target(target_type: TargetType) -> str:
-        """
-        Converts a target type into a boolean string as expected by TyDi eval.
-        """
-        if target_type == TargetType.YES:
-            return 'yes'
-        elif target_type == TargetType.NO:
-            return 'no'
-        elif target_type == TargetType.NO_ANSWER:
-            return 'none'
-        else:
-            raise NotImplementedError(f"Unexpected target type for tydi bool string conversion: {target_type}")
-
-    @staticmethod
-    def _passage_index_to_long_span(passage_index: int) -> NQSpan:
-        if passage_index == -1:
-            return NQSpan.null_span()
-        else:
-            return NQSpan(
-                start_byte=passage_index,
-                end_byte=passage_index,
-                start_token=-1,
-                end_token=-1
-            )
-
-
+from typing import Dict, Any, Tuple, List
+
+import datasets
+
+from primeqa.mrc.metrics.nq_f1.eval_utils import NQLabel, NQSpan
+from primeqa.mrc.metrics.nq_f1.nq_eval import pretty_print, get_metrics_with_answer_stats, score_answers
+from primeqa.mrc.data_models.target_type import TargetType
+
+
+_DESCRIPTION = """
+The F1 score is the harmonic mean of the precision and recall. It can be computed with:
+F1 = 2 * (precision * recall) / (precision + recall).
+"""
+
+_KWARGS_DESCRIPTION = """
+Args:
+    predictions: Predicted labels.
+    references: Ground truth labels.
+
+Returns: metrics dict comprising:
+
+  * LONG ANSWER R@P TABLE.
+  * SHORT ANSWER R@P TABLE.
+"""
+
+_CITATION = """\
+@article{47761,
+title	= {Natural Questions: a Benchmark for Question Answering Research},
+author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
+year	= {2019},
+journal	= {Transactions of the Association of Computational Linguistics}
+}
+"""
+
+
+@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
+class NQF1(datasets.Metric):
+    _common_answer_schema = dict(
+        start_position=datasets.Value("int32"),
+        end_position=datasets.Value("int32"),
+        passage_index=datasets.Value("int32"),
+        yes_no_answer=datasets.Value("int32"),
+        example_id=datasets.Value("string"),
+    )
+    _pred_answer_schema = dict(
+        confidence_score=datasets.Value("float32"),
+    )
+    _ref_answer_schema = dict(
+        language=datasets.Value("string"),  # Kept for schema compatibility (unused in NQF1)
+    )
+
+    def _info(self):
+        return datasets.MetricInfo(
+            description=_DESCRIPTION,
+            citation=_CITATION,
+            inputs_description=_KWARGS_DESCRIPTION,
+            features=datasets.Features(
+                dict(
+                    predictions={**self._common_answer_schema, **self._pred_answer_schema},
+                    references=datasets.Sequence(feature={**self._common_answer_schema, **self._ref_answer_schema})
+                )),
+            reference_urls=["https://github.com/google-research-datasets/natural-questions/blob/master/nq_eval.py"],
+        )
+
+    def _compute(self, *, predictions=None, references=None, **kwargs) -> Dict[str, Any]:
+
+        if not predictions:
+            raise ValueError("No predictions provided")
+        elif not references:
+            raise ValueError("No references provided")
+
+        predictions = dict(map(self._convert_pred_to_entry, predictions))
+        references = dict(map(self._convert_ref_to_entry, references))
+
+        # TODO: parameterize
+        skip_missing_example_ids = False
+        long_non_null_threshold = 2
+        short_non_null_threshold = 2
+
+        long_answer_stats, short_answer_stats = score_answers(
+            gold_annotation_dict=references, pred_dict=predictions,
+            skip_missing_example_ids=skip_missing_example_ids,
+            long_non_null_threshold=long_non_null_threshold,
+            short_non_null_threshold=short_non_null_threshold)
+
+        metrics = pretty_print(long_answer_stats=long_answer_stats, short_answer_stats=short_answer_stats)
+        return metrics
+
+    def _convert_ref_to_entry(self, ref: dict) -> Tuple[str, List[NQLabel]]:
+        """
+        Converts a reference dict into an example_id, [labels] pair.
+        """
+        if not all(ref['example_id'][0] == ref['example_id'][i] for i in range(len(ref['example_id']))):
+            raise ValueError("Found mismatched examples")
+        elif not all(ref['language'][0] == ref['language'][i] for i in range(len(ref['language']))):
+            raise ValueError("Found mismatched languages")
+
+        key = ref['example_id'][0]
+        value = [
+            NQLabel(
+                example_id=ref['example_id'][i],
+                long_answer_span=self._passage_index_to_long_span(ref['passage_index'][i]),
+                short_answer_span_list=[NQSpan(
+                    start_byte=ref['start_position'][i],
+                    end_byte=ref['end_position'][i],
+                    start_token=-1,
+                    end_token=-1)]
+                ,
+                yes_no_answer=self._bool_target(
+                    TargetType(ref['yes_no_answer'][i])
+                ),
+                long_score=0,
+                short_score=0,
+            ) for i in range(len(ref['passage_index']))
+        ]
+        return key, value
+
+    def _convert_pred_to_entry(self, pred: dict) -> Tuple[str, NQLabel]:
+        """
+        Converts a prediction dict into an example_id, label pair.
+        """
+        key = pred['example_id']
+        value = NQLabel(
+            example_id=pred['example_id'],
+            long_answer_span=self._passage_index_to_long_span(pred['passage_index']),
+            short_answer_span_list=[NQSpan(
+                pred['start_position'],
+                pred['end_position'],
+                start_token=-1,
+                end_token=-1)]
+            ,
+            yes_no_answer=self._bool_target(
+                TargetType(pred['yes_no_answer'])
+            ),
+            long_score=pred['confidence_score'],
+            short_score=pred['confidence_score'],
+        )
+        return key, value
+
+    @staticmethod
+    def _bool_target(target_type: TargetType) -> str:
+        """
+        Converts a target type into a boolean string as expected by TyDi eval.
+        """
+        if target_type == TargetType.YES:
+            return 'yes'
+        elif target_type == TargetType.NO:
+            return 'no'
+        elif target_type == TargetType.NO_ANSWER:
+            return 'none'
+        else:
+            raise NotImplementedError(f"Unexpected target type for tydi bool string conversion: {target_type}")
+
+    @staticmethod
+    def _passage_index_to_long_span(passage_index: int) -> NQSpan:
+        if passage_index == -1:
+            return NQSpan.null_span()
+        else:
+            return NQSpan(
+                start_byte=passage_index,
+                end_byte=passage_index,
+                start_token=-1,
+                end_token=-1
+            )
+
+
```

## primeqa/mrc/metrics/squad/evaluate.py

 * *Ordering differences only*

```diff
@@ -1,92 +1,92 @@
-""" Official evaluation script for v1.1 of the SQuAD dataset. """
-
-import argparse
-import json
-import re
-import string
-import sys
-from collections import Counter
-
-
-def normalize_answer(s):
-    """Lower text and remove punctuation, articles and extra whitespace."""
-
-    def remove_articles(text):
-        return re.sub(r"\b(a|an|the)\b", " ", text)
-
-    def white_space_fix(text):
-        return " ".join(text.split())
-
-    def remove_punc(text):
-        exclude = set(string.punctuation)
-        return "".join(ch for ch in text if ch not in exclude)
-
-    def lower(text):
-        return text.lower()
-
-    return white_space_fix(remove_articles(remove_punc(lower(s))))
-
-
-def f1_score(prediction, ground_truth):
-    prediction_tokens = normalize_answer(prediction).split()
-    ground_truth_tokens = normalize_answer(ground_truth).split()
-    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
-    num_same = sum(common.values())
-    if num_same == 0:
-        return 0
-    precision = 1.0 * num_same / len(prediction_tokens)
-    recall = 1.0 * num_same / len(ground_truth_tokens)
-    f1 = (2 * precision * recall) / (precision + recall)
-    return f1
-
-
-def exact_match_score(prediction, ground_truth):
-    return normalize_answer(prediction) == normalize_answer(ground_truth)
-
-
-def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):
-    scores_for_ground_truths = []
-    for ground_truth in ground_truths:
-        score = metric_fn(prediction, ground_truth)
-        scores_for_ground_truths.append(score)
-    return max(scores_for_ground_truths)
-
-
-def evaluate(dataset, predictions):
-    f1 = exact_match = total = 0
-    for article in dataset:
-        for paragraph in article["paragraphs"]:
-            for qa in paragraph["qas"]:
-                total += 1
-                if qa["id"] not in predictions:
-                    message = "Unanswered question " + qa["id"] + " will receive score 0."
-                    print(message, file=sys.stderr)
-                    continue
-                ground_truths = list(map(lambda x: x["text"], qa["answers"]))
-                prediction = predictions[qa["id"]]
-                exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)
-                f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)
-
-    exact_match = 100.0 * exact_match / total
-    f1 = 100.0 * f1 / total
-
-    return {"exact_match": exact_match, "f1": f1}
-
-
-if __name__ == "__main__":
-    expected_version = "1.1"
-    parser = argparse.ArgumentParser(description="Evaluation for SQuAD " + expected_version)
-    parser.add_argument("dataset_file", help="Dataset file")
-    parser.add_argument("prediction_file", help="Prediction File")
-    args = parser.parse_args()
-    with open(args.dataset_file) as dataset_file:
-        dataset_json = json.load(dataset_file)
-        if dataset_json["version"] != expected_version:
-            print(
-                "Evaluation expects v-" + expected_version + ", but got dataset with v-" + dataset_json["version"],
-                file=sys.stderr,
-            )
-        dataset = dataset_json["data"]
-    with open(args.prediction_file) as prediction_file:
-        predictions = json.load(prediction_file)
-    print(json.dumps(evaluate(dataset, predictions)))
+""" Official evaluation script for v1.1 of the SQuAD dataset. """
+
+import argparse
+import json
+import re
+import string
+import sys
+from collections import Counter
+
+
+def normalize_answer(s):
+    """Lower text and remove punctuation, articles and extra whitespace."""
+
+    def remove_articles(text):
+        return re.sub(r"\b(a|an|the)\b", " ", text)
+
+    def white_space_fix(text):
+        return " ".join(text.split())
+
+    def remove_punc(text):
+        exclude = set(string.punctuation)
+        return "".join(ch for ch in text if ch not in exclude)
+
+    def lower(text):
+        return text.lower()
+
+    return white_space_fix(remove_articles(remove_punc(lower(s))))
+
+
+def f1_score(prediction, ground_truth):
+    prediction_tokens = normalize_answer(prediction).split()
+    ground_truth_tokens = normalize_answer(ground_truth).split()
+    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
+    num_same = sum(common.values())
+    if num_same == 0:
+        return 0
+    precision = 1.0 * num_same / len(prediction_tokens)
+    recall = 1.0 * num_same / len(ground_truth_tokens)
+    f1 = (2 * precision * recall) / (precision + recall)
+    return f1
+
+
+def exact_match_score(prediction, ground_truth):
+    return normalize_answer(prediction) == normalize_answer(ground_truth)
+
+
+def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):
+    scores_for_ground_truths = []
+    for ground_truth in ground_truths:
+        score = metric_fn(prediction, ground_truth)
+        scores_for_ground_truths.append(score)
+    return max(scores_for_ground_truths)
+
+
+def evaluate(dataset, predictions):
+    f1 = exact_match = total = 0
+    for article in dataset:
+        for paragraph in article["paragraphs"]:
+            for qa in paragraph["qas"]:
+                total += 1
+                if qa["id"] not in predictions:
+                    message = "Unanswered question " + qa["id"] + " will receive score 0."
+                    print(message, file=sys.stderr)
+                    continue
+                ground_truths = list(map(lambda x: x["text"], qa["answers"]))
+                prediction = predictions[qa["id"]]
+                exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)
+                f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)
+
+    exact_match = 100.0 * exact_match / total
+    f1 = 100.0 * f1 / total
+
+    return {"exact_match": exact_match, "f1": f1}
+
+
+if __name__ == "__main__":
+    expected_version = "1.1"
+    parser = argparse.ArgumentParser(description="Evaluation for SQuAD " + expected_version)
+    parser.add_argument("dataset_file", help="Dataset file")
+    parser.add_argument("prediction_file", help="Prediction File")
+    args = parser.parse_args()
+    with open(args.dataset_file) as dataset_file:
+        dataset_json = json.load(dataset_file)
+        if dataset_json["version"] != expected_version:
+            print(
+                "Evaluation expects v-" + expected_version + ", but got dataset with v-" + dataset_json["version"],
+                file=sys.stderr,
+            )
+        dataset = dataset_json["data"]
+    with open(args.prediction_file) as prediction_file:
+        predictions = json.load(prediction_file)
+    print(json.dumps(evaluate(dataset, predictions)))
```

## primeqa/mrc/metrics/squad/squad.py

 * *Ordering differences only*

```diff
@@ -1,115 +1,115 @@
-# Copyright 2020 The HuggingFace Datasets Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-""" SQuAD metric. """
-
-import datasets
-
-from .evaluate import evaluate
-
-
-_CITATION = """\
-@inproceedings{Rajpurkar2016SQuAD10,
-  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
-  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
-  booktitle={EMNLP},
-  year={2016}
-}
-"""
-
-_DESCRIPTION = """
-This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).
-
-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
-crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
-from the corresponding reading passage, or the question might be unanswerable.
-"""
-
-_KWARGS_DESCRIPTION = """
-Computes SQuAD scores (F1 and EM).
-Args:
-    predictions: List of question-answers dictionaries with the following key-values:
-        - 'id': id of the question-answer pair as given in the references (see below)
-        - 'prediction_text': the text of the answer
-    references: List of question-answers dictionaries with the following key-values:
-        - 'id': id of the question-answer pair (see above),
-        - 'answers': a Dict in the SQuAD dataset format
-            {
-                'text': list of possible texts for the answer, as a list of strings
-                'answer_start': list of start positions for the answer, as a list of ints
-            }
-            Note that answer_start values are not taken into account to compute the metric.
-Returns:
-    'exact_match': Exact match (the normalized answer exactly match the gold answer)
-    'f1': The F-score of predicted tokens versus the gold answer
-Examples:
-
-    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
-    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
-    >>> squad_metric = datasets.load_metric("squad")
-    >>> results = squad_metric.compute(predictions=predictions, references=references)
-    >>> print(results)
-    {'exact_match': 100.0, 'f1': 100.0}
-"""
-
-
-@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
-class SQUAD(datasets.Metric):
-    def _info(self):
-        return datasets.MetricInfo(
-            description=_DESCRIPTION,
-            citation=_CITATION,
-            inputs_description=_KWARGS_DESCRIPTION,
-            features=datasets.Features(
-                {
-                    "predictions": {"id": datasets.Value("string"), "prediction_text": datasets.Value("string")},
-                    "references": {
-                        "id": datasets.Value("string"),
-                        "answers": datasets.features.Sequence(
-                            {
-                                "text": datasets.Value("string"),
-                                "answer_start": datasets.Value("int32"),
-                            }
-                        ),
-                    },
-                }
-            ),
-            codebase_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
-            reference_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
-        )
-
-    def _compute(self, *, predictions, references, **kwargs):
-        
-        if not predictions:
-            raise ValueError("No predictions provided")
-        elif not references:
-            raise ValueError("No references provided")
-        
-        pred_dict = {prediction["id"]: prediction["prediction_text"] for prediction in predictions}
-        dataset = [
-            {
-                "paragraphs": [
-                    {
-                        "qas": [
-                            {
-                                "answers": [{"text": answer_text} for answer_text in ref["answers"]["text"]],
-                                "id": ref["id"],
-                            }
-                            for ref in references
-                        ]
-                    }
-                ]
-            }
-        ]
-        score = evaluate(dataset=dataset, predictions=pred_dict)
-        return score
+# Copyright 2020 The HuggingFace Datasets Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+""" SQuAD metric. """
+
+import datasets
+
+from .evaluate import evaluate
+
+
+_CITATION = """\
+@inproceedings{Rajpurkar2016SQuAD10,
+  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
+  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
+  booktitle={EMNLP},
+  year={2016}
+}
+"""
+
+_DESCRIPTION = """
+This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).
+
+Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
+crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
+from the corresponding reading passage, or the question might be unanswerable.
+"""
+
+_KWARGS_DESCRIPTION = """
+Computes SQuAD scores (F1 and EM).
+Args:
+    predictions: List of question-answers dictionaries with the following key-values:
+        - 'id': id of the question-answer pair as given in the references (see below)
+        - 'prediction_text': the text of the answer
+    references: List of question-answers dictionaries with the following key-values:
+        - 'id': id of the question-answer pair (see above),
+        - 'answers': a Dict in the SQuAD dataset format
+            {
+                'text': list of possible texts for the answer, as a list of strings
+                'answer_start': list of start positions for the answer, as a list of ints
+            }
+            Note that answer_start values are not taken into account to compute the metric.
+Returns:
+    'exact_match': Exact match (the normalized answer exactly match the gold answer)
+    'f1': The F-score of predicted tokens versus the gold answer
+Examples:
+
+    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
+    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
+    >>> squad_metric = datasets.load_metric("squad")
+    >>> results = squad_metric.compute(predictions=predictions, references=references)
+    >>> print(results)
+    {'exact_match': 100.0, 'f1': 100.0}
+"""
+
+
+@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
+class SQUAD(datasets.Metric):
+    def _info(self):
+        return datasets.MetricInfo(
+            description=_DESCRIPTION,
+            citation=_CITATION,
+            inputs_description=_KWARGS_DESCRIPTION,
+            features=datasets.Features(
+                {
+                    "predictions": {"id": datasets.Value("string"), "prediction_text": datasets.Value("string")},
+                    "references": {
+                        "id": datasets.Value("string"),
+                        "answers": datasets.features.Sequence(
+                            {
+                                "text": datasets.Value("string"),
+                                "answer_start": datasets.Value("int32"),
+                            }
+                        ),
+                    },
+                }
+            ),
+            codebase_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
+            reference_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
+        )
+
+    def _compute(self, *, predictions, references, **kwargs):
+        
+        if not predictions:
+            raise ValueError("No predictions provided")
+        elif not references:
+            raise ValueError("No references provided")
+        
+        pred_dict = {prediction["id"]: prediction["prediction_text"] for prediction in predictions}
+        dataset = [
+            {
+                "paragraphs": [
+                    {
+                        "qas": [
+                            {
+                                "answers": [{"text": answer_text} for answer_text in ref["answers"]["text"]],
+                                "id": ref["id"],
+                            }
+                            for ref in references
+                        ]
+                    }
+                ]
+            }
+        ]
+        score = evaluate(dataset=dataset, predictions=pred_dict)
+        return score
```

## primeqa/mrc/metrics/tydi_f1/eval_utils.py

 * *Ordering differences only*

```diff
@@ -1,213 +1,213 @@
-# coding=utf-8
-# Copyright 2020 The Google Research Team Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-"""Utility function for TyDi QA evaluation."""
-
-import collections
-
-
-# A data structure for storing prediction and annotation.
-# When a example has multiple annotations, multiple TyDiLabel will be used.
-TyDiLabel = collections.namedtuple(
-    'TyDiLabel',
-    [
-        'plaintext',  # context.
-        'question_text',  # a question text.
-        'example_id',  # the unique id for each TyDi example.
-        'language',  # language id.
-        'passage_answer_index',  # A index for passage answer among candidates.
-        'passage_span',  # offsets for the passage span
-        'minimal_answer_span',  # A Span object for minimal answer.
-        'yes_no_answer',  # Indicate if the minimal answer is an yes/no answer
-        #   The possible values are "yes", "no", "none".
-        #   (case insensitive)
-        #   If the field is "yes", minimal_answer_span should be empty or null.
-        'passage_score',  # The score for the passage answer prediction.
-        'minimal_score'  # The score for the minimal answer prediction.
-    ])
-
-
-class Span(object):
-    """A class for handling token and byte spans.
-
-      The logic is:
-
-      1) if both start_byte != -1 and end_byte != -1 then the span is defined
-         by byte offsets
-      3) else, this is a null span.
-
-      Null spans means that there is no (passage or minimal) answers.
-
-    """
-
-    def __init__(self, start_byte_offset, end_byte_offset):
-
-        if ((start_byte_offset < 0 and end_byte_offset >= 0) or
-                (start_byte_offset >= 0 and end_byte_offset < 0)):
-            raise ValueError('Inconsistent Null Spans (Byte).')
-
-        if (start_byte_offset >= 0 and end_byte_offset >= 0 and
-                start_byte_offset > end_byte_offset):
-            raise ValueError('Invalid byte spans (start_byte >= end_byte).')
-
-        self.start_byte_offset = start_byte_offset
-        self.end_byte_offset = end_byte_offset
-
-    def is_null_span(self):
-        """A span is a null span if the start and end are both -1.
-
-        This can happen for both gold and predicted values and
-        for both passage answers and minimal answers.
-
-        Returns:
-          boolean flag whether it is null span or not.
-        """
-
-        if (self.start_byte_offset < 0 and self.end_byte_offset < 0):
-            return True
-        return False
-
-    def __str__(self):
-        return '({},{})'.format(self.start_byte_offset, self.end_byte_offset)
-
-    def __repr__(self):
-        return self.__str__()
-
-
-def safe_divide(x, y):
-    """Compute x / y, but return 0 if y is zero."""
-    if y == 0:
-        return 0
-    else:
-        return x / y
-
-
-def safe_average(elements):
-    """Computes average `elements`, but returns 0 if `elements` is empty."""
-    return safe_divide(sum(elements), len(elements))
-
-
-def compute_partial_match_scores(gold_span, pred_span):
-    """Compute byte indices precision, recall and F1 score between span a and b.
-
-    This is used for scoring only minimal answers. See `nonnull_span_equal` for
-    scoring passage answers.
-
-    Args:
-      gold_span: a Span object. End_byte is inclusive (start_byte+byte_len)
-      pred_span: a Span object.  Only compare non-null spans.
-        Then, if the bytes are ot negative, compare byte offsets.
-
-    Returns:
-      precision: byte offset based precision.
-                (# bytes in both gold and pred span) / (# bytes in pred_span)
-      recall: byte offset based recall.
-                (# bytes in both gold and pred span) / (# bytes in gold_span)
-      f1: harmonic mean of precision and recall.
-    """
-    if not isinstance(gold_span, Span):
-        raise TypeError('Gold span must has a Span type.')
-    # if not isinstance(pred_span, NQSpan):
-    #     raise TypeError('Prediction span must has a Span type.')
-    if not isinstance(pred_span, Span):
-        raise TypeError('Prediction span must has a Span type.')
-    if gold_span.is_null_span():
-        raise ValueError(
-            'Null gold span should not be passed for F1 computation.')
-    if pred_span.is_null_span():
-        raise ValueError(
-            'Null prediction span should not be passed for F1 computation.')
-    assert not pred_span.is_null_span()
-    # If there is no overlap, partial score is zero.
-    if ((gold_span.end_byte_offset <= pred_span.start_byte_offset) or
-            (pred_span.end_byte_offset <= gold_span.start_byte_offset)):
-        precision = 0.0
-        recall = 0.0
-
-    else:
-        in_both = (min(gold_span.end_byte_offset, pred_span.end_byte_offset) -
-                   max(gold_span.start_byte_offset, pred_span.start_byte_offset))
-        assert in_both > 0
-        # if gold span starts earlier than pred span.
-        if gold_span.start_byte_offset <= pred_span.start_byte_offset:
-            only_in_gold = pred_span.start_byte_offset - gold_span.start_byte_offset
-            only_in_gold += max(0,
-                                gold_span.end_byte_offset - pred_span.end_byte_offset)
-            only_in_pred = max(pred_span.end_byte_offset - gold_span.end_byte_offset,
-                               0)
-        # if pred span starts earlier than gold span.
-        else:
-            only_in_pred = gold_span.start_byte_offset - pred_span.start_byte_offset
-            only_in_pred += max(0,
-                                pred_span.end_byte_offset - gold_span.end_byte_offset)
-            only_in_gold = max(gold_span.end_byte_offset - pred_span.end_byte_offset,
-                               0)
-        precision = safe_divide(in_both, (in_both + only_in_pred))
-        recall = safe_divide(in_both, (in_both + only_in_gold))
-
-    f1 = safe_divide(2 * precision * recall, precision + recall)
-    return precision, recall, f1
-
-
-def nonnull_span_equal(span_a, span_b):
-    """Given two spans, return if they are equal.
-
-    This is used for scoring only passage answers.
-    See `compute_partial_match_scores` for minimal answers.
-
-    Args:
-      span_a: a Span object.
-      span_b: a Span object.  Only compare non-null spans. First, if the bytes are
-        not negative, compare byte offsets.
-
-    Returns:
-      True or False
-    """
-    assert isinstance(span_a, Span)
-    assert isinstance(span_b, Span)
-    assert not span_a.is_null_span()
-    assert not span_b.is_null_span()
-
-    # if byte offsets are not negative, compare byte offsets
-    if ((span_a.start_byte_offset >= 0 and span_a.end_byte_offset >= 0) and
-            (span_b.start_byte_offset >= 0 and span_b.end_byte_offset >= 0)):
-
-        if ((span_a.start_byte_offset == span_b.start_byte_offset) and
-                (span_a.end_byte_offset == span_b.end_byte_offset)):
-            return True
-
-    return False
-
-
-def gold_has_minimal_answer(gold_label_list, minimal_non_null_threshold):
-    """Gets vote from annotators for judging if there is a minimal answer."""
-    #  We consider if there is a minimal answer if there is an minimal answer span
-    #  or the yes/no answer is not none.
-    gold_has_answer = gold_label_list and sum([
-        ((not label.minimal_answer_span.is_null_span()) or
-         (label.yes_no_answer != 'none')) for label in gold_label_list
-    ]) >= minimal_non_null_threshold
-
-    return bool(gold_has_answer)
-
-
-def gold_has_passage_answer(gold_label_list, passage_non_null_threshold):
-    """Gets vote from annotators for judging if there is a passage answer."""
-
-    gold_has_answer = gold_label_list and (sum([
-        label.passage_answer_index >= 0  # passage answer not null
-        for label in gold_label_list  # for each annotator
-    ]) >= passage_non_null_threshold)
-
-    return bool(gold_has_answer)
+# coding=utf-8
+# Copyright 2020 The Google Research Team Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Utility function for TyDi QA evaluation."""
+
+import collections
+
+
+# A data structure for storing prediction and annotation.
+# When a example has multiple annotations, multiple TyDiLabel will be used.
+TyDiLabel = collections.namedtuple(
+    'TyDiLabel',
+    [
+        'plaintext',  # context.
+        'question_text',  # a question text.
+        'example_id',  # the unique id for each TyDi example.
+        'language',  # language id.
+        'passage_answer_index',  # A index for passage answer among candidates.
+        'passage_span',  # offsets for the passage span
+        'minimal_answer_span',  # A Span object for minimal answer.
+        'yes_no_answer',  # Indicate if the minimal answer is an yes/no answer
+        #   The possible values are "yes", "no", "none".
+        #   (case insensitive)
+        #   If the field is "yes", minimal_answer_span should be empty or null.
+        'passage_score',  # The score for the passage answer prediction.
+        'minimal_score'  # The score for the minimal answer prediction.
+    ])
+
+
+class Span(object):
+    """A class for handling token and byte spans.
+
+      The logic is:
+
+      1) if both start_byte != -1 and end_byte != -1 then the span is defined
+         by byte offsets
+      3) else, this is a null span.
+
+      Null spans means that there is no (passage or minimal) answers.
+
+    """
+
+    def __init__(self, start_byte_offset, end_byte_offset):
+
+        if ((start_byte_offset < 0 and end_byte_offset >= 0) or
+                (start_byte_offset >= 0 and end_byte_offset < 0)):
+            raise ValueError('Inconsistent Null Spans (Byte).')
+
+        if (start_byte_offset >= 0 and end_byte_offset >= 0 and
+                start_byte_offset > end_byte_offset):
+            raise ValueError('Invalid byte spans (start_byte >= end_byte).')
+
+        self.start_byte_offset = start_byte_offset
+        self.end_byte_offset = end_byte_offset
+
+    def is_null_span(self):
+        """A span is a null span if the start and end are both -1.
+
+        This can happen for both gold and predicted values and
+        for both passage answers and minimal answers.
+
+        Returns:
+          boolean flag whether it is null span or not.
+        """
+
+        if (self.start_byte_offset < 0 and self.end_byte_offset < 0):
+            return True
+        return False
+
+    def __str__(self):
+        return '({},{})'.format(self.start_byte_offset, self.end_byte_offset)
+
+    def __repr__(self):
+        return self.__str__()
+
+
+def safe_divide(x, y):
+    """Compute x / y, but return 0 if y is zero."""
+    if y == 0:
+        return 0
+    else:
+        return x / y
+
+
+def safe_average(elements):
+    """Computes average `elements`, but returns 0 if `elements` is empty."""
+    return safe_divide(sum(elements), len(elements))
+
+
+def compute_partial_match_scores(gold_span, pred_span):
+    """Compute byte indices precision, recall and F1 score between span a and b.
+
+    This is used for scoring only minimal answers. See `nonnull_span_equal` for
+    scoring passage answers.
+
+    Args:
+      gold_span: a Span object. End_byte is inclusive (start_byte+byte_len)
+      pred_span: a Span object.  Only compare non-null spans.
+        Then, if the bytes are ot negative, compare byte offsets.
+
+    Returns:
+      precision: byte offset based precision.
+                (# bytes in both gold and pred span) / (# bytes in pred_span)
+      recall: byte offset based recall.
+                (# bytes in both gold and pred span) / (# bytes in gold_span)
+      f1: harmonic mean of precision and recall.
+    """
+    if not isinstance(gold_span, Span):
+        raise TypeError('Gold span must has a Span type.')
+    # if not isinstance(pred_span, NQSpan):
+    #     raise TypeError('Prediction span must has a Span type.')
+    if not isinstance(pred_span, Span):
+        raise TypeError('Prediction span must has a Span type.')
+    if gold_span.is_null_span():
+        raise ValueError(
+            'Null gold span should not be passed for F1 computation.')
+    if pred_span.is_null_span():
+        raise ValueError(
+            'Null prediction span should not be passed for F1 computation.')
+    assert not pred_span.is_null_span()
+    # If there is no overlap, partial score is zero.
+    if ((gold_span.end_byte_offset <= pred_span.start_byte_offset) or
+            (pred_span.end_byte_offset <= gold_span.start_byte_offset)):
+        precision = 0.0
+        recall = 0.0
+
+    else:
+        in_both = (min(gold_span.end_byte_offset, pred_span.end_byte_offset) -
+                   max(gold_span.start_byte_offset, pred_span.start_byte_offset))
+        assert in_both > 0
+        # if gold span starts earlier than pred span.
+        if gold_span.start_byte_offset <= pred_span.start_byte_offset:
+            only_in_gold = pred_span.start_byte_offset - gold_span.start_byte_offset
+            only_in_gold += max(0,
+                                gold_span.end_byte_offset - pred_span.end_byte_offset)
+            only_in_pred = max(pred_span.end_byte_offset - gold_span.end_byte_offset,
+                               0)
+        # if pred span starts earlier than gold span.
+        else:
+            only_in_pred = gold_span.start_byte_offset - pred_span.start_byte_offset
+            only_in_pred += max(0,
+                                pred_span.end_byte_offset - gold_span.end_byte_offset)
+            only_in_gold = max(gold_span.end_byte_offset - pred_span.end_byte_offset,
+                               0)
+        precision = safe_divide(in_both, (in_both + only_in_pred))
+        recall = safe_divide(in_both, (in_both + only_in_gold))
+
+    f1 = safe_divide(2 * precision * recall, precision + recall)
+    return precision, recall, f1
+
+
+def nonnull_span_equal(span_a, span_b):
+    """Given two spans, return if they are equal.
+
+    This is used for scoring only passage answers.
+    See `compute_partial_match_scores` for minimal answers.
+
+    Args:
+      span_a: a Span object.
+      span_b: a Span object.  Only compare non-null spans. First, if the bytes are
+        not negative, compare byte offsets.
+
+    Returns:
+      True or False
+    """
+    assert isinstance(span_a, Span)
+    assert isinstance(span_b, Span)
+    assert not span_a.is_null_span()
+    assert not span_b.is_null_span()
+
+    # if byte offsets are not negative, compare byte offsets
+    if ((span_a.start_byte_offset >= 0 and span_a.end_byte_offset >= 0) and
+            (span_b.start_byte_offset >= 0 and span_b.end_byte_offset >= 0)):
+
+        if ((span_a.start_byte_offset == span_b.start_byte_offset) and
+                (span_a.end_byte_offset == span_b.end_byte_offset)):
+            return True
+
+    return False
+
+
+def gold_has_minimal_answer(gold_label_list, minimal_non_null_threshold):
+    """Gets vote from annotators for judging if there is a minimal answer."""
+    #  We consider if there is a minimal answer if there is an minimal answer span
+    #  or the yes/no answer is not none.
+    gold_has_answer = gold_label_list and sum([
+        ((not label.minimal_answer_span.is_null_span()) or
+         (label.yes_no_answer != 'none')) for label in gold_label_list
+    ]) >= minimal_non_null_threshold
+
+    return bool(gold_has_answer)
+
+
+def gold_has_passage_answer(gold_label_list, passage_non_null_threshold):
+    """Gets vote from annotators for judging if there is a passage answer."""
+
+    gold_has_answer = gold_label_list and (sum([
+        label.passage_answer_index >= 0  # passage answer not null
+        for label in gold_label_list  # for each annotator
+    ]) >= passage_non_null_threshold)
+
+    return bool(gold_has_answer)
```

## primeqa/mrc/metrics/tydi_f1/tydi_eval.py

 * *Ordering differences only*

```diff
@@ -1,554 +1,554 @@
-# coding=utf-8
-# Copyright 2020 The Google Research Team Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# Lint as: python3
-r"""Official evaluation script for the TyDi QA primary tasks.
-
-The primary tasks are the Passage Selection Task (SelectP) and the Minimal
-Answer Span Task Task (AnsSpan). This script is *not* used for the secondary
-task, the SQuAD-compatible gold Passage (GoldP) task.
-
-  ------------------------------------------------------------------------------
-
-  Note that R@P are only meaningful if your model populates the score fields
-  of the prediction JSON format (which is not required).
-
-  ------------------------------------------------------------------------------
-
-  Prediction format (written on multiple lines here for clarity, but each
-  prediction should be a single line in your output file):
-
-  {
-    'example_id': -2226525965842375672,
-    'passage_answer_index': 2,
-    'passage_answer_score': 13.5,
-    'minimal_answer': {'start_byte_offset': 64206, 'end_byte_offset': 64280},
-    'minimal_answer_score': 26.4,
-    'yes_no_answer': 'NONE'
-  }
-
-  The prediction format mirrors the annotation format in defining each passage
-  or minimal answer span both in terms of byte offsets.
-
-  If start_byte_offset >= 0 and end_byte_offset >=0, use byte offsets,
-    else no span is defined (null answer).
-
-  The minimal answer metric takes both minimal answer spans, and the yes/no
-  answer into account. If the 'minimal_answers' list contains any non/null
-  spans, then 'yes_no_answer' should be set to 'NONE'.
-
-  -----------------------------------------------------------------------------
-
-  Metrics:
-
-  Each prediction should be provided with a passage answer score, and a minimal
-  answers score. At evaluation time, the evaluation script will find a score
-  threshold at which F1 is maximized. All predictions with scores below this
-  threshold are ignored (assumed to be null). If the score is not provided,
-  the evaluation script considers all predictions to be valid. The script
-  will also output the maximum recall at precision points of >= 0.5, >= 0.75,
-  and >= 0.9.
-
-  Key methods:
-    Scoring passage answer candidates: score_passage_answer()
-    Scoring minimal answer candidates: score_minimal_answer(),
-                                       eval_utils.compute_partial_match_scores()
-    Computing language-wise F1: compute_macro_f1()
-"""
-
-import collections
-import json
-import logging
-from operator import not_, itemgetter
-
-from primeqa.mrc.metrics.tydi_f1 import eval_utils
-
-
-def score_passage_answer(gold_label_list, pred_label,
-                         passage_non_null_threshold):
-    """Scores a passage answer as correct or not.
-
-    1) First decide if there is a gold passage answer with
-       FLAGS.passage_non_null_threshold.
-    2) The prediction will get a match if:
-       a. There is a gold passage answer.
-       b. The prediction span match exactly with *one* of the non-null gold
-          passage answer index.
-
-    Args:
-      gold_label_list: A list of TyDiLabel, could be None.
-      pred_label: A single TyDiLabel, could be None.
-      passage_non_null_threshold: See FLAGS.passage_non_null_threshold.
-
-    Returns:
-      gold_has_answer, pred_has_answer, is_correct, score
-    """
-    gold_has_answer = eval_utils.gold_has_passage_answer(
-        gold_label_list, passage_non_null_threshold)
-
-    if pred_label is None:
-        return gold_has_answer, not gold_has_answer, False, 0
-
-    pred_has_answer = pred_label.passage_answer_index != -1
-
-    is_correct = False
-    score = pred_label.passage_score
-
-    # Both sides are non-null spans.
-    if gold_has_answer and pred_has_answer:
-        for gold_label in gold_label_list:
-            # while the voting results indicate there is an passage answer, each
-            # annotator might still say there is no passage answer.
-            if gold_label.passage_answer_index < 0:
-                continue
-
-            if gold_label.passage_answer_index == pred_label.passage_answer_index:
-                is_correct = True
-                break
-
-    return gold_has_answer, pred_has_answer, is_correct, score
-
-
-def score_minimal_answer(gold_label_list, pred_label,
-                         span_non_null_threshold):
-    """Scores a minimal answer.
-
-    Outputs score against gold label that gives max F1.
-
-    First decide if there is a gold minimal answer with
-    FLAGS.span_non_null_threshold.
-    If any of the gold label has "yes", or "no", and pred label predicted it
-    correctly, than precision, recall, f1 is all 1.0.
-
-    Args:
-      gold_label_list: A list of TyDiLabel.
-      pred_label: A single TyDiLabel.
-      span_non_null_threshold: See FLAGS.span_non_null_threshold.
-
-    Returns:
-      gold_has_answer, pred_has_answer, (precision, recall, f1), score
-    """
-
-    # There is a gold minimal answer if gold_label_list not empty and non null
-    # answers is over the threshold (sum over annotators).
-    gold_has_answer = eval_utils.gold_has_minimal_answer(
-        gold_label_list, span_non_null_threshold)
-
-    if pred_label is None:
-        return gold_has_answer, not gold_has_answer, (0, 0, 0), 0
-
-    # There is a predicted minimal answer if the predicted minimal label span
-    # is non-null or we have a specific predicted label (such as yes/no).
-    pred_has_answer = (not pred_label.minimal_answer_span.is_null_span()
-                       or pred_label.yes_no_answer != 'none')
-
-    # score is optional.
-    score = pred_label.minimal_score
-    # We find the closest (highest scoring) match between the system's predicted
-    # minimal answer and one of the three gold annotations.
-    max_f1 = 0.0
-    max_precision = 0.0
-    max_recall = 0.0
-
-    # Both sides have minimal answers, which contains yes/no questions.
-    if gold_has_answer and pred_has_answer:
-        if pred_label.yes_no_answer != 'none':  # System predicted a yes/no answer.
-            for gold_label in gold_label_list:
-                if pred_label.yes_no_answer == gold_label.yes_no_answer:
-                    max_f1 = 1.0
-                    max_precision = 1.0
-                    max_recall = 1.0
-                    break
-        else:
-            for gold_label in gold_label_list:
-                if gold_label.minimal_answer_span.is_null_span():
-                    continue
-                # Compute the *micro-F1* (a partial match score for this example).
-                # We also compute a language-wise *macro-F1* later.
-                precision, recall, f1 = eval_utils.compute_partial_match_scores(
-                    gold_label.minimal_answer_span, pred_label.minimal_answer_span)
-                if f1 > max_f1:
-                    max_f1 = f1
-                    max_precision = precision
-                    max_recall = recall
-
-    return (gold_has_answer, pred_has_answer,
-            (max_precision, max_recall, max_f1), score)
-
-
-def byte_slice(text, start, end):
-    byte_str = bytes(text, 'utf-8')
-    return str(byte_str[start:end])
-
-
-def score_answers(gold_annotation_dict, pred_dict, passage_non_null_threshold, span_non_null_threshold, verbose,
-                  skip_missing_example_ids=True, minimal_offsets_per_passage=False):
-    """Scores all answers for all documents.
-
-    Args:
-      gold_annotation_dict: a dict from example id to list of `TyDiLabel`s.
-      pred_dict: a dict from example id to list of `TyDiLabel`s.
-      passage_non_null_threshold:
-      span_non_null_threshold: minimal number of non-null annotations per example to be considered non-null
-      verbose: whether to enable verbose logging
-      skip_missing_example_ids: skip missing examples
-      minimal_offsets_per_passage: whether minimal answer offsets are per passage (as opposed to per document)
-
-    Returns:
-      passage_answer_stats: List of scores for passage answers.
-      minimal_answer_stats: List of scores for minimal answers.
-    """
-    gold_id_set = set(gold_annotation_dict.keys())
-    pred_id_set = set(pred_dict.keys())
-
-    unpredicted = gold_id_set - pred_id_set
-    unexpected = pred_id_set - gold_id_set
-    if unpredicted:
-        logging.warning('Predictions missing for %d examples.', len(unpredicted))
-        logging.info('  Missing ids: %s', sorted(unpredicted))
-    if unexpected:
-        logging.warning(
-            'Found predictions for %d examples that do not appear in the gold data.',
-            len(unexpected))
-        logging.info('  Unexpected ids: %s', sorted(unexpected))
-    if not skip_missing_example_ids and (unpredicted or unexpected):
-        raise ValueError("Gold and pred example ids not the same and skip missing ids not selected")
-
-    passage_answer_stats = []
-    minimal_answer_stats = []
-    example_count = 0
-    for example_id in gold_id_set:
-        example_count += 1
-        gold = gold_annotation_dict[example_id]
-        pred = pred_dict.get(example_id)
-        passage_stats = score_passage_answer(gold, pred, passage_non_null_threshold)
-        minimal_stats = score_minimal_answer(gold, pred, span_non_null_threshold)
-
-        # fix stats for predictions in incorrect passages
-        if minimal_offsets_per_passage and not passage_stats[2] and minimal_stats[1]:
-            minimal_stats = (minimal_stats[0], minimal_stats[1], (0., 0., 0.), minimal_stats[3])
-
-        passage_answer_stats.append(passage_stats)
-        minimal_answer_stats.append(minimal_stats)
-
-        if not verbose:
-            continue
-        if pred is None:
-            continue
-        pred_min_start = pred.minimal_answer_span.start_byte_offset
-        pred_min_end = pred.minimal_answer_span.end_byte_offset
-        gold_min_start = gold[0].minimal_answer_span.start_byte_offset
-        gold_min_end = gold[0].minimal_answer_span.end_byte_offset
-        if gold_min_start >= 0:
-            logging.info('---')
-            logging.info(gold[0].example_id)
-            logging.info(gold[0].question_text)
-            logging.info('gold offsets %d, %d', gold_min_start, gold_min_end)
-            logging.info('pred offsets %d, %d', pred_min_start, pred_min_end)
-            logging.info('gold answer: (%s)',
-                         gold[0].plaintext[gold_min_start:gold_min_end])
-            logging.info('pred answer: (%s)',
-                         gold[0].plaintext[pred_min_start:pred_min_end])
-            logging.info('score %.2f', minimal_answer_stats[-1][-1])
-            logging.info('f1: %.2f, p: %.2f, r: %.2f',
-                         minimal_answer_stats[-1][-2][2],
-                         minimal_answer_stats[-1][-2][0],
-                         minimal_answer_stats[-1][-2][1])
-    # use the 'score' column, which is last
-    passage_answer_stats.sort(key=itemgetter(-1), reverse=True)
-    minimal_answer_stats.sort(key=itemgetter(-1), reverse=True)
-    return passage_answer_stats, minimal_answer_stats
-
-
-def compute_macro_f1(answer_stats, prefix=''):
-    """Computes F1, precision, recall for a list of answer scores.
-
-    This computes the *language-wise macro F1*. For minimal answers,
-    we also compute a partial match score that uses F1, which would be
-    included in this computation via `answer_stats`.
-
-    Args:
-      answer_stats: List of per-example scores.
-      prefix (''): Prefix to prepend to score dictionary.
-
-    Returns:
-      Dictionary mapping measurement names to scores.
-    """
-
-    has_gold, has_pred, f1, _ = list(zip(*answer_stats))
-
-    macro_precision = eval_utils.safe_divide(sum(f1), sum(has_pred))
-    macro_recall = eval_utils.safe_divide(sum(f1), sum(has_gold))
-    macro_f1 = eval_utils.safe_divide(
-        2 * macro_precision * macro_recall,
-        macro_precision + macro_recall)
-
-    return collections.OrderedDict({
-        prefix + 'n': len(answer_stats),
-        prefix + 'f1': macro_f1,
-        prefix + 'precision': macro_precision,
-        prefix + 'recall': macro_recall
-    })
-
-
-def compute_final_f1(passage_answer_stats, minimal_answer_stats):
-    """Computes overall F1 given passage and minimal answers, ignoring scores.
-
-    Note: this assumes that the answers have been thresholded based on scores.
-
-    Arguments:
-       passage_answer_stats: List of passage answer scores.
-       minimal_answer_stats: List of minimal answer scores.
-
-
-    Returns:
-       Dictionary of name (string) -> score.
-    """
-    scores = compute_macro_f1(passage_answer_stats, prefix='passage-answer-')
-    scores.update(compute_macro_f1(
-        minimal_answer_stats, prefix='minimal-answer-'))
-    return scores
-
-
-def compute_pr_curves(answer_stats, targets=None):
-    """Computes PR curve and returns R@P for specific targets.
-
-    The values are computed as follows: find the (precision, recall) point
-    with maximum recall and where precision > target.
-
-    This is only relevant if you return the system scores in your predictions.
-    You may find this useful when attempting to tune the threshold for your
-    system on the dev set before requesting an evaluation on the test set
-    via the leaderboard.
-
-    Arguments:
-      answer_stats: List of statistic tuples from the answer scores.
-      targets (None): List of precision thresholds to target.
-
-    Returns:
-      List of table with rows: [target, r, p, score].
-    """
-    total_f1 = 0
-    total_has_pred = 0
-    total_has_gold = 0
-
-    # Count the number of gold annotations.
-    for has_gold, _, _, _ in answer_stats:
-        total_has_gold += has_gold
-
-    # Keep track of the point of maximum recall for each target.
-
-    max_recall = [0 for _ in targets]
-    max_precision = [0 for _ in targets]
-    max_scores = [0.0 for _ in targets]
-
-    # Only keep track of unique thresholds in this dictionary.
-    scores_to_stats = collections.OrderedDict()
-
-    # Loop through every possible threshold and compute precision + recall.
-    for has_gold, has_pred, is_correct_or_f1, score in answer_stats:
-        if isinstance(is_correct_or_f1, tuple):
-            _, _, f1 = is_correct_or_f1
-        else:
-            f1 = is_correct_or_f1
-        total_f1 += f1
-        total_has_pred += has_pred
-
-        precision = eval_utils.safe_divide(total_f1, total_has_pred)
-        recall = eval_utils.safe_divide(total_f1, total_has_gold)
-
-        # If there are any ties, this will be updated multiple times until the
-        # ties are all counted.
-        scores_to_stats[score] = [precision, recall]
-
-    best_f1 = 0.0
-    best_precision = 0.0
-    best_recall = 0.0
-    best_threshold = 0.0
-
-    for threshold, (precision, recall) in scores_to_stats.items():
-        # Match the thresholds to the find the closest precision above some target.
-        for t, target in enumerate(targets):
-            if precision >= target and recall > max_recall[t]:
-                max_recall[t] = recall
-                max_precision[t] = precision
-                max_scores[t] = threshold
-
-        # Compute optimal threshold.
-        f1 = eval_utils.safe_divide(2 * precision * recall, precision + recall)
-        if f1 > best_f1:
-            best_f1 = f1
-            best_precision = precision
-            best_recall = recall
-            best_threshold = threshold
-
-    return ((best_f1, best_precision, best_recall, best_threshold),
-            list(zip(targets, max_recall, max_precision, max_scores)))
-
-
-def print_r_at_p_table(answer_stats):
-    """Pretty prints the R@P table for default targets."""
-    opt_result, pr_table = compute_pr_curves(
-        answer_stats, targets=[0.5, 0.75, 0.9])
-    f1, precision, recall, threshold = opt_result
-    print('Optimal threshold: {:.5}'.format(threshold))
-    print(' F1     /  P      /  R')
-    print('{: >7.2%} / {: >7.2%} / {: >7.2%}'.format(f1, precision, recall))
-    for target, recall, precision, row in pr_table:
-        print('R@P={}: {:.2%} (actual p={:.2%}, score threshold={:.4})'.format(
-            target, recall, precision, row))
-
-
-def get_metrics_with_answer_stats(passage_answer_stats, minimal_answer_stats):
-    """Generate metrics dict using passage and minimal answer stats."""
-
-    def _get_metric_dict(answer_stats, prefix=''):
-        """Compute all metrics for a set of answer statistics."""
-        opt_result, pr_table = compute_pr_curves(
-            answer_stats, targets=[0.5, 0.75, 0.9])
-        f1, precision, recall, threshold = opt_result
-        metrics = collections.OrderedDict({
-            'best-threshold-f1': f1,
-            'best-threshold-precision': precision,
-            'best-threshold-recall': recall,
-            'best-threshold': threshold,
-        })
-        for target, recall, precision, _ in pr_table:
-            metrics['recall-at-precision>={:.2}'.format(target)] = recall
-            metrics['precision-at-precision>={:.2}'.format(target)] = precision
-
-        # Add prefix before returning.
-        return dict([(prefix + k, v) for k, v in metrics.items()])
-
-    metrics = _get_metric_dict(passage_answer_stats, 'passage-')
-    metrics.update(_get_metric_dict(minimal_answer_stats, 'minimal-'))
-    return metrics
-
-
-def get_latex_str(f1, precision, recall):
-    return '\\fpr' + '{' + ('%.1f' % (f1 * 100)) + '}{' + (
-            '%.1f' % (precision * 100)) + '}{' + ('%.1f' % (recall * 100)) + '}'
-
-
-def pretty_print(tydi_gold_dict, tydi_pred_dict, passage_non_null_threshold=2, span_non_null_threshold=2,
-                 verbose=False, skip_missing_example_ids=False):
-    if any(map(not_, tydi_gold_dict.values())) or any(
-            annotations[0].minimal_answer_span is None for annotations in tydi_gold_dict.values()):
-        logging.warning("At least one example in gold dict has no annotations -- skipping pretty print evaluation")
-        return
-
-    gold_id_set = set(tydi_gold_dict.keys())
-    pred_id_set = set(tydi_pred_dict.keys())
-    sym_diff = gold_id_set.symmetric_difference(pred_id_set)
-
-    if (not skip_missing_example_ids) and sym_diff:
-        raise ValueError('ERROR: the example ids in gold annotations and example '
-                         'ids in the prediction are not equal.')
-    elif skip_missing_example_ids and sym_diff:
-        logging.warning("Skipping {} example ids that are only in either gold or preds".format(len(sym_diff)))
-
-    macro_avg_passage_scores = ([], [], [])
-    macro_avg_minimal_scores = ([], [], [])
-
-    language_list = [  # TODO: support arbitrary languages
-        'english', 'arabic', 'bengali', 'finnish', 'indonesian', 'japanese',
-        'swahili', 'korean', 'russian', 'telugu', 'thai'
-    ]
-
-    per_lang_gold = {}
-    per_lang_pred = {}
-
-    id_to_lang = {}
-
-    for ex_id in gold_id_set:
-        ex = tydi_gold_dict[ex_id]
-        if ex[0].language in per_lang_gold:
-            per_lang_gold[ex[0].language][ex_id] = ex
-        else:
-            per_lang_gold[ex[0].language] = {ex_id: ex}
-        id_to_lang[ex_id] = ex[0].language
-    for ex_id in gold_id_set:
-        ex = tydi_pred_dict[ex_id]
-        if id_to_lang[ex.example_id] in per_lang_pred:
-            per_lang_pred[id_to_lang[ex.example_id]][ex.example_id] = ex
-        else:
-            per_lang_pred[id_to_lang[ex.example_id]] = {ex.example_id: ex}
-
-    for lang in language_list:
-        if lang in per_lang_pred:
-            passage_answer_stats, minimal_answer_stats = score_answers(
-                per_lang_gold.get(lang, {}), per_lang_pred[lang],
-                passage_non_null_threshold, span_non_null_threshold, verbose)
-
-            # Passage selection task
-            opt_result, _ = compute_pr_curves(passage_answer_stats, targets=[0.5])
-            f1, precision, recall, _ = opt_result
-            if lang != 'english':
-                macro_avg_passage_scores[0].append(f1)
-                macro_avg_passage_scores[1].append(precision)
-                macro_avg_passage_scores[2].append(recall)
-            print('Passage & ' + lang + ' & ' + get_latex_str(f1, precision, recall))
-
-            # Minimal answer span task
-            opt_result, _ = compute_pr_curves(minimal_answer_stats, targets=[0.5])
-            f1, precision, recall, _ = opt_result
-            if lang != 'english':
-                macro_avg_minimal_scores[0].append(f1)
-                macro_avg_minimal_scores[1].append(precision)
-                macro_avg_minimal_scores[2].append(recall)
-            print('Minimal Answer & ' + lang + ' & ' +
-                  get_latex_str(f1, precision, recall))
-
-            print('*' * 20)
-            print(lang)
-            print('Language: %s (%d)' % (lang, len(per_lang_gold.get(lang, {}))))
-            print('*' * 20)
-            print('PASSAGE ANSWER R@P TABLE:')
-            print_r_at_p_table(passage_answer_stats)
-            print('*' * 20)
-            print('MINIMAL ANSWER R@P TABLE:')
-            print_r_at_p_table(minimal_answer_stats)
-
-    print('Total # examples in gold: %d, # ex. in pred: %d (including english)' %
-          (len(tydi_gold_dict), len(tydi_pred_dict)))
-
-    f1_list, precision_list, recall_list = macro_avg_passage_scores
-    print('*** Macro Over %d Languages, excluding English **' % len(f1_list))
-    avg_passage_f1 = eval_utils.safe_average(f1_list)
-    avg_passage_recall = eval_utils.safe_average(recall_list)
-    avg_passage_precision = eval_utils.safe_average(precision_list)
-    print('Passage F1:%.3f P:%.3f R:%3f' %
-          (avg_passage_f1, avg_passage_precision, avg_passage_recall))
-    print(get_latex_str(
-        avg_passage_f1, avg_passage_precision, avg_passage_recall))
-
-    f1_list, precision_list, recall_list = macro_avg_minimal_scores
-
-    avg_minimal_f1 = eval_utils.safe_average(f1_list)
-    avg_minimal_recall = eval_utils.safe_average(recall_list)
-    avg_minimal_precision = eval_utils.safe_average(precision_list)
-    print('Minimal F1:%.3f P:%.3f R:%3f' %
-          (avg_minimal_f1, avg_minimal_precision, avg_minimal_recall))
-    print(get_latex_str(
-        avg_minimal_f1, avg_minimal_precision, avg_minimal_recall))
-    print('*** / Aggregate Scores ****')
-
-    aggregate_metrics = {'avg_passage_f1': avg_passage_f1,
-                         'avg_passage_recall': avg_passage_recall,
-                         'avg_passage_precision': avg_passage_precision,
-                         'avg_minimal_f1': avg_minimal_f1,
-                         'avg_minimal_recall': avg_minimal_recall,
-                         'avg_minimal_precision': avg_minimal_precision}
-    print(json.dumps(aggregate_metrics))
-    return aggregate_metrics
+# coding=utf-8
+# Copyright 2020 The Google Research Team Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# Lint as: python3
+r"""Official evaluation script for the TyDi QA primary tasks.
+
+The primary tasks are the Passage Selection Task (SelectP) and the Minimal
+Answer Span Task Task (AnsSpan). This script is *not* used for the secondary
+task, the SQuAD-compatible gold Passage (GoldP) task.
+
+  ------------------------------------------------------------------------------
+
+  Note that R@P are only meaningful if your model populates the score fields
+  of the prediction JSON format (which is not required).
+
+  ------------------------------------------------------------------------------
+
+  Prediction format (written on multiple lines here for clarity, but each
+  prediction should be a single line in your output file):
+
+  {
+    'example_id': -2226525965842375672,
+    'passage_answer_index': 2,
+    'passage_answer_score': 13.5,
+    'minimal_answer': {'start_byte_offset': 64206, 'end_byte_offset': 64280},
+    'minimal_answer_score': 26.4,
+    'yes_no_answer': 'NONE'
+  }
+
+  The prediction format mirrors the annotation format in defining each passage
+  or minimal answer span both in terms of byte offsets.
+
+  If start_byte_offset >= 0 and end_byte_offset >=0, use byte offsets,
+    else no span is defined (null answer).
+
+  The minimal answer metric takes both minimal answer spans, and the yes/no
+  answer into account. If the 'minimal_answers' list contains any non/null
+  spans, then 'yes_no_answer' should be set to 'NONE'.
+
+  -----------------------------------------------------------------------------
+
+  Metrics:
+
+  Each prediction should be provided with a passage answer score, and a minimal
+  answers score. At evaluation time, the evaluation script will find a score
+  threshold at which F1 is maximized. All predictions with scores below this
+  threshold are ignored (assumed to be null). If the score is not provided,
+  the evaluation script considers all predictions to be valid. The script
+  will also output the maximum recall at precision points of >= 0.5, >= 0.75,
+  and >= 0.9.
+
+  Key methods:
+    Scoring passage answer candidates: score_passage_answer()
+    Scoring minimal answer candidates: score_minimal_answer(),
+                                       eval_utils.compute_partial_match_scores()
+    Computing language-wise F1: compute_macro_f1()
+"""
+
+import collections
+import json
+import logging
+from operator import not_, itemgetter
+
+from primeqa.mrc.metrics.tydi_f1 import eval_utils
+
+
+def score_passage_answer(gold_label_list, pred_label,
+                         passage_non_null_threshold):
+    """Scores a passage answer as correct or not.
+
+    1) First decide if there is a gold passage answer with
+       FLAGS.passage_non_null_threshold.
+    2) The prediction will get a match if:
+       a. There is a gold passage answer.
+       b. The prediction span match exactly with *one* of the non-null gold
+          passage answer index.
+
+    Args:
+      gold_label_list: A list of TyDiLabel, could be None.
+      pred_label: A single TyDiLabel, could be None.
+      passage_non_null_threshold: See FLAGS.passage_non_null_threshold.
+
+    Returns:
+      gold_has_answer, pred_has_answer, is_correct, score
+    """
+    gold_has_answer = eval_utils.gold_has_passage_answer(
+        gold_label_list, passage_non_null_threshold)
+
+    if pred_label is None:
+        return gold_has_answer, not gold_has_answer, False, 0
+
+    pred_has_answer = pred_label.passage_answer_index != -1
+
+    is_correct = False
+    score = pred_label.passage_score
+
+    # Both sides are non-null spans.
+    if gold_has_answer and pred_has_answer:
+        for gold_label in gold_label_list:
+            # while the voting results indicate there is an passage answer, each
+            # annotator might still say there is no passage answer.
+            if gold_label.passage_answer_index < 0:
+                continue
+
+            if gold_label.passage_answer_index == pred_label.passage_answer_index:
+                is_correct = True
+                break
+
+    return gold_has_answer, pred_has_answer, is_correct, score
+
+
+def score_minimal_answer(gold_label_list, pred_label,
+                         span_non_null_threshold):
+    """Scores a minimal answer.
+
+    Outputs score against gold label that gives max F1.
+
+    First decide if there is a gold minimal answer with
+    FLAGS.span_non_null_threshold.
+    If any of the gold label has "yes", or "no", and pred label predicted it
+    correctly, than precision, recall, f1 is all 1.0.
+
+    Args:
+      gold_label_list: A list of TyDiLabel.
+      pred_label: A single TyDiLabel.
+      span_non_null_threshold: See FLAGS.span_non_null_threshold.
+
+    Returns:
+      gold_has_answer, pred_has_answer, (precision, recall, f1), score
+    """
+
+    # There is a gold minimal answer if gold_label_list not empty and non null
+    # answers is over the threshold (sum over annotators).
+    gold_has_answer = eval_utils.gold_has_minimal_answer(
+        gold_label_list, span_non_null_threshold)
+
+    if pred_label is None:
+        return gold_has_answer, not gold_has_answer, (0, 0, 0), 0
+
+    # There is a predicted minimal answer if the predicted minimal label span
+    # is non-null or we have a specific predicted label (such as yes/no).
+    pred_has_answer = (not pred_label.minimal_answer_span.is_null_span()
+                       or pred_label.yes_no_answer != 'none')
+
+    # score is optional.
+    score = pred_label.minimal_score
+    # We find the closest (highest scoring) match between the system's predicted
+    # minimal answer and one of the three gold annotations.
+    max_f1 = 0.0
+    max_precision = 0.0
+    max_recall = 0.0
+
+    # Both sides have minimal answers, which contains yes/no questions.
+    if gold_has_answer and pred_has_answer:
+        if pred_label.yes_no_answer != 'none':  # System predicted a yes/no answer.
+            for gold_label in gold_label_list:
+                if pred_label.yes_no_answer == gold_label.yes_no_answer:
+                    max_f1 = 1.0
+                    max_precision = 1.0
+                    max_recall = 1.0
+                    break
+        else:
+            for gold_label in gold_label_list:
+                if gold_label.minimal_answer_span.is_null_span():
+                    continue
+                # Compute the *micro-F1* (a partial match score for this example).
+                # We also compute a language-wise *macro-F1* later.
+                precision, recall, f1 = eval_utils.compute_partial_match_scores(
+                    gold_label.minimal_answer_span, pred_label.minimal_answer_span)
+                if f1 > max_f1:
+                    max_f1 = f1
+                    max_precision = precision
+                    max_recall = recall
+
+    return (gold_has_answer, pred_has_answer,
+            (max_precision, max_recall, max_f1), score)
+
+
+def byte_slice(text, start, end):
+    byte_str = bytes(text, 'utf-8')
+    return str(byte_str[start:end])
+
+
+def score_answers(gold_annotation_dict, pred_dict, passage_non_null_threshold, span_non_null_threshold, verbose,
+                  skip_missing_example_ids=True, minimal_offsets_per_passage=False):
+    """Scores all answers for all documents.
+
+    Args:
+      gold_annotation_dict: a dict from example id to list of `TyDiLabel`s.
+      pred_dict: a dict from example id to list of `TyDiLabel`s.
+      passage_non_null_threshold:
+      span_non_null_threshold: minimal number of non-null annotations per example to be considered non-null
+      verbose: whether to enable verbose logging
+      skip_missing_example_ids: skip missing examples
+      minimal_offsets_per_passage: whether minimal answer offsets are per passage (as opposed to per document)
+
+    Returns:
+      passage_answer_stats: List of scores for passage answers.
+      minimal_answer_stats: List of scores for minimal answers.
+    """
+    gold_id_set = set(gold_annotation_dict.keys())
+    pred_id_set = set(pred_dict.keys())
+
+    unpredicted = gold_id_set - pred_id_set
+    unexpected = pred_id_set - gold_id_set
+    if unpredicted:
+        logging.warning('Predictions missing for %d examples.', len(unpredicted))
+        logging.info('  Missing ids: %s', sorted(unpredicted))
+    if unexpected:
+        logging.warning(
+            'Found predictions for %d examples that do not appear in the gold data.',
+            len(unexpected))
+        logging.info('  Unexpected ids: %s', sorted(unexpected))
+    if not skip_missing_example_ids and (unpredicted or unexpected):
+        raise ValueError("Gold and pred example ids not the same and skip missing ids not selected")
+
+    passage_answer_stats = []
+    minimal_answer_stats = []
+    example_count = 0
+    for example_id in gold_id_set:
+        example_count += 1
+        gold = gold_annotation_dict[example_id]
+        pred = pred_dict.get(example_id)
+        passage_stats = score_passage_answer(gold, pred, passage_non_null_threshold)
+        minimal_stats = score_minimal_answer(gold, pred, span_non_null_threshold)
+
+        # fix stats for predictions in incorrect passages
+        if minimal_offsets_per_passage and not passage_stats[2] and minimal_stats[1]:
+            minimal_stats = (minimal_stats[0], minimal_stats[1], (0., 0., 0.), minimal_stats[3])
+
+        passage_answer_stats.append(passage_stats)
+        minimal_answer_stats.append(minimal_stats)
+
+        if not verbose:
+            continue
+        if pred is None:
+            continue
+        pred_min_start = pred.minimal_answer_span.start_byte_offset
+        pred_min_end = pred.minimal_answer_span.end_byte_offset
+        gold_min_start = gold[0].minimal_answer_span.start_byte_offset
+        gold_min_end = gold[0].minimal_answer_span.end_byte_offset
+        if gold_min_start >= 0:
+            logging.info('---')
+            logging.info(gold[0].example_id)
+            logging.info(gold[0].question_text)
+            logging.info('gold offsets %d, %d', gold_min_start, gold_min_end)
+            logging.info('pred offsets %d, %d', pred_min_start, pred_min_end)
+            logging.info('gold answer: (%s)',
+                         gold[0].plaintext[gold_min_start:gold_min_end])
+            logging.info('pred answer: (%s)',
+                         gold[0].plaintext[pred_min_start:pred_min_end])
+            logging.info('score %.2f', minimal_answer_stats[-1][-1])
+            logging.info('f1: %.2f, p: %.2f, r: %.2f',
+                         minimal_answer_stats[-1][-2][2],
+                         minimal_answer_stats[-1][-2][0],
+                         minimal_answer_stats[-1][-2][1])
+    # use the 'score' column, which is last
+    passage_answer_stats.sort(key=itemgetter(-1), reverse=True)
+    minimal_answer_stats.sort(key=itemgetter(-1), reverse=True)
+    return passage_answer_stats, minimal_answer_stats
+
+
+def compute_macro_f1(answer_stats, prefix=''):
+    """Computes F1, precision, recall for a list of answer scores.
+
+    This computes the *language-wise macro F1*. For minimal answers,
+    we also compute a partial match score that uses F1, which would be
+    included in this computation via `answer_stats`.
+
+    Args:
+      answer_stats: List of per-example scores.
+      prefix (''): Prefix to prepend to score dictionary.
+
+    Returns:
+      Dictionary mapping measurement names to scores.
+    """
+
+    has_gold, has_pred, f1, _ = list(zip(*answer_stats))
+
+    macro_precision = eval_utils.safe_divide(sum(f1), sum(has_pred))
+    macro_recall = eval_utils.safe_divide(sum(f1), sum(has_gold))
+    macro_f1 = eval_utils.safe_divide(
+        2 * macro_precision * macro_recall,
+        macro_precision + macro_recall)
+
+    return collections.OrderedDict({
+        prefix + 'n': len(answer_stats),
+        prefix + 'f1': macro_f1,
+        prefix + 'precision': macro_precision,
+        prefix + 'recall': macro_recall
+    })
+
+
+def compute_final_f1(passage_answer_stats, minimal_answer_stats):
+    """Computes overall F1 given passage and minimal answers, ignoring scores.
+
+    Note: this assumes that the answers have been thresholded based on scores.
+
+    Arguments:
+       passage_answer_stats: List of passage answer scores.
+       minimal_answer_stats: List of minimal answer scores.
+
+
+    Returns:
+       Dictionary of name (string) -> score.
+    """
+    scores = compute_macro_f1(passage_answer_stats, prefix='passage-answer-')
+    scores.update(compute_macro_f1(
+        minimal_answer_stats, prefix='minimal-answer-'))
+    return scores
+
+
+def compute_pr_curves(answer_stats, targets=None):
+    """Computes PR curve and returns R@P for specific targets.
+
+    The values are computed as follows: find the (precision, recall) point
+    with maximum recall and where precision > target.
+
+    This is only relevant if you return the system scores in your predictions.
+    You may find this useful when attempting to tune the threshold for your
+    system on the dev set before requesting an evaluation on the test set
+    via the leaderboard.
+
+    Arguments:
+      answer_stats: List of statistic tuples from the answer scores.
+      targets (None): List of precision thresholds to target.
+
+    Returns:
+      List of table with rows: [target, r, p, score].
+    """
+    total_f1 = 0
+    total_has_pred = 0
+    total_has_gold = 0
+
+    # Count the number of gold annotations.
+    for has_gold, _, _, _ in answer_stats:
+        total_has_gold += has_gold
+
+    # Keep track of the point of maximum recall for each target.
+
+    max_recall = [0 for _ in targets]
+    max_precision = [0 for _ in targets]
+    max_scores = [0.0 for _ in targets]
+
+    # Only keep track of unique thresholds in this dictionary.
+    scores_to_stats = collections.OrderedDict()
+
+    # Loop through every possible threshold and compute precision + recall.
+    for has_gold, has_pred, is_correct_or_f1, score in answer_stats:
+        if isinstance(is_correct_or_f1, tuple):
+            _, _, f1 = is_correct_or_f1
+        else:
+            f1 = is_correct_or_f1
+        total_f1 += f1
+        total_has_pred += has_pred
+
+        precision = eval_utils.safe_divide(total_f1, total_has_pred)
+        recall = eval_utils.safe_divide(total_f1, total_has_gold)
+
+        # If there are any ties, this will be updated multiple times until the
+        # ties are all counted.
+        scores_to_stats[score] = [precision, recall]
+
+    best_f1 = 0.0
+    best_precision = 0.0
+    best_recall = 0.0
+    best_threshold = 0.0
+
+    for threshold, (precision, recall) in scores_to_stats.items():
+        # Match the thresholds to the find the closest precision above some target.
+        for t, target in enumerate(targets):
+            if precision >= target and recall > max_recall[t]:
+                max_recall[t] = recall
+                max_precision[t] = precision
+                max_scores[t] = threshold
+
+        # Compute optimal threshold.
+        f1 = eval_utils.safe_divide(2 * precision * recall, precision + recall)
+        if f1 > best_f1:
+            best_f1 = f1
+            best_precision = precision
+            best_recall = recall
+            best_threshold = threshold
+
+    return ((best_f1, best_precision, best_recall, best_threshold),
+            list(zip(targets, max_recall, max_precision, max_scores)))
+
+
+def print_r_at_p_table(answer_stats):
+    """Pretty prints the R@P table for default targets."""
+    opt_result, pr_table = compute_pr_curves(
+        answer_stats, targets=[0.5, 0.75, 0.9])
+    f1, precision, recall, threshold = opt_result
+    print('Optimal threshold: {:.5}'.format(threshold))
+    print(' F1     /  P      /  R')
+    print('{: >7.2%} / {: >7.2%} / {: >7.2%}'.format(f1, precision, recall))
+    for target, recall, precision, row in pr_table:
+        print('R@P={}: {:.2%} (actual p={:.2%}, score threshold={:.4})'.format(
+            target, recall, precision, row))
+
+
+def get_metrics_with_answer_stats(passage_answer_stats, minimal_answer_stats):
+    """Generate metrics dict using passage and minimal answer stats."""
+
+    def _get_metric_dict(answer_stats, prefix=''):
+        """Compute all metrics for a set of answer statistics."""
+        opt_result, pr_table = compute_pr_curves(
+            answer_stats, targets=[0.5, 0.75, 0.9])
+        f1, precision, recall, threshold = opt_result
+        metrics = collections.OrderedDict({
+            'best-threshold-f1': f1,
+            'best-threshold-precision': precision,
+            'best-threshold-recall': recall,
+            'best-threshold': threshold,
+        })
+        for target, recall, precision, _ in pr_table:
+            metrics['recall-at-precision>={:.2}'.format(target)] = recall
+            metrics['precision-at-precision>={:.2}'.format(target)] = precision
+
+        # Add prefix before returning.
+        return dict([(prefix + k, v) for k, v in metrics.items()])
+
+    metrics = _get_metric_dict(passage_answer_stats, 'passage-')
+    metrics.update(_get_metric_dict(minimal_answer_stats, 'minimal-'))
+    return metrics
+
+
+def get_latex_str(f1, precision, recall):
+    return '\\fpr' + '{' + ('%.1f' % (f1 * 100)) + '}{' + (
+            '%.1f' % (precision * 100)) + '}{' + ('%.1f' % (recall * 100)) + '}'
+
+
+def pretty_print(tydi_gold_dict, tydi_pred_dict, passage_non_null_threshold=2, span_non_null_threshold=2,
+                 verbose=False, skip_missing_example_ids=False):
+    if any(map(not_, tydi_gold_dict.values())) or any(
+            annotations[0].minimal_answer_span is None for annotations in tydi_gold_dict.values()):
+        logging.warning("At least one example in gold dict has no annotations -- skipping pretty print evaluation")
+        return
+
+    gold_id_set = set(tydi_gold_dict.keys())
+    pred_id_set = set(tydi_pred_dict.keys())
+    sym_diff = gold_id_set.symmetric_difference(pred_id_set)
+
+    if (not skip_missing_example_ids) and sym_diff:
+        raise ValueError('ERROR: the example ids in gold annotations and example '
+                         'ids in the prediction are not equal.')
+    elif skip_missing_example_ids and sym_diff:
+        logging.warning("Skipping {} example ids that are only in either gold or preds".format(len(sym_diff)))
+
+    macro_avg_passage_scores = ([], [], [])
+    macro_avg_minimal_scores = ([], [], [])
+
+    language_list = [  # TODO: support arbitrary languages
+        'english', 'arabic', 'bengali', 'finnish', 'indonesian', 'japanese',
+        'swahili', 'korean', 'russian', 'telugu', 'thai'
+    ]
+
+    per_lang_gold = {}
+    per_lang_pred = {}
+
+    id_to_lang = {}
+
+    for ex_id in gold_id_set:
+        ex = tydi_gold_dict[ex_id]
+        if ex[0].language in per_lang_gold:
+            per_lang_gold[ex[0].language][ex_id] = ex
+        else:
+            per_lang_gold[ex[0].language] = {ex_id: ex}
+        id_to_lang[ex_id] = ex[0].language
+    for ex_id in gold_id_set:
+        ex = tydi_pred_dict[ex_id]
+        if id_to_lang[ex.example_id] in per_lang_pred:
+            per_lang_pred[id_to_lang[ex.example_id]][ex.example_id] = ex
+        else:
+            per_lang_pred[id_to_lang[ex.example_id]] = {ex.example_id: ex}
+
+    for lang in language_list:
+        if lang in per_lang_pred:
+            passage_answer_stats, minimal_answer_stats = score_answers(
+                per_lang_gold.get(lang, {}), per_lang_pred[lang],
+                passage_non_null_threshold, span_non_null_threshold, verbose)
+
+            # Passage selection task
+            opt_result, _ = compute_pr_curves(passage_answer_stats, targets=[0.5])
+            f1, precision, recall, _ = opt_result
+            if lang != 'english':
+                macro_avg_passage_scores[0].append(f1)
+                macro_avg_passage_scores[1].append(precision)
+                macro_avg_passage_scores[2].append(recall)
+            print('Passage & ' + lang + ' & ' + get_latex_str(f1, precision, recall))
+
+            # Minimal answer span task
+            opt_result, _ = compute_pr_curves(minimal_answer_stats, targets=[0.5])
+            f1, precision, recall, _ = opt_result
+            if lang != 'english':
+                macro_avg_minimal_scores[0].append(f1)
+                macro_avg_minimal_scores[1].append(precision)
+                macro_avg_minimal_scores[2].append(recall)
+            print('Minimal Answer & ' + lang + ' & ' +
+                  get_latex_str(f1, precision, recall))
+
+            print('*' * 20)
+            print(lang)
+            print('Language: %s (%d)' % (lang, len(per_lang_gold.get(lang, {}))))
+            print('*' * 20)
+            print('PASSAGE ANSWER R@P TABLE:')
+            print_r_at_p_table(passage_answer_stats)
+            print('*' * 20)
+            print('MINIMAL ANSWER R@P TABLE:')
+            print_r_at_p_table(minimal_answer_stats)
+
+    print('Total # examples in gold: %d, # ex. in pred: %d (including english)' %
+          (len(tydi_gold_dict), len(tydi_pred_dict)))
+
+    f1_list, precision_list, recall_list = macro_avg_passage_scores
+    print('*** Macro Over %d Languages, excluding English **' % len(f1_list))
+    avg_passage_f1 = eval_utils.safe_average(f1_list)
+    avg_passage_recall = eval_utils.safe_average(recall_list)
+    avg_passage_precision = eval_utils.safe_average(precision_list)
+    print('Passage F1:%.3f P:%.3f R:%3f' %
+          (avg_passage_f1, avg_passage_precision, avg_passage_recall))
+    print(get_latex_str(
+        avg_passage_f1, avg_passage_precision, avg_passage_recall))
+
+    f1_list, precision_list, recall_list = macro_avg_minimal_scores
+
+    avg_minimal_f1 = eval_utils.safe_average(f1_list)
+    avg_minimal_recall = eval_utils.safe_average(recall_list)
+    avg_minimal_precision = eval_utils.safe_average(precision_list)
+    print('Minimal F1:%.3f P:%.3f R:%3f' %
+          (avg_minimal_f1, avg_minimal_precision, avg_minimal_recall))
+    print(get_latex_str(
+        avg_minimal_f1, avg_minimal_precision, avg_minimal_recall))
+    print('*** / Aggregate Scores ****')
+
+    aggregate_metrics = {'avg_passage_f1': avg_passage_f1,
+                         'avg_passage_recall': avg_passage_recall,
+                         'avg_passage_precision': avg_passage_precision,
+                         'avg_minimal_f1': avg_minimal_f1,
+                         'avg_minimal_recall': avg_minimal_recall,
+                         'avg_minimal_precision': avg_minimal_precision}
+    print(json.dumps(aggregate_metrics))
+    return aggregate_metrics
```

## primeqa/mrc/metrics/tydi_f1/tydi_f1.py

 * *Ordering differences only*

```diff
@@ -1,160 +1,160 @@
-from typing import Dict, Any, Tuple, List
-
-import datasets
-
-from primeqa.mrc.metrics.tydi_f1.eval_utils import Span, TyDiLabel
-from primeqa.mrc.metrics.tydi_f1.tydi_eval import pretty_print
-from primeqa.mrc.data_models.target_type import TargetType
-
-
-_DESCRIPTION = """
-The F1 score is the harmonic mean of the precision and recall. It can be computed with:
-F1 = 2 * (precision * recall) / (precision + recall).  This implementation of F1 is based
-on the TyDi QA leaderboard.
-
-Adapted from https://github.com/google-research-datasets/tydiqa/blob/master/tydi_eval.py.
-"""
-
-_KWARGS_DESCRIPTION = """
-Args:
-    predictions: Predicted labels.
-    references: Ground truth labels.
-    passage_non_null_threshold: threshold for number of null annotations annotations to consider the passage answer as null (default=2)
-    span_non_null_threshold: threshold for number of null annotations annotations to consider the span answer as null (default=2)
-    verbose: dump reference and prediction for debugging purposes
-    
-Returns: metrics dict comprising:
-
-  * minimal_f1: Minimal Answer F1.
-  * minimal_precision: Minimal Answer Precision.
-  * minimal_recall: Minimal Answer Recall.
-  * passage_f1: Passage Answer F1.
-  * passage_precision: Passage Answer Precision.
-  * passage_recall: Passage Answer Recall.
-"""
-
-_CITATION = """\
-@article{tydiqa,
-title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
-author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and 
-           Vitaly Nikolaev and Jennimaria Palomaki}
-year    = {2020},
-journal = {Transactions of the Association for Computational Linguistics}
-}
-"""
-
-
-@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
-class TyDiF1(datasets.Metric):
-    _common_answer_schema = dict(
-        start_position=datasets.Value("int32"),
-        end_position=datasets.Value("int32"),
-        passage_index=datasets.Value("int32"),
-        yes_no_answer=datasets.Value("int32"),
-        example_id=datasets.Value("string"),
-    )
-    _pred_answer_schema = dict(
-        confidence_score=datasets.Value("float32"),
-    )
-    _ref_answer_schema = dict(
-        language=datasets.Value("string"),
-        document_plaintext=datasets.Value("string"),
-        question=datasets.Value("string")
-    )
-
-    def _info(self):
-        return datasets.MetricInfo(
-            description=_DESCRIPTION,
-            citation=_CITATION,
-            inputs_description=_KWARGS_DESCRIPTION,
-            features=datasets.Features(
-                dict(
-                    predictions={**self._common_answer_schema, **self._pred_answer_schema},
-                    references=datasets.Sequence(feature={**self._common_answer_schema, **self._ref_answer_schema})
-                )),
-            reference_urls=["https://github.com/google-research-datasets/tydiqa/blob/master/tydi_eval.py"],
-        )
-
-    def _compute(self, *, predictions=None, references=None, passage_non_null_threshold=2, span_non_null_threshold=2, verbose=False, **kwargs) -> Dict[str, Any]:
-        
-        if not predictions:
-            raise ValueError("No predictions provided")
-        elif not references:
-            raise ValueError("No references provided")
-
-        predictions = dict(map(self._convert_pred_to_entry, predictions))
-        references = dict(map(self._convert_ref_to_entry, references))
-
-        metrics = pretty_print(references, predictions, passage_non_null_threshold=passage_non_null_threshold, span_non_null_threshold=span_non_null_threshold, verbose=verbose)
-        return metrics
-
-    def _convert_ref_to_entry(self, ref: dict) -> Tuple[str, List[TyDiLabel]]:
-        """
-        Converts a reference dict into an example_id, [labels] pair.
-        """
-        if not all(ref['example_id'][0] == ref['example_id'][i] for i in range(len(ref['example_id']))):
-            raise ValueError("Found mismatched examples")
-        elif not all(ref['language'][0] == ref['language'][i] for i in range(len(ref['language']))):
-            raise ValueError("Found mismatched languages")
-
-        key = ref['example_id'][0]
-        value = [
-            TyDiLabel(
-                example_id=ref['example_id'][i],
-                passage_answer_index=ref['passage_index'][i],
-                minimal_answer_span=Span(
-                    ref['start_position'][i],
-                    ref['end_position'][i])
-                ,
-                yes_no_answer=self._bool_target(
-                    TargetType(ref['yes_no_answer'][i])
-                ),
-                passage_score=0,
-                minimal_score=0,
-                language=ref['language'][i],
-                passage_span=None,
-                question_text=ref['question'][i],
-                plaintext=ref['document_plaintext'][i],
-            ) for i in range(len(ref['passage_index']))
-        ]
-        return key, value
-
-    def _convert_pred_to_entry(self, pred: dict) -> Tuple[str, TyDiLabel]:
-        """
-        Converts a prediction dict into an example_id, label pair.
-        """
-        key = pred['example_id']
-        value = TyDiLabel(
-                example_id=pred['example_id'],
-                passage_answer_index=pred['passage_index'],
-                minimal_answer_span=Span(
-                    pred['start_position'],
-                    pred['end_position'])
-                ,
-                yes_no_answer=self._bool_target(
-                    TargetType(pred['yes_no_answer'])
-                ),
-                passage_score=pred['confidence_score'] ,
-                minimal_score=pred['confidence_score'] ,
-                language=None,
-                passage_span=None,
-                question_text='',
-                plaintext='',
-            )
-        return key, value
-
-    @staticmethod
-    def _bool_target(target_type: TargetType) -> str:
-        """
-        Converts a target type into a boolean string as expected by TyDi eval.
-        """
-        if target_type == TargetType.YES:
-            return 'yes'
-        elif target_type == TargetType.NO:
-            return 'no'
-        elif target_type == TargetType.NO_ANSWER:
-            return 'none'
-        else:
-            raise NotImplementedError(f"Unexpected target type for tydi bool string conversion: {target_type}")
-
-
+from typing import Dict, Any, Tuple, List
+
+import datasets
+
+from primeqa.mrc.metrics.tydi_f1.eval_utils import Span, TyDiLabel
+from primeqa.mrc.metrics.tydi_f1.tydi_eval import pretty_print
+from primeqa.mrc.data_models.target_type import TargetType
+
+
+_DESCRIPTION = """
+The F1 score is the harmonic mean of the precision and recall. It can be computed with:
+F1 = 2 * (precision * recall) / (precision + recall).  This implementation of F1 is based
+on the TyDi QA leaderboard.
+
+Adapted from https://github.com/google-research-datasets/tydiqa/blob/master/tydi_eval.py.
+"""
+
+_KWARGS_DESCRIPTION = """
+Args:
+    predictions: Predicted labels.
+    references: Ground truth labels.
+    passage_non_null_threshold: threshold for number of null annotations annotations to consider the passage answer as null (default=2)
+    span_non_null_threshold: threshold for number of null annotations annotations to consider the span answer as null (default=2)
+    verbose: dump reference and prediction for debugging purposes
+    
+Returns: metrics dict comprising:
+
+  * minimal_f1: Minimal Answer F1.
+  * minimal_precision: Minimal Answer Precision.
+  * minimal_recall: Minimal Answer Recall.
+  * passage_f1: Passage Answer F1.
+  * passage_precision: Passage Answer Precision.
+  * passage_recall: Passage Answer Recall.
+"""
+
+_CITATION = """\
+@article{tydiqa,
+title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
+author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and 
+           Vitaly Nikolaev and Jennimaria Palomaki}
+year    = {2020},
+journal = {Transactions of the Association for Computational Linguistics}
+}
+"""
+
+
+@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
+class TyDiF1(datasets.Metric):
+    _common_answer_schema = dict(
+        start_position=datasets.Value("int32"),
+        end_position=datasets.Value("int32"),
+        passage_index=datasets.Value("int32"),
+        yes_no_answer=datasets.Value("int32"),
+        example_id=datasets.Value("string"),
+    )
+    _pred_answer_schema = dict(
+        confidence_score=datasets.Value("float32"),
+    )
+    _ref_answer_schema = dict(
+        language=datasets.Value("string"),
+        document_plaintext=datasets.Value("string"),
+        question=datasets.Value("string")
+    )
+
+    def _info(self):
+        return datasets.MetricInfo(
+            description=_DESCRIPTION,
+            citation=_CITATION,
+            inputs_description=_KWARGS_DESCRIPTION,
+            features=datasets.Features(
+                dict(
+                    predictions={**self._common_answer_schema, **self._pred_answer_schema},
+                    references=datasets.Sequence(feature={**self._common_answer_schema, **self._ref_answer_schema})
+                )),
+            reference_urls=["https://github.com/google-research-datasets/tydiqa/blob/master/tydi_eval.py"],
+        )
+
+    def _compute(self, *, predictions=None, references=None, passage_non_null_threshold=2, span_non_null_threshold=2, verbose=False, **kwargs) -> Dict[str, Any]:
+        
+        if not predictions:
+            raise ValueError("No predictions provided")
+        elif not references:
+            raise ValueError("No references provided")
+
+        predictions = dict(map(self._convert_pred_to_entry, predictions))
+        references = dict(map(self._convert_ref_to_entry, references))
+
+        metrics = pretty_print(references, predictions, passage_non_null_threshold=passage_non_null_threshold, span_non_null_threshold=span_non_null_threshold, verbose=verbose)
+        return metrics
+
+    def _convert_ref_to_entry(self, ref: dict) -> Tuple[str, List[TyDiLabel]]:
+        """
+        Converts a reference dict into an example_id, [labels] pair.
+        """
+        if not all(ref['example_id'][0] == ref['example_id'][i] for i in range(len(ref['example_id']))):
+            raise ValueError("Found mismatched examples")
+        elif not all(ref['language'][0] == ref['language'][i] for i in range(len(ref['language']))):
+            raise ValueError("Found mismatched languages")
+
+        key = ref['example_id'][0]
+        value = [
+            TyDiLabel(
+                example_id=ref['example_id'][i],
+                passage_answer_index=ref['passage_index'][i],
+                minimal_answer_span=Span(
+                    ref['start_position'][i],
+                    ref['end_position'][i])
+                ,
+                yes_no_answer=self._bool_target(
+                    TargetType(ref['yes_no_answer'][i])
+                ),
+                passage_score=0,
+                minimal_score=0,
+                language=ref['language'][i],
+                passage_span=None,
+                question_text=ref['question'][i],
+                plaintext=ref['document_plaintext'][i],
+            ) for i in range(len(ref['passage_index']))
+        ]
+        return key, value
+
+    def _convert_pred_to_entry(self, pred: dict) -> Tuple[str, TyDiLabel]:
+        """
+        Converts a prediction dict into an example_id, label pair.
+        """
+        key = pred['example_id']
+        value = TyDiLabel(
+                example_id=pred['example_id'],
+                passage_answer_index=pred['passage_index'],
+                minimal_answer_span=Span(
+                    pred['start_position'],
+                    pred['end_position'])
+                ,
+                yes_no_answer=self._bool_target(
+                    TargetType(pred['yes_no_answer'])
+                ),
+                passage_score=pred['confidence_score'] ,
+                minimal_score=pred['confidence_score'] ,
+                language=None,
+                passage_span=None,
+                question_text='',
+                plaintext='',
+            )
+        return key, value
+
+    @staticmethod
+    def _bool_target(target_type: TargetType) -> str:
+        """
+        Converts a target type into a boolean string as expected by TyDi eval.
+        """
+        if target_type == TargetType.YES:
+            return 'yes'
+        elif target_type == TargetType.NO:
+            return 'no'
+        elif target_type == TargetType.NO_ANSWER:
+            return 'none'
+        else:
+            raise NotImplementedError(f"Unexpected target type for tydi bool string conversion: {target_type}")
+
+
```

## primeqa/mrc/models/task_model.py

 * *Ordering differences only*

```diff
@@ -1,131 +1,131 @@
-import logging
-from typing import Dict, Type
-
-import torch
-from transformers import PretrainedConfig, PreTrainedModel, MODEL_FOR_PRETRAINING_MAPPING, MODEL_MAPPING
-
-from primeqa.mrc.models.heads.abstract import AbstractTaskHead
-
-
-class ModelForDownstreamTasks(PreTrainedModel):
-    """
-    Language model for downstream tasks.  Tasks are implemented via task heads which subclass `AbstractTaskHead`.
-    """
-
-    def __init__(self,
-                 config: PretrainedConfig,
-                 task_heads: Dict[str, Type[AbstractTaskHead]]):
-        """
-        Args:
-            config: Model config
-            task_heads: dict mapping task head name to constructor
-        """
-        super().__init__(config)
-        self._logger = logging.getLogger(self.__class__.__name__)
-
-        if type(self) is ModelForDownstreamTasks:
-            raise TypeError(f"{ModelForDownstreamTasks.__class__.__name__} is not intended to be directly "
-                            f"instantiated and should be subclassed together with a XPreTrainedModel type. "
-                            f"See {self.model_class_from_config.__name__} or {self.from_config.__name__} "
-                            f"for creating and instantiating these subclasses.")
-
-        if not task_heads:
-            raise ValueError("No task heads provided")
-
-        self._task_head = None
-
-        # Set the model to match the pre-trained name (e.g. self.roberta) so it can be loaded from pretrained
-        setattr(self, self.base_model_prefix, MODEL_MAPPING[config.__class__](config))
-
-        self.task_heads = torch.nn.ModuleDict({
-            name: model(config) for name, model in task_heads.items()
-        })
-        self.init_weights()
-
-    @property
-    def model_(self) -> PreTrainedModel:  # using 'model' instead of 'model_' causes conflicts with some LMs (e.g. BART)
-        """
-        Returns the underlying language model. This is an alias to simplify access.
-        """
-        return getattr(self, self.base_model_prefix)
-
-    @property
-    def task_head(self) -> AbstractTaskHead:
-        """
-        Return the current task head or raises a `ValueError` if it has not yet been set.
-        """
-        if self._task_head is not None:
-            # noinspection PyTypeChecker
-            return self.task_heads[self._task_head]
-        else:
-            raise ValueError(f"Task head is not set.  Call {ModelForDownstreamTasks.set_task_head.__name__} to set it")
-
-    def forward(self,
-                input_ids=None,
-                attention_mask=None,
-                token_type_ids=None,
-                position_ids=None,
-                head_mask=None,
-                inputs_embeds=None,
-                output_attentions=None,
-                output_hidden_states=None,
-                return_dict=None,
-                **kwargs):
-        """
-        Returns task head applied to language model outputs and any additional arguments supplied via `kwargs`.
-        See HF transformers documentation for more details on other parameters.
-        """
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        outputs = self.model_(
-            input_ids,
-            attention_mask=attention_mask,
-            token_type_ids=token_type_ids,
-            position_ids=position_ids,
-            head_mask=head_mask,
-            inputs_embeds=inputs_embeds,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-        )
-
-        return self.task_head(outputs, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)
-
-    @classmethod
-    def model_class_from_config(cls, config: PretrainedConfig) -> Type['ModelForDownstreamTasks']:
-        """
-        Dynamically creates and returns a model class from a PreTrainedConfig.
-        """
-        ptm_base_class = MODEL_FOR_PRETRAINING_MAPPING[config.__class__]
-        model_name = config.__class__.__name__.rstrip('Config')
-        class_name = f'{model_name}{cls.__name__}'
-        model_class = type(class_name, (cls, ptm_base_class), {})
-
-        # noinspection PyTypeChecker
-        return model_class
-
-    @classmethod
-    def from_config(cls, config: PretrainedConfig, *args, **kwargs) -> 'ModelForDownstreamTasks':
-        """
-        Dynamically creates a model class from a PreTrainedConfig and then uses the config
-        with `args` or `kwargs` to instantiate and return a model.
-        """
-        model_class = cls.model_class_from_config(config)
-        model = model_class.from_pretrained(*args, config=config, **kwargs)
-        return model
-
-    def set_task_head(self, task_head: str) -> None:
-        """
-        Args:
-            task_head: name of the task head to activate.
-
-        Raises:
-            KeyError: model does not have task head with name `task_head`.
-        """
-        if task_head not in self.task_heads:
-            raise KeyError(f"Task head '{task_head}' not in task_heads: {list(self.task_heads)}")
-        elif self._task_head is not None:
-            self._logger.info(f"Changing default task head from '{self._task_head}' to '{task_head}'")
-        else:
-            self._logger.info(f"Setting task head for first time to '{self._task_head}'")
-        self._task_head = task_head
+import logging
+from typing import Dict, Type
+
+import torch
+from transformers import PretrainedConfig, PreTrainedModel, MODEL_FOR_PRETRAINING_MAPPING, MODEL_MAPPING
+
+from primeqa.mrc.models.heads.abstract import AbstractTaskHead
+
+
+class ModelForDownstreamTasks(PreTrainedModel):
+    """
+    Language model for downstream tasks.  Tasks are implemented via task heads which subclass `AbstractTaskHead`.
+    """
+
+    def __init__(self,
+                 config: PretrainedConfig,
+                 task_heads: Dict[str, Type[AbstractTaskHead]]):
+        """
+        Args:
+            config: Model config
+            task_heads: dict mapping task head name to constructor
+        """
+        super().__init__(config)
+        self._logger = logging.getLogger(self.__class__.__name__)
+
+        if type(self) is ModelForDownstreamTasks:
+            raise TypeError(f"{ModelForDownstreamTasks.__class__.__name__} is not intended to be directly "
+                            f"instantiated and should be subclassed together with a XPreTrainedModel type. "
+                            f"See {self.model_class_from_config.__name__} or {self.from_config.__name__} "
+                            f"for creating and instantiating these subclasses.")
+
+        if not task_heads:
+            raise ValueError("No task heads provided")
+
+        self._task_head = None
+
+        # Set the model to match the pre-trained name (e.g. self.roberta) so it can be loaded from pretrained
+        setattr(self, self.base_model_prefix, MODEL_MAPPING[config.__class__](config))
+
+        self.task_heads = torch.nn.ModuleDict({
+            name: model(config) for name, model in task_heads.items()
+        })
+        self.init_weights()
+
+    @property
+    def model_(self) -> PreTrainedModel:  # using 'model' instead of 'model_' causes conflicts with some LMs (e.g. BART)
+        """
+        Returns the underlying language model. This is an alias to simplify access.
+        """
+        return getattr(self, self.base_model_prefix)
+
+    @property
+    def task_head(self) -> AbstractTaskHead:
+        """
+        Return the current task head or raises a `ValueError` if it has not yet been set.
+        """
+        if self._task_head is not None:
+            # noinspection PyTypeChecker
+            return self.task_heads[self._task_head]
+        else:
+            raise ValueError(f"Task head is not set.  Call {ModelForDownstreamTasks.set_task_head.__name__} to set it")
+
+    def forward(self,
+                input_ids=None,
+                attention_mask=None,
+                token_type_ids=None,
+                position_ids=None,
+                head_mask=None,
+                inputs_embeds=None,
+                output_attentions=None,
+                output_hidden_states=None,
+                return_dict=None,
+                **kwargs):
+        """
+        Returns task head applied to language model outputs and any additional arguments supplied via `kwargs`.
+        See HF transformers documentation for more details on other parameters.
+        """
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        outputs = self.model_(
+            input_ids,
+            attention_mask=attention_mask,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        return self.task_head(outputs, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)
+
+    @classmethod
+    def model_class_from_config(cls, config: PretrainedConfig) -> Type['ModelForDownstreamTasks']:
+        """
+        Dynamically creates and returns a model class from a PreTrainedConfig.
+        """
+        ptm_base_class = MODEL_FOR_PRETRAINING_MAPPING[config.__class__]
+        model_name = config.__class__.__name__.rstrip('Config')
+        class_name = f'{model_name}{cls.__name__}'
+        model_class = type(class_name, (cls, ptm_base_class), {})
+
+        # noinspection PyTypeChecker
+        return model_class
+
+    @classmethod
+    def from_config(cls, config: PretrainedConfig, *args, **kwargs) -> 'ModelForDownstreamTasks':
+        """
+        Dynamically creates a model class from a PreTrainedConfig and then uses the config
+        with `args` or `kwargs` to instantiate and return a model.
+        """
+        model_class = cls.model_class_from_config(config)
+        model = model_class.from_pretrained(*args, config=config, **kwargs)
+        return model
+
+    def set_task_head(self, task_head: str) -> None:
+        """
+        Args:
+            task_head: name of the task head to activate.
+
+        Raises:
+            KeyError: model does not have task head with name `task_head`.
+        """
+        if task_head not in self.task_heads:
+            raise KeyError(f"Task head '{task_head}' not in task_heads: {list(self.task_heads)}")
+        elif self._task_head is not None:
+            self._logger.info(f"Changing default task head from '{self._task_head}' to '{task_head}'")
+        else:
+            self._logger.info(f"Setting task head for first time to '{self._task_head}'")
+        self._task_head = task_head
```

## primeqa/mrc/models/heads/abstract.py

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-import logging
-from abc import ABCMeta, abstractmethod
-from typing import Union
-
-import torch
-from transformers import PretrainedConfig
-from transformers.file_utils import ModelOutput
-
-
-class AbstractTaskHead(torch.nn.Module, metaclass=ABCMeta):
-    """
-    Base class for task heads.
-    """
-    def __init__(self, config: PretrainedConfig):
-        """
-        Args:
-            config: Language model config.
-        """
-        super().__init__()
-        self._logger = logging.getLogger(self.__class__.__name__)
-        self.config = config
-
-    @abstractmethod
-    def forward(self, model_outputs: Union[tuple, ModelOutput], *args, **kwargs) -> Union[tuple, ModelOutput]:
-        """
-        Compute the task head's forward pass.
-
-        Args:
-            model_outputs: Language model outputs.
-            *args: Additional args for task head.
-            **kwargs: Additional keyword args for task head.
-
-        Returns:
-            Task head result in data structure corresponding to type of `model_outputs`.
-        """
-        raise NotImplementedError
+import logging
+from abc import ABCMeta, abstractmethod
+from typing import Union
+
+import torch
+from transformers import PretrainedConfig
+from transformers.file_utils import ModelOutput
+
+
+class AbstractTaskHead(torch.nn.Module, metaclass=ABCMeta):
+    """
+    Base class for task heads.
+    """
+    def __init__(self, config: PretrainedConfig):
+        """
+        Args:
+            config: Language model config.
+        """
+        super().__init__()
+        self._logger = logging.getLogger(self.__class__.__name__)
+        self.config = config
+
+    @abstractmethod
+    def forward(self, model_outputs: Union[tuple, ModelOutput], *args, **kwargs) -> Union[tuple, ModelOutput]:
+        """
+        Compute the task head's forward pass.
+
+        Args:
+            model_outputs: Language model outputs.
+            *args: Additional args for task head.
+            **kwargs: Additional keyword args for task head.
+
+        Returns:
+            Task head result in data structure corresponding to type of `model_outputs`.
+        """
+        raise NotImplementedError
```

## primeqa/mrc/models/heads/extractive.py

 * *Ordering differences only*

```diff
@@ -1,292 +1,292 @@
-from copy import deepcopy
-from typing import Union, Optional
-
-import torch
-from torch.nn.functional import normalize
-from transformers import PretrainedConfig
-from transformers.file_utils import ModelOutput
-from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions
-from transformers.models.roberta.modeling_roberta import RobertaClassificationHead
-
-from primeqa.mrc.models.heads.abstract import AbstractTaskHead
-from primeqa.mrc.data_models.model_outputs.extractive import ExtractiveQAModelOutput, ExtractiveQAWithConfidenceModelOutput
-from primeqa.mrc.data_models.target_type import TargetType
-
-
-class ExtractiveQAHead(AbstractTaskHead):
-    """
-    Task head for extractive Question Answering.
-    """
-    def __init__(self, config: PretrainedConfig, num_labels_override: Optional[int] = None):
-        """
-        Args:
-            config: Language model config.
-            num_labels_override: Set this to override number of answer types from default `len(TargetType)`.
-        """
-        super().__init__(config)
-        self.num_labels = config.num_labels
-        self.qa_outputs = torch.nn.Linear(config.hidden_size, self.num_labels)
-
-        config_for_classification_head = deepcopy(config)
-        if num_labels_override is None:
-            config_for_classification_head.num_labels = len(TargetType)
-        else:
-            config_for_classification_head.num_labels = num_labels_override
-        self.num_classification_head_labels = config_for_classification_head.num_labels
-
-        dropout_names = ["classifier_dropout", "hidden_dropout_prob", "classifier_dropout_prob"]
-        for name in dropout_names:
-            dropout_value = getattr(config_for_classification_head, name, None)
-            if dropout_value is not None:
-                self._logger.info(f"Loading dropout value {dropout_value} from config attribute '{name}'")
-                config_for_classification_head.classifier_dropout = dropout_value
-                break
-        else:
-            self._logger.warning("No dropout value found -- setting to 0")
-            config_for_classification_head.classifier_dropout = 0.
-
-        self.classifier = RobertaClassificationHead(config_for_classification_head)
-
-    def forward(self,
-                model_outputs: Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions],
-                start_positions=None,
-                end_positions=None,
-                target_type=None,
-                **kwargs) -> Union[tuple, ExtractiveQAModelOutput]:
-        """
-        Compute the task head's forward pass.
-
-        Args:
-            model_outputs: Language model outputs.
-            start_positions: (training only) Ground-truth start positions.
-            end_positions: (training only) Ground-truth end positions.
-            target_type: (training only) Ground-truth target type.
-
-        Returns:
-            Extractive QA task head result in data structure corresponding to type of `model_outputs`.
-        """
-        sequence_output = model_outputs[0]
-
-        # Predict target answer type for the whole question answer pair
-        answer_type_logits = self.classifier(sequence_output)
-
-        # Predict start and end logits by token
-        logits = self.qa_outputs(sequence_output)
-        start_logits, end_logits = logits.split(1, dim=-1)
-        start_logits = start_logits.squeeze(-1)
-        end_logits = end_logits.squeeze(-1)
-
-        total_loss = None
-        if start_positions is not None and end_positions is not None and target_type is not None:
-            # If we are on multi-GPU, split add a dimension
-            if len(start_positions.size()) > 1:
-                start_positions = start_positions.squeeze(-1)
-            if len(end_positions.size()) > 1:
-                end_positions = end_positions.squeeze(-1)
-            if len(target_type.size()) > 1:
-                target_type = target_type.squeeze(-1)
-            # sometimes the start/end positions are outside our model inputs, we ignore these terms
-            ignored_index = start_logits.size(1)
-            start_positions.clamp_(0, ignored_index)
-            end_positions.clamp_(0, ignored_index)
-
-            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)
-            start_loss = loss_fct(start_logits, start_positions)
-            end_loss = loss_fct(end_logits, end_positions)
-            answer_type_loss = loss_fct(answer_type_logits, target_type)
-            total_loss = (start_loss + end_loss + answer_type_loss) / 3
-
-        # (loss), start_logits, end_logits, target_type_logits, (hidden_states), (attentions)
-        return_dict = isinstance(model_outputs, ModelOutput)
-        if not return_dict:
-            output = (start_logits, end_logits, answer_type_logits) + model_outputs[2:]
-            return ((total_loss,) + output) if total_loss is not None else output
-
-        return ExtractiveQAModelOutput(
-            loss=total_loss,
-            start_logits=start_logits,
-            end_logits=end_logits,
-            target_type_logits=answer_type_logits,
-            hidden_states=model_outputs.hidden_states,
-            attentions=model_outputs.attentions,
-        )
-
-
-EXTRACTIVE_HEAD = dict(qa_head=ExtractiveQAHead)
-
-
-class ExtractiveQAWithConfidenceHead(AbstractTaskHead):
-    """
-    Task head for extractive Question Answering supporting confidence calibration.
-    """
-    def __init__(self, config: PretrainedConfig, num_labels_override: Optional[int] = None):
-        """
-        Args:
-            config: Language model config.
-            num_labels_override: Set this to override number of answer types from default `len(TargetType)`.
-        """
-        super().__init__(config)
-        self.num_labels = config.num_labels
-        self.qa_outputs = torch.nn.Linear(config.hidden_size, self.num_labels)
-
-        config_for_classification_head = deepcopy(config)
-        if num_labels_override is None:
-            config_for_classification_head.num_labels = len(TargetType)
-        else:
-            config_for_classification_head.num_labels = num_labels_override
-        self.num_classification_head_labels = config_for_classification_head.num_labels
-
-        dropout_names = ["classifier_dropout", "hidden_dropout_prob", "classifier_dropout_prob"]
-        for name in dropout_names:
-            dropout_value = getattr(config_for_classification_head, name, None)
-            if dropout_value is not None:
-                self._logger.info(f"Loading dropout value {dropout_value} from config attribute '{name}'")
-                config_for_classification_head.classifier_dropout = dropout_value
-                break
-        else:
-            self._logger.warning("No dropout value found -- setting to 0")
-            config_for_classification_head.classifier_dropout = 0.
-
-        self.classifier = RobertaClassificationHead(config_for_classification_head)
-
-        self.output_dropout_rate = getattr(config, "output_dropout_rate", 0.25)
-        self.decoding_times_with_dropout = getattr(config, "decoding_times_with_dropout", 5)
-        self.sep_token_id = getattr(config, "sep_token_id", None)
-
-    def forward(self,
-                model_outputs: Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions],
-                input_ids=None,
-                attention_mask=None,
-                token_type_ids=None,
-                start_positions=None,
-                end_positions=None,
-                target_type=None,
-                **kwargs)-> Union[tuple, ExtractiveQAWithConfidenceModelOutput]:
-        """
-        Compute the task head's forward pass.
-
-        Args:
-            model_outputs: Language model outputs.
-            input_ids: Indices of input sequence tokens in the vocabulary.
-            attention_mask: Mask to avoid performing attention on padding token indices.
-            token_type_ids: Segment token indices to indicate question and context portions of the inputs.
-            start_positions: (training only) Ground-truth start positions.
-            end_positions: (training only) Ground-truth end positions.
-            target_type: (training only) Ground-truth target type.
-
-        Returns:
-            Extractive QA task head result in data structure corresponding to type of `model_outputs`.
-        """
-        sequence_output = model_outputs[0]
-
-        # Predict target answer type for the whole question answer pair
-        answer_type_logits = self.classifier(sequence_output)
-
-        # Predict start and end logits by token
-        logits = self.qa_outputs(sequence_output)
-        start_logits, end_logits = logits.split(1, dim=-1)
-        start_logits = start_logits.squeeze(-1)
-        end_logits = end_logits.squeeze(-1)
-
-        total_loss = None
-        start_stdev = None
-        end_stdev = None
-        query_passage_similarity = None
-        if start_positions is not None and end_positions is not None and target_type is not None:
-            # If we are on multi-GPU, split add a dimension
-            if len(start_positions.size()) > 1:
-                start_positions = start_positions.squeeze(-1)
-            if len(end_positions.size()) > 1:
-                end_positions = end_positions.squeeze(-1)
-            if len(target_type.size()) > 1:
-                target_type = target_type.squeeze(-1)
-            # sometimes the start/end positions are outside our model inputs, we ignore these terms
-            ignored_index = start_logits.size(1)
-            start_positions.clamp_(0, ignored_index)
-            end_positions.clamp_(0, ignored_index)
-
-            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)
-            start_loss = loss_fct(start_logits, start_positions)
-            end_loss = loss_fct(end_logits, end_positions)
-            answer_type_loss = loss_fct(answer_type_logits, target_type)
-
-            total_loss = (start_loss + end_loss + answer_type_loss) / 3
-
-        else:
-            # confidence features only generated for test mode
-            # dropout feature
-            lm_output_dropout_fct = torch.nn.Dropout(self.output_dropout_rate)
-            start_logits_with_dropout = []
-            end_logits_with_dropout = []
-            for k in range(self.decoding_times_with_dropout):
-                logits = self.qa_outputs(lm_output_dropout_fct(sequence_output))
-                s_logits, e_logits = logits.split(1, dim=-1)
-                start_logits_with_dropout.append(s_logits.squeeze(-1))
-                end_logits_with_dropout.append(e_logits.squeeze(-1))
-            start_stdev = torch.std(torch.stack(start_logits_with_dropout), axis=0)
-            end_stdev = torch.std(torch.stack(end_logits_with_dropout), axis=0)
-
-            # colbert style feature to measure the similarity between query and passage
-            if attention_mask is not None and token_type_ids is not None:
-                passage_mark = torch.mul(token_type_ids, attention_mask)
-                query_mark = 1 - token_type_ids
-                # the separator (first 0) between query and passage need be masked
-                first_zero = query_mark.sum(dim=1)
-                query_mark[torch.arange(first_zero.size()[0]), (first_zero - 1)] = 0
-#                query_mask[torch.arange(first_zero.size()[0]), (first_zero - 2)] = 0
-                # <cls> need be masked
-                query_mark[:, 0] = 0
-                normalized_output = normalize(sequence_output, p=2.0, dim = 2)
-                # add 1 to product to make the value positive
-                query_passage_product = normalized_output @ normalized_output.permute(0, 2, 1) + 1.0
-                query_passage_similarity = ((query_passage_product * query_mark.unsqueeze(1)).max(2).values
-                                            * passage_mark).sum(-1) / passage_mark.sum(-1)
-            elif input_ids is not None and self.sep_token_id: # Roberta and XLM-R don't use token_type_ids
-                position_ids = torch.arange(input_ids.size()[1]).repeat(input_ids.size()[0], 1).to(input_ids.device)
-                # Find the position of the first spe_id in input_ids which is the end of query
-                first_sep = (input_ids == self.sep_token_id).long().argmax(-1)
-                query_mark = position_ids < first_sep[..., None]
-                # <cls> need be masked
-                query_mark[:, 0] = 0
-                # Find the position of the last sep_id in input_ids which is the end of passage
-                last_sep = (input_ids == self.sep_token_id).long().cumsum(-1).argmax(-1)
-                attention_mark = position_ids < last_sep[..., None]
-                passage_mark_from_left = position_ids > (first_sep + 1)[..., None]
-                passage_mark = torch.mul(passage_mark_from_left, attention_mark)
-                normalized_output = normalize(sequence_output, p=2.0, dim = 2)
-                # add 1 to product to make the value positive
-                query_passage_product = normalized_output @ normalized_output.permute(0, 2, 1) + 1.0
-                query_passage_similarity = ((query_passage_product * query_mark.unsqueeze(1)).max(2).values
-                                            * passage_mark).sum(-1) / passage_mark.sum(-1)
-            else:
-                query_passage_similarity = torch.zeros(sequence_output.size()[0], dtype=start_logits.dtype)
-
-        # (loss), start_logits, end_logits, target_type_logits,
-        # start_stdev, end_stdev, query_passage_similarity,
-        # (hidden_states), (attentions)
-        return_dict = isinstance(model_outputs, ModelOutput)
-        if not return_dict:
-            if start_stdev is not None and end_stdev is not None and query_passage_similarity is not None:
-                output = (start_logits, end_logits, answer_type_logits, start_stdev, end_stdev,
-                          query_passage_similarity) + model_outputs[2:]
-            else:
-                output = (start_logits, end_logits, answer_type_logits) + model_outputs[2:]
-            return ((total_loss,) + output) if total_loss is not None else output
-
-        return ExtractiveQAWithConfidenceModelOutput(
-            loss=total_loss,
-            start_logits=start_logits,
-            end_logits=end_logits,
-            target_type_logits=answer_type_logits,
-            hidden_states=model_outputs.hidden_states,
-            attentions=model_outputs.attentions,
-            start_stdev=start_stdev,
-            end_stdev=end_stdev,
-            query_passage_similarity=query_passage_similarity
-        )
-
-
-EXTRACTIVE_WITH_CONFIDENCE_HEAD = dict(qa_head=ExtractiveQAWithConfidenceHead)
-
-
-
+from copy import deepcopy
+from typing import Union, Optional
+
+import torch
+from torch.nn.functional import normalize
+from transformers import PretrainedConfig
+from transformers.file_utils import ModelOutput
+from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions
+from transformers.models.roberta.modeling_roberta import RobertaClassificationHead
+
+from primeqa.mrc.models.heads.abstract import AbstractTaskHead
+from primeqa.mrc.data_models.model_outputs.extractive import ExtractiveQAModelOutput, ExtractiveQAWithConfidenceModelOutput
+from primeqa.mrc.data_models.target_type import TargetType
+
+
+class ExtractiveQAHead(AbstractTaskHead):
+    """
+    Task head for extractive Question Answering.
+    """
+    def __init__(self, config: PretrainedConfig, num_labels_override: Optional[int] = None):
+        """
+        Args:
+            config: Language model config.
+            num_labels_override: Set this to override number of answer types from default `len(TargetType)`.
+        """
+        super().__init__(config)
+        self.num_labels = config.num_labels
+        self.qa_outputs = torch.nn.Linear(config.hidden_size, self.num_labels)
+
+        config_for_classification_head = deepcopy(config)
+        if num_labels_override is None:
+            config_for_classification_head.num_labels = len(TargetType)
+        else:
+            config_for_classification_head.num_labels = num_labels_override
+        self.num_classification_head_labels = config_for_classification_head.num_labels
+
+        dropout_names = ["classifier_dropout", "hidden_dropout_prob", "classifier_dropout_prob"]
+        for name in dropout_names:
+            dropout_value = getattr(config_for_classification_head, name, None)
+            if dropout_value is not None:
+                self._logger.info(f"Loading dropout value {dropout_value} from config attribute '{name}'")
+                config_for_classification_head.classifier_dropout = dropout_value
+                break
+        else:
+            self._logger.warning("No dropout value found -- setting to 0")
+            config_for_classification_head.classifier_dropout = 0.
+
+        self.classifier = RobertaClassificationHead(config_for_classification_head)
+
+    def forward(self,
+                model_outputs: Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions],
+                start_positions=None,
+                end_positions=None,
+                target_type=None,
+                **kwargs) -> Union[tuple, ExtractiveQAModelOutput]:
+        """
+        Compute the task head's forward pass.
+
+        Args:
+            model_outputs: Language model outputs.
+            start_positions: (training only) Ground-truth start positions.
+            end_positions: (training only) Ground-truth end positions.
+            target_type: (training only) Ground-truth target type.
+
+        Returns:
+            Extractive QA task head result in data structure corresponding to type of `model_outputs`.
+        """
+        sequence_output = model_outputs[0]
+
+        # Predict target answer type for the whole question answer pair
+        answer_type_logits = self.classifier(sequence_output)
+
+        # Predict start and end logits by token
+        logits = self.qa_outputs(sequence_output)
+        start_logits, end_logits = logits.split(1, dim=-1)
+        start_logits = start_logits.squeeze(-1)
+        end_logits = end_logits.squeeze(-1)
+
+        total_loss = None
+        if start_positions is not None and end_positions is not None and target_type is not None:
+            # If we are on multi-GPU, split add a dimension
+            if len(start_positions.size()) > 1:
+                start_positions = start_positions.squeeze(-1)
+            if len(end_positions.size()) > 1:
+                end_positions = end_positions.squeeze(-1)
+            if len(target_type.size()) > 1:
+                target_type = target_type.squeeze(-1)
+            # sometimes the start/end positions are outside our model inputs, we ignore these terms
+            ignored_index = start_logits.size(1)
+            start_positions.clamp_(0, ignored_index)
+            end_positions.clamp_(0, ignored_index)
+
+            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)
+            start_loss = loss_fct(start_logits, start_positions)
+            end_loss = loss_fct(end_logits, end_positions)
+            answer_type_loss = loss_fct(answer_type_logits, target_type)
+            total_loss = (start_loss + end_loss + answer_type_loss) / 3
+
+        # (loss), start_logits, end_logits, target_type_logits, (hidden_states), (attentions)
+        return_dict = isinstance(model_outputs, ModelOutput)
+        if not return_dict:
+            output = (start_logits, end_logits, answer_type_logits) + model_outputs[2:]
+            return ((total_loss,) + output) if total_loss is not None else output
+
+        return ExtractiveQAModelOutput(
+            loss=total_loss,
+            start_logits=start_logits,
+            end_logits=end_logits,
+            target_type_logits=answer_type_logits,
+            hidden_states=model_outputs.hidden_states,
+            attentions=model_outputs.attentions,
+        )
+
+
+EXTRACTIVE_HEAD = dict(qa_head=ExtractiveQAHead)
+
+
+class ExtractiveQAWithConfidenceHead(AbstractTaskHead):
+    """
+    Task head for extractive Question Answering supporting confidence calibration.
+    """
+    def __init__(self, config: PretrainedConfig, num_labels_override: Optional[int] = None):
+        """
+        Args:
+            config: Language model config.
+            num_labels_override: Set this to override number of answer types from default `len(TargetType)`.
+        """
+        super().__init__(config)
+        self.num_labels = config.num_labels
+        self.qa_outputs = torch.nn.Linear(config.hidden_size, self.num_labels)
+
+        config_for_classification_head = deepcopy(config)
+        if num_labels_override is None:
+            config_for_classification_head.num_labels = len(TargetType)
+        else:
+            config_for_classification_head.num_labels = num_labels_override
+        self.num_classification_head_labels = config_for_classification_head.num_labels
+
+        dropout_names = ["classifier_dropout", "hidden_dropout_prob", "classifier_dropout_prob"]
+        for name in dropout_names:
+            dropout_value = getattr(config_for_classification_head, name, None)
+            if dropout_value is not None:
+                self._logger.info(f"Loading dropout value {dropout_value} from config attribute '{name}'")
+                config_for_classification_head.classifier_dropout = dropout_value
+                break
+        else:
+            self._logger.warning("No dropout value found -- setting to 0")
+            config_for_classification_head.classifier_dropout = 0.
+
+        self.classifier = RobertaClassificationHead(config_for_classification_head)
+
+        self.output_dropout_rate = getattr(config, "output_dropout_rate", 0.25)
+        self.decoding_times_with_dropout = getattr(config, "decoding_times_with_dropout", 5)
+        self.sep_token_id = getattr(config, "sep_token_id", None)
+
+    def forward(self,
+                model_outputs: Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions],
+                input_ids=None,
+                attention_mask=None,
+                token_type_ids=None,
+                start_positions=None,
+                end_positions=None,
+                target_type=None,
+                **kwargs)-> Union[tuple, ExtractiveQAWithConfidenceModelOutput]:
+        """
+        Compute the task head's forward pass.
+
+        Args:
+            model_outputs: Language model outputs.
+            input_ids: Indices of input sequence tokens in the vocabulary.
+            attention_mask: Mask to avoid performing attention on padding token indices.
+            token_type_ids: Segment token indices to indicate question and context portions of the inputs.
+            start_positions: (training only) Ground-truth start positions.
+            end_positions: (training only) Ground-truth end positions.
+            target_type: (training only) Ground-truth target type.
+
+        Returns:
+            Extractive QA task head result in data structure corresponding to type of `model_outputs`.
+        """
+        sequence_output = model_outputs[0]
+
+        # Predict target answer type for the whole question answer pair
+        answer_type_logits = self.classifier(sequence_output)
+
+        # Predict start and end logits by token
+        logits = self.qa_outputs(sequence_output)
+        start_logits, end_logits = logits.split(1, dim=-1)
+        start_logits = start_logits.squeeze(-1)
+        end_logits = end_logits.squeeze(-1)
+
+        total_loss = None
+        start_stdev = None
+        end_stdev = None
+        query_passage_similarity = None
+        if start_positions is not None and end_positions is not None and target_type is not None:
+            # If we are on multi-GPU, split add a dimension
+            if len(start_positions.size()) > 1:
+                start_positions = start_positions.squeeze(-1)
+            if len(end_positions.size()) > 1:
+                end_positions = end_positions.squeeze(-1)
+            if len(target_type.size()) > 1:
+                target_type = target_type.squeeze(-1)
+            # sometimes the start/end positions are outside our model inputs, we ignore these terms
+            ignored_index = start_logits.size(1)
+            start_positions.clamp_(0, ignored_index)
+            end_positions.clamp_(0, ignored_index)
+
+            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)
+            start_loss = loss_fct(start_logits, start_positions)
+            end_loss = loss_fct(end_logits, end_positions)
+            answer_type_loss = loss_fct(answer_type_logits, target_type)
+
+            total_loss = (start_loss + end_loss + answer_type_loss) / 3
+
+        else:
+            # confidence features only generated for test mode
+            # dropout feature
+            lm_output_dropout_fct = torch.nn.Dropout(self.output_dropout_rate)
+            start_logits_with_dropout = []
+            end_logits_with_dropout = []
+            for k in range(self.decoding_times_with_dropout):
+                logits = self.qa_outputs(lm_output_dropout_fct(sequence_output))
+                s_logits, e_logits = logits.split(1, dim=-1)
+                start_logits_with_dropout.append(s_logits.squeeze(-1))
+                end_logits_with_dropout.append(e_logits.squeeze(-1))
+            start_stdev = torch.std(torch.stack(start_logits_with_dropout), axis=0)
+            end_stdev = torch.std(torch.stack(end_logits_with_dropout), axis=0)
+
+            # colbert style feature to measure the similarity between query and passage
+            if attention_mask is not None and token_type_ids is not None:
+                passage_mark = torch.mul(token_type_ids, attention_mask)
+                query_mark = 1 - token_type_ids
+                # the separator (first 0) between query and passage need be masked
+                first_zero = query_mark.sum(dim=1)
+                query_mark[torch.arange(first_zero.size()[0]), (first_zero - 1)] = 0
+#                query_mask[torch.arange(first_zero.size()[0]), (first_zero - 2)] = 0
+                # <cls> need be masked
+                query_mark[:, 0] = 0
+                normalized_output = normalize(sequence_output, p=2.0, dim = 2)
+                # add 1 to product to make the value positive
+                query_passage_product = normalized_output @ normalized_output.permute(0, 2, 1) + 1.0
+                query_passage_similarity = ((query_passage_product * query_mark.unsqueeze(1)).max(2).values
+                                            * passage_mark).sum(-1) / passage_mark.sum(-1)
+            elif input_ids is not None and self.sep_token_id: # Roberta and XLM-R don't use token_type_ids
+                position_ids = torch.arange(input_ids.size()[1]).repeat(input_ids.size()[0], 1).to(input_ids.device)
+                # Find the position of the first spe_id in input_ids which is the end of query
+                first_sep = (input_ids == self.sep_token_id).long().argmax(-1)
+                query_mark = position_ids < first_sep[..., None]
+                # <cls> need be masked
+                query_mark[:, 0] = 0
+                # Find the position of the last sep_id in input_ids which is the end of passage
+                last_sep = (input_ids == self.sep_token_id).long().cumsum(-1).argmax(-1)
+                attention_mark = position_ids < last_sep[..., None]
+                passage_mark_from_left = position_ids > (first_sep + 1)[..., None]
+                passage_mark = torch.mul(passage_mark_from_left, attention_mark)
+                normalized_output = normalize(sequence_output, p=2.0, dim = 2)
+                # add 1 to product to make the value positive
+                query_passage_product = normalized_output @ normalized_output.permute(0, 2, 1) + 1.0
+                query_passage_similarity = ((query_passage_product * query_mark.unsqueeze(1)).max(2).values
+                                            * passage_mark).sum(-1) / passage_mark.sum(-1)
+            else:
+                query_passage_similarity = torch.zeros(sequence_output.size()[0], dtype=start_logits.dtype)
+
+        # (loss), start_logits, end_logits, target_type_logits,
+        # start_stdev, end_stdev, query_passage_similarity,
+        # (hidden_states), (attentions)
+        return_dict = isinstance(model_outputs, ModelOutput)
+        if not return_dict:
+            if start_stdev is not None and end_stdev is not None and query_passage_similarity is not None:
+                output = (start_logits, end_logits, answer_type_logits, start_stdev, end_stdev,
+                          query_passage_similarity) + model_outputs[2:]
+            else:
+                output = (start_logits, end_logits, answer_type_logits) + model_outputs[2:]
+            return ((total_loss,) + output) if total_loss is not None else output
+
+        return ExtractiveQAWithConfidenceModelOutput(
+            loss=total_loss,
+            start_logits=start_logits,
+            end_logits=end_logits,
+            target_type_logits=answer_type_logits,
+            hidden_states=model_outputs.hidden_states,
+            attentions=model_outputs.attentions,
+            start_stdev=start_stdev,
+            end_stdev=end_stdev,
+            query_passage_similarity=query_passage_similarity
+        )
+
+
+EXTRACTIVE_WITH_CONFIDENCE_HEAD = dict(qa_head=ExtractiveQAWithConfidenceHead)
+
+
+
```

## primeqa/mrc/processors/postprocessors/abstract.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-import logging
-from abc import ABCMeta, abstractmethod
-from typing import List, Dict, Any
-
-from datasets import Dataset
-
-
-class AbstractPostProcessor(metaclass=ABCMeta):
-    """
-    Base class for post processors.
-    """
-    def __init__(self,
-                 k: int,
-
-                 max_answer_length: int, single_context_multiple_passages: bool = False):
-        """
-        Args:
-            k: Max number of answers to return.
-            max_answer_length: Maximum Answer Length.
-            single_context_multiple_passages: See `AbstractPreProcessor` for more details.
-        """
-        self._logger = logging.getLogger(self.__class__.__name__)
-        self._k = k
-
-        self._max_answer_length = max_answer_length
-        self._single_context_multiple_passages = single_context_multiple_passages
-
-    @abstractmethod
-    def process(self, examples: Dataset, features: Dataset, predictions: tuple):
-        """
-        Convert data and model predictions into MRC answers.
-        """
-        pass
-
-    @abstractmethod
-    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
-        """
-        Convert examples into references for use with metrics.
-        """
-        pass
-
-    @abstractmethod
-    def process_references_and_predictions(self, examples: Dataset, features: Dataset, predictions):
-        """
-        Convert data and model predictions into MRC answers and references for use in metrics.
-        """
-        pass
+import logging
+from abc import ABCMeta, abstractmethod
+from typing import List, Dict, Any
+
+from datasets import Dataset
+
+
+class AbstractPostProcessor(metaclass=ABCMeta):
+    """
+    Base class for post processors.
+    """
+    def __init__(self,
+                 k: int,
+
+                 max_answer_length: int, single_context_multiple_passages: bool = False):
+        """
+        Args:
+            k: Max number of answers to return.
+            max_answer_length: Maximum Answer Length.
+            single_context_multiple_passages: See `AbstractPreProcessor` for more details.
+        """
+        self._logger = logging.getLogger(self.__class__.__name__)
+        self._k = k
+
+        self._max_answer_length = max_answer_length
+        self._single_context_multiple_passages = single_context_multiple_passages
+
+    @abstractmethod
+    def process(self, examples: Dataset, features: Dataset, predictions: tuple):
+        """
+        Convert data and model predictions into MRC answers.
+        """
+        pass
+
+    @abstractmethod
+    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
+        """
+        Convert examples into references for use with metrics.
+        """
+        pass
+
+    @abstractmethod
+    def process_references_and_predictions(self, examples: Dataset, features: Dataset, predictions):
+        """
+        Convert data and model predictions into MRC answers and references for use in metrics.
+        """
+        pass
```

## primeqa/mrc/processors/postprocessors/extractive.py

 * *Ordering differences only*

```diff
@@ -1,283 +1,283 @@
-from collections import defaultdict
-from itertools import groupby
-from operator import itemgetter
-from typing import List, Dict, Any, Tuple
-import os
-
-import sklearn
-from sklearn.neural_network import MLPClassifier
-import joblib
-
-from datasets import Dataset
-from tqdm import tqdm
-import numpy as np
-from datetime import datetime
-import torch
-import logging
-from transformers import EvalPrediction
-
-from primeqa.mrc.processors.postprocessors.abstract import AbstractPostProcessor
-from primeqa.mrc.processors.postprocessors.scorers import initialize_scorer
-from primeqa.mrc.data_models.target_type import TargetType
-from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
-from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers
-from primeqa.calibration.confidence_scorer import ConfidenceScorer
-
-logger = logging.getLogger(__name__)
-
-
-class ExtractivePostProcessor(AbstractPostProcessor):
-    """
-    Post processor for extractive QA (use with `ExtractiveQAHead`).
-    """
-    def __init__(self,
-                 *args,
-                 n_best_size: int,
-                 scorer_type=SupportedSpanScorers.WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF,
-                 output_confidence_feature: bool = False,
-                 confidence_model_path: str = None,
-                 **kwargs):
-        """
-        Args:
-            *args: Arguments for super class constructor.
-            n_best_size: Max number of start/end logits to consider (max values).
-            scorer_type: Scoring algorithm to use.
-            **kwargs: Keyword Arguments for super class constructor.
-        """
-        super().__init__(*args, **kwargs)
-        self._n_best_size = n_best_size
-        self._score_calculator = initialize_scorer(scorer_type)
-        self._output_confidence_feature = output_confidence_feature
-        if confidence_model_path:
-            self._confidence_scorer = ConfidenceScorer(confidence_model_path)
-        else:
-            self._confidence_scorer = None
-
-    def process(self, examples: Dataset, features: Dataset, predictions: Tuple[np.ndarray, np.ndarray, np.ndarray]):
-        features_itr = groupby(features, key=itemgetter('example_idx'))
-        if len(features) != predictions[0].shape[0] and all(
-                p.shape[0] == predictions[0].shape[0] for p in predictions[1:]):
-            raise ValueError(f"Size mismatch withing {len(features)} features and predictions "
-                             f"of first dim {[p.shape[0] for p in predictions]}")
-
-        if self._output_confidence_feature:
-            all_start_logits, all_end_logits, all_targettype_logits, \
-            all_start_stdev, all_end_stdev, all_query_passage_similarity = predictions
-        else:
-            all_start_logits, all_end_logits, all_targettype_logits = predictions
-            all_start_stdev = None
-            all_end_stdev = None
-            all_query_passage_similarity = None
-
-        # The dictionaries we have to fill.
-        all_predictions = {}
-
-        start_idx = 0
-        for example_idx, example in enumerate(tqdm(examples)):
-            feat_example_idx, example_features = next(features_itr)
-            if feat_example_idx != example_idx:
-                raise ValueError(f"Example id mismatch between example ({example['example_id']}) "
-                                 f"and feature ({feat_example_idx})")
-            example_features = list(example_features)
-            example_id = example_features[0]['example_id']
-            contexts = example["context"]
-            example_start_logits = all_start_logits[start_idx:start_idx+len(example_features)]
-            example_end_logits = all_end_logits[start_idx:start_idx+len(example_features)]
-            example_targettype_preds = all_targettype_logits[start_idx:start_idx+len(example_features)]
-
-            if all_start_stdev is not None and all_end_stdev is not None and all_query_passage_similarity is not None:
-                example_start_stdev = all_start_stdev[start_idx:start_idx+len(example_features)]
-                example_end_stdev = all_end_stdev[start_idx:start_idx+len(example_features)]
-                example_query_passage_similarity = all_query_passage_similarity[start_idx:start_idx+len(example_features)]
-            else:
-                example_start_stdev = None
-                example_end_stdev = None
-                example_query_passage_similarity = None
-            start_idx += len(example_features)
-
-            min_null_prediction = None
-            prelim_predictions = []
-
-            for i, input_feature in enumerate(example_features):
-                if input_feature['example_id'] != example_id:
-                    raise ValueError(f"Example id mismatch between example ({example_id}) "
-                                 f"and feature ({input_feature['example_id']})")
-                start_logits = example_start_logits[i].tolist()
-                end_logits = example_end_logits[i].tolist()
-                target_type_logits = example_targettype_preds[i].tolist()
-
-                if example_start_stdev is not None and example_end_stdev is not None \
-                        and example_query_passage_similarity is not None:
-                    start_stdev = example_start_stdev[i].tolist()
-                    end_stdev = example_end_stdev[i].tolist()
-                    query_passage_similarity = float(example_query_passage_similarity[i])
-                else:
-                    start_stdev = [0.0] * len(start_logits)
-                    end_stdev = [0.0] * len(end_logits)
-                    query_passage_similarity = 0.0
-                offset_mapping = input_feature["offset_mapping"]
-
-                token_is_max_context = input_feature.get("token_is_max_context", None)
-                # Update minimum null prediction.
-                feature_null_score = start_logits[0] + end_logits[0]
-                if min_null_prediction is None or min_null_prediction["score"] > feature_null_score:
-                    min_null_prediction = {
-                        "offsets": (0, 0),
-                        "score": feature_null_score,
-                        "start_logit": start_logits[0],
-                        "end_logit": end_logits[0],
-                    }
-
-                start_indexes = np.argsort(start_logits[:len(offset_mapping)])[-1 : -self._n_best_size - 1 : -1].tolist()
-                end_indexes = np.argsort(end_logits[:len(offset_mapping)])[-1 : -self._n_best_size - 1 : -1].tolist()
-                for start_index in start_indexes:
-                    for end_index in end_indexes:
-                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond
-                    # to part of the input_ids that are not in the context.
-                        if (
-                            start_index >= len(offset_mapping)
-                            or end_index >= len(offset_mapping)
-                            or offset_mapping[start_index] is None
-                            or len(offset_mapping[start_index]) < 2
-                            or offset_mapping[end_index] is None
-                            or len(offset_mapping[end_index]) < 2
-                        ):
-                            continue
-                        # Don't consider answers with a length that is either < 0 or > max_answer_length.
-                        if end_index < start_index or end_index - start_index + 1 > self._max_answer_length:
-                            continue
-                        # Don't consider answer that don't have the maximum context available (if such information is
-                        # provided).
-                        if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):
-                            continue
-
-                        start_position = offset_mapping[start_index][0]
-                        end_position = offset_mapping[end_index][1]
-
-                        if self._single_context_multiple_passages:
-                            passage_candidates = example['passage_candidates']
-                            for context_idx in range(len(passage_candidates['start_positions'])):
-                                passage_start_position = passage_candidates['start_positions'][context_idx]
-                                passage_end_position = passage_candidates['end_positions'][context_idx]
-                                if passage_start_position <= start_position <= end_position <= passage_end_position:
-                                    break
-                            else:
-                                context_idx = -1
-                            passage_text = contexts[0]
-                        else:
-                            context_idx = input_feature['context_idx']
-                            passage_text = contexts[context_idx]
-
-                        span_answer_text = passage_text[offset_mapping[start_index][0]:offset_mapping[end_index][1]]
-                        span_answer_score = self._score_calculator(start_logits[start_index] + end_logits[end_index],
-                                                feature_null_score, target_type_logits)
-                        prelim_predictions.append(
-                        {
-                            'example_id': input_feature['example_id'],
-                            'cls_score': feature_null_score,
-                            'start_logit': start_logits[start_index],
-                            'end_logit': end_logits[end_index],
-                            'span_answer': {
-                                "start_position": start_position,
-                                "end_position": end_position,
-                            },
-                            'span_answer_score' : span_answer_score,
-                            'start_index': start_index,
-                            'end_index':   end_index,
-                            'passage_index' : context_idx,
-                            'target_type_logits': target_type_logits,
-                            'span_answer_text': span_answer_text,
-                            'yes_no_answer': int(TargetType.NO_ANSWER),
-                            'start_stdev': start_stdev[start_index],
-                            'end_stdev': end_stdev[end_index],
-                            'query_passage_similarity': query_passage_similarity
-                        }
-                    )
-            example_predictions = sorted(prelim_predictions, key=itemgetter('span_answer_score'), reverse=True)[:self._k]
-            all_predictions[example_id] = example_predictions
-
-            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid
-            # failure.
-            if len(example_predictions) == 0: 
-                logger.info(f'We do not have any non-null predictions for example {example_id}')
-                example_predictions.append( 
-                    {
-                        'example_id': example_id,
-                        'cls_score': 0.0,
-                        'start_logit': 0.0, 
-                        'end_logit': 0.0, 
-                        'span_answer': {'start_position': -1, 'end_position': -1,},
-                        'span_answer_score': 0.0, 
-                        'span_answer_text': "empty", 
-                        'start_index': -1,
-                        'end_index':   -1,
-                        'passage_index' : -1,
-                        'target_type_logits' : [0, 0, 0, 0, 0],
-                        'yes_no_answer': int(TargetType.NO_ANSWER),
-                        'start_stdev': 0,
-                        'end_stdev': 0,
-                        'query_passage_similarity': 0
-                    })
-
-            # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using
-            # the LogSumExp trick).
-            scores = np.array([pred["span_answer_score"] for pred in example_predictions])
-            exp_scores = np.exp(scores - np.max(scores))
-            probs = exp_scores / exp_scores.sum()
-
-            # Include the probabilities in our predictions.
-            for prob, pred in zip(probs, example_predictions):
-                pred["normalized_span_answer_score"] = prob
-
-            # Confidence score
-            if self._confidence_scorer is not None and self._confidence_scorer.model_exists():
-                scores = self._confidence_scorer.predict_scores(example_predictions)
-                for i in range(len(example_predictions)):
-                    example_predictions[i]["confidence_score"] = scores[i]
-            else:
-                for i in range(len(example_predictions)):
-                    example_predictions[i]["confidence_score"] = example_predictions[i]["normalized_span_answer_score"]
-
-        return all_predictions
-        
-    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
-        references = []
-        for example_idx in range(examples.num_rows):
-            example = examples[example_idx]
-            n_annotators = len(example['target']['start_positions'])
-            label = {
-                'start_position': example['target']['start_positions'],
-                'end_position': example['target']['end_positions'],
-                'passage_index': example['target']['passage_indices'],
-                'yes_no_answer': list(map(TargetType.from_bool_label, example['target']['yes_no_answer'])),  # TODO: decide on schema type for bool ans
-                'example_id': [example['example_id']] * n_annotators,
-                'language': [example['language']] * n_annotators,
-                'document_plaintext': [example['document_plaintext']] * n_annotators,
-                'question': [example['question']]  * n_annotators
-            }
-            references.append(label)
-        return references
-
-    def process_references_and_predictions(self, examples, features, predictions) -> EvalPredictionWithProcessing:
-        references = self.prepare_examples_as_references(examples)
-        predictions = self.process(examples, features, predictions)
-        predictions_for_metric = []
-
-        for example_id, preds in predictions.items():
-            top_pred = preds[0]
-            prediction_for_metric = {
-                'example_id': example_id,
-                'start_position': top_pred['span_answer']['start_position'],
-                'end_position': top_pred['span_answer']['end_position'],
-                'passage_index': top_pred['passage_index'],
-                'yes_no_answer': top_pred['yes_no_answer'],
-                'confidence_score': top_pred['span_answer_score']
-            }
-            predictions_for_metric.append(prediction_for_metric)
-
-        # noinspection PyTypeChecker
-        return EvalPredictionWithProcessing(
-            label_ids=references,
-            predictions=predictions,
-            processed_predictions=predictions_for_metric
-        )
+from collections import defaultdict
+from itertools import groupby
+from operator import itemgetter
+from typing import List, Dict, Any, Tuple
+import os
+
+import sklearn
+from sklearn.neural_network import MLPClassifier
+import joblib
+
+from datasets import Dataset
+from tqdm import tqdm
+import numpy as np
+from datetime import datetime
+import torch
+import logging
+from transformers import EvalPrediction
+
+from primeqa.mrc.processors.postprocessors.abstract import AbstractPostProcessor
+from primeqa.mrc.processors.postprocessors.scorers import initialize_scorer
+from primeqa.mrc.data_models.target_type import TargetType
+from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
+from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers
+from primeqa.calibration.confidence_scorer import ConfidenceScorer
+
+logger = logging.getLogger(__name__)
+
+
+class ExtractivePostProcessor(AbstractPostProcessor):
+    """
+    Post processor for extractive QA (use with `ExtractiveQAHead`).
+    """
+    def __init__(self,
+                 *args,
+                 n_best_size: int,
+                 scorer_type=SupportedSpanScorers.WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF,
+                 output_confidence_feature: bool = False,
+                 confidence_model_path: str = None,
+                 **kwargs):
+        """
+        Args:
+            *args: Arguments for super class constructor.
+            n_best_size: Max number of start/end logits to consider (max values).
+            scorer_type: Scoring algorithm to use.
+            **kwargs: Keyword Arguments for super class constructor.
+        """
+        super().__init__(*args, **kwargs)
+        self._n_best_size = n_best_size
+        self._score_calculator = initialize_scorer(scorer_type)
+        self._output_confidence_feature = output_confidence_feature
+        if confidence_model_path:
+            self._confidence_scorer = ConfidenceScorer(confidence_model_path)
+        else:
+            self._confidence_scorer = None
+
+    def process(self, examples: Dataset, features: Dataset, predictions: Tuple[np.ndarray, np.ndarray, np.ndarray]):
+        features_itr = groupby(features, key=itemgetter('example_idx'))
+        if len(features) != predictions[0].shape[0] and all(
+                p.shape[0] == predictions[0].shape[0] for p in predictions[1:]):
+            raise ValueError(f"Size mismatch withing {len(features)} features and predictions "
+                             f"of first dim {[p.shape[0] for p in predictions]}")
+
+        if self._output_confidence_feature:
+            all_start_logits, all_end_logits, all_targettype_logits, \
+            all_start_stdev, all_end_stdev, all_query_passage_similarity = predictions
+        else:
+            all_start_logits, all_end_logits, all_targettype_logits = predictions
+            all_start_stdev = None
+            all_end_stdev = None
+            all_query_passage_similarity = None
+
+        # The dictionaries we have to fill.
+        all_predictions = {}
+
+        start_idx = 0
+        for example_idx, example in enumerate(tqdm(examples)):
+            feat_example_idx, example_features = next(features_itr)
+            if feat_example_idx != example_idx:
+                raise ValueError(f"Example id mismatch between example ({example['example_id']}) "
+                                 f"and feature ({feat_example_idx})")
+            example_features = list(example_features)
+            example_id = example_features[0]['example_id']
+            contexts = example["context"]
+            example_start_logits = all_start_logits[start_idx:start_idx+len(example_features)]
+            example_end_logits = all_end_logits[start_idx:start_idx+len(example_features)]
+            example_targettype_preds = all_targettype_logits[start_idx:start_idx+len(example_features)]
+
+            if all_start_stdev is not None and all_end_stdev is not None and all_query_passage_similarity is not None:
+                example_start_stdev = all_start_stdev[start_idx:start_idx+len(example_features)]
+                example_end_stdev = all_end_stdev[start_idx:start_idx+len(example_features)]
+                example_query_passage_similarity = all_query_passage_similarity[start_idx:start_idx+len(example_features)]
+            else:
+                example_start_stdev = None
+                example_end_stdev = None
+                example_query_passage_similarity = None
+            start_idx += len(example_features)
+
+            min_null_prediction = None
+            prelim_predictions = []
+
+            for i, input_feature in enumerate(example_features):
+                if input_feature['example_id'] != example_id:
+                    raise ValueError(f"Example id mismatch between example ({example_id}) "
+                                 f"and feature ({input_feature['example_id']})")
+                start_logits = example_start_logits[i].tolist()
+                end_logits = example_end_logits[i].tolist()
+                target_type_logits = example_targettype_preds[i].tolist()
+
+                if example_start_stdev is not None and example_end_stdev is not None \
+                        and example_query_passage_similarity is not None:
+                    start_stdev = example_start_stdev[i].tolist()
+                    end_stdev = example_end_stdev[i].tolist()
+                    query_passage_similarity = float(example_query_passage_similarity[i])
+                else:
+                    start_stdev = [0.0] * len(start_logits)
+                    end_stdev = [0.0] * len(end_logits)
+                    query_passage_similarity = 0.0
+                offset_mapping = input_feature["offset_mapping"]
+
+                token_is_max_context = input_feature.get("token_is_max_context", None)
+                # Update minimum null prediction.
+                feature_null_score = start_logits[0] + end_logits[0]
+                if min_null_prediction is None or min_null_prediction["score"] > feature_null_score:
+                    min_null_prediction = {
+                        "offsets": (0, 0),
+                        "score": feature_null_score,
+                        "start_logit": start_logits[0],
+                        "end_logit": end_logits[0],
+                    }
+
+                start_indexes = np.argsort(start_logits[:len(offset_mapping)])[-1 : -self._n_best_size - 1 : -1].tolist()
+                end_indexes = np.argsort(end_logits[:len(offset_mapping)])[-1 : -self._n_best_size - 1 : -1].tolist()
+                for start_index in start_indexes:
+                    for end_index in end_indexes:
+                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond
+                    # to part of the input_ids that are not in the context.
+                        if (
+                            start_index >= len(offset_mapping)
+                            or end_index >= len(offset_mapping)
+                            or offset_mapping[start_index] is None
+                            or len(offset_mapping[start_index]) < 2
+                            or offset_mapping[end_index] is None
+                            or len(offset_mapping[end_index]) < 2
+                        ):
+                            continue
+                        # Don't consider answers with a length that is either < 0 or > max_answer_length.
+                        if end_index < start_index or end_index - start_index + 1 > self._max_answer_length:
+                            continue
+                        # Don't consider answer that don't have the maximum context available (if such information is
+                        # provided).
+                        if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):
+                            continue
+
+                        start_position = offset_mapping[start_index][0]
+                        end_position = offset_mapping[end_index][1]
+
+                        if self._single_context_multiple_passages:
+                            passage_candidates = example['passage_candidates']
+                            for context_idx in range(len(passage_candidates['start_positions'])):
+                                passage_start_position = passage_candidates['start_positions'][context_idx]
+                                passage_end_position = passage_candidates['end_positions'][context_idx]
+                                if passage_start_position <= start_position <= end_position <= passage_end_position:
+                                    break
+                            else:
+                                context_idx = -1
+                            passage_text = contexts[0]
+                        else:
+                            context_idx = input_feature['context_idx']
+                            passage_text = contexts[context_idx]
+
+                        span_answer_text = passage_text[offset_mapping[start_index][0]:offset_mapping[end_index][1]]
+                        span_answer_score = self._score_calculator(start_logits[start_index] + end_logits[end_index],
+                                                feature_null_score, target_type_logits)
+                        prelim_predictions.append(
+                        {
+                            'example_id': input_feature['example_id'],
+                            'cls_score': feature_null_score,
+                            'start_logit': start_logits[start_index],
+                            'end_logit': end_logits[end_index],
+                            'span_answer': {
+                                "start_position": start_position,
+                                "end_position": end_position,
+                            },
+                            'span_answer_score' : span_answer_score,
+                            'start_index': start_index,
+                            'end_index':   end_index,
+                            'passage_index' : context_idx,
+                            'target_type_logits': target_type_logits,
+                            'span_answer_text': span_answer_text,
+                            'yes_no_answer': int(TargetType.NO_ANSWER),
+                            'start_stdev': start_stdev[start_index],
+                            'end_stdev': end_stdev[end_index],
+                            'query_passage_similarity': query_passage_similarity
+                        }
+                    )
+            example_predictions = sorted(prelim_predictions, key=itemgetter('span_answer_score'), reverse=True)[:self._k]
+            all_predictions[example_id] = example_predictions
+
+            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid
+            # failure.
+            if len(example_predictions) == 0: 
+                logger.info(f'We do not have any non-null predictions for example {example_id}')
+                example_predictions.append( 
+                    {
+                        'example_id': example_id,
+                        'cls_score': 0.0,
+                        'start_logit': 0.0, 
+                        'end_logit': 0.0, 
+                        'span_answer': {'start_position': -1, 'end_position': -1,},
+                        'span_answer_score': 0.0, 
+                        'span_answer_text': "empty", 
+                        'start_index': -1,
+                        'end_index':   -1,
+                        'passage_index' : -1,
+                        'target_type_logits' : [0, 0, 0, 0, 0],
+                        'yes_no_answer': int(TargetType.NO_ANSWER),
+                        'start_stdev': 0,
+                        'end_stdev': 0,
+                        'query_passage_similarity': 0
+                    })
+
+            # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using
+            # the LogSumExp trick).
+            scores = np.array([pred["span_answer_score"] for pred in example_predictions])
+            exp_scores = np.exp(scores - np.max(scores))
+            probs = exp_scores / exp_scores.sum()
+
+            # Include the probabilities in our predictions.
+            for prob, pred in zip(probs, example_predictions):
+                pred["normalized_span_answer_score"] = prob
+
+            # Confidence score
+            if self._confidence_scorer is not None and self._confidence_scorer.model_exists():
+                scores = self._confidence_scorer.predict_scores(example_predictions)
+                for i in range(len(example_predictions)):
+                    example_predictions[i]["confidence_score"] = scores[i]
+            else:
+                for i in range(len(example_predictions)):
+                    example_predictions[i]["confidence_score"] = example_predictions[i]["normalized_span_answer_score"]
+
+        return all_predictions
+        
+    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
+        references = []
+        for example_idx in range(examples.num_rows):
+            example = examples[example_idx]
+            n_annotators = len(example['target']['start_positions'])
+            label = {
+                'start_position': example['target']['start_positions'],
+                'end_position': example['target']['end_positions'],
+                'passage_index': example['target']['passage_indices'],
+                'yes_no_answer': list(map(TargetType.from_bool_label, example['target']['yes_no_answer'])),  # TODO: decide on schema type for bool ans
+                'example_id': [example['example_id']] * n_annotators,
+                'language': [example['language']] * n_annotators,
+                'document_plaintext': [example['document_plaintext']] * n_annotators,
+                'question': [example['question']]  * n_annotators
+            }
+            references.append(label)
+        return references
+
+    def process_references_and_predictions(self, examples, features, predictions) -> EvalPredictionWithProcessing:
+        references = self.prepare_examples_as_references(examples)
+        predictions = self.process(examples, features, predictions)
+        predictions_for_metric = []
+
+        for example_id, preds in predictions.items():
+            top_pred = preds[0]
+            prediction_for_metric = {
+                'example_id': example_id,
+                'start_position': top_pred['span_answer']['start_position'],
+                'end_position': top_pred['span_answer']['end_position'],
+                'passage_index': top_pred['passage_index'],
+                'yes_no_answer': top_pred['yes_no_answer'],
+                'confidence_score': top_pred['span_answer_score']
+            }
+            predictions_for_metric.append(prediction_for_metric)
+
+        # noinspection PyTypeChecker
+        return EvalPredictionWithProcessing(
+            label_ids=references,
+            predictions=predictions,
+            processed_predictions=predictions_for_metric
+        )
```

## primeqa/mrc/processors/postprocessors/natural_questions.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-from collections import defaultdict
-from itertools import groupby
-from operator import itemgetter
-from typing import List, Dict, Any, Tuple
-from datasets import Dataset
-from tqdm import tqdm
-import numpy as np
-from transformers import EvalPrediction
-from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
-from primeqa.mrc.data_models.target_type import TargetType
-
-
-class NaturalQuestionsPostProcessor(ExtractivePostProcessor):
-    """
-    Post processor for NQ.
-    """
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-
-    def process(self, examples: Dataset, features: Dataset, predictions: Tuple[np.ndarray, np.ndarray, np.ndarray]):
-        """
-        Adjust answer start/end positions to original document html.
-        The start/end positions return from super().process() point to the context of document tokens.
-        Args:
-            examples: Dataset examples generated by process_train and process_eval of preprocessoor.
-            features: Features generated by process_train and process_eval of preprocessoor.
-            predictions: Prediction output generated by task head.
-        Returns:
-            Predictions with adjusted answer offset.
-        """
-        
-        predictions = super().process(examples, features, predictions)
-
-        for example in examples:
-            example_id = example['example_id']
-            for pred in predictions[example_id]:
-                start_position = pred['span_answer']['start_position']
-                end_position = pred['span_answer']['end_position']
-                start_token_position = example['context_char_to_token'][start_position]
-                end_token_position = example['context_char_to_token'][end_position - 1]
-                new_start_position = example['document_tokens']['start_byte'][start_token_position]
-                new_end_position = example['document_tokens']['end_byte'][end_token_position]
-                pred['span_answer']['start_position'] = new_start_position
-                pred['span_answer']['end_position'] = new_end_position
-
-                pred['passage_index'] = -1
-                passage_candidates = example['passage_candidates']
-                for context_idx in range(len(passage_candidates['start_positions'])):
-                    passage_start_position = passage_candidates['start_positions'][context_idx]
-                    passage_end_position = passage_candidates['end_positions'][context_idx]
-                    if passage_start_position <= new_start_position <= new_end_position <= passage_end_position:
-                        pred['passage_index'] = context_idx
-                        break
-
-        return predictions
-
-
-    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
-        """
-        Prepare reference for each example.
-        document_plaintext and question are not included to comply with the definition of NQLabel in
-        primeqa.mrc.metrics.nq_f1.eval_utils.
-        Args:
-            examples: Dataset examples generated by process_train and process_eval of preprocessoor.
-        Returns:
-            List of answer labels in the format of NQLabel.
-        """
-        references = []
-        for example_idx in range(examples.num_rows):
-            example = examples[example_idx]
-            n_annotators = len(example['target']['start_positions'])
-            label = {
-                'start_position': example['target']['start_positions'],
-                'end_position': example['target']['end_positions'],
-                'passage_index': example['target']['passage_indices'],
-                'yes_no_answer': list(map(TargetType.from_bool_label, example['target']['yes_no_answer'])),  # TODO: decide on schema type for bool ans
-                'example_id': [example['example_id']] * n_annotators,
-                'language': [example['language']] * n_annotators,
-            }
-            references.append(label)
-        return references
+from collections import defaultdict
+from itertools import groupby
+from operator import itemgetter
+from typing import List, Dict, Any, Tuple
+from datasets import Dataset
+from tqdm import tqdm
+import numpy as np
+from transformers import EvalPrediction
+from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
+from primeqa.mrc.data_models.target_type import TargetType
+
+
+class NaturalQuestionsPostProcessor(ExtractivePostProcessor):
+    """
+    Post processor for NQ.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def process(self, examples: Dataset, features: Dataset, predictions: Tuple[np.ndarray, np.ndarray, np.ndarray]):
+        """
+        Adjust answer start/end positions to original document html.
+        The start/end positions return from super().process() point to the context of document tokens.
+        Args:
+            examples: Dataset examples generated by process_train and process_eval of preprocessoor.
+            features: Features generated by process_train and process_eval of preprocessoor.
+            predictions: Prediction output generated by task head.
+        Returns:
+            Predictions with adjusted answer offset.
+        """
+        
+        predictions = super().process(examples, features, predictions)
+
+        for example in examples:
+            example_id = example['example_id']
+            for pred in predictions[example_id]:
+                start_position = pred['span_answer']['start_position']
+                end_position = pred['span_answer']['end_position']
+                start_token_position = example['context_char_to_token'][start_position]
+                end_token_position = example['context_char_to_token'][end_position - 1]
+                new_start_position = example['document_tokens']['start_byte'][start_token_position]
+                new_end_position = example['document_tokens']['end_byte'][end_token_position]
+                pred['span_answer']['start_position'] = new_start_position
+                pred['span_answer']['end_position'] = new_end_position
+
+                pred['passage_index'] = -1
+                passage_candidates = example['passage_candidates']
+                for context_idx in range(len(passage_candidates['start_positions'])):
+                    passage_start_position = passage_candidates['start_positions'][context_idx]
+                    passage_end_position = passage_candidates['end_positions'][context_idx]
+                    if passage_start_position <= new_start_position <= new_end_position <= passage_end_position:
+                        pred['passage_index'] = context_idx
+                        break
+
+        return predictions
+
+
+    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
+        """
+        Prepare reference for each example.
+        document_plaintext and question are not included to comply with the definition of NQLabel in
+        primeqa.mrc.metrics.nq_f1.eval_utils.
+        Args:
+            examples: Dataset examples generated by process_train and process_eval of preprocessoor.
+        Returns:
+            List of answer labels in the format of NQLabel.
+        """
+        references = []
+        for example_idx in range(examples.num_rows):
+            example = examples[example_idx]
+            n_annotators = len(example['target']['start_positions'])
+            label = {
+                'start_position': example['target']['start_positions'],
+                'end_position': example['target']['end_positions'],
+                'passage_index': example['target']['passage_indices'],
+                'yes_no_answer': list(map(TargetType.from_bool_label, example['target']['yes_no_answer'])),  # TODO: decide on schema type for bool ans
+                'example_id': [example['example_id']] * n_annotators,
+                'language': [example['language']] * n_annotators,
+            }
+            references.append(label)
+        return references
```

## primeqa/mrc/processors/postprocessors/scorers.py

 * *Ordering differences only*

```diff
@@ -1,93 +1,93 @@
-import logging
-from functools import partial
-
-from typing import List, Optional, Callable, Union
-from enum import Enum
-
-from primeqa.mrc.data_models.target_type import TargetType
-
-
-class SupportedSpanScorers(Enum):
-    """
-    Enumeration of supported scoring algorithms.
-    """
-    SCORE_DIFF_BASED = 'score_diff_based'
-    TARGET_TYPE_WEIGHTED_SCORE_DIFF = 'target_type_weighted_score_diff'
-    WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF = 'weighted_sum_target_type_and_score_diff'
-
-    @classmethod
-    def get_supported(cls):
-        """
-        Returns the names of the supported scoring algorithms.
-        """
-        return [entry.value for entry in cls]
-
-
-def initialize_scorer(scorer_type: Union[str, SupportedSpanScorers],
-                      target_type_weight: Optional[float] = 0.5) -> Callable:
-    """
-    Factory method to initialize scorer.
-
-    Args:
-        scorer_type: Which scoring algorithm to use.
-        target_type_weight: How much weight [0-1] to put on target type logits vs start/end logits.
-
-    Returns:
-        Initialized scorer.
-    """
-    if not isinstance(scorer_type, SupportedSpanScorers):
-        scorer_type = SupportedSpanScorers(scorer_type)
-    if scorer_type == SupportedSpanScorers.SCORE_DIFF_BASED:
-        logging.debug("\tInitialized scorer %s" % compute_score_diff_between_span_and_cls.__name__)
-        return compute_score_diff_between_span_and_cls
-
-    elif scorer_type == SupportedSpanScorers.TARGET_TYPE_WEIGHTED_SCORE_DIFF:
-        logging.debug("\tInitialized scorer %s" %
-                      compute_short_answer_type_weighted_score_diff_between_span_and_cls.__name__)
-        return compute_short_answer_type_weighted_score_diff_between_span_and_cls
-
-    elif scorer_type == SupportedSpanScorers.WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF:
-        logging.debug(
-            "\tInitialized scorer %s with weight %s for the target type score and weight %s for "
-            "the score diff score" % (
-                compute_weighted_sum_short_answer_type_score_diff_between_span_and_cls.__name__,
-                target_type_weight, 1 - target_type_weight))
-
-        return partial(
-            compute_weighted_sum_short_answer_type_score_diff_between_span_and_cls,
-            target_type_weight=target_type_weight)
-    else:
-        raise ValueError('Unsupported scorer type: %s' % scorer_type)
-
-
-def compute_score_diff_between_span_and_cls(
-        span_score: float, null_span_score: float, *args, **kwargs):
-    """
-    Compute score as difference between span score (e.g. `start=i`, `end=j`) and null span score (e.g. CLS).
-    """
-    return span_score - null_span_score
-
-
-def compute_short_answer_type_weighted_score_diff_between_span_and_cls(
-        span_score: float, null_span_score: float,
-        target_type_logits: List, *args, **kwargs):
-    """
-    Compute score as product of target type logit (for span answer) and `compute_score_diff_between_span_and_cls`
-    """
-    score_diff = compute_score_diff_between_span_and_cls(
-        span_score=span_score, null_span_score=null_span_score)
-    return score_diff * target_type_logits[int(TargetType.SPAN_ANSWER)]
-
-
-def compute_weighted_sum_short_answer_type_score_diff_between_span_and_cls(
-        span_score: float, null_span_score: float,
-        target_type_logits: List, target_type_weight: float, *args, **kwargs):
-    """
-    Compute score as sum of products `target_type_weight` times target type logit (for span answer) and
-    `1 - target_type_weight` times `compute_score_diff_between_span_and_cls`.
-    """
-    score_diff = compute_score_diff_between_span_and_cls(
-        span_score=span_score, null_span_score=null_span_score)
-
-    return (1 - target_type_weight) * score_diff + target_type_weight * target_type_logits[
-        int(TargetType.SPAN_ANSWER)]
+import logging
+from functools import partial
+
+from typing import List, Optional, Callable, Union
+from enum import Enum
+
+from primeqa.mrc.data_models.target_type import TargetType
+
+
+class SupportedSpanScorers(Enum):
+    """
+    Enumeration of supported scoring algorithms.
+    """
+    SCORE_DIFF_BASED = 'score_diff_based'
+    TARGET_TYPE_WEIGHTED_SCORE_DIFF = 'target_type_weighted_score_diff'
+    WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF = 'weighted_sum_target_type_and_score_diff'
+
+    @classmethod
+    def get_supported(cls):
+        """
+        Returns the names of the supported scoring algorithms.
+        """
+        return [entry.value for entry in cls]
+
+
+def initialize_scorer(scorer_type: Union[str, SupportedSpanScorers],
+                      target_type_weight: Optional[float] = 0.5) -> Callable:
+    """
+    Factory method to initialize scorer.
+
+    Args:
+        scorer_type: Which scoring algorithm to use.
+        target_type_weight: How much weight [0-1] to put on target type logits vs start/end logits.
+
+    Returns:
+        Initialized scorer.
+    """
+    if not isinstance(scorer_type, SupportedSpanScorers):
+        scorer_type = SupportedSpanScorers(scorer_type)
+    if scorer_type == SupportedSpanScorers.SCORE_DIFF_BASED:
+        logging.debug("\tInitialized scorer %s" % compute_score_diff_between_span_and_cls.__name__)
+        return compute_score_diff_between_span_and_cls
+
+    elif scorer_type == SupportedSpanScorers.TARGET_TYPE_WEIGHTED_SCORE_DIFF:
+        logging.debug("\tInitialized scorer %s" %
+                      compute_short_answer_type_weighted_score_diff_between_span_and_cls.__name__)
+        return compute_short_answer_type_weighted_score_diff_between_span_and_cls
+
+    elif scorer_type == SupportedSpanScorers.WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF:
+        logging.debug(
+            "\tInitialized scorer %s with weight %s for the target type score and weight %s for "
+            "the score diff score" % (
+                compute_weighted_sum_short_answer_type_score_diff_between_span_and_cls.__name__,
+                target_type_weight, 1 - target_type_weight))
+
+        return partial(
+            compute_weighted_sum_short_answer_type_score_diff_between_span_and_cls,
+            target_type_weight=target_type_weight)
+    else:
+        raise ValueError('Unsupported scorer type: %s' % scorer_type)
+
+
+def compute_score_diff_between_span_and_cls(
+        span_score: float, null_span_score: float, *args, **kwargs):
+    """
+    Compute score as difference between span score (e.g. `start=i`, `end=j`) and null span score (e.g. CLS).
+    """
+    return span_score - null_span_score
+
+
+def compute_short_answer_type_weighted_score_diff_between_span_and_cls(
+        span_score: float, null_span_score: float,
+        target_type_logits: List, *args, **kwargs):
+    """
+    Compute score as product of target type logit (for span answer) and `compute_score_diff_between_span_and_cls`
+    """
+    score_diff = compute_score_diff_between_span_and_cls(
+        span_score=span_score, null_span_score=null_span_score)
+    return score_diff * target_type_logits[int(TargetType.SPAN_ANSWER)]
+
+
+def compute_weighted_sum_short_answer_type_score_diff_between_span_and_cls(
+        span_score: float, null_span_score: float,
+        target_type_logits: List, target_type_weight: float, *args, **kwargs):
+    """
+    Compute score as sum of products `target_type_weight` times target type logit (for span answer) and
+    `1 - target_type_weight` times `compute_score_diff_between_span_and_cls`.
+    """
+    score_diff = compute_score_diff_between_span_and_cls(
+        span_score=span_score, null_span_score=null_span_score)
+
+    return (1 - target_type_weight) * score_diff + target_type_weight * target_type_logits[
+        int(TargetType.SPAN_ANSWER)]
```

## primeqa/mrc/processors/postprocessors/squad.py

 * *Ordering differences only*

```diff
@@ -1,56 +1,56 @@
-from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
-from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
-
-from datasets import Dataset
-from tqdm import tqdm
-
-from typing import List, Dict, Any, Tuple
-
-
-class SQUADPostProcessor(ExtractivePostProcessor):
-    """
-    Post processor for extractive QA (use with `ExtractiveQAHead`).
-    """
-    
-    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
-        references = []
-        for example_idx in range(examples.num_rows):
-            example = examples[example_idx]
-            answers = {"text": example["answer_text"], 
-                       "answer_start": example['target']['start_positions'] }
-            label = {
-                'id': example['example_id'],
-                'answers': answers
-            }
-            references.append(label)
-        return references
-    
-    def prepare_predictions_for_squad(self, examples, predictions):
-        contexts = {}
-        for _, example in enumerate(tqdm(examples)):
-            contexts[example["example_id"]] = example["context"]
-        predictions_for_metric = []
-        for example_id, preds in predictions.items():
-            top_pred = preds[0]
-            context = contexts[example_id][0]
-            prediction_text = context[top_pred['span_answer']['start_position'] : top_pred['span_answer']['end_position']]
-            prediction_for_metric = {
-                'id': example_id,
-                'prediction_text': prediction_text
-            }
-            predictions_for_metric.append(prediction_for_metric)
-        return predictions_for_metric
-    
-    def process_references_and_predictions(self, examples, features, predictions) -> EvalPredictionWithProcessing:
-        references = self.prepare_examples_as_references(examples)        
-        predictions = self.process(examples, features, predictions)
-        predictions_for_metric = self.prepare_predictions_for_squad(examples, predictions)
-
-        # noinspection PyTypeChecker
-        return EvalPredictionWithProcessing(
-            label_ids=references,
-            predictions=predictions,
-            processed_predictions=predictions_for_metric
-        )
-
+from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor
+from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing
+
+from datasets import Dataset
+from tqdm import tqdm
+
+from typing import List, Dict, Any, Tuple
+
+
+class SQUADPostProcessor(ExtractivePostProcessor):
+    """
+    Post processor for extractive QA (use with `ExtractiveQAHead`).
+    """
+    
+    def prepare_examples_as_references(self, examples: Dataset) -> List[Dict[str, Any]]:
+        references = []
+        for example_idx in range(examples.num_rows):
+            example = examples[example_idx]
+            answers = {"text": example["answer_text"], 
+                       "answer_start": example['target']['start_positions'] }
+            label = {
+                'id': example['example_id'],
+                'answers': answers
+            }
+            references.append(label)
+        return references
+    
+    def prepare_predictions_for_squad(self, examples, predictions):
+        contexts = {}
+        for _, example in enumerate(tqdm(examples)):
+            contexts[example["example_id"]] = example["context"]
+        predictions_for_metric = []
+        for example_id, preds in predictions.items():
+            top_pred = preds[0]
+            context = contexts[example_id][0]
+            prediction_text = context[top_pred['span_answer']['start_position'] : top_pred['span_answer']['end_position']]
+            prediction_for_metric = {
+                'id': example_id,
+                'prediction_text': prediction_text
+            }
+            predictions_for_metric.append(prediction_for_metric)
+        return predictions_for_metric
+    
+    def process_references_and_predictions(self, examples, features, predictions) -> EvalPredictionWithProcessing:
+        references = self.prepare_examples_as_references(examples)        
+        predictions = self.process(examples, features, predictions)
+        predictions_for_metric = self.prepare_predictions_for_squad(examples, predictions)
+
+        # noinspection PyTypeChecker
+        return EvalPredictionWithProcessing(
+            label_ids=references,
+            predictions=predictions,
+            processed_predictions=predictions_for_metric
+        )
+
```

## primeqa/mrc/processors/preprocessors/abstract.py

 * *Ordering differences only*

```diff
@@ -1,164 +1,164 @@
-import logging
-from abc import ABCMeta, abstractmethod
-from typing import Optional, Tuple, Type
-
-from datasets.arrow_dataset import Batch
-from transformers import PreTrainedTokenizerFast, BatchEncoding
-from datasets import Dataset
-
-
-class AbstractPreProcessor(metaclass=ABCMeta):
-    """
-    Abstract preprocessor which provides interface for all preprocessors.
-    """
-
-    def __init__(self,
-                 tokenizer: PreTrainedTokenizerFast,
-                 stride: int,
-                 max_seq_len: Optional[int] = None,
-                 negative_sampling_prob_when_has_answer: float = 0.01,
-                 negative_sampling_prob_when_no_answer: float = 0.04,
-                 num_workers: Optional[int] = None,
-                 load_from_cache_file: bool = True,
-                 max_q_char_len: int = 128,
-                 single_context_multiple_passages: bool = False,
-                 max_contexts: Optional[int] = None):
-        """
-        Args:
-            tokenizer:
-                Tokenizer used to prepare model inputs.
-            stride:
-                Step size to move sliding window across context.
-            max_seq_len:
-                Maximum length of question and context inputs to the model (in word pieces/bpes).
-                Uses tokenizer default if not given.
-            negative_sampling_prob_when_has_answer:
-                Probability to select a negative feature from an example which has an answer.
-            negative_sampling_prob_when_no_answer:
-                Probability to select a negative feature from an example which does not have an answer.
-            num_workers:
-                Number of workers to use for preprocessing.
-                Uses all available logical cores by default.
-            load_from_cache_file:
-                Whether to attempt loading features from cache file.
-            max_q_char_len :
-                Max length allowed per question (in characters).
-                Remainder will be trimmed.
-            single_context_multiple_passages:
-                Iff true allow multiple context passages from the same example in the same feature span.
-                Note some preprocessors may override this parameter.
-            max_contexts:
-                Maximum number of contexts to search per example.
-                Remainder will be trimmed.
-                Defaults to searching all contexts.
-        """
-        self._logger = logging.getLogger(self.__class__.__name__)
-        self._tokenizer = tokenizer
-        self._stride = stride
-        self._max_seq_len = max_seq_len
-        self._negative_sampling_prob_when_has_answer = negative_sampling_prob_when_has_answer
-        self._negative_sampling_prob_when_no_answer = negative_sampling_prob_when_no_answer
-        self._num_workers = num_workers
-        self._load_from_cache_file = load_from_cache_file
-        self._max_q_char_len = max_q_char_len
-        self._single_context_multiple_passages = single_context_multiple_passages
-        self._max_contexts = max_contexts
-
-        if not (0. <= self._negative_sampling_prob_when_has_answer <= 1.):
-            raise ValueError(f"Expected 0 <= negative_sampling_prob_when_has_answer <= 1 but got: "
-                             f"{self._negative_sampling_prob_when_has_answer:.02f}")
-
-        if not (0. <= self._negative_sampling_prob_when_has_answer <= 1.):
-            raise ValueError(f"Expected 0 <= negative_sampling_prob_when_no_answer <= 1 but got: "
-                             f"{self._negative_sampling_prob_when_no_answer:.02f}")
-
-    @abstractmethod
-    def process_train(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
-        """
-        Process training examples into features.
-
-        Args:
-            examples: examples to process into features.
-
-        Returns:
-            tuple (examples, features) comprising examples adapted into standardized format and processed input features for model.
-        """
-        pass
-
-    @abstractmethod
-    def process_eval(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
-        """
-        Process eval examples into features.
-
-        Args:
-            examples: examples to process into features.
-
-        Returns:
-            tuple (examples, features) comprising examples adapted into standardized format and processed input features for model.
-        """
-        pass
-
-    @abstractmethod
-    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
-        """
-        Convert dataset into standardized format accepted by the preprocessor.
-        This method will likely need to be overridden when subclassing.
-
-        Args:
-            dataset: data to adapt.
-            is_train: whether the dataset is for training.
-
-        Returns:
-            Adapted dataset.
-        """
-        pass
-
-    @abstractmethod
-    def label_features_for_subsampling(self, tokenized_examples: BatchEncoding, examples: Batch) -> BatchEncoding:
-        """
-        Annotate each training feature with a 'subsample_type' of type `SubsampleType` for subsampling.
-
-        Args:
-            tokenized_examples: featurized examples to annotate.
-            examples: original examples corresponding to the `tokenized_examples` features.
-
-        Returns: `tokenized_examples` annotated with 'subsample_type' for subsampling.
-
-        """
-        pass
-
-    @abstractmethod
-    def subsample_features(self, dataset: Dataset) -> Dataset:
-        """
-        Subsample training features according to 'subsample_type':
-
-        * All positive features are kept.
-        * All negative features from an example that has an answer are kept with probability `self._negative_sampling_prob_when_has_answer`.
-        * All negative features from an example that has no answer are kept with probability `self._negative_sampling_prob_when_no_answer`.
-
-        Args:
-            dataset: features to subsample.
-
-        Returns:
-            subsampled features.
-        """
-        pass
-
-    @abstractmethod
-    def validate_schema(self, dataset: Dataset, is_train: bool, pre_adaptation: bool = True) -> None:
-        """
-        Validate the data schema is correct for this preprocessor.
-
-        Args:
-            dataset: data to validate schema of
-            is_train: whether the data is for training
-            pre_adaptation: whether adapt_dataset has been called. This allows for optional fields
-                            (e.g. example_id) to be imputed during adaptation.
-
-        Returns:
-            None
-
-        Raises:
-            ValueError: The data is not in the correct schema.
-        """
-        pass
+import logging
+from abc import ABCMeta, abstractmethod
+from typing import Optional, Tuple, Type
+
+from datasets.arrow_dataset import Batch
+from transformers import PreTrainedTokenizerFast, BatchEncoding
+from datasets import Dataset
+
+
+class AbstractPreProcessor(metaclass=ABCMeta):
+    """
+    Abstract preprocessor which provides interface for all preprocessors.
+    """
+
+    def __init__(self,
+                 tokenizer: PreTrainedTokenizerFast,
+                 stride: int,
+                 max_seq_len: Optional[int] = None,
+                 negative_sampling_prob_when_has_answer: float = 0.01,
+                 negative_sampling_prob_when_no_answer: float = 0.04,
+                 num_workers: Optional[int] = None,
+                 load_from_cache_file: bool = True,
+                 max_q_char_len: int = 128,
+                 single_context_multiple_passages: bool = False,
+                 max_contexts: Optional[int] = None):
+        """
+        Args:
+            tokenizer:
+                Tokenizer used to prepare model inputs.
+            stride:
+                Step size to move sliding window across context.
+            max_seq_len:
+                Maximum length of question and context inputs to the model (in word pieces/bpes).
+                Uses tokenizer default if not given.
+            negative_sampling_prob_when_has_answer:
+                Probability to select a negative feature from an example which has an answer.
+            negative_sampling_prob_when_no_answer:
+                Probability to select a negative feature from an example which does not have an answer.
+            num_workers:
+                Number of workers to use for preprocessing.
+                Uses all available logical cores by default.
+            load_from_cache_file:
+                Whether to attempt loading features from cache file.
+            max_q_char_len :
+                Max length allowed per question (in characters).
+                Remainder will be trimmed.
+            single_context_multiple_passages:
+                Iff true allow multiple context passages from the same example in the same feature span.
+                Note some preprocessors may override this parameter.
+            max_contexts:
+                Maximum number of contexts to search per example.
+                Remainder will be trimmed.
+                Defaults to searching all contexts.
+        """
+        self._logger = logging.getLogger(self.__class__.__name__)
+        self._tokenizer = tokenizer
+        self._stride = stride
+        self._max_seq_len = max_seq_len
+        self._negative_sampling_prob_when_has_answer = negative_sampling_prob_when_has_answer
+        self._negative_sampling_prob_when_no_answer = negative_sampling_prob_when_no_answer
+        self._num_workers = num_workers
+        self._load_from_cache_file = load_from_cache_file
+        self._max_q_char_len = max_q_char_len
+        self._single_context_multiple_passages = single_context_multiple_passages
+        self._max_contexts = max_contexts
+
+        if not (0. <= self._negative_sampling_prob_when_has_answer <= 1.):
+            raise ValueError(f"Expected 0 <= negative_sampling_prob_when_has_answer <= 1 but got: "
+                             f"{self._negative_sampling_prob_when_has_answer:.02f}")
+
+        if not (0. <= self._negative_sampling_prob_when_has_answer <= 1.):
+            raise ValueError(f"Expected 0 <= negative_sampling_prob_when_no_answer <= 1 but got: "
+                             f"{self._negative_sampling_prob_when_no_answer:.02f}")
+
+    @abstractmethod
+    def process_train(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
+        """
+        Process training examples into features.
+
+        Args:
+            examples: examples to process into features.
+
+        Returns:
+            tuple (examples, features) comprising examples adapted into standardized format and processed input features for model.
+        """
+        pass
+
+    @abstractmethod
+    def process_eval(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
+        """
+        Process eval examples into features.
+
+        Args:
+            examples: examples to process into features.
+
+        Returns:
+            tuple (examples, features) comprising examples adapted into standardized format and processed input features for model.
+        """
+        pass
+
+    @abstractmethod
+    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
+        """
+        Convert dataset into standardized format accepted by the preprocessor.
+        This method will likely need to be overridden when subclassing.
+
+        Args:
+            dataset: data to adapt.
+            is_train: whether the dataset is for training.
+
+        Returns:
+            Adapted dataset.
+        """
+        pass
+
+    @abstractmethod
+    def label_features_for_subsampling(self, tokenized_examples: BatchEncoding, examples: Batch) -> BatchEncoding:
+        """
+        Annotate each training feature with a 'subsample_type' of type `SubsampleType` for subsampling.
+
+        Args:
+            tokenized_examples: featurized examples to annotate.
+            examples: original examples corresponding to the `tokenized_examples` features.
+
+        Returns: `tokenized_examples` annotated with 'subsample_type' for subsampling.
+
+        """
+        pass
+
+    @abstractmethod
+    def subsample_features(self, dataset: Dataset) -> Dataset:
+        """
+        Subsample training features according to 'subsample_type':
+
+        * All positive features are kept.
+        * All negative features from an example that has an answer are kept with probability `self._negative_sampling_prob_when_has_answer`.
+        * All negative features from an example that has no answer are kept with probability `self._negative_sampling_prob_when_no_answer`.
+
+        Args:
+            dataset: features to subsample.
+
+        Returns:
+            subsampled features.
+        """
+        pass
+
+    @abstractmethod
+    def validate_schema(self, dataset: Dataset, is_train: bool, pre_adaptation: bool = True) -> None:
+        """
+        Validate the data schema is correct for this preprocessor.
+
+        Args:
+            dataset: data to validate schema of
+            is_train: whether the data is for training
+            pre_adaptation: whether adapt_dataset has been called. This allows for optional fields
+                            (e.g. example_id) to be imputed during adaptation.
+
+        Returns:
+            None
+
+        Raises:
+            ValueError: The data is not in the correct schema.
+        """
+        pass
```

## primeqa/mrc/processors/preprocessors/base.py

```diff
@@ -1,412 +1,413 @@
-import itertools
-from lib2to3.pgen2.tokenize import tokenize
-import random
-import uuid
-from operator import sub
-from typing import List, Iterable, Tuple, Any, Dict, Union
-
-from datasets.arrow_dataset import Batch
-from transformers import BatchEncoding
-from datasets import Dataset
-from datasets.features.features import Sequence, Value
-
-from primeqa.mrc.processors.preprocessors.abstract import AbstractPreProcessor
-from primeqa.mrc.data_models.subsample_type import SubsampleType
-from primeqa.mrc.data_models.target_type import TargetType
-
-
-class BasePreProcessor(AbstractPreProcessor):
-    """
-    Base class which implements core preprocessing functionality.
-
-    Processes datasets with the following schema.
-
-    * 'question': `str`
-    * 'context': `list[str]`
-
-    Optional fields which will be imputed if not provided:
-
-    * 'example_id': `str`
-    * 'language': `str`
-
-    Required for training data:
-
-    * 'target': `{'start_positions': list[int], 'end_positions': list[int], 'passage_indices': list[int], 'yes_no_answer': list[str] }`
-
-    Required for `single_context_multiple_passages=True`:
-
-    * 'passage_candidates' : `{ 'start_positions': list[int], 'end_positions': list[int] }`
-
-    Notes for subclassing:
-
-    * Override adapt_dataset to format data following above schema
-    * Just before returning dataset from overridden adapt_dataset include line `dataset = super().adapt_dataset(dataset, is_train)`
-    * See `TyDiQAPreProcessor` as an example
-    """
-    _del_keys = ["overflow_to_sample_mapping"]
-    _feature_types = {'question': Value(dtype='string', id=None),
-                      'context': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}
-    _train_feature_types = {
-        'target': {'start_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
-                   'end_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
-                   'passage_indices': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
-                   'yes_no_answer': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}
-    _example_id_type = {'example_id': Value(dtype='string', id=None)}
-    _single_context_type = {'passage_candidates': {
-        'start_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
-        'end_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)
-    }}
-    _language_feature_type = {'language': Value(dtype='string', id=None)}
-
-    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
-        if 'example_id' not in dataset.features:
-            dataset = dataset.map(  # Map instead of add column to allow caching
-                self._insert_example_ids,
-                batched=True,
-                load_from_cache_file=self._load_from_cache_file,
-                num_proc=self._num_workers,
-            )
-        if 'language' not in dataset.features:
-            dataset = dataset.add_column('language', ['UNKNOWN'] * dataset.num_rows)
-        self.validate_schema(dataset, is_train, pre_adaptation=False)
-        return dataset
-
-    def process_train(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
-        return self._process(examples, is_train=True)
-
-    def process_eval(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
-        return self._process(examples, is_train=False)
-
-    def _process(self, examples: Dataset, is_train: bool) -> Tuple[Dataset, Dataset]:
-        """
-        Provides implementation for public processing methods.
-        """
-        examples = self.adapt_dataset(examples, is_train)
-        if examples.num_rows == 0:
-            raise ValueError("No examples to process")
-
-        features = examples.map(
-            self._process_batch,
-            fn_kwargs=dict(is_train=is_train),
-            batched=True,
-            with_indices=True,
-            num_proc=self._num_workers,
-            remove_columns=examples.column_names,
-            load_from_cache_file=self._load_from_cache_file,
-            desc=f"Running tokenizer on {'train' if is_train else 'eval'} dataset",
-        )
-        if is_train:
-            features = self.subsample_features(features)
-        return examples, features
-
-    def _process_batch(self, examples: Batch, indices: List[int], is_train: bool) -> BatchEncoding:
-        """
-        Process a batch of examples into features
-        """
-        examples_question = examples['question']
-        examples_context = examples['context']
-        if isinstance(examples_question, str):  # wrap single (question, [context]) pair in list
-            examples_question = [examples_question]
-            examples_context = [examples_context]
-        examples_question = [q.lstrip()[:self._max_q_char_len] for q in examples_question]
-        
-        # create 1:1 question:context lists
-        expanded_examples_question = []
-        expanded_examples_idx = []
-        for i, (question, context) in enumerate(zip(examples_question, examples_context)):
-            context = self._trim_to_max_contexts(context, examples, i)
-            n_context_for_example = len(context)
-            if self._single_context_multiple_passages and n_context_for_example != 1:
-                raise ValueError("Must have exactly one context for each question "
-                                 "to use single_context_multiple_passages")
-            expanded_examples_question.extend(itertools.repeat(question, n_context_for_example))
-            expanded_examples_idx.extend(itertools.repeat(i, n_context_for_example))
-        expanded_examples_context = list(itertools.chain.from_iterable(examples_context))
-
-        tokenized_examples = self._tokenizer(
-            expanded_examples_question if self._pad_on_right else expanded_examples_context,
-            expanded_examples_context if self._pad_on_right else expanded_examples_question,
-            stride=self._stride,
-            max_length=self._max_seq_len,
-            truncation='only_second' if self._pad_on_right else 'only_first',
-            return_overflowing_tokens=True,
-            return_offsets_mapping=True,
-        )
-
-        tokenized_examples['example_idx'] = [expanded_examples_idx[oidx] for oidx in
-                                             tokenized_examples["overflow_to_sample_mapping"]]
-        tokenized_examples['example_id'] = [examples['example_id'][eidx] for eidx in tokenized_examples['example_idx']]
-
-        if not self._single_context_multiple_passages:  # context_idx only defined in this case
-            spans_per_example = self._generate_previous_spans_per_example(tokenized_examples['example_idx'],
-                                                                          tokenized_examples[
-                                                                              "overflow_to_sample_mapping"])
-            tokenized_examples['context_idx'] = list(
-                map(sub, tokenized_examples["overflow_to_sample_mapping"], spans_per_example))
-
-        if is_train:
-            tokenized_examples = self._create_train_targets(tokenized_examples, examples)
-            tokenized_examples = self.label_features_for_subsampling(tokenized_examples, examples)
-        else:
-            tokenized_examples = self._create_eval_targets(tokenized_examples)
-
-        tokenized_examples['example_idx'] = [indices[eidx] for eidx in tokenized_examples['example_idx']]
-
-        for key in self._del_keys:
-            tokenized_examples.pop(key, None)
-        
-        return tokenized_examples
-
-    def _create_train_targets(self, tokenized_examples: BatchEncoding, examples: Batch) -> BatchEncoding:
-        """
-        Create start/end position and target type targets for training.
-        """
-        target = examples['target']
-
-        # Since one context might give us several features if it has a long context,
-        # and each example can have many contexts, we need a map from a feature to ts corresponding example.
-        # This key gives us just that.
-        example_mapping = tokenized_examples['example_idx']
-        # The offset mappings will give us a map from token to character position in the original context. This will
-        # help us compute the start_positions and end_positions.
-        offset_mapping = tokenized_examples["offset_mapping"]
-
-        # Let's label those examples!
-        tokenized_examples["start_positions"] = []
-        tokenized_examples["end_positions"] = []
-        tokenized_examples["target_type"] = []
-
-        for i, offsets in enumerate(offset_mapping):
-            # We will label impossible answers with the index of the CLS token.
-            input_ids = tokenized_examples["input_ids"][i]
-            cls_index = input_ids.index(self._tokenizer.cls_token_id)
-
-            # Grab the sequence corresponding to that example (to know what is the context and what is the question).
-            sequence_ids = tokenized_examples.sequence_ids(i)
-
-            # One example can give several spans, this is the index of the example containing this span of text.
-            example_index = example_mapping[i]
-            t = target[example_index]
-            passage_index = t['passage_indices'][0]
-            start_position = t['start_positions'][0]
-            end_position = t['end_positions'][0]
-            yes_no_answer = TargetType.from_bool_label(t['yes_no_answer'][0])
-
-            # Start/end character index of the answer in the text.
-            start_char = start_position
-            end_char = end_position
-
-            # Start token index of the current span in the text.
-            token_start_index = 0
-            while sequence_ids[token_start_index] != (1 if self._pad_on_right else 0):
-                token_start_index += 1
-
-            # End token index of the current span in the text.
-            token_end_index = len(input_ids) - 1
-            while sequence_ids[token_end_index] != (1 if self._pad_on_right else 0):
-                token_end_index -= 1
-
-            if passage_index == -1:
-                window_contains_correct_passage = False
-            else:
-                if self._single_context_multiple_passages:
-                    passage_candidates = examples['passage_candidates'][example_index]
-                    passage_start_position = passage_candidates['start_positions'][passage_index]
-                    passage_end_position = passage_candidates['end_positions'][passage_index]
-                    window_contains_correct_passage = self._spans_intersect(
-                        (passage_start_position, passage_end_position),
-                        (offsets[token_start_index][0], offsets[token_end_index][1])
-                    )
-                else:
-                    context_idx = tokenized_examples['context_idx'][i]
-                    window_contains_correct_passage = passage_index == context_idx
-
-            if window_contains_correct_passage and start_position == -1:  # Passage or Y/N Answer
-                tokenized_examples["start_positions"].append(cls_index)
-                tokenized_examples["end_positions"].append(cls_index)
-                tt = yes_no_answer
-                if tt not in (TargetType.YES, TargetType.NO):
-                    tt = TargetType.PASSAGE_ANSWER
-                tokenized_examples["target_type"].append(tt)
-            elif not window_contains_correct_passage or start_position == -1:  # No Answer
-                tokenized_examples["start_positions"].append(cls_index)
-                tokenized_examples["end_positions"].append(cls_index)
-                tokenized_examples["target_type"].append(TargetType.NO_ANSWER)
-            else:  # Span Answer
-                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).
-                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
-                    tokenized_examples["start_positions"].append(cls_index)
-                    tokenized_examples["end_positions"].append(cls_index)
-                    tokenized_examples["target_type"].append(TargetType.PASSAGE_ANSWER)
-                else:
-                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.
-                    # Note: we could go after the last offset if the answer is the last word (edge case).
-                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
-                        token_start_index += 1
-                    tokenized_examples["start_positions"].append(token_start_index - 1)
-                    while offsets[token_end_index][1] >= end_char:
-                        token_end_index -= 1
-                    tokenized_examples["end_positions"].append(token_end_index + 1)
-                    tokenized_examples["target_type"].append(TargetType.SPAN_ANSWER)
-
-        return tokenized_examples
-
-    def _create_eval_targets(self, tokenized_examples: BatchEncoding) -> BatchEncoding:
-        """
-        Adjust offset mapping to prevent predicting invalid offsets.
-        """
-        context_index = 1 if self._pad_on_right else 0
-        for i in range(len(tokenized_examples["input_ids"])):
-            # Grab the sequence corresponding to that example (to know what is the context and what is the question).
-            sequence_ids = tokenized_examples.sequence_ids(i)
-
-            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token
-            # position is part of the context or not.
-            tokenized_examples["offset_mapping"][i] = [
-                (o if sequence_ids[k] == context_index else None)
-                for k, o in enumerate(tokenized_examples["offset_mapping"][i])
-            ]
-
-        return tokenized_examples
-    
-    def label_features_for_subsampling(self, tokenized_examples: BatchEncoding, examples: Batch) -> BatchEncoding:
-        if self._subsample_all_features:
-            self._logger.warning("Keeping all negative training instances -- "
-                                 "this may create an unbalanced training set and increase training time significantly")
-            return tokenized_examples
-        elif self._subsample_no_features:
-            self._logger.warning("Removing all negative training instances -- only positives will be used")
-
-        example_mapping = tokenized_examples['example_idx']
-
-        tokenized_examples['subsample_type'] = []
-        for i in range(len(tokenized_examples['input_ids'])):
-            if tokenized_examples['target_type'][i] != TargetType.NO_ANSWER:
-                st = SubsampleType.POSITIVE
-            else:
-                example_idx = example_mapping[i]
-                passage_mapping = examples['target'][example_idx]['passage_indices']
-                passage_idx = passage_mapping[0]
-                has_answer = passage_idx != -1
-                if has_answer:
-                    st = SubsampleType.NEGATIVE_HAS_ANSWER
-                else:
-                    st = SubsampleType.NEGATIVE_NO_ANSWER
-            tokenized_examples['subsample_type'].append(st)
-
-        return tokenized_examples
-
-    def subsample_features(self, dataset: Dataset) -> Dataset:
-        if self._subsample_all_features:
-            return dataset
-
-        keep_indices = [i for i, st in enumerate(dataset['subsample_type']) if self._keep_feature(st)]
-        try:
-            dataset = dataset.select(keep_indices)
-        except IndexError as ex:
-            raise ValueError("No features remaining after subsampling") from ex
-        dataset = dataset.remove_columns('subsample_type')
-        return dataset
-
-    def _keep_feature(self, st: SubsampleType) -> bool:
-        """
-        Return True iff this training feature should be kept based on the subsample type.
-
-        Raises:
-            NotImplementedError: invalid SubsampleType value.
-        """
-        if st == SubsampleType.POSITIVE:
-            return True
-        elif st == SubsampleType.NEGATIVE_HAS_ANSWER:
-            return random.random() < self._negative_sampling_prob_when_has_answer
-        elif st == SubsampleType.NEGATIVE_NO_ANSWER:
-            return random.random() < self._negative_sampling_prob_when_no_answer
-        else:
-            raise NotImplementedError(f"Unexpected subsample type: {st}")
-
-    @property
-    def _pad_on_right(self) -> bool:
-        """
-        Returns true iff tokenizer pads on right side.
-        """
-        return self._tokenizer.padding_side == "right"
-
-    @property
-    def _subsample_all_features(self) -> bool:
-        """
-        Returns true iff subsampling is configured to keep all features.
-        """
-        return self._negative_sampling_prob_when_has_answer == self._negative_sampling_prob_when_no_answer == 1.
-
-    @property
-    def _subsample_no_features(self) -> bool:
-        """
-        Returns true iff subsampling is configured to keep no negative features.
-        """
-        return self._negative_sampling_prob_when_has_answer == self._negative_sampling_prob_when_no_answer == 0.
-
-    @staticmethod
-    def _generate_previous_spans_per_example(example_idx: List[int], sample_mapping: List[int]) -> Iterable[int]:
-        """
-        Yields cumulative number of spans from previous examples.
-        """
-        group_start_idx = 0
-        for _, group in itertools.groupby(example_idx):
-            group_len = None
-            for group_len, _ in enumerate(group, 1):
-                pass
-            if group_len is None:  # this should never be triggered
-                raise ValueError("Unexpected group length None")
-            yield from itertools.repeat(sample_mapping[group_start_idx], group_len)
-            group_start_idx += group_len
-
-    def validate_schema(self, dataset: Dataset, is_train: bool, pre_adaptation: bool = True) -> None:
-        cls = type(self) if pre_adaptation else BasePreProcessor
-        items = cls._feature_types.items()
-        if is_train:
-            items = itertools.chain(items, cls._train_feature_types.items())
-        if not pre_adaptation:
-            items = itertools.chain(items, cls._example_id_type.items(), cls._language_feature_type.items())
-        if self._single_context_multiple_passages:
-            items = itertools.chain(items, cls._single_context_type.items())
-        for feature_name, feature_type in items:
-            if feature_name not in dataset.features:
-                raise ValueError(f"Expected but did not find feature '{feature_name}' in dataset")
-            elif dataset.features[feature_name] != feature_type:
-                raise ValueError(F"Feature type mismatch for feature '{feature_name}'. "
-                                 F"Expected {feature_type} but found {dataset.features[feature_name]}")
-
-    @staticmethod
-    def _spans_intersect(s1: Tuple[int, int], s2: Tuple[int, int]) -> bool:
-        """
-        Returns true iff two spans s1, s2 intersect.
-        """
-        return (s1[0] <= s2[0] <= s1[1]) or (s2[0] <= s1[0] <= s2[1]) or \
-               (s1[0] <= s2[1] <= s1[1]) or (s2[0] <= s1[1] <= s2[1])
-
-    def _trim_to_max_contexts(self,
-                              context: Union[List[str], List[List[str]]],
-                              examples: Batch,
-                              example_idx: int) -> Union[List[str], List[List[str]]]:
-        """
-        Trims each example to at most max_contexts if it is set.
-        """
-        if self._max_contexts is None:
-            pass
-        elif self._single_context_multiple_passages:
-            passage_candidates = examples['passage_candidates'][example_idx]
-            if len(passage_candidates['start_positions']) > self._max_contexts:
-                context[0] = context[0][:passage_candidates['end_positions'][self._max_contexts - 1]]
-        else:
-            context = context[:self._max_contexts]
-        return context
-
-    @staticmethod
-    def _insert_example_ids(examples: Batch) -> Batch:
-        """
-        Add arbitrary, unique example_ids to examples.
-        """
-        n_examples = len(examples['question'])
-        example_id = [str(uuid.uuid4()) for _ in range(n_examples)]
-        examples['example_id'] = example_id
-        return examples
+import itertools
+from lib2to3.pgen2.tokenize import tokenize
+import random
+import uuid
+from operator import sub
+from typing import List, Iterable, Tuple, Any, Dict, Union
+
+from datasets.arrow_dataset import Batch
+from transformers import BatchEncoding
+from datasets import Dataset
+from datasets.features.features import Sequence, Value
+
+from primeqa.mrc.processors.preprocessors.abstract import AbstractPreProcessor
+from primeqa.mrc.data_models.subsample_type import SubsampleType
+from primeqa.mrc.data_models.target_type import TargetType
+
+
+class BasePreProcessor(AbstractPreProcessor):
+    """
+    Base class which implements core preprocessing functionality.
+
+    Processes datasets with the following schema.
+
+    * 'question': `str`
+    * 'context': `list[str]`
+
+    Optional fields which will be imputed if not provided:
+
+    * 'example_id': `str`
+    * 'language': `str`
+
+    Required for training data:
+
+    * 'target': `{'start_positions': list[int], 'end_positions': list[int], 'passage_indices': list[int], 'yes_no_answer': list[str] }`
+
+    Required for `single_context_multiple_passages=True`:
+
+    * 'passage_candidates' : `{ 'start_positions': list[int], 'end_positions': list[int] }`
+
+    Notes for subclassing:
+
+    * Override adapt_dataset to format data following above schema
+    * Just before returning dataset from overridden adapt_dataset include line `dataset = super().adapt_dataset(dataset, is_train)`
+    * See `TyDiQAPreProcessor` as an example
+    """
+    _del_keys = ["overflow_to_sample_mapping"]
+    _feature_types = {'question': Value(dtype='string', id=None),
+                      'context': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}
+    _train_feature_types = {
+        'target': {'start_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
+                   'end_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
+                   'passage_indices': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
+                   'yes_no_answer': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}
+    _example_id_type = {'example_id': Value(dtype='string', id=None)}
+    _single_context_type = {'passage_candidates': {
+        'start_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
+        'end_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)
+    }}
+    _language_feature_type = {'language': Value(dtype='string', id=None)}
+
+    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
+        if 'example_id' not in dataset.features:
+            dataset = dataset.map(  # Map instead of add column to allow caching
+                self._insert_example_ids,
+                batched=True,
+                load_from_cache_file=self._load_from_cache_file,
+                num_proc=self._num_workers,
+            )
+        if 'language' not in dataset.features:
+            dataset = dataset.add_column('language', ['UNKNOWN'] * dataset.num_rows)
+        self.validate_schema(dataset, is_train, pre_adaptation=False)
+        return dataset
+
+    def process_train(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
+        return self._process(examples, is_train=True)
+
+    def process_eval(self, examples: Dataset) -> Tuple[Dataset, Dataset]:
+        return self._process(examples, is_train=False)
+
+    def _process(self, examples: Dataset, is_train: bool) -> Tuple[Dataset, Dataset]:
+        """
+        Provides implementation for public processing methods.
+        """
+        examples = self.adapt_dataset(examples, is_train)
+        if examples.num_rows == 0:
+            raise ValueError("No examples to process")
+
+        features = examples.map(
+            self._process_batch,
+            fn_kwargs=dict(is_train=is_train),
+            batched=True,
+            with_indices=True,
+            num_proc=self._num_workers,
+            remove_columns=examples.column_names,
+            load_from_cache_file=self._load_from_cache_file,
+            desc=f"Running tokenizer on {'train' if is_train else 'eval'} dataset",
+        )
+        if is_train:
+            features = self.subsample_features(features)
+        return examples, features
+
+    def _process_batch(self, examples: Batch, indices: List[int], is_train: bool) -> BatchEncoding:
+        """
+        Process a batch of examples into features
+        """
+        examples_question = examples['question']
+        examples_context = examples['context']
+        if isinstance(examples_question, str):  # wrap single (question, [context]) pair in list
+            examples_question = [examples_question]
+            examples_context = [examples_context]
+        examples_question = [q.lstrip()[:self._max_q_char_len] for q in examples_question]
+        
+        # create 1:1 question:context lists
+        expanded_examples_question = []
+        expanded_examples_idx = []
+        for i, (question, context) in enumerate(zip(examples_question, examples_context)):
+            context = self._trim_to_max_contexts(context, examples, i)
+            n_context_for_example = len(context)
+            if self._single_context_multiple_passages and n_context_for_example != 1:
+                raise ValueError("Must have exactly one context for each question "
+                                 "to use single_context_multiple_passages")
+            expanded_examples_question.extend(itertools.repeat(question, n_context_for_example))
+            expanded_examples_idx.extend(itertools.repeat(i, n_context_for_example))
+        expanded_examples_context = list(itertools.chain.from_iterable(examples_context))
+
+        tokenized_examples = self._tokenizer(
+            expanded_examples_question if self._pad_on_right else expanded_examples_context,
+            expanded_examples_context if self._pad_on_right else expanded_examples_question,
+            stride=self._stride,
+            max_length=self._max_seq_len,
+            truncation='only_second' if self._pad_on_right else 'only_first',
+            return_overflowing_tokens=True,
+            return_offsets_mapping=True,
+        )
+
+        tokenized_examples['example_idx'] = [expanded_examples_idx[oidx] for oidx in
+                                             tokenized_examples["overflow_to_sample_mapping"]]
+        tokenized_examples['example_id'] = [examples['example_id'][eidx] for eidx in tokenized_examples['example_idx']]
+
+        if not self._single_context_multiple_passages:  # context_idx only defined in this case
+            spans_per_example = self._generate_previous_spans_per_example(tokenized_examples['example_idx'],
+                                                                          tokenized_examples[
+                                                                              "overflow_to_sample_mapping"])
+            tokenized_examples['context_idx'] = list(
+                map(sub, tokenized_examples["overflow_to_sample_mapping"], spans_per_example))
+
+        if is_train:
+            tokenized_examples = self._create_train_targets(tokenized_examples, examples)
+            tokenized_examples = self.label_features_for_subsampling(tokenized_examples, examples)
+        else:
+            tokenized_examples = self._create_eval_targets(tokenized_examples)
+
+        tokenized_examples['example_idx'] = [indices[eidx] for eidx in tokenized_examples['example_idx']]
+
+        for key in self._del_keys:
+            tokenized_examples.pop(key, None)
+        
+        return tokenized_examples
+
+    def _create_train_targets(self, tokenized_examples: BatchEncoding, examples: Batch) -> BatchEncoding:
+        """
+        Create start/end position and target type targets for training.
+        """
+        target = examples['target']
+
+        # Since one context might give us several features if it has a long context,
+        # and each example can have many contexts, we need a map from a feature to ts corresponding example.
+        # This key gives us just that.
+        example_mapping = tokenized_examples['example_idx']
+        # The offset mappings will give us a map from token to character position in the original context. This will
+        # help us compute the start_positions and end_positions.
+        offset_mapping = tokenized_examples["offset_mapping"]
+
+        # Let's label those examples!
+        tokenized_examples["start_positions"] = []
+        tokenized_examples["end_positions"] = []
+        tokenized_examples["target_type"] = []
+
+        for i, offsets in enumerate(offset_mapping):
+            # We will label impossible answers with the index of the CLS token.
+            input_ids = tokenized_examples["input_ids"][i]
+            cls_index = input_ids.index(self._tokenizer.cls_token_id)
+
+            # Grab the sequence corresponding to that example (to know what is the context and what is the question).
+            sequence_ids = tokenized_examples.sequence_ids(i)
+
+            # One example can give several spans, this is the index of the example containing this span of text.
+            example_index = example_mapping[i]
+            t = target[example_index]
+            passage_index = t['passage_indices'][0]
+            start_position = t['start_positions'][0]
+            end_position = t['end_positions'][0]
+            yes_no_answer = TargetType.from_bool_label(t['yes_no_answer'][0])
+
+            # Start/end character index of the answer in the text.
+            start_char = start_position
+            end_char = end_position
+
+            # Start token index of the current span in the text.
+            token_start_index = 0
+            while sequence_ids[token_start_index] != (1 if self._pad_on_right else 0):
+                token_start_index += 1
+
+            # End token index of the current span in the text.
+            token_end_index = len(input_ids) - 1
+            while sequence_ids[token_end_index] != (1 if self._pad_on_right else 0):
+                token_end_index -= 1
+
+            if passage_index == -1:
+                window_contains_correct_passage = False
+            else:
+                if self._single_context_multiple_passages:
+                    passage_candidates = examples['passage_candidates'][example_index]
+                    passage_start_position = passage_candidates['start_positions'][passage_index]
+                    passage_end_position = passage_candidates['end_positions'][passage_index]
+                    window_contains_correct_passage = self._spans_intersect(
+                        (passage_start_position, passage_end_position),
+                        (offsets[token_start_index][0], offsets[token_end_index][1])
+                    )
+                else:
+                    context_idx = tokenized_examples['context_idx'][i]
+                    window_contains_correct_passage = passage_index == context_idx
+
+            if window_contains_correct_passage and start_position == -1:  # Passage or Y/N Answer
+                tokenized_examples["start_positions"].append(cls_index)
+                tokenized_examples["end_positions"].append(cls_index)
+                tt = yes_no_answer
+                if tt not in (TargetType.YES, TargetType.NO):
+                    tt = TargetType.PASSAGE_ANSWER
+                tokenized_examples["target_type"].append(tt)
+            elif not window_contains_correct_passage or start_position == -1:  # No Answer
+                tokenized_examples["start_positions"].append(cls_index)
+                tokenized_examples["end_positions"].append(cls_index)
+                tokenized_examples["target_type"].append(TargetType.NO_ANSWER)
+            else:  # Span Answer
+                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).
+                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
+                    tokenized_examples["start_positions"].append(cls_index)
+                    tokenized_examples["end_positions"].append(cls_index)
+                    tokenized_examples["target_type"].append(TargetType.PASSAGE_ANSWER)
+                else:
+                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.
+                    # Note: we could go after the last offset if the answer is the last word (edge case).
+                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
+                        token_start_index += 1
+                    tokenized_examples["start_positions"].append(token_start_index - 1)
+                    while offsets[token_end_index][1] >= end_char:
+                        token_end_index -= 1
+                    tokenized_examples["end_positions"].append(token_end_index + 1)
+                    tokenized_examples["target_type"].append(TargetType.SPAN_ANSWER)
+
+        return tokenized_examples
+
+    def _create_eval_targets(self, tokenized_examples: BatchEncoding) -> BatchEncoding:
+        """
+        Adjust offset mapping to prevent predicting invalid offsets.
+        """
+        context_index = 1 if self._pad_on_right else 0
+        for i in range(len(tokenized_examples["input_ids"])):
+            # Grab the sequence corresponding to that example (to know what is the context and what is the question).
+            sequence_ids = tokenized_examples.sequence_ids(i)
+
+            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token
+            # position is part of the context or not.
+            tokenized_examples["offset_mapping"][i] = [
+                (o if sequence_ids[k] == context_index else None)
+                for k, o in enumerate(tokenized_examples["offset_mapping"][i])
+            ]
+
+        return tokenized_examples
+    
+    def label_features_for_subsampling(self, tokenized_examples: BatchEncoding, examples: Batch) -> BatchEncoding:
+        if self._subsample_all_features:
+            self._logger.warning("Keeping all negative training instances -- "
+                                 "this may create an unbalanced training set and increase training time significantly")
+            return tokenized_examples
+        elif self._subsample_no_features:
+            self._logger.warning("Removing all negative training instances -- only positives will be used")
+
+        example_mapping = tokenized_examples['example_idx']
+
+        tokenized_examples['subsample_type'] = []
+        for i in range(len(tokenized_examples['input_ids'])):
+            if tokenized_examples['target_type'][i] != TargetType.NO_ANSWER:
+                st = SubsampleType.POSITIVE
+            else:
+                example_idx = example_mapping[i]
+                passage_mapping = examples['target'][example_idx]['passage_indices']
+                passage_idx = passage_mapping[0]
+                has_answer = passage_idx != -1
+                if has_answer:
+                    st = SubsampleType.NEGATIVE_HAS_ANSWER
+                else:
+                    st = SubsampleType.NEGATIVE_NO_ANSWER
+            tokenized_examples['subsample_type'].append(st)
+
+        return tokenized_examples
+
+    def subsample_features(self, dataset: Dataset) -> Dataset:
+        if self._subsample_all_features:
+            return dataset
+
+        keep_indices = [i for i, st in enumerate(dataset['subsample_type']) if self._keep_feature(st)]
+        if len(keep_indices) == 0:
+             raise ValueError("No features remaining after subsampling")
+
+        dataset = dataset.select(keep_indices)
+
+        dataset = dataset.remove_columns('subsample_type')
+        return dataset
+
+    def _keep_feature(self, st: SubsampleType) -> bool:
+        """
+        Return True iff this training feature should be kept based on the subsample type.
+
+        Raises:
+            NotImplementedError: invalid SubsampleType value.
+        """
+        if st == SubsampleType.POSITIVE:
+            return True
+        elif st == SubsampleType.NEGATIVE_HAS_ANSWER:
+            return random.random() < self._negative_sampling_prob_when_has_answer
+        elif st == SubsampleType.NEGATIVE_NO_ANSWER:
+            return random.random() < self._negative_sampling_prob_when_no_answer
+        else:
+            raise NotImplementedError(f"Unexpected subsample type: {st}")
+
+    @property
+    def _pad_on_right(self) -> bool:
+        """
+        Returns true iff tokenizer pads on right side.
+        """
+        return self._tokenizer.padding_side == "right"
+
+    @property
+    def _subsample_all_features(self) -> bool:
+        """
+        Returns true iff subsampling is configured to keep all features.
+        """
+        return self._negative_sampling_prob_when_has_answer == self._negative_sampling_prob_when_no_answer == 1.
+
+    @property
+    def _subsample_no_features(self) -> bool:
+        """
+        Returns true iff subsampling is configured to keep no negative features.
+        """
+        return self._negative_sampling_prob_when_has_answer == self._negative_sampling_prob_when_no_answer == 0.
+
+    @staticmethod
+    def _generate_previous_spans_per_example(example_idx: List[int], sample_mapping: List[int]) -> Iterable[int]:
+        """
+        Yields cumulative number of spans from previous examples.
+        """
+        group_start_idx = 0
+        for _, group in itertools.groupby(example_idx):
+            group_len = None
+            for group_len, _ in enumerate(group, 1):
+                pass
+            if group_len is None:  # this should never be triggered
+                raise ValueError("Unexpected group length None")
+            yield from itertools.repeat(sample_mapping[group_start_idx], group_len)
+            group_start_idx += group_len
+
+    def validate_schema(self, dataset: Dataset, is_train: bool, pre_adaptation: bool = True) -> None:
+        cls = type(self) if pre_adaptation else BasePreProcessor
+        items = cls._feature_types.items()
+        if is_train:
+            items = itertools.chain(items, cls._train_feature_types.items())
+        if not pre_adaptation:
+            items = itertools.chain(items, cls._example_id_type.items(), cls._language_feature_type.items())
+        if self._single_context_multiple_passages:
+            items = itertools.chain(items, cls._single_context_type.items())
+        for feature_name, feature_type in items:
+            if feature_name not in dataset.features:
+                raise ValueError(f"Expected but did not find feature '{feature_name}' in dataset")
+            elif dataset.features[feature_name] != feature_type:
+                raise ValueError(F"Feature type mismatch for feature '{feature_name}'. "
+                                 F"Expected {feature_type} but found {dataset.features[feature_name]}")
+
+    @staticmethod
+    def _spans_intersect(s1: Tuple[int, int], s2: Tuple[int, int]) -> bool:
+        """
+        Returns true iff two spans s1, s2 intersect.
+        """
+        return (s1[0] <= s2[0] <= s1[1]) or (s2[0] <= s1[0] <= s2[1]) or \
+               (s1[0] <= s2[1] <= s1[1]) or (s2[0] <= s1[1] <= s2[1])
+
+    def _trim_to_max_contexts(self,
+                              context: Union[List[str], List[List[str]]],
+                              examples: Batch,
+                              example_idx: int) -> Union[List[str], List[List[str]]]:
+        """
+        Trims each example to at most max_contexts if it is set.
+        """
+        if self._max_contexts is None:
+            pass
+        elif self._single_context_multiple_passages:
+            passage_candidates = examples['passage_candidates'][example_idx]
+            if len(passage_candidates['start_positions']) > self._max_contexts:
+                context[0] = context[0][:passage_candidates['end_positions'][self._max_contexts - 1]]
+        else:
+            context = context[:self._max_contexts]
+        return context
+
+    @staticmethod
+    def _insert_example_ids(examples: Batch) -> Batch:
+        """
+        Add arbitrary, unique example_ids to examples.
+        """
+        n_examples = len(examples['question'])
+        example_id = [str(uuid.uuid4()) for _ in range(n_examples)]
+        examples['example_id'] = example_id
+        return examples
```

## primeqa/mrc/processors/preprocessors/natural_questions.py

 * *Ordering differences only*

```diff
@@ -1,233 +1,233 @@
-import functools
-from operator import itemgetter
-from typing import Optional, List
-from transformers import BatchEncoding
-from datasets import Dataset
-from datasets.arrow_dataset import Example, Batch
-from datasets.features.features import Sequence, Value, ClassLabel
-from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
-
-
-
-class NaturalQuestionsPreProcessor(BasePreProcessor):
-    """
-    Preprocessor for NQ data.
-    Note this preprocessor only supports `single_context_multiple_passages=True` and will
-    override the value accordingly.
-    """
-
-    _feature_types = {'question': {'text': Value(dtype='string', id=None),
-                                   'tokens': Sequence(feature=Value(dtype='string', id=None),
-                                                       length=-1, id=None)},
-                      'document': {'html': Value(dtype='string', id=None),
-                                   'title': Value(dtype='string', id=None),
-                                   'tokens': Sequence(feature={'is_html': Value(dtype='bool', id=None),
-                                                               'token': Value(dtype='string', id=None),
-                                                               "start_byte": Value("int64"),
-                                                               "end_byte": Value("int64")},
-                                                    length=-1, id=None),
-                                   'url': Value(dtype='string', id=None)}}
-    _train_feature_types = {
-        'annotations': Sequence(feature={
-            'id': Value(dtype='string', id=None),
-             'long_answer': {'end_byte': Value(dtype='int64', id=None),
-                             'end_token': Value(dtype='int64', id=None),
-                             'start_byte': Value(dtype='int64', id=None),
-                             'start_token': Value(dtype='int64', id=None),
-                             "candidate_index": Value("int64")},
-            'short_answers': Sequence(feature={'end_byte': Value(dtype='int64', id=None),
-                                               'end_token': Value(dtype='int64', id=None),
-                                               'start_byte': Value(dtype='int64', id=None),
-                                               'start_token': Value(dtype='int64', id=None),
-                                               'text': Value(dtype='string', id=None)},
-                                      length=-1, id=None),
-            'yes_no_answer': ClassLabel(num_classes=2, names=['NO', 'YES'], id=None)},
-            length=-1, id=None),
-    }
-    _single_context_type = {
-        'long_answer_candidates': Sequence(feature={'end_byte': Value(dtype='int64', id=None),
-                                                    'end_token': Value(dtype='int64', id=None),
-                                                    'start_byte': Value(dtype='int64', id=None),
-                                                    'start_token': Value(dtype='int64', id=None),
-                                                    'top_level': Value(dtype='bool', id=None)},
-                                  length=-1, id=None),
-    }
-    _yes_no_answer_to_str = {-1: 'NONE', 0: 'NO', 1: 'YES'}
-    _LIST_TAGS = {'<ol', '<ul', '<dl', '<li', '<dd', '<dt'}
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        if not self._single_context_multiple_passages:
-            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
-            self._single_context_multiple_passages = True
-
-
-    def adapt_dataset(self, dataset: Dataset, is_train: bool, keep_html: bool=True) -> Dataset:
-        """
-        Process dataset examples to rename fields, create context and set answer offset.
-        Args:
-             dataset: Dataset to be processed.
-             is_train: True for training otherwise False.
-             keep_html: True if keep html token in context otherwise false.
-        Returns:
-             Precossed dataset.
-        """
-        
-        self.validate_schema(dataset, is_train)
-        dataset = dataset.map(
-            functools.partial(self._rename_examples_create_context_and_adjust_offset, is_train=is_train, keep_html=keep_html),
-            load_from_cache_file=self._load_from_cache_file,
-            num_proc=self._num_workers
-        )
-        dataset = super().adapt_dataset(dataset, is_train)
-        return dataset
-
-
-    def _rename_examples_create_context_and_adjust_offset(self, example: Example, is_train: bool, keep_html: bool=True):
-        """
-        Rename examples to BasePreProcessor schema,
-        create context from document token,
-        and set the start/end positions of target and passage candidates to the new context.
-        
-        Args:
-             example: Dataset example.
-             is_train: True for training otherwise False.
-             keep_html: True if keep html token in context otherwise false.
-        Returns:
-             Precossed example.
-        """
-
-        # rename example
-        example['example_id'] = example['id']
-        example['question'] = example['question']['text']
-        example['language'] = 'english'
-        example['document_html'] = example['document']['html']
-        example['document_tokens'] = example['document']['tokens']
-
-        passage_candidates = {}
-        passage_candidates['start_positions'] = example['long_answer_candidates']['start_byte']
-        passage_candidates['end_positions'] = example['long_answer_candidates']['end_byte']
-        example['passage_candidates'] = passage_candidates
-        del example['long_answer_candidates']
-
-        example['target'] = self.get_annotations(example['annotations'], example['passage_candidates'])
-
-        # create context from document tokens, and build alignment between char and token.
-        context = ""
-        char_to_token = []
-        token_to_char = []
-        num_tokens = len(example['document_tokens']['token'])
-
-        for i in range(num_tokens):
-            if not keep_html and example['document_tokens']['is_html'][i]:
-                token_to_char.append(-1)
-                continue
-            if context:
-                char_to_token.append(-1)
-                context += ' '
-            for j in range(len(example['document_tokens']['token'][i])):
-                char_to_token.append(i)
-            token_to_char.append(len(context))
-            context += example['document_tokens']['token'][i]
-
-        example['context'] = [context]
-        example['context_char_to_token'] = char_to_token
-        example['context_token_to_char'] = token_to_char
-
-        # change target offsets to document token based context (only needed by training)
-        if not is_train:
-           return example
-
-        for i in range(len(example['target']['passage_indices'])):
-            pidx = example['target']['passage_indices'][i]
-            if pidx == -1 or example['target']['start_positions'][i] == -1:
-                continue
-
-            for j in range(num_tokens):
-                if example['context_token_to_char'][j] == -1:
-                    continue
-                if example['document_tokens']['start_byte'][j] >= example['target']['start_positions'][i]:
-                    break
-            if j < num_tokens:
-                example['target']['start_positions'][i] = example['context_token_to_char'][j]
-            else:
-                raise ValueError('Start position of short answer can not be set to the token based context.')
-
-            for j in range(num_tokens - 1, -1, -1):
-                if example['context_token_to_char'][j] == -1:
-                    continue
-                if example['document_tokens']['end_byte'][j] <= example['target']['end_positions'][i]:
-                    break
-            if j >= 0:
-                example['target']['end_positions'][i] = example['context_token_to_char'][j] + \
-                                                        len(example['document_tokens']['token'][j])
-            else:
-                raise ValueError('End position of short answer can not be set to the token based context.')
-
-        num_passages = len(example['passage_candidates']['start_positions'])
-        for i in range(num_passages):
-            passage_start_position = example['passage_candidates']['start_positions'][i]
-            passage_end_position = example['passage_candidates']['end_positions'][i]
-
-            for j in range(num_tokens):
-                if example['context_token_to_char'][j] == -1:
-                    continue
-                if example['document_tokens']['start_byte'][j] >= passage_start_position:
-                    break
-            if j < num_tokens:
-                example['passage_candidates']['start_positions'][i] = example['context_token_to_char'][j]
-            else:
-                raise ValueError('Start position of passage candidate can not be set to the token based context.')
-
-            for j in range(num_tokens - 1, -1, -1):
-                if example['context_token_to_char'][j] == -1:
-                    continue
-                if example['document_tokens']['end_byte'][j] <= passage_end_position:
-                    break
-            if j >= 0:
-                example['passage_candidates']['end_positions'][i] = example['context_token_to_char'][j] + \
-                                                                    len(example['document_tokens']['token'][j])
-            else:
-                raise ValueError('End position of passage candidate can not be set to the token based context.')
-
-        return example
-
-
-    def get_annotations(self, annotations, paragraphs):
-        """
-        Process NQ annotations into preprocessor format.
-        Args:
-             annotations: Annotations of NQ example.
-             paragraphs: Passage_candidates of NQ example.
-        Returns:
-             Annotations in preprocessor format.
-        """
-
-        nq_annotations = {}
-        nq_annotations['end_positions'] = []
-        nq_annotations['start_positions'] = []
-        nq_annotations['passage_indices'] = []
-        nq_annotations['yes_no_answer'] = []
-        for i in range(len(annotations['id'])):
-            start_byte, end_byte = annotations['short_answers'][i]['start_byte'],annotations['short_answers'][i]['end_byte']
-            
-            if len(start_byte) == 0:
-                start_byte = -1
-                end_byte = -1
-                candidate_index = -1
-            else:
-                start_byte = start_byte[0]
-                end_byte = end_byte[0]
-                candidate_index = annotations['long_answer'][i]['candidate_index']
-            if end_byte < start_byte:
-                self._logger.error("end_byte < start_byte")
-
-            yes_no_answer = annotations['yes_no_answer'][i]
-            yes_no_answer = self._yes_no_answer_to_str[yes_no_answer]
-                
-            nq_annotations['end_positions'].append(end_byte)
-            nq_annotations['start_positions'].append(start_byte)
-            nq_annotations['passage_indices'].append(candidate_index)
-            nq_annotations['yes_no_answer'].append(yes_no_answer)
-        return nq_annotations
-
+import functools
+from operator import itemgetter
+from typing import Optional, List
+from transformers import BatchEncoding
+from datasets import Dataset
+from datasets.arrow_dataset import Example, Batch
+from datasets.features.features import Sequence, Value, ClassLabel
+from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
+
+
+
+class NaturalQuestionsPreProcessor(BasePreProcessor):
+    """
+    Preprocessor for NQ data.
+    Note this preprocessor only supports `single_context_multiple_passages=True` and will
+    override the value accordingly.
+    """
+
+    _feature_types = {'question': {'text': Value(dtype='string', id=None),
+                                   'tokens': Sequence(feature=Value(dtype='string', id=None),
+                                                       length=-1, id=None)},
+                      'document': {'html': Value(dtype='string', id=None),
+                                   'title': Value(dtype='string', id=None),
+                                   'tokens': Sequence(feature={'is_html': Value(dtype='bool', id=None),
+                                                               'token': Value(dtype='string', id=None),
+                                                               "start_byte": Value("int64"),
+                                                               "end_byte": Value("int64")},
+                                                    length=-1, id=None),
+                                   'url': Value(dtype='string', id=None)}}
+    _train_feature_types = {
+        'annotations': Sequence(feature={
+            'id': Value(dtype='string', id=None),
+             'long_answer': {'end_byte': Value(dtype='int64', id=None),
+                             'end_token': Value(dtype='int64', id=None),
+                             'start_byte': Value(dtype='int64', id=None),
+                             'start_token': Value(dtype='int64', id=None),
+                             "candidate_index": Value("int64")},
+            'short_answers': Sequence(feature={'end_byte': Value(dtype='int64', id=None),
+                                               'end_token': Value(dtype='int64', id=None),
+                                               'start_byte': Value(dtype='int64', id=None),
+                                               'start_token': Value(dtype='int64', id=None),
+                                               'text': Value(dtype='string', id=None)},
+                                      length=-1, id=None),
+            'yes_no_answer': ClassLabel(num_classes=2, names=['NO', 'YES'], id=None)},
+            length=-1, id=None),
+    }
+    _single_context_type = {
+        'long_answer_candidates': Sequence(feature={'end_byte': Value(dtype='int64', id=None),
+                                                    'end_token': Value(dtype='int64', id=None),
+                                                    'start_byte': Value(dtype='int64', id=None),
+                                                    'start_token': Value(dtype='int64', id=None),
+                                                    'top_level': Value(dtype='bool', id=None)},
+                                  length=-1, id=None),
+    }
+    _yes_no_answer_to_str = {-1: 'NONE', 0: 'NO', 1: 'YES'}
+    _LIST_TAGS = {'<ol', '<ul', '<dl', '<li', '<dd', '<dt'}
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        if not self._single_context_multiple_passages:
+            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
+            self._single_context_multiple_passages = True
+
+
+    def adapt_dataset(self, dataset: Dataset, is_train: bool, keep_html: bool=True) -> Dataset:
+        """
+        Process dataset examples to rename fields, create context and set answer offset.
+        Args:
+             dataset: Dataset to be processed.
+             is_train: True for training otherwise False.
+             keep_html: True if keep html token in context otherwise false.
+        Returns:
+             Precossed dataset.
+        """
+        
+        self.validate_schema(dataset, is_train)
+        dataset = dataset.map(
+            functools.partial(self._rename_examples_create_context_and_adjust_offset, is_train=is_train, keep_html=keep_html),
+            load_from_cache_file=self._load_from_cache_file,
+            num_proc=self._num_workers
+        )
+        dataset = super().adapt_dataset(dataset, is_train)
+        return dataset
+
+
+    def _rename_examples_create_context_and_adjust_offset(self, example: Example, is_train: bool, keep_html: bool=True):
+        """
+        Rename examples to BasePreProcessor schema,
+        create context from document token,
+        and set the start/end positions of target and passage candidates to the new context.
+        
+        Args:
+             example: Dataset example.
+             is_train: True for training otherwise False.
+             keep_html: True if keep html token in context otherwise false.
+        Returns:
+             Precossed example.
+        """
+
+        # rename example
+        example['example_id'] = example['id']
+        example['question'] = example['question']['text']
+        example['language'] = 'english'
+        example['document_html'] = example['document']['html']
+        example['document_tokens'] = example['document']['tokens']
+
+        passage_candidates = {}
+        passage_candidates['start_positions'] = example['long_answer_candidates']['start_byte']
+        passage_candidates['end_positions'] = example['long_answer_candidates']['end_byte']
+        example['passage_candidates'] = passage_candidates
+        del example['long_answer_candidates']
+
+        example['target'] = self.get_annotations(example['annotations'], example['passage_candidates'])
+
+        # create context from document tokens, and build alignment between char and token.
+        context = ""
+        char_to_token = []
+        token_to_char = []
+        num_tokens = len(example['document_tokens']['token'])
+
+        for i in range(num_tokens):
+            if not keep_html and example['document_tokens']['is_html'][i]:
+                token_to_char.append(-1)
+                continue
+            if context:
+                char_to_token.append(-1)
+                context += ' '
+            for j in range(len(example['document_tokens']['token'][i])):
+                char_to_token.append(i)
+            token_to_char.append(len(context))
+            context += example['document_tokens']['token'][i]
+
+        example['context'] = [context]
+        example['context_char_to_token'] = char_to_token
+        example['context_token_to_char'] = token_to_char
+
+        # change target offsets to document token based context (only needed by training)
+        if not is_train:
+           return example
+
+        for i in range(len(example['target']['passage_indices'])):
+            pidx = example['target']['passage_indices'][i]
+            if pidx == -1 or example['target']['start_positions'][i] == -1:
+                continue
+
+            for j in range(num_tokens):
+                if example['context_token_to_char'][j] == -1:
+                    continue
+                if example['document_tokens']['start_byte'][j] >= example['target']['start_positions'][i]:
+                    break
+            if j < num_tokens:
+                example['target']['start_positions'][i] = example['context_token_to_char'][j]
+            else:
+                raise ValueError('Start position of short answer can not be set to the token based context.')
+
+            for j in range(num_tokens - 1, -1, -1):
+                if example['context_token_to_char'][j] == -1:
+                    continue
+                if example['document_tokens']['end_byte'][j] <= example['target']['end_positions'][i]:
+                    break
+            if j >= 0:
+                example['target']['end_positions'][i] = example['context_token_to_char'][j] + \
+                                                        len(example['document_tokens']['token'][j])
+            else:
+                raise ValueError('End position of short answer can not be set to the token based context.')
+
+        num_passages = len(example['passage_candidates']['start_positions'])
+        for i in range(num_passages):
+            passage_start_position = example['passage_candidates']['start_positions'][i]
+            passage_end_position = example['passage_candidates']['end_positions'][i]
+
+            for j in range(num_tokens):
+                if example['context_token_to_char'][j] == -1:
+                    continue
+                if example['document_tokens']['start_byte'][j] >= passage_start_position:
+                    break
+            if j < num_tokens:
+                example['passage_candidates']['start_positions'][i] = example['context_token_to_char'][j]
+            else:
+                raise ValueError('Start position of passage candidate can not be set to the token based context.')
+
+            for j in range(num_tokens - 1, -1, -1):
+                if example['context_token_to_char'][j] == -1:
+                    continue
+                if example['document_tokens']['end_byte'][j] <= passage_end_position:
+                    break
+            if j >= 0:
+                example['passage_candidates']['end_positions'][i] = example['context_token_to_char'][j] + \
+                                                                    len(example['document_tokens']['token'][j])
+            else:
+                raise ValueError('End position of passage candidate can not be set to the token based context.')
+
+        return example
+
+
+    def get_annotations(self, annotations, paragraphs):
+        """
+        Process NQ annotations into preprocessor format.
+        Args:
+             annotations: Annotations of NQ example.
+             paragraphs: Passage_candidates of NQ example.
+        Returns:
+             Annotations in preprocessor format.
+        """
+
+        nq_annotations = {}
+        nq_annotations['end_positions'] = []
+        nq_annotations['start_positions'] = []
+        nq_annotations['passage_indices'] = []
+        nq_annotations['yes_no_answer'] = []
+        for i in range(len(annotations['id'])):
+            start_byte, end_byte = annotations['short_answers'][i]['start_byte'],annotations['short_answers'][i]['end_byte']
+            
+            if len(start_byte) == 0:
+                start_byte = -1
+                end_byte = -1
+                candidate_index = -1
+            else:
+                start_byte = start_byte[0]
+                end_byte = end_byte[0]
+                candidate_index = annotations['long_answer'][i]['candidate_index']
+            if end_byte < start_byte:
+                self._logger.error("end_byte < start_byte")
+
+            yes_no_answer = annotations['yes_no_answer'][i]
+            yes_no_answer = self._yes_no_answer_to_str[yes_no_answer]
+                
+            nq_annotations['end_positions'].append(end_byte)
+            nq_annotations['start_positions'].append(start_byte)
+            nq_annotations['passage_indices'].append(candidate_index)
+            nq_annotations['yes_no_answer'].append(yes_no_answer)
+        return nq_annotations
+
```

## primeqa/mrc/processors/preprocessors/squad.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-from datasets import Dataset
-from datasets.arrow_dataset import Example
-from datasets.features.features import Sequence, Value
-
-from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
-
-
-class SQUADPreprocessor(BasePreProcessor):
-    """
-    Preprocessor for the SQuAD 1.1 data.
-    Note this preprocessor only supports `single_context_multiple_passages=True` and will
-    override the value accordingly.
-    """
-    
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        if not self._single_context_multiple_passages:
-            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
-            self._single_context_multiple_passages = True
-
-    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
-        dataset = dataset.map(self._augment_examples,
-                              load_from_cache_file=self._load_from_cache_file,
-                              num_proc=self._num_workers
-                              )
-        dataset = super().adapt_dataset(dataset, is_train)
-        return dataset
-    
-
-    def _augment_examples(self, example: Example):
-        """Rename examples from SQUAD schema to `BasePreProcessor` schema."""
-        
-        example["example_id"] = example.pop("id")
-        
-        target = example.pop('answers')
-        target["start_positions"] = target.pop("answer_start")
-        target["end_positions"] = [s + len(t) for (s,t) in zip(target["start_positions"],target["text"])]
-        target["passage_indices"] = [0 for _ in target["start_positions"]]
-        target["yes_no_answer"] = ['NONE' for _ in target["start_positions"]]
-        example['target'] = target
-        
-        # this is to fix issue in the XQUAD.ZH dataset
-        # the answer offset is not correct
-        # rely on the original answer text
-        example["answer_text"] = target.pop("text")
-        
-        passage_candidates = {"start_positions": [0],
-                               "end_positions" : [len(example["context"])]}
-        example['passage_candidates'] = passage_candidates
-        context = [ example['context'] ]
-        example['context'] = context  
-      
+from datasets import Dataset
+from datasets.arrow_dataset import Example
+from datasets.features.features import Sequence, Value
+
+from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
+
+
+class SQUADPreprocessor(BasePreProcessor):
+    """
+    Preprocessor for the SQuAD 1.1 data.
+    Note this preprocessor only supports `single_context_multiple_passages=True` and will
+    override the value accordingly.
+    """
+    
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        if not self._single_context_multiple_passages:
+            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
+            self._single_context_multiple_passages = True
+
+    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
+        dataset = dataset.map(self._augment_examples,
+                              load_from_cache_file=self._load_from_cache_file,
+                              num_proc=self._num_workers
+                              )
+        dataset = super().adapt_dataset(dataset, is_train)
+        return dataset
+    
+
+    def _augment_examples(self, example: Example):
+        """Rename examples from SQUAD schema to `BasePreProcessor` schema."""
+        
+        example["example_id"] = example.pop("id")
+        
+        target = example.pop('answers')
+        target["start_positions"] = target.pop("answer_start")
+        target["end_positions"] = [s + len(t) for (s,t) in zip(target["start_positions"],target["text"])]
+        target["passage_indices"] = [0 for _ in target["start_positions"]]
+        target["yes_no_answer"] = ['NONE' for _ in target["start_positions"]]
+        example['target'] = target
+        
+        # this is to fix issue in the XQUAD.ZH dataset
+        # the answer offset is not correct
+        # rely on the original answer text
+        example["answer_text"] = target.pop("text")
+        
+        passage_candidates = {"start_positions": [0],
+                               "end_positions" : [len(example["context"])]}
+        example['passage_candidates'] = passage_candidates
+        context = [ example['context'] ]
+        example['context'] = context  
+      
         return example
```

## primeqa/mrc/processors/preprocessors/tydiqa.py

 * *Ordering differences only*

```diff
@@ -1,98 +1,98 @@
-from datasets import Dataset
-from datasets.arrow_dataset import Example
-from datasets.features.features import Sequence, Value
-
-from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
-
-
-class TyDiQAPreprocessor(BasePreProcessor):
-    """
-    Preprocessor for TyDi QA data.
-    Note this preprocessor only supports `single_context_multiple_passages=True` and will
-    override the value accordingly.
-    """
-    _feature_types = {'question_text': Value(dtype='string', id=None),
-                      'document_plaintext': Value(dtype='string', id=None)}
-    _train_feature_types = {
-        'annotations': Sequence(feature={
-                   'minimal_answers_end_byte': Value(dtype='int32', id=None),
-                   'minimal_answers_start_byte': Value(dtype='int32', id=None),
-                   'passage_answer_candidate_index': Value(dtype='int32', id=None),
-                   'yes_no_answer': Value(dtype='string', id=None)})
-    }
-    _rename_fields = {'question_text': 'question', 'annotations': 'target',
-                      'passage_answer_candidates': 'passage_candidates'}
-    _rename_target = {'passage_answer_candidate_index': 'passage_indices',
-                      'minimal_answers_start_byte': 'start_positions',
-                      'minimal_answers_end_byte': 'end_positions'}
-    _rename_passages = {'plaintext_start_byte': 'start_positions',
-                        'plaintext_end_byte': 'end_positions'}
-    _single_context_type = {'passage_answer_candidates': Sequence(
-        feature={'plaintext_start_byte': Value(dtype='int32', id=None),
-                 'plaintext_end_byte': Value(dtype='int32', id=None)})}
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        if not self._single_context_multiple_passages:
-            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
-            self._single_context_multiple_passages = True
-
-    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
-        self.validate_schema(dataset, is_train)
-        dataset = dataset.rename_columns(self._rename_fields)
-        dataset = dataset.map(self._rename_examples,
-                              load_from_cache_file=self._load_from_cache_file,
-                              num_proc=self._num_workers
-                              )
-        dataset = dataset.map(self._convert_start_and_end_positions_from_bytes_to_chars,
-                              load_from_cache_file=self._load_from_cache_file,
-                              num_proc=self._num_workers
-                              )
-        dataset = super().adapt_dataset(dataset, is_train)
-        return dataset
-    
-    @staticmethod
-    def _convert_start_and_end_positions_from_bytes_to_chars(example: Example):
-        """
-        Converts the target start/end positions from bytes to character offsets.
-        """
-        context = example['document_plaintext']
-        context_bytes = context.encode('utf-8')
-
-        for i in range(len(example['target']['passage_indices'])):
-            pidx = example['target']['passage_indices'][i]
-            if pidx == -1 or example['target']['start_positions'][i] == -1:
-                continue
-
-            example['target']['start_positions'][i] = len(
-                context_bytes[:example['target']['start_positions'][i]].decode('utf-8', errors='replace'))
-            example['target']['end_positions'][i] = len(
-                context_bytes[:example['target']['end_positions'][i]].decode('utf-8', errors='replace'))
-
-        num_passages = len(example['passage_candidates']['start_positions'])
-        for i in range(num_passages):
-            passage_start_position = example['passage_candidates']['start_positions'][i]
-            passage_end_position = example['passage_candidates']['end_positions'][i]
-            example['passage_candidates']['start_positions'][i] = len(
-                context_bytes[:passage_start_position].decode('utf-8', errors='replace'))
-            example['passage_candidates']['end_positions'][i] = len(
-                context_bytes[:passage_end_position].decode('utf-8', errors='replace'))
-
-        example['context'] = [context]
-        return example
-
-    def _rename_examples(self, example: Example):
-        """
-        Rename examples from TyDi schema to `BasePreProcessor` schema.
-        """
-        target = example['target']
-        for old_key, new_key in self._rename_target.items():
-            target[new_key] = target.pop(old_key)
-        # TODO text extraction by byte from document_plaintext (generative support)
-        example['target'] = target
-
-        passage_candidates = example['passage_candidates']
-        for old_key, new_key in self._rename_passages.items():
-            passage_candidates[new_key] = passage_candidates.pop(old_key)
-        example['passage_candidates'] = passage_candidates
-        return example
+from datasets import Dataset
+from datasets.arrow_dataset import Example
+from datasets.features.features import Sequence, Value
+
+from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
+
+
+class TyDiQAPreprocessor(BasePreProcessor):
+    """
+    Preprocessor for TyDi QA data.
+    Note this preprocessor only supports `single_context_multiple_passages=True` and will
+    override the value accordingly.
+    """
+    _feature_types = {'question_text': Value(dtype='string', id=None),
+                      'document_plaintext': Value(dtype='string', id=None)}
+    _train_feature_types = {
+        'annotations': Sequence(feature={
+                   'minimal_answers_end_byte': Value(dtype='int32', id=None),
+                   'minimal_answers_start_byte': Value(dtype='int32', id=None),
+                   'passage_answer_candidate_index': Value(dtype='int32', id=None),
+                   'yes_no_answer': Value(dtype='string', id=None)})
+    }
+    _rename_fields = {'question_text': 'question', 'annotations': 'target',
+                      'passage_answer_candidates': 'passage_candidates'}
+    _rename_target = {'passage_answer_candidate_index': 'passage_indices',
+                      'minimal_answers_start_byte': 'start_positions',
+                      'minimal_answers_end_byte': 'end_positions'}
+    _rename_passages = {'plaintext_start_byte': 'start_positions',
+                        'plaintext_end_byte': 'end_positions'}
+    _single_context_type = {'passage_answer_candidates': Sequence(
+        feature={'plaintext_start_byte': Value(dtype='int32', id=None),
+                 'plaintext_end_byte': Value(dtype='int32', id=None)})}
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        if not self._single_context_multiple_passages:
+            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
+            self._single_context_multiple_passages = True
+
+    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
+        self.validate_schema(dataset, is_train)
+        dataset = dataset.rename_columns(self._rename_fields)
+        dataset = dataset.map(self._rename_examples,
+                              load_from_cache_file=self._load_from_cache_file,
+                              num_proc=self._num_workers
+                              )
+        dataset = dataset.map(self._convert_start_and_end_positions_from_bytes_to_chars,
+                              load_from_cache_file=self._load_from_cache_file,
+                              num_proc=self._num_workers
+                              )
+        dataset = super().adapt_dataset(dataset, is_train)
+        return dataset
+    
+    @staticmethod
+    def _convert_start_and_end_positions_from_bytes_to_chars(example: Example):
+        """
+        Converts the target start/end positions from bytes to character offsets.
+        """
+        context = example['document_plaintext']
+        context_bytes = context.encode('utf-8')
+
+        for i in range(len(example['target']['passage_indices'])):
+            pidx = example['target']['passage_indices'][i]
+            if pidx == -1 or example['target']['start_positions'][i] == -1:
+                continue
+
+            example['target']['start_positions'][i] = len(
+                context_bytes[:example['target']['start_positions'][i]].decode('utf-8', errors='replace'))
+            example['target']['end_positions'][i] = len(
+                context_bytes[:example['target']['end_positions'][i]].decode('utf-8', errors='replace'))
+
+        num_passages = len(example['passage_candidates']['start_positions'])
+        for i in range(num_passages):
+            passage_start_position = example['passage_candidates']['start_positions'][i]
+            passage_end_position = example['passage_candidates']['end_positions'][i]
+            example['passage_candidates']['start_positions'][i] = len(
+                context_bytes[:passage_start_position].decode('utf-8', errors='replace'))
+            example['passage_candidates']['end_positions'][i] = len(
+                context_bytes[:passage_end_position].decode('utf-8', errors='replace'))
+
+        example['context'] = [context]
+        return example
+
+    def _rename_examples(self, example: Example):
+        """
+        Rename examples from TyDi schema to `BasePreProcessor` schema.
+        """
+        target = example['target']
+        for old_key, new_key in self._rename_target.items():
+            target[new_key] = target.pop(old_key)
+        # TODO text extraction by byte from document_plaintext (generative support)
+        example['target'] = target
+
+        passage_candidates = example['passage_candidates']
+        for old_key, new_key in self._rename_passages.items():
+            passage_candidates[new_key] = passage_candidates.pop(old_key)
+        example['passage_candidates'] = passage_candidates
+        return example
```

## primeqa/mrc/processors/preprocessors/tydiqa_google.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-from operator import itemgetter
-from typing import Optional
-
-from datasets import Dataset
-from datasets.features.features import Sequence, Value
-
-from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
-
-class TyDiQAGooglePreprocessor(BasePreProcessor):
-    """
-    Preprocessor for TyDi QA data in Google format (*NOT* HuggingFace).
-    Note this preprocessor only supports `single_context_multiple_passages=True` and will
-    override the value accordingly.
-    """
-    _feature_types = {'question_text': Value(dtype='string', id=None),
-                      'document_plaintext': Value(dtype='string', id=None)}
-    _train_feature_types = {
-        'annotations': Sequence(feature={
-                   'minimal_answers_end_byte': Value(dtype='int32', id=None),
-                   'minimal_answers_start_byte': Value(dtype='int32', id=None),
-                   'passage_answer_candidate_index': Value(dtype='int32', id=None),
-                   'yes_no_answer': Value(dtype='string', id=None)})
-    }
-    _example_id_type = {'example_id': Value(dtype='string', id=None)}
-    _byte_itemgetter = itemgetter('plaintext_start_byte', 'plaintext_end_byte')
-    _rename_fields = {'question_text': 'question', 'annotations': 'target',
-                      'passage_answer_candidates': 'passage_candidates'}
-    _rename_target = {'passage_answer_candidate_index': 'passage_indices',
-                      'minimal_answers_start_byte': 'start_positions',
-                      'minimal_answers_end_byte': 'end_positions'}
-    _rename_passages = {'plaintext_start_byte': 'start_positions',
-                        'plaintext_end_byte': 'end_positions'}
-    _single_context_type = {'passage_answer_candidates': Sequence(
-        feature={'plaintext_start_byte': Value(dtype='int32', id=None),
-                 'plaintext_end_byte': Value(dtype='int32', id=None)})}
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        if not self._single_context_multiple_passages:
-            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
-            self._single_context_multiple_passages = True
-
-    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
-        dataset = dataset.rename_columns(self._rename_fields)
-        dataset = dataset.map(self._rename_examples,
-                              load_from_cache_file=self._load_from_cache_file,
-                              num_proc=self._num_workers
-                              )
-        dataset = dataset.map(self._convert_start_and_end_positions_from_bytes_to_chars,
-                              load_from_cache_file=self._load_from_cache_file,
-                              num_proc=self._num_workers
-        )
-        dataset = dataset.cast_column('example_id',Value(dtype="string",id=None))
-        dataset = super().adapt_dataset(dataset, is_train)
-        return dataset
-    
-    def _convert_start_and_end_positions_from_bytes_to_chars(self, example):
-        """
-        Converts the target start/end positions from bytes to character offsets.
-        """
-        context = example['document_plaintext']
-        context_bytes = context.encode('utf-8')
-
-        for i in range(len(example['target']['passage_indices'])):
-            pidx = example['target']['passage_indices'][i]
-            if pidx == -1 or example['target']['start_positions'][i] == -1:
-                continue
-
-            example['target']['start_positions'][i] = len(
-                context_bytes[:example['target']['start_positions'][i]].decode('utf-8', errors='replace'))
-            example['target']['end_positions'][i] = len(
-                context_bytes[:example['target']['end_positions'][i]].decode('utf-8', errors='replace'))
-
-        num_passages = len(example['passage_candidates']['start_positions'])
-        for i in range(num_passages):
-            passage_start_position = example['passage_candidates']['start_positions'][i]
-            passage_end_position = example['passage_candidates']['end_positions'][i]
-            example['passage_candidates']['start_positions'][i] = len(
-                context_bytes[:passage_start_position].decode('utf-8', errors='replace'))
-            example['passage_candidates']['end_positions'][i] = len(
-                context_bytes[:passage_end_position].decode('utf-8', errors='replace'))
-
-        example['context'] = [context]
-        return example
-
-    def _rename_examples(self, example):
-        """
-        Rename examples from TyDi schema to `BasePreProcessor` schema.
-        """
-        target = example['target']
-        
-        new_target = {}
-        new_target['yes_no_answer'] = []
-        for old_key, new_key in self._rename_target.items():
-            new_target[new_key] = []
-        
-        for annotation in target:
-            new_target['passage_indices'].append(annotation['passage_answer']['candidate_index'])
-            new_target['start_positions'].append(annotation['minimal_answer']['plaintext_start_byte'])
-            new_target['end_positions'].append(annotation['minimal_answer']['plaintext_end_byte'])
-            new_target['yes_no_answer'].append(annotation['yes_no_answer'])
-        example['target'] = new_target
-
-        passage_candidates = example['passage_candidates']
-        new_passage_candidates = {}
-        for old_key, new_key in self._rename_passages.items():
-            new_passage_candidates[new_key] = []
-        for candidate in passage_candidates:
-            for old_key, new_key in self._rename_passages.items():
-                new_passage_candidates[new_key].append(candidate[old_key])
-        example['passage_candidates'] = new_passage_candidates
-        return example
+from operator import itemgetter
+from typing import Optional
+
+from datasets import Dataset
+from datasets.features.features import Sequence, Value
+
+from primeqa.mrc.processors.preprocessors.base import BasePreProcessor
+
+class TyDiQAGooglePreprocessor(BasePreProcessor):
+    """
+    Preprocessor for TyDi QA data in Google format (*NOT* HuggingFace).
+    Note this preprocessor only supports `single_context_multiple_passages=True` and will
+    override the value accordingly.
+    """
+    _feature_types = {'question_text': Value(dtype='string', id=None),
+                      'document_plaintext': Value(dtype='string', id=None)}
+    _train_feature_types = {
+        'annotations': Sequence(feature={
+                   'minimal_answers_end_byte': Value(dtype='int32', id=None),
+                   'minimal_answers_start_byte': Value(dtype='int32', id=None),
+                   'passage_answer_candidate_index': Value(dtype='int32', id=None),
+                   'yes_no_answer': Value(dtype='string', id=None)})
+    }
+    _example_id_type = {'example_id': Value(dtype='string', id=None)}
+    _byte_itemgetter = itemgetter('plaintext_start_byte', 'plaintext_end_byte')
+    _rename_fields = {'question_text': 'question', 'annotations': 'target',
+                      'passage_answer_candidates': 'passage_candidates'}
+    _rename_target = {'passage_answer_candidate_index': 'passage_indices',
+                      'minimal_answers_start_byte': 'start_positions',
+                      'minimal_answers_end_byte': 'end_positions'}
+    _rename_passages = {'plaintext_start_byte': 'start_positions',
+                        'plaintext_end_byte': 'end_positions'}
+    _single_context_type = {'passage_answer_candidates': Sequence(
+        feature={'plaintext_start_byte': Value(dtype='int32', id=None),
+                 'plaintext_end_byte': Value(dtype='int32', id=None)})}
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        if not self._single_context_multiple_passages:
+            self._logger.info(f"{self.__class__.__name__} only supports single context multiple passages -- enabling")
+            self._single_context_multiple_passages = True
+
+    def adapt_dataset(self, dataset: Dataset, is_train: bool) -> Dataset:
+        dataset = dataset.rename_columns(self._rename_fields)
+        dataset = dataset.map(self._rename_examples,
+                              load_from_cache_file=self._load_from_cache_file,
+                              num_proc=self._num_workers
+                              )
+        dataset = dataset.map(self._convert_start_and_end_positions_from_bytes_to_chars,
+                              load_from_cache_file=self._load_from_cache_file,
+                              num_proc=self._num_workers
+        )
+        dataset = dataset.cast_column('example_id',Value(dtype="string",id=None))
+        dataset = super().adapt_dataset(dataset, is_train)
+        return dataset
+    
+    def _convert_start_and_end_positions_from_bytes_to_chars(self, example):
+        """
+        Converts the target start/end positions from bytes to character offsets.
+        """
+        context = example['document_plaintext']
+        context_bytes = context.encode('utf-8')
+
+        for i in range(len(example['target']['passage_indices'])):
+            pidx = example['target']['passage_indices'][i]
+            if pidx == -1 or example['target']['start_positions'][i] == -1:
+                continue
+
+            example['target']['start_positions'][i] = len(
+                context_bytes[:example['target']['start_positions'][i]].decode('utf-8', errors='replace'))
+            example['target']['end_positions'][i] = len(
+                context_bytes[:example['target']['end_positions'][i]].decode('utf-8', errors='replace'))
+
+        num_passages = len(example['passage_candidates']['start_positions'])
+        for i in range(num_passages):
+            passage_start_position = example['passage_candidates']['start_positions'][i]
+            passage_end_position = example['passage_candidates']['end_positions'][i]
+            example['passage_candidates']['start_positions'][i] = len(
+                context_bytes[:passage_start_position].decode('utf-8', errors='replace'))
+            example['passage_candidates']['end_positions'][i] = len(
+                context_bytes[:passage_end_position].decode('utf-8', errors='replace'))
+
+        example['context'] = [context]
+        return example
+
+    def _rename_examples(self, example):
+        """
+        Rename examples from TyDi schema to `BasePreProcessor` schema.
+        """
+        target = example['target']
+        
+        new_target = {}
+        new_target['yes_no_answer'] = []
+        for old_key, new_key in self._rename_target.items():
+            new_target[new_key] = []
+        
+        for annotation in target:
+            new_target['passage_indices'].append(annotation['passage_answer']['candidate_index'])
+            new_target['start_positions'].append(annotation['minimal_answer']['plaintext_start_byte'])
+            new_target['end_positions'].append(annotation['minimal_answer']['plaintext_end_byte'])
+            new_target['yes_no_answer'].append(annotation['yes_no_answer'])
+        example['target'] = new_target
+
+        passage_candidates = example['passage_candidates']
+        new_passage_candidates = {}
+        for old_key, new_key in self._rename_passages.items():
+            new_passage_candidates[new_key] = []
+        for candidate in passage_candidates:
+            for old_key, new_key in self._rename_passages.items():
+                new_passage_candidates[new_key].append(candidate[old_key])
+        example['passage_candidates'] = new_passage_candidates
+        return example
```

## primeqa/mrc/trainers/mrc.py

 * *Ordering differences only*

```diff
@@ -1,279 +1,279 @@
-import inspect
-import json
-import logging
-import os
-from typing import Optional
-
-import datasets
-import torch
-from datasets import Dataset
-from packaging import version
-from torch.utils.data import DataLoader
-from transformers import Trainer, is_datasets_available
-from transformers.trainer_pt_utils import IterableDatasetShard
-
-logger = logging.getLogger(__name__)
-
-
-class MRCTrainer(Trainer):
-    def __init__(self, *args, eval_examples=None, eval_dataset=None, post_process_function=None, **kwargs):
-        """
-        MRC training and evaluation.
-
-        Args:
-            *args: Arguments for super-class constructor.
-            eval_examples: Eval examples `Dataset` from `BasePreprocessor.process_eval`.
-            eval_dataset: Eval features `Dataset` from `BasePreprocessor.process_eval`.
-            post_process_function:  Function to create predictions from model outputs.
-            **kwargs: Keyword arguments for super-class constructor.
-        """
-        super().__init__(*args, **kwargs)
-        self.eval_examples = eval_examples
-        self.eval_dataset = eval_dataset
-        self.post_process_function = post_process_function
-
-    def _remove_unused_columns(self, dataset: "datasets.Dataset", description: Optional[str] = None):
-        """
-        Infer needed `Dataset` columns matching model and (active) model head argument names.
-        Remove unneeded columns from `dataset`.
-
-        Since this is a private method being overridden we override the calling methods as well.
-
-        Args:
-            dataset: `Dataset` to remove unneeded columns from
-            description: `dataset` description
-
-        Returns:
-            `dataset` with unneeded columns removed.
-        """
-        if not self.args.remove_unused_columns:
-            return dataset
-        if self._signature_columns is None:
-            # Inspect model and task head forward signature to keep only the arguments it accepts.
-            model_signature = inspect.signature(self.model.forward)
-            task_head_signature = inspect.signature(self.model.task_head.forward)
-
-            signature_columns = set(model_signature.parameters.keys())
-            signature_columns |= task_head_signature.parameters.keys()
-            signature_columns -= {'kwargs'}
-
-            # Labels may be named label or label_ids, the default data collator handles that.
-            signature_columns |= {"label", "label_ids"}
-
-            self._signature_columns = list(signature_columns)
-
-        columns = [k for k in self._signature_columns if k in dataset.column_names]
-        ignored_columns = list(set(dataset.column_names) - set(self._signature_columns))
-        if len(ignored_columns) > 0:
-            dset_description = "" if description is None else f"in the {description} set "
-            logger.info(
-                f"The following columns {dset_description} don't have a corresponding argument in "
-                f"`{self.model.__class__.__name__}.forward` and have been ignored: {', '.join(ignored_columns)}."
-            )
-
-        if version.parse(datasets.__version__) < version.parse("1.4.0"):
-            dataset.set_format(
-                type=dataset.format["type"], columns=columns, format_kwargs=dataset.format["format_kwargs"]
-            )
-            return dataset
-        else:
-            return dataset.remove_columns(ignored_columns)
-
-    def get_train_dataloader(self) -> DataLoader:
-        """
-        Returns the training torch `DataLoader`.
-
-        Will use no sampler if `self.train_dataset` does not implement `__len__`, a random sampler (adapted
-        to distributed training if necessary) otherwise.
-
-        Subclass and override this method if you want to inject some custom behavior.
-        """
-        if self.train_dataset is None:
-            raise ValueError("Trainer: training requires a train_dataset.")
-
-        train_dataset = self.train_dataset
-        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):
-            train_dataset = self._remove_unused_columns(train_dataset, description="training")
-
-        if isinstance(train_dataset, torch.utils.data.IterableDataset):
-            if self.args.world_size > 1:
-                train_dataset = IterableDatasetShard(
-                    train_dataset,
-                    batch_size=self.args.train_batch_size,
-                    drop_last=self.args.dataloader_drop_last,
-                    num_processes=self.args.world_size,
-                    process_index=self.args.process_index,
-                )
-
-            return DataLoader(
-                train_dataset,
-                batch_size=self.args.per_device_train_batch_size,
-                collate_fn=self.data_collator,
-                num_workers=self.args.dataloader_num_workers,
-                pin_memory=self.args.dataloader_pin_memory,
-            )
-
-        train_sampler = self._get_train_sampler()
-
-        return DataLoader(
-            train_dataset,
-            batch_size=self.args.train_batch_size,
-            sampler=train_sampler,
-            collate_fn=self.data_collator,
-            drop_last=self.args.dataloader_drop_last,
-            num_workers=self.args.dataloader_num_workers,
-            pin_memory=self.args.dataloader_pin_memory,
-        )
-
-    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:
-        """
-        Returns the evaluation torch `DataLoader`.
-
-        Subclass and override this method if you want to inject some custom behavior.
-
-        Args:
-            eval_dataset: If provided, will override `self.eval_dataset`. If it is an `datasets.Dataset`, columns not
-                          accepted by the `model.forward()` method are automatically removed.
-                          It must implement `__len__`.
-
-        """
-
-        if eval_dataset is None and self.eval_dataset is None:
-            raise ValueError("Trainer: evaluation requires an eval_dataset.")
-        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
-
-        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):
-            eval_dataset = self._remove_unused_columns(eval_dataset, description="evaluation")
-
-        if isinstance(eval_dataset, torch.utils.data.IterableDataset):
-            if self.args.world_size > 1:
-                eval_dataset = IterableDatasetShard(
-                    eval_dataset,
-                    batch_size=self.args.per_device_eval_batch_size,
-                    drop_last=self.args.dataloader_drop_last,
-                    num_processes=self.args.world_size,
-                    process_index=self.args.process_index,
-                )
-            return DataLoader(
-                eval_dataset,
-                batch_size=self.args.eval_batch_size,
-                collate_fn=self.data_collator,
-                num_workers=self.args.dataloader_num_workers,
-                pin_memory=self.args.dataloader_pin_memory,
-            )
-
-        eval_sampler = self._get_eval_sampler(eval_dataset)
-
-        return DataLoader(
-            eval_dataset,
-            sampler=eval_sampler,
-            batch_size=self.args.eval_batch_size,
-            collate_fn=self.data_collator,
-            drop_last=self.args.dataloader_drop_last,
-            num_workers=self.args.dataloader_num_workers,
-            pin_memory=self.args.dataloader_pin_memory,
-        )
-
-    # TODO: when implementing test support implement `get_test_dataloader`
-
-    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval"):
-        """
-        Evaluate model using either eval data passed to method (if given).
-        Otherwise use data given to constructor at instantiation.
-
-        Args:
-            eval_examples: Eval examples `Dataset` from `BasePreprocessor.process_eval`.
-            eval_dataset: Eval features `Dataset` from `BasePreprocessor.process_eval`.
-            ignore_keys: Keys to ignore in evaluation loop.
-            metric_key_prefix: Append this prefix to metric names.
-
-        Returns:
-            Evaluation metrics if post-processing and metric computation functions
-            were provided to constructor at instantiation, otherwise an empty dict.
-        """
-        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
-        eval_dataloader = self.get_eval_dataloader(eval_dataset)
-        eval_examples = self.eval_examples if eval_examples is None else eval_examples
-
-        # # Temporarily disable metric computation, we will do it in the loop here.
-        compute_metrics = self.compute_metrics
-        self.compute_metrics = None
-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
-        try:
-            output = eval_loop(
-                eval_dataloader,
-                description="Evaluation",
-                # No point gathering the predictions if there are no metrics, otherwise we defer to
-                # self.args.prediction_loss_only
-                # gather predictions if running in eval mode
-                prediction_loss_only=self.args.prediction_loss_only, #True if compute_metrics is None else None,
-                ignore_keys=ignore_keys,
-            )
-        finally:
-            self.compute_metrics = compute_metrics
-
-        if self.post_process_function is not None:
-            eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)
-
-            # TODO: return eval_preds and metrics, write save function for preds
-            with open(os.path.join(self.args.output_dir, 'eval_predictions.json'), 'w') as f:
-                json.dump(eval_preds.predictions, f, indent=4)
-            with open(os.path.join(self.args.output_dir, 'eval_predictions_processed.json'), 'w') as f:
-                json.dump(eval_preds.processed_predictions, f, indent=4)
-            with open(os.path.join(self.args.output_dir, 'eval_references.json'), 'w') as f:
-                json.dump(eval_preds.label_ids, f, indent=4)
-
-        if self.post_process_function is not None and self.compute_metrics is not None:
-            metrics = self.compute_metrics(eval_preds)
-
-            # Prefix all keys with metric_key_prefix + '_'
-            for key in list(metrics.keys()):
-                if not key.startswith(f"{metric_key_prefix}_"):
-                    metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
-
-            self.log(metrics)
-        else:
-            metrics = {}
-
-        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
-        return metrics
-    
-    def predict(self, eval_dataset=None, eval_examples=None, ignore_keys=None):
-        """
-        Obtain the predictions using either eval data passed to method (if given).
-        Otherwise use data given to constructor at instantiation.
-
-        Args:
-            eval_examples: Eval examples `Dataset` from `BasePreprocessor.process_eval`.
-            eval_dataset: Eval features `Dataset` from `BasePreprocessor.process_eval`.
-            ignore_keys: Keys to ignore in evaluation loop.
-
-        Returns:
-            Answer predictions if post-processing function was provided to constructor 
-            at instantiation, otherwise an empty dict.
-        """
-        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
-        eval_dataloader = self.get_eval_dataloader(eval_dataset)
-        eval_examples = self.eval_examples if eval_examples is None else eval_examples
-
-        
-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
-        try:
-            output = eval_loop(
-                eval_dataloader,
-                description="Evaluation",
-                # No point gathering the predictions if there are no metrics, otherwise we defer to
-                # self.args.prediction_loss_only
-                # gather predictions if running in eval mode
-                prediction_loss_only=self.args.prediction_loss_only, #True if compute_metrics is None else None,
-                ignore_keys=ignore_keys,
-            )
-        finally:
-            pass
-
-        if self.post_process_function is not None:
-            eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)
-        else:
-            eval_preds = {}
-
-        return eval_preds
+import inspect
+import json
+import logging
+import os
+from typing import Optional
+
+import datasets
+import torch
+from datasets import Dataset
+from packaging import version
+from torch.utils.data import DataLoader
+from transformers import Trainer, is_datasets_available
+from transformers.trainer_pt_utils import IterableDatasetShard
+
+logger = logging.getLogger(__name__)
+
+
+class MRCTrainer(Trainer):
+    def __init__(self, *args, eval_examples=None, eval_dataset=None, post_process_function=None, **kwargs):
+        """
+        MRC training and evaluation.
+
+        Args:
+            *args: Arguments for super-class constructor.
+            eval_examples: Eval examples `Dataset` from `BasePreprocessor.process_eval`.
+            eval_dataset: Eval features `Dataset` from `BasePreprocessor.process_eval`.
+            post_process_function:  Function to create predictions from model outputs.
+            **kwargs: Keyword arguments for super-class constructor.
+        """
+        super().__init__(*args, **kwargs)
+        self.eval_examples = eval_examples
+        self.eval_dataset = eval_dataset
+        self.post_process_function = post_process_function
+
+    def _remove_unused_columns(self, dataset: "datasets.Dataset", description: Optional[str] = None):
+        """
+        Infer needed `Dataset` columns matching model and (active) model head argument names.
+        Remove unneeded columns from `dataset`.
+
+        Since this is a private method being overridden we override the calling methods as well.
+
+        Args:
+            dataset: `Dataset` to remove unneeded columns from
+            description: `dataset` description
+
+        Returns:
+            `dataset` with unneeded columns removed.
+        """
+        if not self.args.remove_unused_columns:
+            return dataset
+        if self._signature_columns is None:
+            # Inspect model and task head forward signature to keep only the arguments it accepts.
+            model_signature = inspect.signature(self.model.forward)
+            task_head_signature = inspect.signature(self.model.task_head.forward)
+
+            signature_columns = set(model_signature.parameters.keys())
+            signature_columns |= task_head_signature.parameters.keys()
+            signature_columns -= {'kwargs'}
+
+            # Labels may be named label or label_ids, the default data collator handles that.
+            signature_columns |= {"label", "label_ids"}
+
+            self._signature_columns = list(signature_columns)
+
+        columns = [k for k in self._signature_columns if k in dataset.column_names]
+        ignored_columns = list(set(dataset.column_names) - set(self._signature_columns))
+        if len(ignored_columns) > 0:
+            dset_description = "" if description is None else f"in the {description} set "
+            logger.info(
+                f"The following columns {dset_description} don't have a corresponding argument in "
+                f"`{self.model.__class__.__name__}.forward` and have been ignored: {', '.join(ignored_columns)}."
+            )
+
+        if version.parse(datasets.__version__) < version.parse("1.4.0"):
+            dataset.set_format(
+                type=dataset.format["type"], columns=columns, format_kwargs=dataset.format["format_kwargs"]
+            )
+            return dataset
+        else:
+            return dataset.remove_columns(ignored_columns)
+
+    def get_train_dataloader(self) -> DataLoader:
+        """
+        Returns the training torch `DataLoader`.
+
+        Will use no sampler if `self.train_dataset` does not implement `__len__`, a random sampler (adapted
+        to distributed training if necessary) otherwise.
+
+        Subclass and override this method if you want to inject some custom behavior.
+        """
+        if self.train_dataset is None:
+            raise ValueError("Trainer: training requires a train_dataset.")
+
+        train_dataset = self.train_dataset
+        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):
+            train_dataset = self._remove_unused_columns(train_dataset, description="training")
+
+        if isinstance(train_dataset, torch.utils.data.IterableDataset):
+            if self.args.world_size > 1:
+                train_dataset = IterableDatasetShard(
+                    train_dataset,
+                    batch_size=self.args.train_batch_size,
+                    drop_last=self.args.dataloader_drop_last,
+                    num_processes=self.args.world_size,
+                    process_index=self.args.process_index,
+                )
+
+            return DataLoader(
+                train_dataset,
+                batch_size=self.args.per_device_train_batch_size,
+                collate_fn=self.data_collator,
+                num_workers=self.args.dataloader_num_workers,
+                pin_memory=self.args.dataloader_pin_memory,
+            )
+
+        train_sampler = self._get_train_sampler()
+
+        return DataLoader(
+            train_dataset,
+            batch_size=self.args.train_batch_size,
+            sampler=train_sampler,
+            collate_fn=self.data_collator,
+            drop_last=self.args.dataloader_drop_last,
+            num_workers=self.args.dataloader_num_workers,
+            pin_memory=self.args.dataloader_pin_memory,
+        )
+
+    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:
+        """
+        Returns the evaluation torch `DataLoader`.
+
+        Subclass and override this method if you want to inject some custom behavior.
+
+        Args:
+            eval_dataset: If provided, will override `self.eval_dataset`. If it is an `datasets.Dataset`, columns not
+                          accepted by the `model.forward()` method are automatically removed.
+                          It must implement `__len__`.
+
+        """
+
+        if eval_dataset is None and self.eval_dataset is None:
+            raise ValueError("Trainer: evaluation requires an eval_dataset.")
+        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
+
+        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):
+            eval_dataset = self._remove_unused_columns(eval_dataset, description="evaluation")
+
+        if isinstance(eval_dataset, torch.utils.data.IterableDataset):
+            if self.args.world_size > 1:
+                eval_dataset = IterableDatasetShard(
+                    eval_dataset,
+                    batch_size=self.args.per_device_eval_batch_size,
+                    drop_last=self.args.dataloader_drop_last,
+                    num_processes=self.args.world_size,
+                    process_index=self.args.process_index,
+                )
+            return DataLoader(
+                eval_dataset,
+                batch_size=self.args.eval_batch_size,
+                collate_fn=self.data_collator,
+                num_workers=self.args.dataloader_num_workers,
+                pin_memory=self.args.dataloader_pin_memory,
+            )
+
+        eval_sampler = self._get_eval_sampler(eval_dataset)
+
+        return DataLoader(
+            eval_dataset,
+            sampler=eval_sampler,
+            batch_size=self.args.eval_batch_size,
+            collate_fn=self.data_collator,
+            drop_last=self.args.dataloader_drop_last,
+            num_workers=self.args.dataloader_num_workers,
+            pin_memory=self.args.dataloader_pin_memory,
+        )
+
+    # TODO: when implementing test support implement `get_test_dataloader`
+
+    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval"):
+        """
+        Evaluate model using either eval data passed to method (if given).
+        Otherwise use data given to constructor at instantiation.
+
+        Args:
+            eval_examples: Eval examples `Dataset` from `BasePreprocessor.process_eval`.
+            eval_dataset: Eval features `Dataset` from `BasePreprocessor.process_eval`.
+            ignore_keys: Keys to ignore in evaluation loop.
+            metric_key_prefix: Append this prefix to metric names.
+
+        Returns:
+            Evaluation metrics if post-processing and metric computation functions
+            were provided to constructor at instantiation, otherwise an empty dict.
+        """
+        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
+        eval_dataloader = self.get_eval_dataloader(eval_dataset)
+        eval_examples = self.eval_examples if eval_examples is None else eval_examples
+
+        # # Temporarily disable metric computation, we will do it in the loop here.
+        compute_metrics = self.compute_metrics
+        self.compute_metrics = None
+        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
+        try:
+            output = eval_loop(
+                eval_dataloader,
+                description="Evaluation",
+                # No point gathering the predictions if there are no metrics, otherwise we defer to
+                # self.args.prediction_loss_only
+                # gather predictions if running in eval mode
+                prediction_loss_only=self.args.prediction_loss_only, #True if compute_metrics is None else None,
+                ignore_keys=ignore_keys,
+            )
+        finally:
+            self.compute_metrics = compute_metrics
+
+        if self.post_process_function is not None:
+            eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)
+
+            # TODO: return eval_preds and metrics, write save function for preds
+            with open(os.path.join(self.args.output_dir, 'eval_predictions.json'), 'w') as f:
+                json.dump(eval_preds.predictions, f, indent=4)
+            with open(os.path.join(self.args.output_dir, 'eval_predictions_processed.json'), 'w') as f:
+                json.dump(eval_preds.processed_predictions, f, indent=4)
+            with open(os.path.join(self.args.output_dir, 'eval_references.json'), 'w') as f:
+                json.dump(eval_preds.label_ids, f, indent=4)
+
+        if self.post_process_function is not None and self.compute_metrics is not None:
+            metrics = self.compute_metrics(eval_preds)
+
+            # Prefix all keys with metric_key_prefix + '_'
+            for key in list(metrics.keys()):
+                if not key.startswith(f"{metric_key_prefix}_"):
+                    metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
+
+            self.log(metrics)
+        else:
+            metrics = {}
+
+        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
+        return metrics
+    
+    def predict(self, eval_dataset=None, eval_examples=None, ignore_keys=None):
+        """
+        Obtain the predictions using either eval data passed to method (if given).
+        Otherwise use data given to constructor at instantiation.
+
+        Args:
+            eval_examples: Eval examples `Dataset` from `BasePreprocessor.process_eval`.
+            eval_dataset: Eval features `Dataset` from `BasePreprocessor.process_eval`.
+            ignore_keys: Keys to ignore in evaluation loop.
+
+        Returns:
+            Answer predictions if post-processing function was provided to constructor 
+            at instantiation, otherwise an empty dict.
+        """
+        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
+        eval_dataloader = self.get_eval_dataloader(eval_dataset)
+        eval_examples = self.eval_examples if eval_examples is None else eval_examples
+
+        
+        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
+        try:
+            output = eval_loop(
+                eval_dataloader,
+                description="Evaluation",
+                # No point gathering the predictions if there are no metrics, otherwise we defer to
+                # self.args.prediction_loss_only
+                # gather predictions if running in eval mode
+                prediction_loss_only=self.args.prediction_loss_only, #True if compute_metrics is None else None,
+                ignore_keys=ignore_keys,
+            )
+        finally:
+            pass
+
+        if self.post_process_function is not None:
+            eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)
+        else:
+            eval_preds = {}
+
+        return eval_preds
```

## primeqa/qg/run_qg.py

```diff
@@ -1,218 +1,271 @@
-from transformers import (
-    HfArgumentParser,
-    Seq2SeqTrainingArguments,
-    set_seed,
-)
-from primeqa.qg.processors.data_loader import QGDataLoader
-from dataclasses import dataclass,field
-from primeqa.qg.models.qg_model import QGModel
-from primeqa.qg.trainers.qg_trainer import QGTrainer
-from primeqa.qg.metrics.generation_metrics import rouge_metrics
-from primeqa.qg.utils.data_collator import T2TDataCollator
-from typing import Optional
-
-import json
-import logging
-import os
-import sys
-logger = logging.getLogger(__name__)
-
-@dataclass
-class ModelArguments:
-    """
-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
-    """
-
-    model_name_or_path: str = field(
-       default='t5-base', metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models" ,
-                                    "choices":["t5-base", "t5-small", "google/mt5-small","google/mt5-base"]}
-    )
-    modality: str = field(
-       default='table', metadata={"help": "Whether to generate questions from tables or passages",
-                                  "choices":["table", "passage"]}
-    )
-    tokenizer_name: Optional[str] = field(
-        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
-    )
-    cache_dir: Optional[str] = field(
-        default=None, metadata={"help": "Where do we want to store the pretrained models downloaded from s3"}
-    )
-
-@dataclass
-class DataTrainingArguments:
-    """
-    Arguments pertaining to what data we are going to input our model for training and eval.
-    """
-
-    dataset_name:Optional[str] = field(
-        default="wikisql", metadata={"help": "Name of the dataset to train the qg model", 
-                                    "choices": ["wikisql", "squad", "squad_v2", "tydiqa"]}
-    )
-    train_file_path: Optional[str] = field(
-        default='train_data.pt',
-        metadata={"help": "Path for cached train dataset"},
-    )
-    valid_file_path: Optional[str] = field(
-        default='valid_data.pt',
-        metadata={"help": "Path for cached valid dataset"},
-    )
-    max_len: Optional[int] = field(
-        default=512,
-        metadata={"help": "Max input length for the source text"},
-    )
-    target_max_len: Optional[int] = field(
-        default=32,
-        metadata={"help": "Max input length for the target text"},
-    )
-
-@dataclass
-class InferenceArguments:
-    do_generate: Optional[bool] = field(
-        default=False, metadata={"help": "Whether to generate questions"}
-    )
-    num_questions_per_instance: Optional[int] = field(
-        default=5, metadata={"help": "Number of questions to generate per table/passage"}
-    )
-    max_where_clauses: Optional[int] = field(
-        default=1, metadata={"help": "Max number of filters in generated question"}
-    )
-    data_path:str = field(
-        default='examples/qg/sample_table.json', metadata={"help": "Path to JSON file with LIST of tables/passages. Each table \
-                              should be a dict with keys 'header' and 'rows', and passages should be str"}
-    )
-    generate_aggregate: Optional[bool] = field(
-        default=False, metadata={"help": "Whether to generate aggregate questions with max, min, sum, etc."}
-    )
-    gen_output_path: Optional[str] = field(
-        default='examples/qg/sample_generation.json', metadata={"help": "path to JSON file where generated questions will be saved"} 
-    )
-
-def main(raw_args):
-    print(raw_args)
-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments, InferenceArguments))
-
-    if len(raw_args) == 2 and raw_args[1].endswith(".json"):
-        model_args, data_args, training_args, inference_args = parser.parse_json_file(json_file=raw_args[1])
-    elif len(raw_args) == 1:
-        model_args, data_args, training_args = parser.parse_dict(raw_args[0])
-    else:
-        model_args, data_args, training_args, inference_args = parser.parse_args_into_dataclasses()
-    
-    # These rguments has to be hardcoded in order for Trainer to work
-    training_args.predict_with_generate=True
-    training_args.remove_unused_columns = False
-    training_args.prediction_loss_only = False
-    
-    if (
-        os.path.exists(training_args.output_dir)
-        and os.listdir(training_args.output_dir)
-        and training_args.do_train
-        and not training_args.overwrite_output_dir
-    ):
-        raise ValueError(
-            f"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome."
-        )
-
-    # Setup logging
-    logging.basicConfig(
-        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
-        datefmt="%m/%d/%Y %H:%M:%S",
-        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,
-    )
-    logger.warning(
-        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
-        training_args.local_rank,
-        training_args.device,
-        training_args.n_gpu,
-        bool(training_args.local_rank != -1),
-        training_args.fp16,
-    )
-    logger.info("Training/evaluation parameters %s", training_args)
-
-    # Set seed
-    set_seed(training_args.seed)
-
-    qg_model = QGModel(model_args.model_name_or_path, modality=model_args.modality)
-
-    if training_args.do_train or training_args.do_eval:
-        qgdl = QGDataLoader(
-            tokenizer=qg_model.tokenizer,
-            dataset_name=data_args.dataset_name,
-            input_max_len=data_args.max_len,
-            target_max_len=data_args.target_max_len
-            )
-        
-        train_dataset = qgdl.create("train")
-        valid_dataset = qgdl.create("validation")
-
-        compute_metrics = rouge_metrics(qg_model.tokenizer)
-        
-
-        trainer = QGTrainer(
-            model=qg_model.model,
-            tokenizer = qg_model.tokenizer,
-            args=training_args,
-            train_dataset=train_dataset,
-            valid_dataset=valid_dataset,
-            data_collator=T2TDataCollator(),
-            compute_metrics=compute_metrics
-        )
-
-    if training_args.do_train:
-        trainer.train(
-        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
-        )
-        train_result = trainer.save_model()
-
-        metrics = train_result.metrics
-        trainer.log_metrics("train", metrics)
-        trainer.save_metrics("train", metrics)
-        trainer.save_state()
-    
-    # Inference
-    if inference_args.do_generate:     
-        # There are some arguments to control the type of questions generated such as probability of aggregations, number of where clauses etc. (contd.)
-        # These arguments can optionally be provided by the user as inference arguments. 
-        # Check out the notebook at primeqa/notebooks/qg/tableqginference.ipynb for more details.    
-        
-        # aggregation proobablities
-        agg_prob = [1., 0., 0., 0., 0., 0.]
-        if inference_args.generate_aggregate:
-            agg_prob = [0., 0.2, 0.2, 0.2, 0.2, 0.2]
-        
-        # where clauses
-        where_prob = [0.]*5
-        for i in range(1, 5):
-            if i <= inference_args.max_where_clauses:
-                where_prob [i] = 1.
-        where_prob = [w/sum(where_prob) for w in where_prob]
-
-        with open(inference_args.data_path) as fp:
-            data_list = json.load(fp)
-        
-        generated_questions = qg_model.generate_questions(
-                                data_list,
-                                inference_args.num_questions_per_instance,
-                                agg_prob,
-                                where_prob
-                                )
-        with open(inference_args.gen_output_path, 'w') as fp:
-            json.dump(generated_questions, fp)
-
-    # Evaluation
-    if training_args.do_eval and training_args.local_rank in [-1, 0]:
-        logger.info("*** Evaluate ***")
-
-        metrics = trainer.evaluate()
-        output_eval_file = os.path.join(training_args.output_dir, "eval_results.txt")
-        with open(output_eval_file, "w") as writer:
-            logger.info("***** Eval results *****")
-            for key in sorted(metrics.keys()):
-                logger.info("  %s = %s", key, str(metrics[key]))
-                writer.write("%s = %s\n" % (key, str(metrics[key])))
-        
-        trainer.log_metrics("eval", metrics)
-        trainer.save_metrics("eval", metrics)
-
-if __name__ == "__main__":
-    main(sys.argv)
+import json
+import logging
+import os
+import sys
+from dataclasses import dataclass, field
+from typing import List, Optional
+
+import datasets
+from datasets import list_datasets, load_dataset
+from primeqa.qg.metrics.generation_metrics import rouge_metrics
+from primeqa.qg.models.qg_model import QGModel
+from primeqa.qg.trainers.qg_trainer import QGTrainer
+from primeqa.qg.processors.data_loader import QGDataLoader
+from primeqa.qg.utils.data_collator import T2TDataCollator
+from transformers import (
+    HfArgumentParser,
+    Seq2SeqTrainingArguments,
+    set_seed,
+)
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        default="t5-base",
+        metadata={
+            "help": "Path to pretrained model or model identifier from huggingface.co/models",
+            "choices": ["t5-base", "t5-small", "google/mt5-small", "google/mt5-base"],
+        },
+    )
+    modality: str = field(
+        default="table",
+        metadata={
+            "help": "Whether to generate questions from tables or passages",
+            "choices": ["table", "passage"],
+        },
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Pretrained tokenizer name or path if not the same as model_name"
+        },
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Where do we want to store the pretrained models downloaded from s3"
+        },
+    )
+
+
+@dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+    """
+
+    dataset_name: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Name of the dataset to train the qg model",
+            "choices": list_datasets(),
+        },
+    )
+    dataset_config: Optional[str] = field(
+        default=None,
+        metadata={
+            "help": "Config of the dataset loaded, e.g. 'secondary_task' for TyDiQA"
+        },
+    )
+    train_file_path: Optional[str] = field(
+        default="train_data.pt",
+        metadata={"help": "Path for cached train dataset"},
+    )
+    valid_file_path: Optional[str] = field(
+        default="valid_data.pt",
+        metadata={"help": "Path for cached valid dataset"},
+    )
+    max_len: Optional[int] = field(
+        default=512,
+        metadata={"help": "Max input length for the source text"},
+    )
+    target_max_len: Optional[int] = field(
+        default=32,
+        metadata={"help": "Max input length for the target text"},
+    )
+
+
+@dataclass
+class InferenceArguments:
+    do_generate: Optional[bool] = field(
+        default=False, metadata={"help": "Whether to generate questions"}
+    )
+    num_questions_per_instance: Optional[int] = field(
+        default=5,
+        metadata={"help": "Number of questions to generate per table/passage"},
+    )
+    max_where_clauses: Optional[int] = field(
+        default=1, metadata={"help": "Max number of filters in generated question"}
+    )
+    data_path: str = field(
+        default="primeqa/qg/sample_table.json",
+        metadata={
+            "help": "Path to JSON file with LIST of tables/passages. Each table \
+                              should be a dict with keys 'header' and 'rows', and passages should be str"
+        },
+    )
+    generate_aggregate: Optional[bool] = field(
+        default=False,
+        metadata={
+            "help": "Whether to generate aggregate questions with max, min, sum, etc."
+        },
+    )
+    gen_output_path: Optional[str] = field(
+        default="sample_generation.json",
+        metadata={"help": "path to JSON file where generated questions will be saved"},
+    )
+
+
+def main(raw_args):
+    print(raw_args)
+    parser = HfArgumentParser(
+        (
+            ModelArguments,
+            DataTrainingArguments,
+            Seq2SeqTrainingArguments,
+            InferenceArguments,
+        )
+    )
+
+    if len(raw_args) == 2 and raw_args[1].endswith(".json"):
+        model_args, data_args, training_args, inference_args = parser.parse_json_file(
+            json_file=raw_args[1]
+        )
+    elif len(raw_args) == 1:
+        model_args, data_args, training_args = parser.parse_dict(raw_args[0])
+    else:
+        (
+            model_args,
+            data_args,
+            training_args,
+            inference_args,
+        ) = parser.parse_args_into_dataclasses()
+
+    # These arguments has to be hardcoded in order for Trainer to work
+    training_args.predict_with_generate = True
+    training_args.remove_unused_columns = False
+    training_args.prediction_loss_only = False
+
+    if (
+        os.path.exists(training_args.output_dir)
+        and os.listdir(training_args.output_dir)
+        and training_args.do_train
+        and not training_args.overwrite_output_dir
+    ):
+        raise ValueError(
+            f"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome."
+        )
+
+    # Setup logging
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,
+    )
+    logger.warning(
+        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
+        training_args.local_rank,
+        training_args.device,
+        training_args.n_gpu,
+        bool(training_args.local_rank != -1),
+        training_args.fp16,
+    )
+    logger.info("Training/evaluation parameters %s", training_args)
+
+    # Set seed
+    set_seed(training_args.seed)
+
+    qg_model = QGModel(model_args.model_name_or_path, modality=model_args.modality)
+
+    qgdl = QGDataLoader(
+        tokenizer=qg_model.tokenizer,
+        dataset_name=data_args.dataset_name,
+        modality=model_args.modality,
+        input_max_len=data_args.max_len,
+        target_max_len=data_args.target_max_len,
+    )
+    if training_args.do_train or training_args.do_eval:
+        train_dataset = qgdl.create(
+            dataset_split="train", dataset_config=data_args.dataset_config
+        )
+        valid_dataset = qgdl.create(
+            dataset_split="validation", dataset_config=data_args.dataset_config
+        )
+
+        compute_metrics = rouge_metrics(qg_model.tokenizer)
+
+        trainer = QGTrainer(
+            model=qg_model.model,
+            tokenizer=qg_model.tokenizer,
+            args=training_args,
+            train_dataset=train_dataset,
+            eval_dataset=valid_dataset,
+            data_collator=T2TDataCollator(),
+            compute_metrics=compute_metrics,
+        )
+
+    if training_args.do_train:
+        trainer.train(
+            model_path=model_args.model_name_or_path
+            if os.path.isdir(model_args.model_name_or_path)
+            else None
+        )
+        train_result = trainer.save_model()
+
+        metrics = train_result.metrics
+        trainer.log_metrics("train", metrics)
+        trainer.save_metrics("train", metrics)
+        trainer.save_state()
+
+    # Inference
+    if inference_args.do_generate:
+        # There are some arguments to control the type of questions generated such as probability of aggregations, number of where clauses etc. (contd.)
+        # These arguments can optionally be provided by the user as inference arguments.
+        # Check out the notebook at primeqa/notebooks/qg/tableqginference.ipynb for more details.
+
+        # aggregation proobablities
+        agg_prob = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]
+        if inference_args.generate_aggregate:
+            agg_prob = [0.0, 0.2, 0.2, 0.2, 0.2, 0.2]
+
+        # where clauses
+        where_prob = [0.0] * 5
+        for i in range(1, 5):
+            if i <= inference_args.max_where_clauses:
+                where_prob[i] = 1.0
+        where_prob = [w / sum(where_prob) for w in where_prob]
+
+        with open(inference_args.data_path) as fp:
+            data_list = json.load(fp)
+
+        generated_questions = qg_model.generate_questions(
+            data_list, inference_args.num_questions_per_instance, agg_prob, where_prob
+        )
+        with open(inference_args.gen_output_path, "w") as fp:
+            json.dump(generated_questions, fp)
+
+    # Evaluation
+    if training_args.do_eval and training_args.local_rank in [-1, 0]:
+        logger.info("*** Evaluate ***")
+
+        metrics = trainer.evaluate()
+        output_eval_file = os.path.join(training_args.output_dir, "eval_results.txt")
+        with open(output_eval_file, "w") as writer:
+            logger.info("***** Eval results *****")
+            for key in sorted(metrics.keys()):
+                logger.info("  %s = %s", key, str(metrics[key]))
+                writer.write("%s = %s\n" % (key, str(metrics[key])))
+
+        trainer.log_metrics("eval", metrics)
+        trainer.save_metrics("eval", metrics)
+
+
+if __name__ == "__main__":
+    main(sys.argv)
```

## primeqa/qg/models/qg_model.py

```diff
@@ -1,93 +1,95 @@
-from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
-from primeqa.qg.utils.constants import QGSpecialTokens
-from primeqa.qg.models.table_qg.sql_sampler import SimpleSqlSampler
-from primeqa.qg.models.passage_qg.answer_sampler import AnswerSampler
-
-
-
-class QGModel():
-    def __init__(self,model_path, modality='table'):
-        """ Table Question Generation Model gets initialized based on either pre-trained model path or
-        the model name. One example could be 't5-base'.
-
-        Args:
-            model_path (String): Either Name of the model or the path to the pre-trained model
-        """        
-        self._model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
-        self._tokenizer = AutoTokenizer.from_pretrained(model_path)
-        # adding special tokens to tokenizer which will be used to convert SQL and Passage+Answer to string
-        # expanding token embeddings in model
-        
-        sql_tokens_list = [QGSpecialTokens.sep, QGSpecialTokens.cond, QGSpecialTokens.ans,
-                        QGSpecialTokens.header, QGSpecialTokens.hsep]
-        for sql_token in sql_tokens_list:
-            if sql_token not in self._tokenizer.vocab: # add only when special-tokens aren't already there
-                self._tokenizer.add_tokens([sql_token])
-        self._model.resize_token_embeddings(len(self._tokenizer.vocab))
-
-        self.modality = modality 
-        if self.modality == 'passage':
-            self.answer_sampler = AnswerSampler()
-        elif self.modality == 'table':
-            self.sql_sampler = SimpleSqlSampler()
-
-    
-    @property
-    def model(self):
-        """ Propery of TableQG model.
-
-        Returns:
-            Sequence to sequence model object (based on model name)
-        """
-        return self._model
-
-    @property
-    def tokenizer(self):
-        """ Property of TableQG model.
-
-        Returns:
-            Tokenizer class object based on the model name/ path
-        """
-        return self._tokenizer
-
-    def generate_questions(self, 
-                data_list, 
-                num_questions_per_instance=5, 
-                agg_prob=[], 
-                num_where_prob=[], 
-                ineq_prob=0.0,
-                answers_list=[],
-                id_list=[]):
-                
-        if type(data_list) == dict:
-            data_list = [data_list]
-
-        if self.modality == 'table':
-            input_str_list, sql_list, id_question_list = self.sql_sampler.controlled_sample_sql(data_list, num_questions_per_instance, agg_prob, num_where_prob, ineq_prob, id_list)
-            answer_list = [s['answer'] for s in sql_list]
-        elif self.modality == 'passage':
-            input_str_list, answer_list, id_question_list , id_context_map = self.answer_sampler.create_qg_input(data_list, num_questions_per_instance, answers_list, id_list)
-
-        input_ids = self._tokenizer(input_str_list, 
-            return_tensors='pt', 
-            padding=True,
-            truncation=True).input_ids
-
-        generated_ids = self._model.generate(input_ids,
-            max_length=60, 
-            num_beams=10,
-            repetition_penalty=2.5,
-            length_penalty=1.0,
-            early_stopping=True)
-
-        questions = [self._tokenizer.decode(g, skip_special_tokens=True,
-                            clean_up_tokenization_spaces=True) for g in generated_ids]
-        
-        if id_question_list == [] :
-            questions_dict = [{'question': questions[i], 'answer': answer_list[i]} for i in range(len(questions))]
-        elif self.modality == 'passage' :
-            questions_dict = [{'context_id':id_question_list[i], 'context':id_context_map.get(id_question_list[i]),'question': questions[i], 'answer': answer_list[i]} for i in range(len(questions))]
-        else:
-            questions_dict = [{'context_id':id_question_list[i], 'question': questions[i], 'answer': answer_list[i]} for i in range(len(questions))]
-        
-        return questions_dict
+from primeqa.qg.models.passage_qg.answer_sampler import AnswerSampler
+from primeqa.qg.models.table_qg.sql_sampler import SimpleSqlSampler
+from primeqa.qg.utils.constants import QGSpecialTokens
+from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
+
+
+class QGModel():
+    def __init__(self,model_path, modality='table'):
+        """ Table Question Generation Model gets initialized based on either pre-trained model path or
+        the model name. One example could be 't5-base'.
+
+        Args:
+            model_path (str): Either Name of the model or the path to the pre-trained model
+            modality (str, optional): The modality specifies what data is predicted based on which input. Possible options include 'table' and 'passage'.
+        """        
+        self._model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
+        self._tokenizer = AutoTokenizer.from_pretrained(model_path)
+        
+        special_tokens_list = []
+
+        self.modality = modality 
+        if self.modality == 'passage':
+            special_tokens_list.append(QGSpecialTokens.sep)
+            self.answer_sampler = AnswerSampler()
+        elif self.modality == 'table':
+            special_tokens_list.extend([QGSpecialTokens.sep, QGSpecialTokens.cond, QGSpecialTokens.ans,
+                            QGSpecialTokens.header, QGSpecialTokens.hsep])
+            self.sql_sampler = SimpleSqlSampler()
+
+        # adding special tokens to tokenizer which will be used to convert SQL and Passage+Answer to string
+        for special_token in special_tokens_list:
+            if special_token not in self._tokenizer.vocab: # add only when special-tokens aren't already there
+                self._tokenizer.add_tokens([special_token])
+        # expanding token embeddings in model
+        self._model.resize_token_embeddings(len(self._tokenizer.vocab))
+    
+    @property
+    def model(self):
+        """ Propery of TableQG model.
+
+        Returns:
+            Sequence to sequence model object (based on model name)
+        """
+        return self._model
+
+    @property
+    def tokenizer(self):
+        """ Property of TableQG model.
+
+        Returns:
+            Tokenizer class object based on the model name/ path
+        """
+        return self._tokenizer
+
+    def generate_questions(self, 
+                data_list, 
+                num_questions_per_instance=5, 
+                agg_prob=[], 
+                num_where_prob=[], 
+                ineq_prob=0.0,
+                answers_list=[],
+                id_list=[]):
+                
+        if type(data_list) == dict:
+            data_list = [data_list]
+
+        if self.modality == 'table':
+            input_str_list, sql_list, id_question_list = self.sql_sampler.controlled_sample_sql(data_list, num_questions_per_instance, agg_prob, num_where_prob, ineq_prob, id_list)
+            answer_list = [s['answer'] for s in sql_list]
+        elif self.modality == 'passage':
+            input_str_list, answer_list, id_question_list , id_context_map = self.answer_sampler.create_qg_input(data_list, num_questions_per_instance, answers_list, id_list)
+
+        input_ids = self._tokenizer(input_str_list, 
+            return_tensors='pt', 
+            padding=True,
+            truncation=True).input_ids
+
+        generated_ids = self._model.generate(input_ids,
+            max_length=60, 
+            num_beams=10,
+            repetition_penalty=2.5,
+            length_penalty=1.0,
+            early_stopping=True)
+
+        questions = [self._tokenizer.decode(g, skip_special_tokens=True,
+                            clean_up_tokenization_spaces=True) for g in generated_ids]
+        
+        if id_question_list == [] :
+            questions_dict = [{'question': questions[i], 'answer': answer_list[i]} for i in range(len(questions))]
+        elif self.modality == 'passage' :
+            questions_dict = [{'context_id':id_question_list[i], 'context':id_context_map.get(id_question_list[i]),'question': questions[i], 'answer': answer_list[i]} for i in range(len(questions))]
+        else:
+            questions_dict = [{'context_id':id_question_list[i], 'question': questions[i], 'answer': answer_list[i]} for i in range(len(questions))]
+        
+        return questions_dict
```

## primeqa/qg/models/passage_qg/answer_sampler.py

```diff
@@ -1,86 +1,86 @@
-from primeqa.qg.utils.constants import QGSpecialTokens
-import numpy as np
-import stanza
-from stanza.models.common.doc import Document
-from stanza.pipeline.core import Pipeline
-
-
-stanza.download(lang="multilingual")
-
-class AnswerSampler():
-    """
-	Class for sampling answer tokens from passage, to be used for PassageQG inference.
-	"""
-    def __init__(self):
-        """_summary_
-        We use NERs from stanza library here. Currently we support four languages: Arabic, English, Finnish and Russian.
-        These languages are in the intersection between TyDi QA data and Stanza NER.
-        """
-        self.lang_codes = {'ar': {'name':'Arabic', 'method_available': 'NER'},
-                           'en': {'name':'English', 'method_available':'NER'},
-                           'fi': {'name':'Finnish', 'method_available':'NER'},
-                           'id': {'name':'Indonesian', 'method_available':'UD'},
-                           'ko': {'name':'Korean', 'method_available':'UD'},
-                           'ru': {'name':'Russian', 'method_available':'NER'},
-                           'te': {'name':'Telugu', 'method_available':'UD'}
-                        }
-        self.lang_identify = Pipeline(lang="multilingual", processors="langid", verbose=False)
-        self.ner_models = {}
-        for lang in self.lang_codes:
-            if self.lang_codes[lang]['method_available'] == 'NER':
-                self.ner_models[lang] = stanza.Pipeline(lang=lang, processors='tokenize,ner', verbose=False)
-                print('Loaded NER model for ',self.lang_codes[lang]['name'])            
-    
-    def detect_language(self, text):
-        """
-        detect the language of an input passage, used later for identifying the right NER model to use
-        """
-        doc = Document([], text=text)
-        self.lang_identify(doc)
-        if doc.lang not in self.lang_codes:
-            NotImplementedError('This language is not supported: ' + doc.lang)
-        return doc.lang
-
-    def get_named_entities(self, text):
-        """
-        pick the right NER model to use based on detected language of the passage
-        """
-        lang = self.detect_language(text)
-        print('Input language', lang)
-        if self.lang_codes[lang]['method_available'] == 'NER':
-            output = self.ner_models[lang](text)
-            return [ent.text for ent in output.ents]
-        else:
-            NotImplementedError
-    
-    def create_qg_input(self, 
-                        data_list, 
-                        num_questions_per_instance = 5, 
-                        answers_list=[],
-                        id_list=[]):
-        """
-        create the input for qg training: If the answers are provided in answers_list, use them. 
-        Otherwise sampling named entities as possible answers from text passages.
-        """
-        input_str_list = []
-        ans_list = []
-        id_question_list = []
-        id_context_map = dict()
-        for i, data in enumerate(data_list):
-            # If answers_list provided, then use them to generate questions. Otherwise use NER to
-            # sample answers.
-            if len(answers_list) > i and answers_list[i] != []: 
-                answers = answers_list[i]
-            else:
-                answers = self.get_named_entities(data)
-            if num_questions_per_instance < len(answers):
-                answers = np.random.choice(answers, num_questions_per_instance, replace=False)
-            for ans in answers:
-                text = ans + ' '+QGSpecialTokens.sep+' ' + data        
-                input_str_list.append(text)
-                ans_list.append(ans)
-                if len(id_list) > i and id_list[i] != None :
-                    id_question_list.append(id_list[i])
-                    id_context_map[id_list[i]]=data
-
-        return input_str_list, ans_list, id_question_list, id_context_map
+from primeqa.qg.utils.constants import QGSpecialTokens
+import numpy as np
+import stanza
+from stanza.models.common.doc import Document
+from stanza.pipeline.core import Pipeline
+
+
+stanza.download(lang="multilingual")
+
+class AnswerSampler():
+    """
+	Class for sampling answer tokens from passage, to be used for PassageQG inference.
+	"""
+    def __init__(self):
+        """_summary_
+        We use NERs from stanza library here. Currently we support four languages: Arabic, English, Finnish and Russian.
+        These languages are in the intersection between TyDi QA data and Stanza NER.
+        """
+        self.lang_codes = {'ar': {'name':'Arabic', 'method_available': 'NER'},
+                           'en': {'name':'English', 'method_available':'NER'},
+                           'fi': {'name':'Finnish', 'method_available':'NER'},
+                           'id': {'name':'Indonesian', 'method_available':'UD'},
+                           'ko': {'name':'Korean', 'method_available':'UD'},
+                           'ru': {'name':'Russian', 'method_available':'NER'},
+                           'te': {'name':'Telugu', 'method_available':'UD'}
+                        }
+        self.lang_identify = Pipeline(lang="multilingual", processors="langid", verbose=False)
+        self.ner_models = {}
+        for lang in self.lang_codes:
+            if self.lang_codes[lang]['method_available'] == 'NER':
+                self.ner_models[lang] = stanza.Pipeline(lang=lang, processors='tokenize,ner', verbose=False)
+                print('Loaded NER model for ',self.lang_codes[lang]['name'])            
+    
+    def detect_language(self, text):
+        """
+        detect the language of an input passage, used later for identifying the right NER model to use
+        """
+        doc = Document([], text=text)
+        self.lang_identify(doc)
+        if doc.lang not in self.lang_codes:
+            raise NotImplementedError('This language is not supported: ' + doc.lang)
+        return doc.lang
+
+    def get_named_entities(self, text):
+        """
+        pick the right NER model to use based on detected language of the passage
+        """
+        lang = self.detect_language(text)
+        print('Input language', lang)
+        if self.lang_codes[lang]['method_available'] == 'NER':
+            output = self.ner_models[lang](text)
+            return [ent.text for ent in output.ents]
+        else:
+            raise NotImplementedError
+    
+    def create_qg_input(self, 
+                        data_list, 
+                        num_questions_per_instance = 5, 
+                        answers_list=[],
+                        id_list=[]):
+        """
+        create the input for qg training: If the answers are provided in answers_list, use them. 
+        Otherwise sampling named entities as possible answers from text passages.
+        """
+        input_str_list = []
+        ans_list = []
+        id_question_list = []
+        id_context_map = dict()
+        for i, data in enumerate(data_list):
+            # If answers_list provided, then use them to generate questions. Otherwise use NER to
+            # sample answers.
+            if len(answers_list) > i and answers_list[i] != []: 
+                answers = answers_list[i]
+            else:
+                answers = self.get_named_entities(data)
+            if num_questions_per_instance < len(answers):
+                answers = np.random.choice(answers, num_questions_per_instance, replace=False)
+            for ans in answers:
+                text = ans + ' '+QGSpecialTokens.sep+' ' + data        
+                input_str_list.append(text)
+                ans_list.append(ans)
+                if len(id_list) > i and id_list[i] != None :
+                    id_question_list.append(id_list[i])
+                    id_context_map[id_list[i]]=data
+
+        return input_str_list, ans_list, id_question_list, id_context_map
```

## primeqa/qg/models/table_qg/sql_sampler.py

 * *Ordering differences only*

```diff
@@ -1,467 +1,467 @@
-import math
-from copy import deepcopy
-import numpy as np
-from primeqa.qg.utils.constants import SqlOperants, QGSpecialTokens
-
-class SimpleSqlSampler():
-    """ A simple sql sampler to sample sqls based on number of where clause conditions and other parameters
-    """
-    def __init__(self):
-        self.sql_tokens = QGSpecialTokens
-
-    @staticmethod
-    def add_column_types(table):
-        """Adds a data type list to the table dict based on values in the cells in that column.
-        The data type for a column is either real or text.
-        Args:
-            table ([dict]): [The table Dict containing headers and rows]
-
-        Returns:
-            [Dict]: [Table Dict with  key 'type' containing list of data type for every column]
-        """
-
-        header = table['header']
-        rows = table['rows']
-
-        # identifying column types. Initializing with real
-        types = ['real'] * len(header)
-        for r in rows:
-            for i in range(len(header)):
-                x = str(r[i]).replace(',','')
-                try:
-                    _ = float(x)
-                except ValueError:
-                    # if not able to convert string to float in any cell the whole column is 'text'
-                    types[i] = 'text'
-
-        # converting str to float for real columns
-        for r in range(len(rows)):
-            for i in range(len(header)):
-                if types[i] == 'real':
-                    x = str(rows[r][i]).replace(',','')
-                    rows[r][i] = float(x)
-
-        table['types'] = types
-        table['rows'] = rows
-        return table
-
-    def sql_execution(self, where_clause, select_column, agg_op, table):
-        """ This function executes the sql on a given table and returns the answer.
-
-        Args:
-            where_clause ([type]): [description]
-            select_column ([type]): [description]
-            agg_op ([type]): [description]
-            table ([type]): [description]
-
-        Returns:
-            [String]: [Answer after executing sql on the given table]
-        """
-        
-
-        selected_cells = []
-        
-        if (len(where_clause)>0):
-            for row_id in where_clause['rows']:
-                selected_cells.append(table['rows'][row_id][select_column])
-        else:
-            for row in table['rows']:
-                selected_cells.append(row[select_column])
-        
-        if table['types'][select_column] == 'real':
-            selected_cells = [float(str(s).replace(',','')) for s in selected_cells]
-        else:
-            selected_cells = [s.lower() for s in selected_cells]
-        
-        #agg_op list -> ['select', 'maximum', 'minimum', 'count', 'sum', 'average']
-        if agg_op == 0:
-            answer =  selected_cells
-        elif agg_op == 1:
-            answer = [max(selected_cells)]
-        elif agg_op == 2:
-            answer = [min(selected_cells)]
-        elif agg_op == 3:
-            answer = [len(selected_cells)]
-        elif agg_op == 4:
-            answer = [sum(selected_cells)]
-        elif agg_op == 5:
-            answer = [sum(selected_cells)/len(selected_cells)]
-        
-        return answer
-    
-    @staticmethod
-    def _get_inequality_conds(col, num_conditions=5):
-        unique_set = np.unique(col)
-        conds_list = []
-
-        for val in unique_set:
-            greater_id_list = []
-            lesser_id_list = []
-            for i in range(len(col)):
-                if col[i] > val:
-                    greater_id_list.append(i)
-                elif col[i] < val:
-                    lesser_id_list.append(i)
-            if len(lesser_id_list) > 0:
-                conds_list.append([str(val), 2, lesser_id_list])
-            if len(greater_id_list) > 0:
-                conds_list.append([str(val), 1, greater_id_list])
-
-        # Many inequality conditions can be generated for a real column. This makes
-        # it computationally expensive later when creating multiple where clauses. 
-        # We will sample inequalities here for that reason.
-        sampled_idx = np.random.choice(len(conds_list), min(
-            num_conditions, len(conds_list)), replace=False)
-        sampled_conds_list = [conds_list[i] for i in sampled_idx]
-
-        return sampled_conds_list
-
-
-    def _get_column_freq(self, table, if_ineq=False):
-        """ Calculates frequency of a column in the table.
-
-        Args:
-            table ([Dict]): [Table Dictionary containing header and rows]
-            if_ineq (bool, optional): [if there are inequality conditions or not]. Defaults to False.
-
-        Returns:
-            [List]: [Column List]
-        """
-        rows = table['rows']
-        types = table['types']
-
-        if if_ineq:
-            num_real_cols = len([t for t in types if t == 'real'])
-            if num_real_cols > 0:
-                num_ineq_conds = max(round(50/num_real_cols), 1)
-
-        # creating column lists
-        cols = [[] for _ in range(len(rows[0]))]
-        for r in rows:
-            for i, cell in enumerate(r):
-                cols[i].append(cell)
-
-        # finding unique in each column
-        cols_list = []
-        for j, col in enumerate(cols):
-            cdict = {}
-            for i, cell in enumerate(col):
-                if cell not in cdict:
-                    cdict[cell] = [i]
-                else:
-                    cdict[cell].append(i)
-            clist = []
-            for c in cdict:
-                clist.append([c, 0, cdict[c]])
-            # adding inequality conditions
-            if types[j] == 'real' and if_ineq:
-                clist.extend(self._get_inequality_conds(col, num_ineq_conds))
-
-            cols_list.append(clist)
-        return cols_list
-
-    
-    def _check_condition(self, conds, cols_list):
-        all_rows = []
-        for c in conds:
-            for cell in cols_list[c[0]]:
-                if cell[0] == c[2] and cell[1] == c[1]:  # if conditions match
-                    all_rows.append(cell[2])
-        intersection_len = len(set(all_rows[0]).intersection(*all_rows))
-
-        # check if the condition gives non zero number of rows
-        if intersection_len == 0:
-            return False
-
-        # check if any subset condition can produce same set of rows
-        for r in all_rows:
-            rows = deepcopy(all_rows)
-            rows.remove(r)
-            rlen = len(set(rows[0]).intersection(*rows))
-            if rlen == intersection_len:
-                return False
-        return True
-
-    def _get_unique_conditions(self, wlist):
-        wdict = {}
-        for wc in wlist:
-            conds_str = str(sorted([str(c) for c in wc['conds']]))
-            wdict[conds_str] = wc
-        wlist = []
-        for key in wdict:
-            wlist.append(wdict[key])
-        return wlist
-
-    def get_where_clauses(self, table, num_where=2, if_ineq=False):
-        cols_list = self._get_column_freq(table, if_ineq)
-        where_dict = {}
-        if num_where==0:
-            return where_dict
-
-        where1_list = []
-        for i, c in enumerate(cols_list):
-            for cell in c:
-                wc = {'conds': [[i, cell[1], cell[0]]], 'rows': cell[2]}
-                where1_list.append(wc)
-        where_dict['nw-1'] = where1_list
-
-        # removing cells which only appear once before going to multiple where
-        cc_list = []
-        for i in range(len(table['header'])):
-            cc = []
-            for cell in cols_list[i]:
-                if len(cell[2]) > 1:
-                    cc.append(cell)
-            cc_list.append(cc)
-        cols_list = cc_list
-
-        if num_where >= 2:
-            where2_list = []
-            for i in range(len(cols_list)):
-                colA = cols_list[i]
-                for j in range(i+1, len(cols_list)):
-                    colB = cols_list[j]
-                    for ca in colA:
-                        for cb in colB:
-                            intersection = list(set(ca[2]) & set(cb[2]))
-                            conds = [[i, ca[1], ca[0]], [j, cb[1], cb[0]]]
-                            # if len(intersection) < len(colA[ca]) and len(intersection) < len(colB[cb]) and len(intersection) > 0:
-                            if self._check_condition(conds, cols_list):
-                                wc = {'conds': [[i, ca[1], ca[0]], [
-                                    j, cb[1], cb[0]]], 'rows': intersection}
-                                where2_list.append(wc)
-            where_dict['nw-2'] = where2_list
-
-        if num_where >= 3:
-            where3_list = []
-            for w2 in where2_list:
-                if len(w2['rows']) > 1:
-
-                    for i in range(len(cols_list)):
-                        if i not in [c[0] for c in w2['conds']]:
-                            for cc in cols_list[i]:
-                                conds = deepcopy(w2['conds'])
-                                conds.append([i, cc[1], cc[0]])
-                                if self._check_condition(conds, cols_list):
-                                    intersection = list(
-                                        set(w2['rows']) & set(cc[2]))
-                                    wc = {'conds': conds, 'rows': intersection}
-                                    where3_list.append(wc)
-            where3_list = self._get_unique_conditions(where3_list)
-            where_dict['nw-3'] = where3_list
-
-        if num_where == 4:
-            where4_list = []
-            for w3 in where3_list:
-                if len(w3['rows']) > 1:
-
-                    for i in range(len(cols_list)):
-                        if i not in [c[0] for c in w3['conds']]:
-                            for cc in cols_list[i]:
-                                conds = deepcopy(w3['conds'])
-                                conds.append([i, cc[1], cc[0]])
-                                if self._check_condition(conds, cols_list):
-                                    intersection = list(
-                                        set(w3['rows']) & set(cc[2]))
-                                    wc = {'conds': conds, 'rows': intersection}
-                                    where4_list.append(wc)
-            where4_list = self._get_unique_conditions(where4_list)
-            where_dict['nw-4'] = where4_list
-        return where_dict
-
-
-    def sample_sql(self, table, num_sample, num_where, agg_op=0, if_ineq=False):
-        """ This function samples sqls from a given table based on values for the parameters
-        num_sample -> number of sql queries to sample, num_where -> number of where condtioned desired in every sampled sql Query etc.
-        Args:
-            table ([Dict]): [Table dictionary with header and rows]
-            num_sample ([int]): [Number of sqls to sample]
-            num_where ([int]): [Number of where clause conditions every sampled sql should have]
-            agg_op (int, optional): [Whether to sample aggregate queries or not]. Defaults to 0.
-            if_ineq (bool, optional): [description]. Defaults to False.
-            
-        Returns:
-            [List,Dict]: [Sampled sql query list in readable string format and dict format]
-        """
-        header = table['header']
-        types = table['types']
-        where_list = [] 
-        multiple_where_dict = self.get_where_clauses(table, num_where, if_ineq)
-        if num_where > 0 :
-            where_list = multiple_where_dict['nw-' + str(num_where)]
-        real_cols = [i for i in range(len(types)) if types[i] == 'real']
-
-        if_agg = 0
-        if agg_op != 0:
-            if_agg = 1
-
-        filtered_where_list = []
-        for wc in where_list:
-            cols_in_where = [c[0] for c in wc['conds']]
-            if if_agg and len(wc['rows']) > 1 and len(set(real_cols) - set(cols_in_where)) >= 1:
-                filtered_where_list.append(wc)
-            elif not if_agg and len(wc['rows']) == 1:
-                filtered_where_list.append(wc)
-
-        if len(filtered_where_list) == 0 and num_where > 0 :
-            return []
-        
-        sampled_where_list = []
-        
-        if num_where > 0:
-            sample_ids = np.random.choice(
-                len(filtered_where_list), num_sample, replace=True)
-            sampled_where_list = [filtered_where_list[i] for i in sample_ids]
-
-        if if_agg:
-            clist = real_cols
-        else:
-            clist = list(range(len(header)))
-
-        sql_string_list = []
-        sql_list = []
-        
-        if (len(sampled_where_list)>0):
-            for wc in sampled_where_list:
-                try:
-                    possible_cols = list(set(clist) - set([c[0] for c in wc['conds']]))
-                    select_column = np.random.choice(possible_cols)
-                    sql = {'sel': select_column, 'agg': agg_op}
-                    sql['conds'] = wc['conds']
-                    answer = self.sql_execution(wc, select_column, agg_op, table)
-                    sql_dict = self.readable_sql(sql, header, answer)
-
-                    sql_list.append(sql_dict)
-                    sql_string_list.append(self.convert_sql_to_string(sql_dict, table))
-                except:
-                    print('\nQuery creation error')
-        else :
-            possible_cols = list(set(clist))
-            select_column = np.random.choice(possible_cols)
-            sql = {'sel': select_column, 'agg': agg_op}
-            wc={}
-            answer = self.sql_execution(wc, select_column, agg_op, table)
-            sql_dict = self.readable_sql(sql, header, answer)
-            sql_list.append(sql_dict)
-            sql_string_list.append(self.convert_sql_to_string(sql_dict, table))
-
-
-        return sql_string_list, sql_list
-
-    def readable_sql(self, sql, header, answer):
-        """Convert Non-readable SQL to readable SQL dict
-
-        Args:
-            sql ([Dict]): [Sql query]
-            header ([List]): [Headers in table]
-            answer ([String]): [Answer]
-
-        Returns:
-            [Dict]: [Sql Dict in readable format]
-        """
-        sql_dict = {}
-        sql_dict['col'] = [SqlOperants.agg_ops[sql['agg']], header[sql['sel']]]
-        conds = []
-        if "conds" in sql :
-            for c in sql['conds']:
-                cond = [header[c[0]], SqlOperants.cond_ops_string[c[1]], str(c[2])]
-                conds.append(cond)
-            sql_dict['conds'] = conds
-        sql_dict['answer'] = str(answer[0])
-        return sql_dict
-
-    def convert_sql_to_string(self, sql_dict, table=[], tokenizer='T5'):
-        """Convert sql query in Dict format to string
-
-        Args:
-            sql_dict ([Dict]): [Sql Query to convert to string]
-            table (list, optional): [Table]. Defaults to [].
-            tokenizer (str, optional): [description]. Defaults to 'T5'.
-        Returns:
-            [String]: [Sql query in string format]
-        """
-        tokens = self.sql_tokens
-
-        sql_str =  str(sql_dict['col'][0]) + ' '+tokens.sep+' ' + str(sql_dict['col'][1])
-        if "conds" in sql_dict :
-            for cond in sql_dict['conds']:
-                sql_str += ' '+tokens.sep+' '
-                sql_str += (' '+tokens.cond+' ').join([str(c) for c in cond])
-        sql_str += ' '+tokens.ans+' ' + str(sql_dict['answer'])
-
-        table['header'] = [str(h) for h in table['header']]
-        sql_str += ' '+tokens.header+' ' + (' '+tokens.hsep+' ').join(table['header'])
-
-        return sql_str
-
-    def controlled_sample_sql(self, table_list, num_samples_per_table=5, agg_prob=[], num_where_prob=[], ineq_prob=0.0,id_list=[]):
-        if agg_prob == []:
-            # agg_prob = [0.6, 0.1, 0.1, 0.0, 0.1, 0.1] #['select', 'maximum', 'minimum', 'count', 'sum', 'average']
-            # ['select', 'maximum', 'minimum', 'count', 'sum', 'average']
-            agg_prob = [1.0, 0., 0., 0., 0., 0.]
-        if num_where_prob == []:
-            # num_where_prob = [0.0, 0.45, 0.3, 0.2, 0.05] # [0,1,2,3,4], there is no zero where clause case
-            # [0,1,2,3,4], there is no zero where clause case
-            num_where_prob = [0.0, 1., 0., 0., 0.]
-
-        sample_batch_size = 1
-        sample_size_list = [sample_batch_size] * \
-            math.floor(num_samples_per_table/sample_batch_size)
-        if num_samples_per_table % sample_batch_size != 0:
-            sample_size_list.append(num_samples_per_table % sample_batch_size)
-
-        all_sql_str_list = []
-        all_sql_list = []
-        all_id_list = []
-        # for table in tqdm(table_list):
-        for i, table in enumerate(table_list):
-            if 'types' not in table:
-                table = self.add_column_types(table)
-            for num_samples in sample_size_list:
-                agg_op = np.random.choice(len(agg_prob), 1, True, agg_prob)[0]
-                num_where = np.random.choice(
-                    len(num_where_prob), 1, True, num_where_prob)[0]
-
-                # if to use ineq.
-                if_ineq = np.random.choice(2, 1, True, [1-ineq_prob, ineq_prob])[0]
-                sql_str_list, sql_list = self.sample_sql(table, num_samples,
-                                    num_where, agg_op, if_ineq)
-
-                num_trials = 0
-                while len(sql_str_list) < num_samples:
-                    diff = num_samples - len(sql_str_list)
-                    if 'real' not in table['types'] and agg_op != 0:
-                        agg_op = 0
-                    elif num_where > 1:
-                        if if_ineq:
-                            num_where -= 1
-                        else:
-                            if_ineq = 1
-                    elif num_where == 1 and agg_op != 0:
-                        if not if_ineq:
-                            if_ineq = 1
-                        else:
-                            agg_op = 0
-                    elif num_where == 1 and agg_op == 0:
-                        agg_op = np.random.choice([1, 2, 3, 4, 5])
-                    diff_sql_str_list, diff_sql_list = self.sample_sql(
-                                        table, diff, num_where, agg_op, if_ineq)
-                    
-                    sql_str_list.extend(diff_sql_str_list)
-                    sql_list.extend(diff_sql_list)
-
-                    num_trials += 1
-                    if num_trials > 5:  # if we cant get it in 5 trials, let us skip this instances
-                        print('Unsuccessful.')
-                        break
-
-                all_sql_str_list.extend(sql_str_list)
-                if i < len(id_list) and id_list[i]!=None :
-                    id_num = id_list[i]
-                elif(len(id_list)>0):
-                    id_num = "NA"
-                if(len(id_list)>0):
-                    id_num_list= [id_num] * len(sql_str_list)
-                    all_id_list.extend(id_num_list)
-                all_sql_list.extend(sql_list)
+import math
+from copy import deepcopy
+import numpy as np
+from primeqa.qg.utils.constants import SqlOperants, QGSpecialTokens
+
+class SimpleSqlSampler():
+    """ A simple sql sampler to sample sqls based on number of where clause conditions and other parameters
+    """
+    def __init__(self):
+        self.sql_tokens = QGSpecialTokens
+
+    @staticmethod
+    def add_column_types(table):
+        """Adds a data type list to the table dict based on values in the cells in that column.
+        The data type for a column is either real or text.
+        Args:
+            table ([dict]): [The table Dict containing headers and rows]
+
+        Returns:
+            [Dict]: [Table Dict with  key 'type' containing list of data type for every column]
+        """
+
+        header = table['header']
+        rows = table['rows']
+
+        # identifying column types. Initializing with real
+        types = ['real'] * len(header)
+        for r in rows:
+            for i in range(len(header)):
+                x = str(r[i]).replace(',','')
+                try:
+                    _ = float(x)
+                except ValueError:
+                    # if not able to convert string to float in any cell the whole column is 'text'
+                    types[i] = 'text'
+
+        # converting str to float for real columns
+        for r in range(len(rows)):
+            for i in range(len(header)):
+                if types[i] == 'real':
+                    x = str(rows[r][i]).replace(',','')
+                    rows[r][i] = float(x)
+
+        table['types'] = types
+        table['rows'] = rows
+        return table
+
+    def sql_execution(self, where_clause, select_column, agg_op, table):
+        """ This function executes the sql on a given table and returns the answer.
+
+        Args:
+            where_clause ([type]): [description]
+            select_column ([type]): [description]
+            agg_op ([type]): [description]
+            table ([type]): [description]
+
+        Returns:
+            [String]: [Answer after executing sql on the given table]
+        """
+        
+
+        selected_cells = []
+        
+        if (len(where_clause)>0):
+            for row_id in where_clause['rows']:
+                selected_cells.append(table['rows'][row_id][select_column])
+        else:
+            for row in table['rows']:
+                selected_cells.append(row[select_column])
+        
+        if table['types'][select_column] == 'real':
+            selected_cells = [float(str(s).replace(',','')) for s in selected_cells]
+        else:
+            selected_cells = [s.lower() for s in selected_cells]
+        
+        #agg_op list -> ['select', 'maximum', 'minimum', 'count', 'sum', 'average']
+        if agg_op == 0:
+            answer =  selected_cells
+        elif agg_op == 1:
+            answer = [max(selected_cells)]
+        elif agg_op == 2:
+            answer = [min(selected_cells)]
+        elif agg_op == 3:
+            answer = [len(selected_cells)]
+        elif agg_op == 4:
+            answer = [sum(selected_cells)]
+        elif agg_op == 5:
+            answer = [sum(selected_cells)/len(selected_cells)]
+        
+        return answer
+    
+    @staticmethod
+    def _get_inequality_conds(col, num_conditions=5):
+        unique_set = np.unique(col)
+        conds_list = []
+
+        for val in unique_set:
+            greater_id_list = []
+            lesser_id_list = []
+            for i in range(len(col)):
+                if col[i] > val:
+                    greater_id_list.append(i)
+                elif col[i] < val:
+                    lesser_id_list.append(i)
+            if len(lesser_id_list) > 0:
+                conds_list.append([str(val), 2, lesser_id_list])
+            if len(greater_id_list) > 0:
+                conds_list.append([str(val), 1, greater_id_list])
+
+        # Many inequality conditions can be generated for a real column. This makes
+        # it computationally expensive later when creating multiple where clauses. 
+        # We will sample inequalities here for that reason.
+        sampled_idx = np.random.choice(len(conds_list), min(
+            num_conditions, len(conds_list)), replace=False)
+        sampled_conds_list = [conds_list[i] for i in sampled_idx]
+
+        return sampled_conds_list
+
+
+    def _get_column_freq(self, table, if_ineq=False):
+        """ Calculates frequency of a column in the table.
+
+        Args:
+            table ([Dict]): [Table Dictionary containing header and rows]
+            if_ineq (bool, optional): [if there are inequality conditions or not]. Defaults to False.
+
+        Returns:
+            [List]: [Column List]
+        """
+        rows = table['rows']
+        types = table['types']
+
+        if if_ineq:
+            num_real_cols = len([t for t in types if t == 'real'])
+            if num_real_cols > 0:
+                num_ineq_conds = max(round(50/num_real_cols), 1)
+
+        # creating column lists
+        cols = [[] for _ in range(len(rows[0]))]
+        for r in rows:
+            for i, cell in enumerate(r):
+                cols[i].append(cell)
+
+        # finding unique in each column
+        cols_list = []
+        for j, col in enumerate(cols):
+            cdict = {}
+            for i, cell in enumerate(col):
+                if cell not in cdict:
+                    cdict[cell] = [i]
+                else:
+                    cdict[cell].append(i)
+            clist = []
+            for c in cdict:
+                clist.append([c, 0, cdict[c]])
+            # adding inequality conditions
+            if types[j] == 'real' and if_ineq:
+                clist.extend(self._get_inequality_conds(col, num_ineq_conds))
+
+            cols_list.append(clist)
+        return cols_list
+
+    
+    def _check_condition(self, conds, cols_list):
+        all_rows = []
+        for c in conds:
+            for cell in cols_list[c[0]]:
+                if cell[0] == c[2] and cell[1] == c[1]:  # if conditions match
+                    all_rows.append(cell[2])
+        intersection_len = len(set(all_rows[0]).intersection(*all_rows))
+
+        # check if the condition gives non zero number of rows
+        if intersection_len == 0:
+            return False
+
+        # check if any subset condition can produce same set of rows
+        for r in all_rows:
+            rows = deepcopy(all_rows)
+            rows.remove(r)
+            rlen = len(set(rows[0]).intersection(*rows))
+            if rlen == intersection_len:
+                return False
+        return True
+
+    def _get_unique_conditions(self, wlist):
+        wdict = {}
+        for wc in wlist:
+            conds_str = str(sorted([str(c) for c in wc['conds']]))
+            wdict[conds_str] = wc
+        wlist = []
+        for key in wdict:
+            wlist.append(wdict[key])
+        return wlist
+
+    def get_where_clauses(self, table, num_where=2, if_ineq=False):
+        cols_list = self._get_column_freq(table, if_ineq)
+        where_dict = {}
+        if num_where==0:
+            return where_dict
+
+        where1_list = []
+        for i, c in enumerate(cols_list):
+            for cell in c:
+                wc = {'conds': [[i, cell[1], cell[0]]], 'rows': cell[2]}
+                where1_list.append(wc)
+        where_dict['nw-1'] = where1_list
+
+        # removing cells which only appear once before going to multiple where
+        cc_list = []
+        for i in range(len(table['header'])):
+            cc = []
+            for cell in cols_list[i]:
+                if len(cell[2]) > 1:
+                    cc.append(cell)
+            cc_list.append(cc)
+        cols_list = cc_list
+
+        if num_where >= 2:
+            where2_list = []
+            for i in range(len(cols_list)):
+                colA = cols_list[i]
+                for j in range(i+1, len(cols_list)):
+                    colB = cols_list[j]
+                    for ca in colA:
+                        for cb in colB:
+                            intersection = list(set(ca[2]) & set(cb[2]))
+                            conds = [[i, ca[1], ca[0]], [j, cb[1], cb[0]]]
+                            # if len(intersection) < len(colA[ca]) and len(intersection) < len(colB[cb]) and len(intersection) > 0:
+                            if self._check_condition(conds, cols_list):
+                                wc = {'conds': [[i, ca[1], ca[0]], [
+                                    j, cb[1], cb[0]]], 'rows': intersection}
+                                where2_list.append(wc)
+            where_dict['nw-2'] = where2_list
+
+        if num_where >= 3:
+            where3_list = []
+            for w2 in where2_list:
+                if len(w2['rows']) > 1:
+
+                    for i in range(len(cols_list)):
+                        if i not in [c[0] for c in w2['conds']]:
+                            for cc in cols_list[i]:
+                                conds = deepcopy(w2['conds'])
+                                conds.append([i, cc[1], cc[0]])
+                                if self._check_condition(conds, cols_list):
+                                    intersection = list(
+                                        set(w2['rows']) & set(cc[2]))
+                                    wc = {'conds': conds, 'rows': intersection}
+                                    where3_list.append(wc)
+            where3_list = self._get_unique_conditions(where3_list)
+            where_dict['nw-3'] = where3_list
+
+        if num_where == 4:
+            where4_list = []
+            for w3 in where3_list:
+                if len(w3['rows']) > 1:
+
+                    for i in range(len(cols_list)):
+                        if i not in [c[0] for c in w3['conds']]:
+                            for cc in cols_list[i]:
+                                conds = deepcopy(w3['conds'])
+                                conds.append([i, cc[1], cc[0]])
+                                if self._check_condition(conds, cols_list):
+                                    intersection = list(
+                                        set(w3['rows']) & set(cc[2]))
+                                    wc = {'conds': conds, 'rows': intersection}
+                                    where4_list.append(wc)
+            where4_list = self._get_unique_conditions(where4_list)
+            where_dict['nw-4'] = where4_list
+        return where_dict
+
+
+    def sample_sql(self, table, num_sample, num_where, agg_op=0, if_ineq=False):
+        """ This function samples sqls from a given table based on values for the parameters
+        num_sample -> number of sql queries to sample, num_where -> number of where condtioned desired in every sampled sql Query etc.
+        Args:
+            table ([Dict]): [Table dictionary with header and rows]
+            num_sample ([int]): [Number of sqls to sample]
+            num_where ([int]): [Number of where clause conditions every sampled sql should have]
+            agg_op (int, optional): [Whether to sample aggregate queries or not]. Defaults to 0.
+            if_ineq (bool, optional): [description]. Defaults to False.
+            
+        Returns:
+            [List,Dict]: [Sampled sql query list in readable string format and dict format]
+        """
+        header = table['header']
+        types = table['types']
+        where_list = [] 
+        multiple_where_dict = self.get_where_clauses(table, num_where, if_ineq)
+        if num_where > 0 :
+            where_list = multiple_where_dict['nw-' + str(num_where)]
+        real_cols = [i for i in range(len(types)) if types[i] == 'real']
+
+        if_agg = 0
+        if agg_op != 0:
+            if_agg = 1
+
+        filtered_where_list = []
+        for wc in where_list:
+            cols_in_where = [c[0] for c in wc['conds']]
+            if if_agg and len(wc['rows']) > 1 and len(set(real_cols) - set(cols_in_where)) >= 1:
+                filtered_where_list.append(wc)
+            elif not if_agg and len(wc['rows']) == 1:
+                filtered_where_list.append(wc)
+
+        if len(filtered_where_list) == 0 and num_where > 0 :
+            return []
+        
+        sampled_where_list = []
+        
+        if num_where > 0:
+            sample_ids = np.random.choice(
+                len(filtered_where_list), num_sample, replace=True)
+            sampled_where_list = [filtered_where_list[i] for i in sample_ids]
+
+        if if_agg:
+            clist = real_cols
+        else:
+            clist = list(range(len(header)))
+
+        sql_string_list = []
+        sql_list = []
+        
+        if (len(sampled_where_list)>0):
+            for wc in sampled_where_list:
+                try:
+                    possible_cols = list(set(clist) - set([c[0] for c in wc['conds']]))
+                    select_column = np.random.choice(possible_cols)
+                    sql = {'sel': select_column, 'agg': agg_op}
+                    sql['conds'] = wc['conds']
+                    answer = self.sql_execution(wc, select_column, agg_op, table)
+                    sql_dict = self.readable_sql(sql, header, answer)
+
+                    sql_list.append(sql_dict)
+                    sql_string_list.append(self.convert_sql_to_string(sql_dict, table))
+                except:
+                    print('\nQuery creation error')
+        else :
+            possible_cols = list(set(clist))
+            select_column = np.random.choice(possible_cols)
+            sql = {'sel': select_column, 'agg': agg_op}
+            wc={}
+            answer = self.sql_execution(wc, select_column, agg_op, table)
+            sql_dict = self.readable_sql(sql, header, answer)
+            sql_list.append(sql_dict)
+            sql_string_list.append(self.convert_sql_to_string(sql_dict, table))
+
+
+        return sql_string_list, sql_list
+
+    def readable_sql(self, sql, header, answer):
+        """Convert Non-readable SQL to readable SQL dict
+
+        Args:
+            sql ([Dict]): [Sql query]
+            header ([List]): [Headers in table]
+            answer ([String]): [Answer]
+
+        Returns:
+            [Dict]: [Sql Dict in readable format]
+        """
+        sql_dict = {}
+        sql_dict['col'] = [SqlOperants.agg_ops[sql['agg']], header[sql['sel']]]
+        conds = []
+        if "conds" in sql :
+            for c in sql['conds']:
+                cond = [header[c[0]], SqlOperants.cond_ops_string[c[1]], str(c[2])]
+                conds.append(cond)
+            sql_dict['conds'] = conds
+        sql_dict['answer'] = str(answer[0])
+        return sql_dict
+
+    def convert_sql_to_string(self, sql_dict, table=[], tokenizer='T5'):
+        """Convert sql query in Dict format to string
+
+        Args:
+            sql_dict ([Dict]): [Sql Query to convert to string]
+            table (list, optional): [Table]. Defaults to [].
+            tokenizer (str, optional): [description]. Defaults to 'T5'.
+        Returns:
+            [String]: [Sql query in string format]
+        """
+        tokens = self.sql_tokens
+
+        sql_str =  str(sql_dict['col'][0]) + ' '+tokens.sep+' ' + str(sql_dict['col'][1])
+        if "conds" in sql_dict :
+            for cond in sql_dict['conds']:
+                sql_str += ' '+tokens.sep+' '
+                sql_str += (' '+tokens.cond+' ').join([str(c) for c in cond])
+        sql_str += ' '+tokens.ans+' ' + str(sql_dict['answer'])
+
+        table['header'] = [str(h) for h in table['header']]
+        sql_str += ' '+tokens.header+' ' + (' '+tokens.hsep+' ').join(table['header'])
+
+        return sql_str
+
+    def controlled_sample_sql(self, table_list, num_samples_per_table=5, agg_prob=[], num_where_prob=[], ineq_prob=0.0,id_list=[]):
+        if agg_prob == []:
+            # agg_prob = [0.6, 0.1, 0.1, 0.0, 0.1, 0.1] #['select', 'maximum', 'minimum', 'count', 'sum', 'average']
+            # ['select', 'maximum', 'minimum', 'count', 'sum', 'average']
+            agg_prob = [1.0, 0., 0., 0., 0., 0.]
+        if num_where_prob == []:
+            # num_where_prob = [0.0, 0.45, 0.3, 0.2, 0.05] # [0,1,2,3,4], there is no zero where clause case
+            # [0,1,2,3,4], there is no zero where clause case
+            num_where_prob = [0.0, 1., 0., 0., 0.]
+
+        sample_batch_size = 1
+        sample_size_list = [sample_batch_size] * \
+            math.floor(num_samples_per_table/sample_batch_size)
+        if num_samples_per_table % sample_batch_size != 0:
+            sample_size_list.append(num_samples_per_table % sample_batch_size)
+
+        all_sql_str_list = []
+        all_sql_list = []
+        all_id_list = []
+        # for table in tqdm(table_list):
+        for i, table in enumerate(table_list):
+            if 'types' not in table:
+                table = self.add_column_types(table)
+            for num_samples in sample_size_list:
+                agg_op = np.random.choice(len(agg_prob), 1, True, agg_prob)[0]
+                num_where = np.random.choice(
+                    len(num_where_prob), 1, True, num_where_prob)[0]
+
+                # if to use ineq.
+                if_ineq = np.random.choice(2, 1, True, [1-ineq_prob, ineq_prob])[0]
+                sql_str_list, sql_list = self.sample_sql(table, num_samples,
+                                    num_where, agg_op, if_ineq)
+
+                num_trials = 0
+                while len(sql_str_list) < num_samples:
+                    diff = num_samples - len(sql_str_list)
+                    if 'real' not in table['types'] and agg_op != 0:
+                        agg_op = 0
+                    elif num_where > 1:
+                        if if_ineq:
+                            num_where -= 1
+                        else:
+                            if_ineq = 1
+                    elif num_where == 1 and agg_op != 0:
+                        if not if_ineq:
+                            if_ineq = 1
+                        else:
+                            agg_op = 0
+                    elif num_where == 1 and agg_op == 0:
+                        agg_op = np.random.choice([1, 2, 3, 4, 5])
+                    diff_sql_str_list, diff_sql_list = self.sample_sql(
+                                        table, diff, num_where, agg_op, if_ineq)
+                    
+                    sql_str_list.extend(diff_sql_str_list)
+                    sql_list.extend(diff_sql_list)
+
+                    num_trials += 1
+                    if num_trials > 5:  # if we cant get it in 5 trials, let us skip this instances
+                        print('Unsuccessful.')
+                        break
+
+                all_sql_str_list.extend(sql_str_list)
+                if i < len(id_list) and id_list[i]!=None :
+                    id_num = id_list[i]
+                elif(len(id_list)>0):
+                    id_num = "NA"
+                if(len(id_list)>0):
+                    id_num_list= [id_num] * len(sql_str_list)
+                    all_id_list.extend(id_num_list)
+                all_sql_list.extend(sql_list)
         return all_sql_str_list, all_sql_list, all_id_list
```

## primeqa/qg/processors/data_loader.py

```diff
@@ -1,46 +1,60 @@
-from primeqa.qg.processors.passage_qg.tydiqa_processor import TydiQADataset
-from datasets import Dataset
-from primeqa.qg.processors.passage_qg.squad_processor import SquadDataset
-from primeqa.qg.processors.table_qg.wikisql_processor import WikiSqlDataset
-
-class QGDataLoader():
-	def __init__(self, 
-				tokenizer,
-				dataset_name='wikisql', 
-				input_max_len=512,
-				target_max_len=32
-				):
-		self.tokenizer = tokenizer
-		self.input_max_len = input_max_len
-		self.target_max_len = target_max_len
-
-		if dataset_name == 'wikisql':
-			self.dataset = WikiSqlDataset()
-		elif dataset_name in ['squad', 'squad_v2']:
-			self.dataset = SquadDataset(dataset_name)	
-		elif dataset_name in ['tydiqa']:	
-			self.dataset = TydiQADataset()
-		else:
-			raise NotImplementedError("this data not supported")
-		
-	def convert_to_features(self, example_batch):
-		input_encodings = self.tokenizer.batch_encode_plus(example_batch['input'], 
-										pad_to_max_length=True, max_length=self.input_max_len)
-		target_encodings = self.tokenizer.batch_encode_plus(example_batch['question'], 
-										pad_to_max_length=True, max_length=self.target_max_len)
-		encodings = {
-			'input_ids': input_encodings['input_ids'], 
-			'attention_mask': input_encodings['attention_mask'],
-			'target_ids': target_encodings['input_ids'],
-			'target_attention_mask': target_encodings['attention_mask']
-		}
-		return encodings
-
-	def create(self, data_split='train'):
-		processed_data_dict = self.dataset.preprocess_data_for_qg(data_split) # list of dict
-
-		processed_data = Dataset.from_dict(processed_data_dict)
-		tokenized_data =  processed_data.map(self.convert_to_features, batched=True)
-		columns = ['input_ids', 'attention_mask', 'target_ids', 'target_attention_mask']
-		tokenized_data.set_format(type='torch', columns=columns)
-		return tokenized_data
+import logging
+
+from datasets import Dataset, DatasetDict, load_dataset
+from primeqa.qg.processors.passage_qg.qg_processor import QGProcessor
+from primeqa.qg.processors.table_qg.sql_processor import SqlProcessor
+
+logger = logging.getLogger(__name__)
+
+
+class QGDataLoader:
+    def __init__(
+        self,
+        tokenizer,
+        modality,
+        input_max_len,
+        target_max_len,
+        dataset_name=None,
+        dataset_config=None,
+        dataset_split=None,
+    ):
+        self.dataset_name = dataset_name
+        self.dataset_config = dataset_config
+        self.dataset_split = dataset_split
+
+        if modality == "table":
+            self.processor = SqlProcessor(tokenizer, input_max_len, target_max_len)
+        elif modality == "passage":
+            self.processor = QGProcessor(tokenizer, input_max_len, target_max_len)
+
+    def create(
+        self,
+        dataset: Dataset = None,
+        dataset_split: str = "train",
+        dataset_config: str = None,
+    ) -> Dataset:
+        if dataset is None:
+            # load dataset first
+            assert self.dataset_name is not None
+            dataset_split = (
+                dataset_split if dataset_split is not None else self.dataset_split
+            )
+            dataset_config = (
+                dataset_config if dataset_config is not None else self.dataset_config
+            )
+            # select `secondary task` config for TyDi QA if none is given
+            if self.dataset_name == "tydiqa" and dataset_config is None:
+                dataset_config = "secondary_task"
+                logger.info(
+                    "Defaulting to config '{dataset_config}' for dataset {self.dataset_name}"
+                )
+            dataset = load_dataset(
+                self.dataset_name, name=dataset_config, split=dataset_split
+            )
+            if isinstance(dataset, DatasetDict):
+                raise ValueError(
+                    "Loaded dataset is of type DatasetDict, did you choose a split?"
+                )
+
+        # NOTE works only if data has correct format
+        return self.processor(dataset)
```

## primeqa/qg/trainers/qg_trainer.py

```diff
@@ -1,18 +1,8 @@
-from transformers import Seq2SeqTrainer
-
-
-class QGTrainer(Seq2SeqTrainer):
-
-    def __init__(self,*args,train_dataset,valid_dataset,data_collator,**kwargs):
-        """Question Generation Trainer for training and Evaluation. This class extends the Trainer class from huggingface.
-
-        Args:
-            train_dataset (Dataset): Train Dataset Generator
-            valid_dataset (Dataset): Validation Dataset Generator
-            data_collator (Dataset): Test Dataset Generator
-        """  
-        super().__init__(*args,
-                train_dataset=train_dataset,
-                eval_dataset=valid_dataset,
-                data_collator=data_collator,
-                **kwargs)
+from transformers import Seq2SeqTrainer
+
+class QGTrainer(Seq2SeqTrainer):
+    """ The trainer class for QG. All related functionality should go to this class. """
+
+    def __init__(self, *args, **kwargs):
+        """ Custom intialization for the QGTrainer should be added here. """
+        super().__init__(*args, **kwargs)
```

## primeqa/qg/utils/constants.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-
-from dataclasses import dataclass
-# constants for SQL in WikiSQL
-
-@dataclass
-class SqlOperants:
-	agg_ops = ['select', 'maximum', 'minimum', 'count', 'sum', 'average']
-	cond_ops = ['=', '>', '<', 'OP']
-	cond_ops_string = ['equal', 'greater', 'lesser', 'OP']
-
-@dataclass
-class QGSpecialTokens:
-	sep = '<<sep>>'
-	cond = '<<cond>>'
-	ans = '<<answer>>'
-	header = '<<header>>'
+
+from dataclasses import dataclass
+# constants for SQL in WikiSQL
+
+@dataclass
+class SqlOperants:
+	agg_ops = ['select', 'maximum', 'minimum', 'count', 'sum', 'average']
+	cond_ops = ['=', '>', '<', 'OP']
+	cond_ops_string = ['equal', 'greater', 'lesser', 'OP']
+
+@dataclass
+class QGSpecialTokens:
+	sep = '<<sep>>'
+	cond = '<<cond>>'
+	ans = '<<answer>>'
+	header = '<<header>>'
 	hsep = '<<hsep>>'
```

## primeqa/qg/utils/data_collator.py

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-from dataclasses import dataclass,field
-from typing import Optional, List, Dict
-import torch
-
-
-@dataclass
-class T2TDataCollator:
-    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:
-        """
-        Take a list of samples from a Dataset and collate them into a batch.
-        Returns:
-            A dictionary of tensors
-        """
-        input_ids = torch.stack([example['input_ids'] for example in batch])
-        lm_labels = torch.stack([example['target_ids'] for example in batch])
-        lm_labels[lm_labels[:, :] == 0] = -100
-        attention_mask = torch.stack([example['attention_mask'] for example in batch])
-        decoder_attention_mask = torch.stack([example['target_attention_mask'] for example in batch])
-        
-
-        return {
-            'input_ids': input_ids, 
-            'attention_mask': attention_mask,
-            'labels': lm_labels, 
-            'decoder_attention_mask': decoder_attention_mask
+from dataclasses import dataclass,field
+from typing import Optional, List, Dict
+import torch
+
+
+@dataclass
+class T2TDataCollator:
+    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:
+        """
+        Take a list of samples from a Dataset and collate them into a batch.
+        Returns:
+            A dictionary of tensors
+        """
+        input_ids = torch.stack([example['input_ids'] for example in batch])
+        lm_labels = torch.stack([example['target_ids'] for example in batch])
+        lm_labels[lm_labels[:, :] == 0] = -100
+        attention_mask = torch.stack([example['attention_mask'] for example in batch])
+        decoder_attention_mask = torch.stack([example['target_attention_mask'] for example in batch])
+        
+
+        return {
+            'input_ids': input_ids, 
+            'attention_mask': attention_mask,
+            'labels': lm_labels, 
+            'decoder_attention_mask': decoder_attention_mask
         }
```

## primeqa/tableqa/run_tableqa.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-import logging
-from primeqa.tableqa.metrics.answer_accuracy import compute_denotation_accuracy
-from primeqa.tableqa.models.tableqa_model import TableQAModel
-from primeqa.tableqa.postprocessor.wikisql import WikiSQLPostprocessor
-from primeqa.tableqa.preprocessors.dataset import TableQADataset
-from primeqa.tableqa.trainers.tableqa_trainer import TableQATrainer
-from dataclasses import dataclass, field
-from transformers import TapasConfig
-from transformers import (
-    DataCollator,
-    HfArgumentParser,
-    TrainingArguments,
-    set_seed,default_data_collator,
-)
-import pandas as pd
-from primeqa.tableqa.utils.data_collator import TapasCollator
-from primeqa.tableqa.preprocessors.wikisql_preprocessor import load_data
-import os
-
-@dataclass
-class TableQAArguments:
-    """
-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
-    """
-    data_path_root: str = field(
-       default='primeqa/tableqa/preprocessors/data/wikisql/', metadata={"help": "root path to store the preprocessed dataset"}
-    )
-    train_data_path: str = field(
-       default='primeqa/tableqa/preprocessors/data/wikisql/', metadata={"help": "Train data path for training on user's own dataset"}
-    )
-    dev_data_path: str = field(
-       default='primeqa/tableqa/preprocessors/data/wikisql/', metadata={"help": "Dev data path for training on user's own dataset"}
-    )
-
-    dataset_name: str = field(
-       default='wikisql', metadata={"help": "Name of the dataset to train the tapas model on"}
-    )
-    num_aggregation_labels: int = field(
-       default=4, metadata={"help": "Total number of aggregation labels"}
-    )
-    use_answer_as_supervision: bool = field(
-        default=True, metadata={"help": "Whether to use answer as supervision or not"}
-    )
-    answer_loss_cutoff: float = field(
-        default=0.664694, metadata={"help": "Answer loss cutoff"}
-    )
-    cell_selection_preference: float = field(
-        default=0.207951, metadata={"help": "Cell selection preference"}
-    )
-
-    huber_loss_delta: float = field(
-        default=0.121194, metadata={"help": "Huber loss delta"}
-    )
-    init_cell_selection_weights_to_zero: bool = field(
-        default=True, metadata={"help": "Init cell selection weights to zero or not"}
-    )
-    select_one_column: bool = field(
-        default=True, metadata={"help": "select one column"}
-    )
-    allow_empty_column_selection: bool = field(
-        default=True, metadata={"help": "Allow empty column selection"}
-    )
-    temperature: float = field(
-        default=0.0352513, metadata={"help": "temperature"}
-    )
-def run_table_qa(data_args,model_args,training_args):
-    logger = logging.getLogger(__name__)
-    tqa_parser = HfArgumentParser(TableQAArguments)
-    tqa_args = tqa_parser.parse_json_file(json_file=os.path.abspath(data_args.tableqa_config_file))[0]
-    config = TapasConfig(tqa_args)
-    tableqa_model = TableQAModel(model_args.model_name_or_path,config=config)
-    model = tableqa_model.model
-    tokenizer = tableqa_model.tokenizer
-    post_obj = WikiSQLPostprocessor(tokenizer,tqa_args)
-    if data_args.dataset_name=="wikisql":
-        train_dataset,eval_dataset = load_data(tqa_args.data_path_root,tokenizer)
-    else:
-        tqadataset = TableQADataset(tqa_args.data_path_root,data_args.train_file,data_args.eval_file ,tokenizer)
-        train_dataset,eval_dataset= tqadataset.load_data()
-    trainer = TableQATrainer(model=model,
-                            args=training_args,
-                            train_dataset=train_dataset if training_args.do_train else None,
-                            eval_dataset=eval_dataset if training_args.do_eval else None,
-                            tokenizer=tableqa_model.tokenizer,
-                            data_collator=TapasCollator(),
-                            post_process_function= post_obj.postprocess_prediction,
-                            compute_metrics=compute_denotation_accuracy  
-                            )
-    if training_args.do_train:
-        train_result = trainer.train()
-        trainer.save_model()
-        metrics = train_result.metrics
-        trainer.log_metrics("train", metrics)
-        trainer.save_metrics("train", metrics)
-        trainer.save_state()
-    if training_args.do_eval:
-        logger.info("*** Evaluate ***")
-        metrics = trainer.evaluate()
-        trainer.log_metrics("eval", metrics)
-        trainer.save_metrics("eval", metrics)
+import logging
+from primeqa.tableqa.metrics.answer_accuracy import compute_denotation_accuracy
+from primeqa.tableqa.models.tableqa_model import TableQAModel
+from primeqa.tableqa.postprocessor.wikisql import WikiSQLPostprocessor
+from primeqa.tableqa.preprocessors.dataset import TableQADataset
+from primeqa.tableqa.trainers.tableqa_trainer import TableQATrainer
+from dataclasses import dataclass, field
+from transformers import TapasConfig
+from transformers import (
+    DataCollator,
+    HfArgumentParser,
+    TrainingArguments,
+    set_seed,default_data_collator,
+)
+import pandas as pd
+from primeqa.tableqa.utils.data_collator import TapasCollator
+from primeqa.tableqa.preprocessors.wikisql_preprocessor import load_data
+import os
+
+@dataclass
+class TableQAArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+    data_path_root: str = field(
+       default='primeqa/tableqa/preprocessors/data/wikisql/', metadata={"help": "root path to store the preprocessed dataset"}
+    )
+    train_data_path: str = field(
+       default='primeqa/tableqa/preprocessors/data/wikisql/', metadata={"help": "Train data path for training on user's own dataset"}
+    )
+    dev_data_path: str = field(
+       default='primeqa/tableqa/preprocessors/data/wikisql/', metadata={"help": "Dev data path for training on user's own dataset"}
+    )
+
+    dataset_name: str = field(
+       default='wikisql', metadata={"help": "Name of the dataset to train the tapas model on"}
+    )
+    num_aggregation_labels: int = field(
+       default=4, metadata={"help": "Total number of aggregation labels"}
+    )
+    use_answer_as_supervision: bool = field(
+        default=True, metadata={"help": "Whether to use answer as supervision or not"}
+    )
+    answer_loss_cutoff: float = field(
+        default=0.664694, metadata={"help": "Answer loss cutoff"}
+    )
+    cell_selection_preference: float = field(
+        default=0.207951, metadata={"help": "Cell selection preference"}
+    )
+
+    huber_loss_delta: float = field(
+        default=0.121194, metadata={"help": "Huber loss delta"}
+    )
+    init_cell_selection_weights_to_zero: bool = field(
+        default=True, metadata={"help": "Init cell selection weights to zero or not"}
+    )
+    select_one_column: bool = field(
+        default=True, metadata={"help": "select one column"}
+    )
+    allow_empty_column_selection: bool = field(
+        default=True, metadata={"help": "Allow empty column selection"}
+    )
+    temperature: float = field(
+        default=0.0352513, metadata={"help": "temperature"}
+    )
+def run_table_qa(data_args,model_args,training_args):
+    logger = logging.getLogger(__name__)
+    tqa_parser = HfArgumentParser(TableQAArguments)
+    tqa_args = tqa_parser.parse_json_file(json_file=os.path.abspath(data_args.tableqa_config_file))[0]
+    config = TapasConfig(tqa_args)
+    tableqa_model = TableQAModel(model_args.model_name_or_path,config=config)
+    model = tableqa_model.model
+    tokenizer = tableqa_model.tokenizer
+    post_obj = WikiSQLPostprocessor(tokenizer,tqa_args)
+    if data_args.dataset_name=="wikisql":
+        train_dataset,eval_dataset = load_data(tqa_args.data_path_root,tokenizer)
+    else:
+        tqadataset = TableQADataset(tqa_args.data_path_root,data_args.train_file,data_args.eval_file ,tokenizer)
+        train_dataset,eval_dataset= tqadataset.load_data()
+    trainer = TableQATrainer(model=model,
+                            args=training_args,
+                            train_dataset=train_dataset if training_args.do_train else None,
+                            eval_dataset=eval_dataset if training_args.do_eval else None,
+                            tokenizer=tableqa_model.tokenizer,
+                            data_collator=TapasCollator(),
+                            post_process_function= post_obj.postprocess_prediction,
+                            compute_metrics=compute_denotation_accuracy  
+                            )
+    if training_args.do_train:
+        train_result = trainer.train()
+        trainer.save_model()
+        metrics = train_result.metrics
+        trainer.log_metrics("train", metrics)
+        trainer.save_metrics("train", metrics)
+        trainer.save_state()
+    if training_args.do_eval:
+        logger.info("*** Evaluate ***")
+        metrics = trainer.evaluate()
+        trainer.log_metrics("eval", metrics)
+        trainer.save_metrics("eval", metrics)
```

## primeqa/tableqa/metrics/answer_accuracy.py

 * *Ordering differences only*

```diff
@@ -1,23 +1,23 @@
-import ast
-import logging
-logger = logging.getLogger(__name__)
-
-def compute_denotation_accuracy(predicted_answers,gold_answers):
-    """Computes denotation accuracy based on predicted and gold answers
-
-    Args:
-        predicted_answers (List): List of predicted answers
-        gold_answers (List): List of gold answers
-
-    Returns:
-        Dict: Metrics score
-    """
-    
-    exact_match = []
-    for pred,gold in zip(predicted_answers,gold_answers):
-        correct = 0
-        if pred==gold:
-            correct = 1
-        exact_match.append(correct)
-    accuracy = sum(exact_match) / len(exact_match)
+import ast
+import logging
+logger = logging.getLogger(__name__)
+
+def compute_denotation_accuracy(predicted_answers,gold_answers):
+    """Computes denotation accuracy based on predicted and gold answers
+
+    Args:
+        predicted_answers (List): List of predicted answers
+        gold_answers (List): List of gold answers
+
+    Returns:
+        Dict: Metrics score
+    """
+    
+    exact_match = []
+    for pred,gold in zip(predicted_answers,gold_answers):
+        correct = 0
+        if pred==gold:
+            correct = 1
+        exact_match.append(correct)
+    accuracy = sum(exact_match) / len(exact_match)
     return {"Denotation accuracy":accuracy}
```

## primeqa/tableqa/models/tableqa_model.py

```diff
@@ -1,76 +1,76 @@
-from pickle import NONE
-from transformers import TapasConfig,TapasTokenizer, TapasForQuestionAnswering
-import pandas as pd
-
-class TableQAModel():
-    def __init__(self,model_name_path,config=None):
-        """TableQA model class
-
-        Args:
-            model_name_path (str): Path to the pre-trained model.
-            config (_type_, optional): _description_. Defaults to None.
-        """
-        self._model = TapasForQuestionAnswering.from_pretrained(model_name_path)
-        self.config = config
-        self._tokenizer = TapasTokenizer.from_pretrained(model_name_path)
-
-
-    @property
-    def model(self):
-        """ Propery of TableQA model.
-        Returns:
-            Sequence to sequence model object (based on model name)
-        """
-        return self._model
-
-    @property
-    def tokenizer(self):
-        """ Property of TableQG model.
-        Returns:
-            Tokenizer class object based on the model name/ path
-        """
-        return self._tokenizer
-        
-
-        
-
-    def predict_from_dict(self,data_dict,queries_list):
-        """This function takes a table dictionary and a list of queries as input and returns the answer to the queries using the TableQA model.
-
-        Args:
-            data_dict (Dict): Table in dict format
-            queries_list (List): List of queries
-
-        Returns:
-            Dict: Returns a dictionary of query and the predicted answer.
-        """
-        table = pd.DataFrame.from_dict(data_dict)
-        inputs = self._tokenizer(table=table, queries=queries_list, padding='max_length', return_tensors="pt")
-        outputs = self._model(**inputs)
-        predicted_answer_coordinates = self._tokenizer.convert_logits_to_predictions(inputs,
-                                                                                    outputs.logits.detach(),
-                                                                                    None)
-        answers = []
-        for coordinates in predicted_answer_coordinates[0]:
-            if len(coordinates) == 1:
-            # only a single cell:
-                answers.append(table.iat[coordinates[0]])
-            else:
-                cell_values = []
-                for coordinate in coordinates:
-                    cell_values.append(table.iat[coordinate])
-                answers.append(", ".join(cell_values))
-        query_answer_dict = {}
-        for query, answer in zip(queries_list, answers):
-            query_answer_dict[query] = answer
-        return query_answer_dict
-                                                                                                    
-    
-    
-
-
-    
-
-
-
-
+from pickle import NONE
+from transformers import TapasConfig,TapasTokenizer, TapasForQuestionAnswering
+import pandas as pd
+
+class TableQAModel():
+    def __init__(self,model_name_path,config=None):
+        """TableQA model class
+
+        Args:
+            model_name_path (str): Path to the pre-trained model.
+            config (_type_, optional): _description_. Defaults to None.
+        """
+        self._model = TapasForQuestionAnswering.from_pretrained(model_name_path)
+        self.config = config
+        self._tokenizer = TapasTokenizer.from_pretrained(model_name_path)
+
+
+    @property
+    def model(self):
+        """ Propery of TableQA model.
+        Returns:
+            Sequence to sequence model object (based on model name)
+        """
+        return self._model
+
+    @property
+    def tokenizer(self):
+        """ Property of TableQG model.
+        Returns:
+            Tokenizer class object based on the model name/ path
+        """
+        return self._tokenizer
+        
+
+        
+
+    def predict(self,data_dict,queries_list):
+        """This function takes a table dictionary and a list of queries as input and returns the answer to the queries using the TableQA model.
+
+        Args:
+            data_dict (Dict): Table in dict format
+            queries_list (List): List of queries
+
+        Returns:
+            Dict: Returns a dictionary of query and the predicted answer.
+        """
+        table = pd.DataFrame.from_dict(data_dict)
+        inputs = self._tokenizer(table=table, queries=queries_list, padding='max_length', return_tensors="pt")
+        outputs = self._model(**inputs)
+        predicted_answer_coordinates = self._tokenizer.convert_logits_to_predictions(inputs,
+                                                                                    outputs.logits.detach(),
+                                                                                    None)
+        answers = []
+        for coordinates in predicted_answer_coordinates[0]:
+            if len(coordinates) == 1:
+            # only a single cell:
+                answers.append(table.iat[coordinates[0]])
+            else:
+                cell_values = []
+                for coordinate in coordinates:
+                    cell_values.append(table.iat[coordinate])
+                answers.append(", ".join(cell_values))
+        query_answer_dict = {}
+        for query, answer in zip(queries_list, answers):
+            query_answer_dict[query] = answer
+        return query_answer_dict
+                                                                                                    
+    
+    
+
+
+    
+
+
+
+
```

## primeqa/tableqa/postprocessor/wikisql.py

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-import argparse
-import logging
-from os import pread
-import torch
-import pandas as pd
-import os
-import ast
-
-logger = logging.getLogger(__name__)
-
-        
-class WikiSQLPostprocessor:
-    def __init__(self,tokenizer,args):
-        self.tokenizer = tokenizer
-        self.args = args
-
-    def postprocess_prediction(self,eval_examples,eval_dataset, predictions):
-        """Post process wikisql prediction using dev dataset and predictions
-
-        Args:
-            eval_examples (Examples): examples
-            eval_dataset (Dataset): Development dataset object
-            predictions (Tensor): prediction for each instance
-
-        Returns:
-            _type_: _description_
-        """
-        eval_preds = []
-        gold_answers= []
-        for i, data in enumerate(eval_dataset):
-            data = {key: val.unsqueeze(0) for key, val in data.items()}
-            answer_coordinates = self.tokenizer.convert_logits_to_predictions(data,torch.tensor(predictions[i,:]))
-            gold_data = eval_dataset.data.iloc[i].to_dict()
-            table_file = gold_data['table_file']
-            gold_answer = ast.literal_eval(gold_data['answer_text'])
-            predicted_answer = self.get_predicted_answer_text(table_file,answer_coordinates)
-            eval_preds.append([str(i).lower() for i in predicted_answer])
-            gold_answers.append([str(i).lower() for i in gold_answer])
-        
-
-        return eval_preds,gold_answers
-
-    def get_predicted_answer_text(self,table_file,answer_coordinates):
-        """The functions takes answer coordinates as input and predicts the answer texts
-
-        Args:
-            table_file (String): Name of the table file
-            answer_coordinates (int,int): Row column index
-
-        Returns:
-            _type_: _description_
-        """
-        answers = []
-        table = pd.read_csv(os.path.join(self.args.data_path_root,table_file),index_col=0).astype(str)
-        for coordinates in answer_coordinates[0]:
-            if len(coordinates) == 1:
-            # only a single cell:
-                answers.append(table.iat[coordinates[0]])
-            else:
-                cell_values = []
-                for coordinate in coordinates:
-                    cell_values.append(table.iat[coordinate])
-                answers.append(", ".join(cell_values))
-        return answers
-    
+import argparse
+import logging
+from os import pread
+import torch
+import pandas as pd
+import os
+import ast
+
+logger = logging.getLogger(__name__)
+
+        
+class WikiSQLPostprocessor:
+    def __init__(self,tokenizer,args):
+        self.tokenizer = tokenizer
+        self.args = args
+
+    def postprocess_prediction(self,eval_examples,eval_dataset, predictions):
+        """Post process wikisql prediction using dev dataset and predictions
+
+        Args:
+            eval_examples (Examples): examples
+            eval_dataset (Dataset): Development dataset object
+            predictions (Tensor): prediction for each instance
+
+        Returns:
+            _type_: _description_
+        """
+        eval_preds = []
+        gold_answers= []
+        for i, data in enumerate(eval_dataset):
+            data = {key: val.unsqueeze(0) for key, val in data.items()}
+            answer_coordinates = self.tokenizer.convert_logits_to_predictions(data,torch.tensor(predictions[i,:]))
+            gold_data = eval_dataset.data.iloc[i].to_dict()
+            table_file = gold_data['table_file']
+            gold_answer = ast.literal_eval(gold_data['answer_text'])
+            predicted_answer = self.get_predicted_answer_text(table_file,answer_coordinates)
+            eval_preds.append([str(i).lower() for i in predicted_answer])
+            gold_answers.append([str(i).lower() for i in gold_answer])
+        
+
+        return eval_preds,gold_answers
+
+    def get_predicted_answer_text(self,table_file,answer_coordinates):
+        """The functions takes answer coordinates as input and predicts the answer texts
+
+        Args:
+            table_file (String): Name of the table file
+            answer_coordinates (int,int): Row column index
+
+        Returns:
+            _type_: _description_
+        """
+        answers = []
+        table = pd.read_csv(os.path.join(self.args.data_path_root,table_file),index_col=0).astype(str)
+        for coordinates in answer_coordinates[0]:
+            if len(coordinates) == 1:
+            # only a single cell:
+                answers.append(table.iat[coordinates[0]])
+            else:
+                cell_values = []
+                for coordinate in coordinates:
+                    cell_values.append(table.iat[coordinate])
+                answers.append(", ".join(cell_values))
+        return answers
+
```

## primeqa/tableqa/preprocessors/convert_to_sqa_format.py

 * *Ordering differences only*

```diff
@@ -1,446 +1,446 @@
-# coding=utf-8
-# Copyright 2019 The Google AI Language Team Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# Lint as: python3
-# From https://github.com/NielsRogge/tapas_utils/blob/master/Parsing_answer_texts_to_answer_coordinates_for_TAPAS.ipynb
-
-"""This module implements a simple parser that can be used for TAPAS.
-
-Given a table, a question and one or more answer_texts, it will parse the texts
-to populate other fields (e.g. answer_coordinates, float_value) that are required
-by TAPAS.
-
-Please note that exceptions in this module are concise and not parameterized,
-since they are used as counter names in a BEAM pipeline.
-"""
-
-import enum
-from typing import Callable, List, Text, Optional
-
-import six
-import struct
-import unicodedata
-import re
-
-import frozendict
-import numpy as np
-import scipy.optimize
-import ast
-
-class SupervisionMode(enum.Enum):
-  # Don't filter out any supervised information.
-  NONE = 0
-  # Remove all the supervised signals and recompute them by parsing answer
-  # texts.
-  REMOVE_ALL = 2
-  # Same as above but discard ambiguous examples
-  # (where an answer matches multiple cells).
-  REMOVE_ALL_STRICT = 3
-
-
-def _find_matching_coordinates(table, answer_text,
-                               normalize):
-  normalized_text = normalize(answer_text)
-  for row_index, row in table.iterrows():
-    for column_index, cell in enumerate(row):
-      if normalized_text == normalize(str(cell)):
-        yield (row_index, column_index)
-
-
-def _compute_cost_matrix_inner(
-    table,
-    answer_texts,
-    normalize,
-    discard_ambiguous_examples,
-):
-  """Returns a cost matrix M where the value M[i,j] contains a matching cost from answer i to cell j.
-
-  The matrix is a binary matrix and -1 is used to indicate a possible match from
-  a given answer_texts to a specific cell table. The cost matrix can then be
-  usedto compute the optimal assignments that minimizes the cost using the
-  hungarian algorithm (see scipy.optimize.linear_sum_assignment).
-
-  Args:
-    table: a Pandas dataframe.
-    answer_texts: a list of strings.
-    normalize: a function that normalizes a string.
-    discard_ambiguous_examples: If true discard if answer has multiple matches.
-
-  Raises:
-    ValueError if:
-      - we cannot correctly construct the cost matrix or the text-cell
-      assignment is ambiguous.
-      - we cannot find a matching cell for a given answer_text.
-
-  Returns:
-    A numpy matrix with shape (num_answer_texts, num_rows * num_columns).
-  """
-  max_candidates = 0
-  n_rows, n_columns = table.shape[0], table.shape[1]
-  num_cells = n_rows * n_columns
-  num_candidates = np.zeros((n_rows, n_columns))
-  cost_matrix = np.zeros((len(answer_texts), num_cells))
-
-  for index, answer_text in enumerate(answer_texts):
-    found = 0
-    for row, column in _find_matching_coordinates(table, answer_text,
-                                                  normalize):
-      found += 1
-      cost_matrix[index, (row * len(table.columns)) + column] = -1
-      num_candidates[row, column] += 1
-      max_candidates = max(max_candidates, num_candidates[row, column])
-    if found == 0:
-      return None
-    if discard_ambiguous_examples and found > 1:
-      raise ValueError("Found multiple cells for answers")
-
-  # TODO(piccinno): Shall we allow ambiguous assignments?
-  if max_candidates > 1:
-    raise ValueError("Assignment is ambiguous")
-  #print("the cost matrix is",cost_matrix)
-  return cost_matrix
-
-
-def _compute_cost_matrix(
-    table,
-    answer_texts,
-    discard_ambiguous_examples,
-):
-  """Computes cost matrix."""
-  # print("table and answer text is",table,answer_texts)
-  for index, normalize_fn in enumerate(STRING_NORMALIZATIONS):
-    try:
-      result = _compute_cost_matrix_inner(
-          table,
-          answer_texts,
-          normalize_fn,
-          discard_ambiguous_examples,
-      )
-      if result is None:
-        continue
-      return result
-    except ValueError:
-      if index == len(STRING_NORMALIZATIONS) - 1:
-        raise
-  return None
-
-
-def _parse_answer_coordinates(table,
-                              answer_texts,
-                              discard_ambiguous_examples):
-  """Populates answer_coordinates using answer_texts.
-
-  Args:
-    table: a Table message, needed to compute the answer coordinates.
-    answer_texts: a list of strings
-    discard_ambiguous_examples: If true discard if answer has multiple matches.
-
-  Raises:
-    ValueError if the conversion fails.
-  """
-  
-  cost_matrix = _compute_cost_matrix(
-      table,
-      answer_texts,
-      discard_ambiguous_examples,
-  )
-  if cost_matrix is None:
-    return
-  row_indices, column_indices = scipy.optimize.linear_sum_assignment(
-      cost_matrix)
- 
-  # create answer coordinates as list of tuples
-  answer_coordinates = []
-  for row_index in row_indices:
-    flatten_position = column_indices[row_index]
-    row_coordinate = flatten_position // len(table.columns)
-    column_coordinate = flatten_position % len(table.columns)
-    answer_coordinates.append((row_coordinate, column_coordinate))
-
-  return answer_coordinates
-
-
-### START OF UTILITIES FROM TEXT_UTILS.PY ###
-
-def wtq_normalize(x):
-  """Returns the normalized version of x.
-  This normalization function is taken from WikiTableQuestions github, hence the
-  wtq prefix. For more information, see
-  https://github.com/ppasupat/WikiTableQuestions/blob/master/evaluator.py
-  Args:
-    x: the object (integer type or string) to normalize.
-  Returns:
-    A normalized string.
-  """
-  x = x if isinstance(x, six.text_type) else six.text_type(x)
-  # Remove diacritics.
-  x = "".join(
-      c for c in unicodedata.normalize("NFKD", x)
-      if unicodedata.category(c) != "Mn")
-  # Normalize quotes and dashes.
-  x = re.sub(u"[‘’´`]", "'", x)
-  x = re.sub(u"[“”]", '"', x)
-  x = re.sub(u"[‐‑‒–—−]", "-", x)
-  x = re.sub(u"[‐]", "", x)
-  while True:
-    old_x = x
-    # Remove citations.
-    x = re.sub(u"((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[•♦†‡*#+])*$", "",
-               x.strip())
-    # Remove details in parenthesis.
-    x = re.sub(u"(?<!^)( \\([^)]*\\))*$", "", x.strip())
-    # Remove outermost quotation mark.
-    x = re.sub(u'^"([^"]*)"$', r"\1", x.strip())
-    if x == old_x:
-      break
-  # Remove final '.'.
-  if x and x[-1] == ".":
-    x = x[:-1]
-  # Collapse whitespaces and convert to lower case.
-  x = re.sub(r"\s+", " ", x, flags=re.U).lower().strip()
-  x = re.sub("<[^<]+?>", "", x)
-  x = x.replace("\n", " ")
-  return x
-
-
-_TOKENIZER = re.compile(r"\w+|[^\w\s]+", re.UNICODE)
-
-
-def tokenize_string(x):
-  return list(_TOKENIZER.findall(x.lower()))
-
-
-# List of string normalization functions to be applied in order. We go from
-# simplest to more complex normalization procedures.
-STRING_NORMALIZATIONS = (
-    lambda x: x,
-    lambda x: x.lower(),
-    tokenize_string,
-    wtq_normalize,
-)
-
-
-def to_float32(v):
-  """If v is a float reduce precision to that of a 32 bit float."""
-  if not isinstance(v, float):
-    return v
-  return struct.unpack("!f", struct.pack("!f", v))[0]
-
-def _split_thousands(delimeter,string_value):
-  if string_value.split(delimeter) is not None:
-    return True
-  else:
-    return False
-
-
-def convert_to_float(value):
-  """Converts value to a float using a series of increasingly complex heuristics.
-  Args:
-    value: object that needs to be converted. Allowed types include
-      float/int/strings.
-  Returns:
-    A float interpretation of value.
-  Raises:
-    ValueError if the float conversion of value fails.
-  """
-  if isinstance(value, float):
-    return value
-  if isinstance(value, int):
-    return float(value)
-  if not isinstance(value, six.string_types):
-    raise ValueError("Argument value is not a string. Can't parse it as float")
-  sanitized = value
-
-  try:
-    # Example: 1,000.7
-    if "." in sanitized and "," in sanitized:
-      return float(sanitized.replace(",", ""))
-    # 1,000
-    if "," in sanitized and _split_thousands(",", sanitized):
-      return float(sanitized.replace(",", ""))
-    # 5,5556
-    if "," in sanitized and sanitized.count(",") == 1 and not _split_thousands(
-        ",", sanitized):
-      return float(sanitized.replace(",", "."))
-    # 0.0.0.1
-    if sanitized.count(".") > 1:
-      return float(sanitized.replace(".", ""))
-    # 0,0,0,1
-    if sanitized.count(",") > 1:
-      return float(sanitized.replace(",", ""))
-    return float(sanitized)
-  except ValueError:
-    # Avoid adding the sanitized value in the error message.
-    raise ValueError("Unable to convert value to float")
-
-### END OF UTILITIES FROM TEXT_UTILS.PY ###
-
-def _parse_answer_float(answer_texts, float_value):
-  if len(answer_texts) > 1:
-    raise ValueError("Cannot convert to multiple answers to single float")
-  float_value = convert_to_float(answer_texts[0])
-  float_value = float_value
-
-  return answer_texts, float_value
-
-
-def _has_single_float_answer_equal_to(question, answer_texts, target):
-  """Returns true if the question has a single answer whose value equals to target."""
-  if len(answer_texts) != 1:
-    return False
-  try:
-    float_value = convert_to_float(answer_texts[0])
-    # In general answer_float is derived by applying the same conver_to_float
-    # function at interaction creation time, hence here we use exact match to
-    # avoid any false positive.
-    return to_float32(float_value) == to_float32(target)
-  except ValueError:
-    return False
-
-
-def _parse_question(
-    table,
-    original_question,
-    answer_texts,
-    answer_coordinates,
-    float_value,
-    aggregation_function,
-    clear_fields,
-    discard_ambiguous_examples,
-):
-  """Parses question's answer_texts fields to possibly populate additional fields.
-
-  Args:
-    table: a Pandas dataframe, needed to compute the answer coordinates.
-    original_question: a string.
-    answer_texts: a list of strings, serving as the answer to the question.
-    anser_coordinates:
-    float_value: a float, serves as float value signal. 
-    aggregation_function: 
-    clear_fields: A list of strings indicating which fields need to be cleared
-      and possibly repopulated.
-    discard_ambiguous_examples: If true, discard ambiguous examples.
-
-  Returns:
-    A Question message with answer_coordinates or float_value field populated.
-
-  Raises:
-    ValueError if we cannot parse correctly the question message.
-  """
-  question = original_question
-  # If we have a float value signal we just copy its string representation to
-  # the answer text (if multiple answers texts are present OR the answer text
-  # cannot be parsed to float OR the float value is different), after clearing
-  # this field.
-  if "float_value" in clear_fields and float_value is not None:
-    if not _has_single_float_answer_equal_to(question, answer_texts, float_value):
-      del answer_texts[:]
-      float_value = float(float_value)
-      if float_value.is_integer():
-        number_str = str(int(float_value))
-      else:
-        number_str = str(float_value)
-      answer_texts = []
-      answer_texts.append(number_str)
-
-  if not answer_texts:
-    raise ValueError("No answer_texts provided")
-
-  for field_name in clear_fields:
-    if field_name == "answer_coordinates":
-        answer_coordinates = None
-    if field_name == "float_value":
-        float_value = None
-    if field_name == "aggregation_function":
-        aggregation_function = None
-
-  error_message = ""
-  if not answer_coordinates:
-    try:
-      answer_coordinates = _parse_answer_coordinates(
-          table,
-          answer_texts,
-          discard_ambiguous_examples,
-      )
-    except ValueError as exc:
-      error_message += "[answer_coordinates: {}]".format(str(exc))
-      if discard_ambiguous_examples:
-        raise ValueError(f"Cannot parse answer: {error_message}")
-  #print("Answer coordinates are",answer_coordinates)
-  if not float_value:
-    try:
-      answer_texts, float_value = _parse_answer_float(answer_texts, float_value)
-
-    except ValueError as exc:
-      error_message += "[float_value: {}]".format(str(exc))
-
-  # Raises an exception if we cannot set any of the two fields.
-  if not answer_coordinates and not float_value:
-    raise ValueError("Cannot parse answer: {}".format(error_message))
-
-  return question, answer_texts, answer_coordinates, float_value, aggregation_function
-
-
-# TODO(piccinno): Use some sort of introspection here to get the field names of
-# the proto.
-_CLEAR_FIELDS = frozendict.frozendict({
-    SupervisionMode.REMOVE_ALL: [
-        "answer_coordinates", "float_value", "aggregation_function"
-    ],
-    SupervisionMode.REMOVE_ALL_STRICT: [
-        "answer_coordinates", "float_value", "aggregation_function"
-    ]
-})
-
-
-def parse_question(table, question, answer_texts, answer_coordinates=None, float_value=None, aggregation_function=None,
-                    mode=SupervisionMode.REMOVE_ALL):
-    """Parses answer_text field of a question to populate additional fields required by TAPAS.
-
-    Args:
-        table: a Pandas dataframe, needed to compute the answer coordinates. Note that one should apply .astype(str)
-        before supplying the table to this function. 
-        question: a string.
-        answer_texts: a list of strings, containing one or more answer texts that serve as answer to the question.
-        answer_coordinates: optional answer coordinates supervision signal, if you already have those. 
-        float_value: optional float supervision signal, if you already have this. 
-        aggregation_function: optional aggregation function supervised signal, if you already have this. 
-        mode: see SupervisionMode enum for more information.
-
-    Returns:
-        A list with the question, populated answer_coordinates or float_value.
-
-    Raises:
-        ValueError if we cannot parse correctly the question string.
-    """
-    # if ".0" in answer_texts[0]:
-    #   answer_texts=[(str(int(ast.literal_eval(answer_texts[0]))))]
-    answer_texts = list(set(answer_texts))
-    if mode == SupervisionMode.NONE:
-        return question, answer_texts
-
-    clear_fields = _CLEAR_FIELDS.get(mode, None)
-    if clear_fields is None:
-        raise ValueError(f"Mode {mode.name} is not supported")
-
-    return _parse_question(
-        table,
-        question,
-        answer_texts,
-        answer_coordinates,
-        float_value,
-        aggregation_function,
-        clear_fields,
-        discard_ambiguous_examples=mode == SupervisionMode.REMOVE_ALL_STRICT,
+# coding=utf-8
+# Copyright 2019 The Google AI Language Team Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# Lint as: python3
+# From https://github.com/NielsRogge/tapas_utils/blob/master/Parsing_answer_texts_to_answer_coordinates_for_TAPAS.ipynb
+
+"""This module implements a simple parser that can be used for TAPAS.
+
+Given a table, a question and one or more answer_texts, it will parse the texts
+to populate other fields (e.g. answer_coordinates, float_value) that are required
+by TAPAS.
+
+Please note that exceptions in this module are concise and not parameterized,
+since they are used as counter names in a BEAM pipeline.
+"""
+
+import enum
+from typing import Callable, List, Text, Optional
+
+import six
+import struct
+import unicodedata
+import re
+
+import frozendict
+import numpy as np
+import scipy.optimize
+import ast
+
+class SupervisionMode(enum.Enum):
+  # Don't filter out any supervised information.
+  NONE = 0
+  # Remove all the supervised signals and recompute them by parsing answer
+  # texts.
+  REMOVE_ALL = 2
+  # Same as above but discard ambiguous examples
+  # (where an answer matches multiple cells).
+  REMOVE_ALL_STRICT = 3
+
+
+def _find_matching_coordinates(table, answer_text,
+                               normalize):
+  normalized_text = normalize(answer_text)
+  for row_index, row in table.iterrows():
+    for column_index, cell in enumerate(row):
+      if normalized_text == normalize(str(cell)):
+        yield (row_index, column_index)
+
+
+def _compute_cost_matrix_inner(
+    table,
+    answer_texts,
+    normalize,
+    discard_ambiguous_examples,
+):
+  """Returns a cost matrix M where the value M[i,j] contains a matching cost from answer i to cell j.
+
+  The matrix is a binary matrix and -1 is used to indicate a possible match from
+  a given answer_texts to a specific cell table. The cost matrix can then be
+  usedto compute the optimal assignments that minimizes the cost using the
+  hungarian algorithm (see scipy.optimize.linear_sum_assignment).
+
+  Args:
+    table: a Pandas dataframe.
+    answer_texts: a list of strings.
+    normalize: a function that normalizes a string.
+    discard_ambiguous_examples: If true discard if answer has multiple matches.
+
+  Raises:
+    ValueError if:
+      - we cannot correctly construct the cost matrix or the text-cell
+      assignment is ambiguous.
+      - we cannot find a matching cell for a given answer_text.
+
+  Returns:
+    A numpy matrix with shape (num_answer_texts, num_rows * num_columns).
+  """
+  max_candidates = 0
+  n_rows, n_columns = table.shape[0], table.shape[1]
+  num_cells = n_rows * n_columns
+  num_candidates = np.zeros((n_rows, n_columns))
+  cost_matrix = np.zeros((len(answer_texts), num_cells))
+
+  for index, answer_text in enumerate(answer_texts):
+    found = 0
+    for row, column in _find_matching_coordinates(table, answer_text,
+                                                  normalize):
+      found += 1
+      cost_matrix[index, (row * len(table.columns)) + column] = -1
+      num_candidates[row, column] += 1
+      max_candidates = max(max_candidates, num_candidates[row, column])
+    if found == 0:
+      return None
+    if discard_ambiguous_examples and found > 1:
+      raise ValueError("Found multiple cells for answers")
+
+  # TODO(piccinno): Shall we allow ambiguous assignments?
+  if max_candidates > 1:
+    raise ValueError("Assignment is ambiguous")
+  #print("the cost matrix is",cost_matrix)
+  return cost_matrix
+
+
+def _compute_cost_matrix(
+    table,
+    answer_texts,
+    discard_ambiguous_examples,
+):
+  """Computes cost matrix."""
+  # print("table and answer text is",table,answer_texts)
+  for index, normalize_fn in enumerate(STRING_NORMALIZATIONS):
+    try:
+      result = _compute_cost_matrix_inner(
+          table,
+          answer_texts,
+          normalize_fn,
+          discard_ambiguous_examples,
+      )
+      if result is None:
+        continue
+      return result
+    except ValueError:
+      if index == len(STRING_NORMALIZATIONS) - 1:
+        raise
+  return None
+
+
+def _parse_answer_coordinates(table,
+                              answer_texts,
+                              discard_ambiguous_examples):
+  """Populates answer_coordinates using answer_texts.
+
+  Args:
+    table: a Table message, needed to compute the answer coordinates.
+    answer_texts: a list of strings
+    discard_ambiguous_examples: If true discard if answer has multiple matches.
+
+  Raises:
+    ValueError if the conversion fails.
+  """
+  
+  cost_matrix = _compute_cost_matrix(
+      table,
+      answer_texts,
+      discard_ambiguous_examples,
+  )
+  if cost_matrix is None:
+    return
+  row_indices, column_indices = scipy.optimize.linear_sum_assignment(
+      cost_matrix)
+ 
+  # create answer coordinates as list of tuples
+  answer_coordinates = []
+  for row_index in row_indices:
+    flatten_position = column_indices[row_index]
+    row_coordinate = flatten_position // len(table.columns)
+    column_coordinate = flatten_position % len(table.columns)
+    answer_coordinates.append((row_coordinate, column_coordinate))
+
+  return answer_coordinates
+
+
+### START OF UTILITIES FROM TEXT_UTILS.PY ###
+
+def wtq_normalize(x):
+  """Returns the normalized version of x.
+  This normalization function is taken from WikiTableQuestions github, hence the
+  wtq prefix. For more information, see
+  https://github.com/ppasupat/WikiTableQuestions/blob/master/evaluator.py
+  Args:
+    x: the object (integer type or string) to normalize.
+  Returns:
+    A normalized string.
+  """
+  x = x if isinstance(x, six.text_type) else six.text_type(x)
+  # Remove diacritics.
+  x = "".join(
+      c for c in unicodedata.normalize("NFKD", x)
+      if unicodedata.category(c) != "Mn")
+  # Normalize quotes and dashes.
+  x = re.sub(u"[‘’´`]", "'", x)
+  x = re.sub(u"[“”]", '"', x)
+  x = re.sub(u"[‐‑‒–—−]", "-", x)
+  x = re.sub(u"[‐]", "", x)
+  while True:
+    old_x = x
+    # Remove citations.
+    x = re.sub(u"((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[•♦†‡*#+])*$", "",
+               x.strip())
+    # Remove details in parenthesis.
+    x = re.sub(u"(?<!^)( \\([^)]*\\))*$", "", x.strip())
+    # Remove outermost quotation mark.
+    x = re.sub(u'^"([^"]*)"$', r"\1", x.strip())
+    if x == old_x:
+      break
+  # Remove final '.'.
+  if x and x[-1] == ".":
+    x = x[:-1]
+  # Collapse whitespaces and convert to lower case.
+  x = re.sub(r"\s+", " ", x, flags=re.U).lower().strip()
+  x = re.sub("<[^<]+?>", "", x)
+  x = x.replace("\n", " ")
+  return x
+
+
+_TOKENIZER = re.compile(r"\w+|[^\w\s]+", re.UNICODE)
+
+
+def tokenize_string(x):
+  return list(_TOKENIZER.findall(x.lower()))
+
+
+# List of string normalization functions to be applied in order. We go from
+# simplest to more complex normalization procedures.
+STRING_NORMALIZATIONS = (
+    lambda x: x,
+    lambda x: x.lower(),
+    tokenize_string,
+    wtq_normalize,
+)
+
+
+def to_float32(v):
+  """If v is a float reduce precision to that of a 32 bit float."""
+  if not isinstance(v, float):
+    return v
+  return struct.unpack("!f", struct.pack("!f", v))[0]
+
+def _split_thousands(delimeter,string_value):
+  if string_value.split(delimeter) is not None:
+    return True
+  else:
+    return False
+
+
+def convert_to_float(value):
+  """Converts value to a float using a series of increasingly complex heuristics.
+  Args:
+    value: object that needs to be converted. Allowed types include
+      float/int/strings.
+  Returns:
+    A float interpretation of value.
+  Raises:
+    ValueError if the float conversion of value fails.
+  """
+  if isinstance(value, float):
+    return value
+  if isinstance(value, int):
+    return float(value)
+  if not isinstance(value, six.string_types):
+    raise ValueError("Argument value is not a string. Can't parse it as float")
+  sanitized = value
+
+  try:
+    # Example: 1,000.7
+    if "." in sanitized and "," in sanitized:
+      return float(sanitized.replace(",", ""))
+    # 1,000
+    if "," in sanitized and _split_thousands(",", sanitized):
+      return float(sanitized.replace(",", ""))
+    # 5,5556
+    if "," in sanitized and sanitized.count(",") == 1 and not _split_thousands(
+        ",", sanitized):
+      return float(sanitized.replace(",", "."))
+    # 0.0.0.1
+    if sanitized.count(".") > 1:
+      return float(sanitized.replace(".", ""))
+    # 0,0,0,1
+    if sanitized.count(",") > 1:
+      return float(sanitized.replace(",", ""))
+    return float(sanitized)
+  except ValueError:
+    # Avoid adding the sanitized value in the error message.
+    raise ValueError("Unable to convert value to float")
+
+### END OF UTILITIES FROM TEXT_UTILS.PY ###
+
+def _parse_answer_float(answer_texts, float_value):
+  if len(answer_texts) > 1:
+    raise ValueError("Cannot convert to multiple answers to single float")
+  float_value = convert_to_float(answer_texts[0])
+  float_value = float_value
+
+  return answer_texts, float_value
+
+
+def _has_single_float_answer_equal_to(question, answer_texts, target):
+  """Returns true if the question has a single answer whose value equals to target."""
+  if len(answer_texts) != 1:
+    return False
+  try:
+    float_value = convert_to_float(answer_texts[0])
+    # In general answer_float is derived by applying the same conver_to_float
+    # function at interaction creation time, hence here we use exact match to
+    # avoid any false positive.
+    return to_float32(float_value) == to_float32(target)
+  except ValueError:
+    return False
+
+
+def _parse_question(
+    table,
+    original_question,
+    answer_texts,
+    answer_coordinates,
+    float_value,
+    aggregation_function,
+    clear_fields,
+    discard_ambiguous_examples,
+):
+  """Parses question's answer_texts fields to possibly populate additional fields.
+
+  Args:
+    table: a Pandas dataframe, needed to compute the answer coordinates.
+    original_question: a string.
+    answer_texts: a list of strings, serving as the answer to the question.
+    anser_coordinates:
+    float_value: a float, serves as float value signal. 
+    aggregation_function: 
+    clear_fields: A list of strings indicating which fields need to be cleared
+      and possibly repopulated.
+    discard_ambiguous_examples: If true, discard ambiguous examples.
+
+  Returns:
+    A Question message with answer_coordinates or float_value field populated.
+
+  Raises:
+    ValueError if we cannot parse correctly the question message.
+  """
+  question = original_question
+  # If we have a float value signal we just copy its string representation to
+  # the answer text (if multiple answers texts are present OR the answer text
+  # cannot be parsed to float OR the float value is different), after clearing
+  # this field.
+  if "float_value" in clear_fields and float_value is not None:
+    if not _has_single_float_answer_equal_to(question, answer_texts, float_value):
+      del answer_texts[:]
+      float_value = float(float_value)
+      if float_value.is_integer():
+        number_str = str(int(float_value))
+      else:
+        number_str = str(float_value)
+      answer_texts = []
+      answer_texts.append(number_str)
+
+  if not answer_texts:
+    raise ValueError("No answer_texts provided")
+
+  for field_name in clear_fields:
+    if field_name == "answer_coordinates":
+        answer_coordinates = None
+    if field_name == "float_value":
+        float_value = None
+    if field_name == "aggregation_function":
+        aggregation_function = None
+
+  error_message = ""
+  if not answer_coordinates:
+    try:
+      answer_coordinates = _parse_answer_coordinates(
+          table,
+          answer_texts,
+          discard_ambiguous_examples,
+      )
+    except ValueError as exc:
+      error_message += "[answer_coordinates: {}]".format(str(exc))
+      if discard_ambiguous_examples:
+        raise ValueError(f"Cannot parse answer: {error_message}")
+  #print("Answer coordinates are",answer_coordinates)
+  if not float_value:
+    try:
+      answer_texts, float_value = _parse_answer_float(answer_texts, float_value)
+
+    except ValueError as exc:
+      error_message += "[float_value: {}]".format(str(exc))
+
+  # Raises an exception if we cannot set any of the two fields.
+  if not answer_coordinates and not float_value:
+    raise ValueError("Cannot parse answer: {}".format(error_message))
+
+  return question, answer_texts, answer_coordinates, float_value, aggregation_function
+
+
+# TODO(piccinno): Use some sort of introspection here to get the field names of
+# the proto.
+_CLEAR_FIELDS = frozendict.frozendict({
+    SupervisionMode.REMOVE_ALL: [
+        "answer_coordinates", "float_value", "aggregation_function"
+    ],
+    SupervisionMode.REMOVE_ALL_STRICT: [
+        "answer_coordinates", "float_value", "aggregation_function"
+    ]
+})
+
+
+def parse_question(table, question, answer_texts, answer_coordinates=None, float_value=None, aggregation_function=None,
+                    mode=SupervisionMode.REMOVE_ALL):
+    """Parses answer_text field of a question to populate additional fields required by TAPAS.
+
+    Args:
+        table: a Pandas dataframe, needed to compute the answer coordinates. Note that one should apply .astype(str)
+        before supplying the table to this function. 
+        question: a string.
+        answer_texts: a list of strings, containing one or more answer texts that serve as answer to the question.
+        answer_coordinates: optional answer coordinates supervision signal, if you already have those. 
+        float_value: optional float supervision signal, if you already have this. 
+        aggregation_function: optional aggregation function supervised signal, if you already have this. 
+        mode: see SupervisionMode enum for more information.
+
+    Returns:
+        A list with the question, populated answer_coordinates or float_value.
+
+    Raises:
+        ValueError if we cannot parse correctly the question string.
+    """
+    # if ".0" in answer_texts[0]:
+    #   answer_texts=[(str(int(ast.literal_eval(answer_texts[0]))))]
+    answer_texts = list(set(answer_texts))
+    if mode == SupervisionMode.NONE:
+        return question, answer_texts
+
+    clear_fields = _CLEAR_FIELDS.get(mode, None)
+    if clear_fields is None:
+        raise ValueError(f"Mode {mode.name} is not supported")
+
+    return _parse_question(
+        table,
+        question,
+        answer_texts,
+        answer_coordinates,
+        float_value,
+        aggregation_function,
+        clear_fields,
+        discard_ambiguous_examples=mode == SupervisionMode.REMOVE_ALL_STRICT,
     )
```

## primeqa/tableqa/preprocessors/dataset.py

 * *Ordering differences only*

```diff
@@ -1,85 +1,85 @@
-from cmath import nan
-import torch
-import pandas as pd
-import ast
-
-class DatasetProcessor(torch.utils.data.Dataset):
-    
-    def __init__(self, data, tokenizer,table_csv_path):
-        """ Basic tableqa dataset processor class which tokenizes every instance and returns the input ids and labels
-
-        Args:
-            data (object): Table QA instance
-            tokenizer (TapasTokenizer): Instance of Tapas Tokenizer class
-            table_csv_path (str): path to table in csv format
-        """
-        self.data = data
-        self.tokenizer = tokenizer
-        self.table_csv_path = table_csv_path
-    def __getitem__(self, idx):
-        """Returns encoded instance
-
-        Args:
-            idx (int): Index of data
-
-        Returns:
-            encoding: Returns tokenized table instance
-        """
-        item = self.data.iloc[idx]
-        table = pd.read_csv(self.table_csv_path + item.table_file,index_col=0).astype(str) # be sure to make your table data text only
-        answer_coordinates=ast.literal_eval(str(item.answer_coordinates))
-
-        encoding = self.tokenizer(table=table,
-                                queries=item.question,
-                                answer_coordinates=answer_coordinates,
-                                answer_text=item.answer_text,
-                                truncation=True,
-                                padding="max_length",
-                                return_tensors="pt",
-                                max_column_id=32,
-                                max_row_id=64,
-        )
-     
-        encoding = {key: val.squeeze(0) for key, val in encoding.items()}
-        if item.float_answer:
-            encoding["float_answer"] = torch.tensor(item.float_answer)
-        return encoding
-    def __len__(self):
-        """Calculates length of data/total number of training instances
-
-        Returns:
-            int: Total number of training instances
-        """
-        return len(self.data)
-
-
-class TableQADataset:
-    def __init__(self,data_path_root,train_dataset_path,dev_dataset_path,tokenizer=None):
-        """General tableqa datset class which creates Dataset class object based on train data and tables
-
-        Args:
-            data_path_root (str): Root directory of the dataset for eg. ./wikisql. Tables will be stored inside $data_path_root/tables
-            train_dataset_path (str): Path to the training dataset in tsv format with id, question, answer coordinates and answer text
-            dev_dataset_path (str): Path to the development dataset in tsv format.
-            tokenizer (TapasTokenizer, optional): Tapas Tokenizer object. Defaults to None.
-        """
-        self.tokenizer = tokenizer
-        self.data_path_root = data_path_root
-        self.train_dataset_path = train_dataset_path
-        self.dev_dataset_path = dev_dataset_path
-        
-    def load_data(self):
-        """This function preprocesses raw data and returns the processes dataset for train and dev
-
-        Returns:
-            Dataset, Dataset: Returns train dataset and eval dataset
-        """
-        train_data = pd.read_csv(self.train_dataset_path, sep='\t')
-        train_dataset = DatasetProcessor(train_data, self.tokenizer,self.data_path_root)
-
-        dev_data = pd.read_csv(self.dev_dataset_path, sep='\t')
-        dev_dataset = DatasetProcessor(dev_data, self.tokenizer,self.data_path_root)
-        return train_dataset,dev_dataset
-
-
-
+from cmath import nan
+import torch
+import pandas as pd
+import ast
+
+class DatasetProcessor(torch.utils.data.Dataset):
+    
+    def __init__(self, data, tokenizer,table_csv_path):
+        """ Basic tableqa dataset processor class which tokenizes every instance and returns the input ids and labels
+
+        Args:
+            data (object): Table QA instance
+            tokenizer (TapasTokenizer): Instance of Tapas Tokenizer class
+            table_csv_path (str): path to table in csv format
+        """
+        self.data = data
+        self.tokenizer = tokenizer
+        self.table_csv_path = table_csv_path
+    def __getitem__(self, idx):
+        """Returns encoded instance
+
+        Args:
+            idx (int): Index of data
+
+        Returns:
+            encoding: Returns tokenized table instance
+        """
+        item = self.data.iloc[idx]
+        table = pd.read_csv(self.table_csv_path + item.table_file,index_col=0).astype(str) # be sure to make your table data text only
+        answer_coordinates=ast.literal_eval(str(item.answer_coordinates))
+
+        encoding = self.tokenizer(table=table,
+                                queries=item.question,
+                                answer_coordinates=answer_coordinates,
+                                answer_text=item.answer_text,
+                                truncation=True,
+                                padding="max_length",
+                                return_tensors="pt",
+                                max_column_id=32,
+                                max_row_id=64,
+        )
+     
+        encoding = {key: val.squeeze(0) for key, val in encoding.items()}
+        if item.float_answer:
+            encoding["float_answer"] = torch.tensor(item.float_answer)
+        return encoding
+    def __len__(self):
+        """Calculates length of data/total number of training instances
+
+        Returns:
+            int: Total number of training instances
+        """
+        return len(self.data)
+
+
+class TableQADataset:
+    def __init__(self,data_path_root,train_dataset_path,dev_dataset_path,tokenizer=None):
+        """General tableqa datset class which creates Dataset class object based on train data and tables
+
+        Args:
+            data_path_root (str): Root directory of the dataset for eg. ./wikisql. Tables will be stored inside $data_path_root/tables
+            train_dataset_path (str): Path to the training dataset in tsv format with id, question, answer coordinates and answer text
+            dev_dataset_path (str): Path to the development dataset in tsv format.
+            tokenizer (TapasTokenizer, optional): Tapas Tokenizer object. Defaults to None.
+        """
+        self.tokenizer = tokenizer
+        self.data_path_root = data_path_root
+        self.train_dataset_path = train_dataset_path
+        self.dev_dataset_path = dev_dataset_path
+        
+    def load_data(self):
+        """This function preprocesses raw data and returns the processes dataset for train and dev
+
+        Returns:
+            Dataset, Dataset: Returns train dataset and eval dataset
+        """
+        train_data = pd.read_csv(self.train_dataset_path, sep='\t')
+        train_dataset = DatasetProcessor(train_data, self.tokenizer,self.data_path_root)
+
+        dev_data = pd.read_csv(self.dev_dataset_path, sep='\t')
+        dev_dataset = DatasetProcessor(dev_data, self.tokenizer,self.data_path_root)
+        return train_dataset,dev_dataset
+
+
+
```

## primeqa/tableqa/preprocessors/wikisql_preprocessor.py

```diff
@@ -1,118 +1,118 @@
-from primeqa.tableqa.preprocessors.convert_to_sqa_format import parse_question
-from primeqa.qg.processors.table_qg.wikisql_processor import WikiSqlDataset
-from primeqa.qg.models.table_qg.sql_sampler import SimpleSqlSampler
-
-import pandas as pd
-import csv
-import nlp
-from nlp import load_dataset
-import os
-from primeqa.tableqa.preprocessors.dataset import DatasetProcessor
-from pathlib import Path
-
-def preprocess_wikisql(output_dir,dataset,split):
-    """Preprocesses wikisql dataset downloaded from huggingface. Converts it to a format accepted by tapas
-
-    Args:
-        output_dir (str): Directory path to store converted intermediate data 
-        dataset (Dataset): Downloaded wikisql dataset
-        split (str): The dataset split whether train or dev
-
-    Returns:
-        str,str: Returns path to the output directory and path to the processed dataset
-    """
-    path = Path(output_dir)
-    path.mkdir(parents=True, exist_ok=True)
-    tables_path = Path(output_dir+"/tables/")
-    tables_path.mkdir(parents=True, exist_ok=True)
-    data_path = os.path.join(output_dir,split+".tsv")
-    file_to_write = open(data_path,"wt")
-    tsv_writer = csv.writer(file_to_write, delimiter='\t')
-    tsv_writer.writerow(['id','question','table_file','answer_coordinates','answer_text','float_answer','aggregation_label'])
-    for id, d in enumerate(dataset):
-        question = d['question']
-        qid = "wikisql_"+str(id)
-        table = d['table']
-        
-        sql = d['sql']
-        answer_text,table =  get_answer(table,sql)
-        answer_text = [str(i) for i in answer_text]
-        table_id,processed_table,min_tokens = preprocess_table(table)
-        if answer_text==['']:
-            continue
-        if min_tokens > 150:
-            continue
-        table_df = pd.DataFrame.from_dict(processed_table)
-        parsed_data = parse_question(table_df,question,answer_text)
-        table_df.to_csv(os.path.join(output_dir,"tables/"+str(table_id)+".csv"), sep=',')
-        answer_coordinates = parsed_data[2]
-        if answer_coordinates=="" or answer_coordinates==None or answer_coordinates==[]:
-            continue
-        float_answer = parsed_data[3]
-        aggregation_label = parsed_data[4]
-        tsv_writer.writerow([qid,question,"tables/"+str(table_id)+".csv",answer_coordinates,answer_text,float_answer,aggregation_label])
-    print("Preprocessing done")
-    file_to_write.close()
-    return output_dir,data_path
-
-
-def preprocess_table(table):
-    """This method preprocess the table and converts it into format such as {column_header: [list of values for that column in every row ]}
-
-    Args:
-        table (Dict): The table dictionary as provided with wikisql dataset from huggingface
-
-    Returns:
-        str,Dict,int: Returns the id of the table, a dictionary of processed table in required format, and the minimum number of tokens in the table which is #rows * #columns
-    """
-    header = table['header']
-    id = table['id']
-    rows = table['rows']
-    min_tokens = len(header)*len(rows)
-    table_data = {}
-    for i,h in enumerate(header):
-        table_data[h] = [r[i] for r in rows]
-    return id,table_data,min_tokens
-
-
-def get_answer(table,sql):
-    """ Executes the SQL provided with wikisql dataset on the table and fetches the answer text
-
-    Args:
-        table (Dict): Table Dictonary
-        sql (str): sql string
-
-    Returns:
-        str,Dict: Returns the answer text and the corrected table
-    """
-    answer_text = None
-    table = SimpleSqlSampler.add_column_types(table)
-    answer_text= WikiSqlDataset._execute_sql(sql,table)
-    return answer_text,table
-
-def load_data(out_dir,tokenizer,subset_train=-1,subset_dev=-1):
-    """Main function which downloads the wikisql data from huggingface, converts it into required format and preprocessed it and returns the Dataset objects.
-
-    Args:
-        out_dir (str): Output directory to store intemediate converted data
-        tokenizer (TapasTokenizer): Tokenizer to tokenize the data and get encodings
-
-    Returns:
-        Dataset,Dataset: Returns processed train and dev Dataset objects
-    """
-    print("Preprocessing wikisql dataset")
-    dataset_dev = load_dataset('wikisql', split=nlp.Split.VALIDATION)
-    dataset_train = load_dataset('wikisql', split=nlp.Split.TRAIN)
-    if(subset_dev>-1):
-        dataset_dev=dataset_dev.select(range(subset_dev))
-    if(subset_train>-1):
-        dataset_train=dataset_train.select(range(subset_train))
-    root_dir,train_data_path = preprocess_wikisql(out_dir,dataset_train,"train")
-    root_dir,dev_data_path  = preprocess_wikisql(out_dir,dataset_dev,"dev")
-    dev_data = pd.read_csv(dev_data_path, sep='\t')
-    dev_dataset = DatasetProcessor(dev_data, tokenizer,root_dir)
-    train_data = pd.read_csv(train_data_path, sep='\t')
-    train_dataset = DatasetProcessor(train_data, tokenizer,root_dir)
-    return train_dataset,dev_dataset
-
-
+from primeqa.tableqa.preprocessors.convert_to_sqa_format import parse_question
+from primeqa.qg.processors.table_qg.sql_processor import SqlProcessor
+from primeqa.qg.models.table_qg.sql_sampler import SimpleSqlSampler
+
+import pandas as pd
+import csv
+import nlp
+from nlp import load_dataset
+import os
+from primeqa.tableqa.preprocessors.dataset import DatasetProcessor
+from pathlib import Path
+
+def preprocess_wikisql(output_dir,dataset,split):
+    """Preprocesses wikisql dataset downloaded from huggingface. Converts it to a format accepted by tapas
+
+    Args:
+        output_dir (str): Directory path to store converted intermediate data 
+        dataset (Dataset): Downloaded wikisql dataset
+        split (str): The dataset split whether train or dev
+
+    Returns:
+        str,str: Returns path to the output directory and path to the processed dataset
+    """
+    path = Path(output_dir)
+    path.mkdir(parents=True, exist_ok=True)
+    tables_path = Path(output_dir+"/tables/")
+    tables_path.mkdir(parents=True, exist_ok=True)
+    data_path = os.path.join(output_dir,split+".tsv")
+    file_to_write = open(data_path,"wt")
+    tsv_writer = csv.writer(file_to_write, delimiter='\t')
+    tsv_writer.writerow(['id','question','table_file','answer_coordinates','answer_text','float_answer','aggregation_label'])
+    for id, d in enumerate(dataset):
+        question = d['question']
+        qid = "wikisql_"+str(id)
+        table = d['table']
+        
+        sql = d['sql']
+        answer_text,table =  get_answer(table,sql)
+        answer_text = [str(i) for i in answer_text]
+        table_id,processed_table,min_tokens = preprocess_table(table)
+        if answer_text==['']:
+            continue
+        if min_tokens > 150:
+            continue
+        table_df = pd.DataFrame.from_dict(processed_table)
+        parsed_data = parse_question(table_df,question,answer_text)
+        table_df.to_csv(os.path.join(output_dir,"tables/"+str(table_id)+".csv"), sep=',')
+        answer_coordinates = parsed_data[2]
+        if answer_coordinates=="" or answer_coordinates==None or answer_coordinates==[]:
+            continue
+        float_answer = parsed_data[3]
+        aggregation_label = parsed_data[4]
+        tsv_writer.writerow([qid,question,"tables/"+str(table_id)+".csv",answer_coordinates,answer_text,float_answer,aggregation_label])
+    print("Preprocessing done")
+    file_to_write.close()
+    return output_dir,data_path
+
+
+def preprocess_table(table):
+    """This method preprocess the table and converts it into format such as {column_header: [list of values for that column in every row ]}
+
+    Args:
+        table (Dict): The table dictionary as provided with wikisql dataset from huggingface
+
+    Returns:
+        str,Dict,int: Returns the id of the table, a dictionary of processed table in required format, and the minimum number of tokens in the table which is #rows * #columns
+    """
+    header = table['header']
+    id = table['id']
+    rows = table['rows']
+    min_tokens = len(header)*len(rows)
+    table_data = {}
+    for i,h in enumerate(header):
+        table_data[h] = [r[i] for r in rows]
+    return id,table_data,min_tokens
+
+
+def get_answer(table,sql):
+    """ Executes the SQL provided with wikisql dataset on the table and fetches the answer text
+
+    Args:
+        table (Dict): Table Dictonary
+        sql (str): sql string
+
+    Returns:
+        str,Dict: Returns the answer text and the corrected table
+    """
+    answer_text = None
+    table = SimpleSqlSampler.add_column_types(table)
+    answer_text= SqlProcessor._execute_sql(sql,table)
+    return answer_text,table
+
+def load_data(out_dir,tokenizer,subset_train=-1,subset_dev=-1):
+    """Main function which downloads the wikisql data from huggingface, converts it into required format and preprocessed it and returns the Dataset objects.
+
+    Args:
+        out_dir (str): Output directory to store intemediate converted data
+        tokenizer (TapasTokenizer): Tokenizer to tokenize the data and get encodings
+
+    Returns:
+        Dataset,Dataset: Returns processed train and dev Dataset objects
+    """
+    print("Preprocessing wikisql dataset")
+    dataset_dev = load_dataset('wikisql', split=nlp.Split.VALIDATION)
+    dataset_train = load_dataset('wikisql', split=nlp.Split.TRAIN)
+    if(subset_dev>-1):
+        dataset_dev=dataset_dev.select(range(subset_dev))
+    if(subset_train>-1):
+        dataset_train=dataset_train.select(range(subset_train))
+    root_dir,train_data_path = preprocess_wikisql(out_dir,dataset_train,"train")
+    root_dir,dev_data_path  = preprocess_wikisql(out_dir,dataset_dev,"dev")
+    dev_data = pd.read_csv(dev_data_path, sep='\t')
+    dev_dataset = DatasetProcessor(dev_data, tokenizer,root_dir)
+    train_data = pd.read_csv(train_data_path, sep='\t')
+    train_dataset = DatasetProcessor(train_data, tokenizer,root_dir)
+    return train_dataset,dev_dataset
+
+
```

## primeqa/tableqa/trainers/tableqa_trainer.py

 * *Ordering differences only*

```diff
@@ -1,59 +1,59 @@
-from transformers import Trainer
-import os
-import json
-class TableQATrainer(Trainer):
-    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):
-        """Basic tableQA trainer which extends huggingface's transformers Trainer class
-
-        Args:
-            eval_examples (_type_, optional): _description_. Defaults to None.
-            post_process_function (_type_, optional): _description_. Defaults to None.
-        """
-        super().__init__(*args, **kwargs)
-        self.eval_examples = eval_examples
-        self.post_process_function = post_process_function
-        
-
-
-    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval"):
-        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
-        eval_dataloader = self.get_eval_dataloader(eval_dataset)
-        eval_examples = self.eval_examples if eval_examples is None else eval_examples
-
-        # Temporarily disable metric computation, we will do it in the loop here.
-        compute_metrics = self.compute_metrics
-        self.compute_metrics = None
-        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
-        try:
-            output = eval_loop(
-                eval_dataloader,
-                description="Evaluation",
-                # No point gathering the predictions if there are no metrics, otherwise we defer to
-                # self.args.prediction_loss_only
-                #prediction_loss_only=True if compute_metrics is None else None,
-                ignore_keys=ignore_keys,
-            )
-        finally:
-            self.compute_metrics = compute_metrics
-
-      
-        if self.post_process_function is not None:
-            eval_preds,gold_answers = self.post_process_function(eval_examples, eval_dataset, output.predictions)
-            with open(os.path.join(self.args.output_dir, 'eval_predictions.json'), 'w') as f:
-                json.dump(eval_preds, f, indent=4)
-        if self.compute_metrics is not None:
-            metrics = self.compute_metrics(eval_preds,gold_answers)
-
-            # Prefix all keys with metric_key_prefix + '_'
-            for key in list(metrics.keys()):
-                if not key.startswith(f"{metric_key_prefix}_"):
-                    metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
-
-            self.log(metrics)
-        else:
-            metrics = {}
-
-      
-
-        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
+from transformers import Trainer
+import os
+import json
+class TableQATrainer(Trainer):
+    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):
+        """Basic tableQA trainer which extends huggingface's transformers Trainer class
+
+        Args:
+            eval_examples (_type_, optional): _description_. Defaults to None.
+            post_process_function (_type_, optional): _description_. Defaults to None.
+        """
+        super().__init__(*args, **kwargs)
+        self.eval_examples = eval_examples
+        self.post_process_function = post_process_function
+        
+
+
+    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval"):
+        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
+        eval_dataloader = self.get_eval_dataloader(eval_dataset)
+        eval_examples = self.eval_examples if eval_examples is None else eval_examples
+
+        # Temporarily disable metric computation, we will do it in the loop here.
+        compute_metrics = self.compute_metrics
+        self.compute_metrics = None
+        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop
+        try:
+            output = eval_loop(
+                eval_dataloader,
+                description="Evaluation",
+                # No point gathering the predictions if there are no metrics, otherwise we defer to
+                # self.args.prediction_loss_only
+                #prediction_loss_only=True if compute_metrics is None else None,
+                ignore_keys=ignore_keys,
+            )
+        finally:
+            self.compute_metrics = compute_metrics
+
+      
+        if self.post_process_function is not None:
+            eval_preds,gold_answers = self.post_process_function(eval_examples, eval_dataset, output.predictions)
+            with open(os.path.join(self.args.output_dir, 'eval_predictions.json'), 'w') as f:
+                json.dump(eval_preds, f, indent=4)
+        if self.compute_metrics is not None:
+            metrics = self.compute_metrics(eval_preds,gold_answers)
+
+            # Prefix all keys with metric_key_prefix + '_'
+            for key in list(metrics.keys()):
+                if not key.startswith(f"{metric_key_prefix}_"):
+                    metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
+
+            self.log(metrics)
+        else:
+            metrics = {}
+
+      
+
+        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
         return metrics
```

## primeqa/tableqa/utils/data_collator.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-from dataclasses import dataclass,field
-from typing import Optional, List, Dict
-import torch
-
-
-@dataclass
-class TapasCollator:
-    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:
-        """
-        Take a list of samples from a Dataset and collate them into a batch.
-        Returns:
-            A dictionary of tensors
-        """
-        input_ids = torch.stack([example['input_ids'] for example in batch])
-        labels = torch.stack([example['labels'] for example in batch])
-        attention_mask = torch.stack([example['attention_mask'] for example in batch])
-        token_type_ids = torch.stack([example['token_type_ids'] for example in batch])
-        numeric_values = torch.stack([example['numeric_values'] for example in batch])
-        numeric_values_scale = torch.stack([example['numeric_values_scale'] for example in batch])
-
-        return {
-            'input_ids': input_ids, 
-            'attention_mask': attention_mask,
-            'labels': labels, 
-            'token_type_ids': token_type_ids,
-            'numeric_values': numeric_values,
-            'numeric_values_scale':numeric_values_scale,
-
+from dataclasses import dataclass,field
+from typing import Optional, List, Dict
+import torch
+
+
+@dataclass
+class TapasCollator:
+    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:
+        """
+        Take a list of samples from a Dataset and collate them into a batch.
+        Returns:
+            A dictionary of tensors
+        """
+        input_ids = torch.stack([example['input_ids'] for example in batch])
+        labels = torch.stack([example['labels'] for example in batch])
+        attention_mask = torch.stack([example['attention_mask'] for example in batch])
+        token_type_ids = torch.stack([example['token_type_ids'] for example in batch])
+        numeric_values = torch.stack([example['numeric_values'] for example in batch])
+        numeric_values_scale = torch.stack([example['numeric_values_scale'] for example in batch])
+
+        return {
+            'input_ids': input_ids, 
+            'attention_mask': attention_mask,
+            'labels': labels, 
+            'token_type_ids': token_type_ids,
+            'numeric_values': numeric_values,
+            'numeric_values_scale':numeric_values_scale,
+
         }
```

## primeqa/util/args_helper.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-from argparse import ArgumentParser
-from enum import Enum
-import logging
-
-logger = logging.getLogger(__name__)
-
-
-def fill_from_dict(defaults, a_dict):
-    set_values = []
-    for arg, val in a_dict.items():
-        if val is None:
-            continue
-        set_values.append(arg)
-        parts = arg.split('.')  # set nested objects with names like obj1.attr
-        toset = defaults
-        for part in parts[:-1]:
-            toset = toset.__dict__[part]
-        name = parts[-1]
-        d = toset.__dict__[name]
-        if isinstance(d, Enum):
-            toset.__dict__[name] = type(d)[val]
-        else:
-            toset.__dict__[name] = val
-    return set_values
-
-
-def _nested_get(obj, arg):
-    parts = arg.split('.')  # set nested objects with names like obj1.attr
-    toget = obj
-    for part in parts[:-1]:
-        if not hasattr(toget, '__dict__') or part not in toget.__dict__:
-            return None
-        toget = toget.__dict__[part]
-    return toget.__dict__[parts[-1]]
-
-
-def name_value_list(obj, prefix=''):
-    name_values = []
-    for attr, value in obj.__dict__.items():
-        # ignore members that start with '_'
-        if attr.startswith('_'):
-            continue
-        if hasattr(value, '__dict__'):
-            name_values.extend(name_value_list(value, prefix=prefix + attr + '.'))
-        else:
-            name_values.append((prefix+attr, value))
-    return name_values
-
-
-def fill_from_args(defaults):
-    """
-    Builds an argument parser, parses the arguments, updates and returns the object 'defaults'
-    :param defaults: an object with fields to be filled from command line arguments
-    :return:
-    """
-    parser = ArgumentParser()
-    # if defaults has a __required_args__ we set those to be required on the command line
-    required_args = []
-    names_values = name_value_list(defaults)
-    # print(names_values)
-    names = set([n for n, v in names_values])
-    if hasattr(defaults, '__required_args__'):
-        required_args = defaults.__required_args__
-        for reqarg in required_args:
-            if reqarg.startswith('_'):
-                raise ValueError(f'arguments should not start with an underscore ({reqarg})')
-            if reqarg not in names:
-                raise ValueError(f'argument "{reqarg}" is required, but not present in __init__')
-    for attr, value in names_values:
-        help_str = None
-        # if it is a tuple, we assume the second is the help string
-        # if type(value) is tuple and len(value) == 2 and type(value[1]) is str:
-        #     help_str = value[1]
-        #     value = value[0]
-
-        # check if it is a type we can take on the command line
-        if type(value) not in [str, int, float, bool] and not isinstance(value, Enum):
-            raise ValueError(f'Error on {attr}: cannot have {type(value)} as argument')
-        if type(value) is bool and value:
-            raise ValueError(f'Error on {attr}: boolean arguments (flags) must be false by default')
-
-        # also handle str to enum conversion
-        t = type(value)
-        if isinstance(value, Enum):
-            t = str
-
-        # don't pass defaults to argparse, just pass None we'll keep the default value if the arg value is None
-        if t is bool:
-            # support bool with store_true (required false by default)
-            parser.add_argument('--'+attr, default=None, action='store_true', help=help_str)
-        else:
-            parser.add_argument('--'+attr, type=t, default=None, help=help_str, required=(attr in required_args))
-    args = parser.parse_args()
-    # now update the passed object with the arguments
-    defaults.__passed_args__ = fill_from_dict(defaults, args.__dict__)
-    # call _post_argparse() if the method is defined
-    try:
-        defaults._post_argparse()
-    except AttributeError:
-        pass
-    return defaults
+from argparse import ArgumentParser
+from enum import Enum
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+def fill_from_dict(defaults, a_dict):
+    set_values = []
+    for arg, val in a_dict.items():
+        if val is None:
+            continue
+        set_values.append(arg)
+        parts = arg.split('.')  # set nested objects with names like obj1.attr
+        toset = defaults
+        for part in parts[:-1]:
+            toset = toset.__dict__[part]
+        name = parts[-1]
+        d = toset.__dict__[name]
+        if isinstance(d, Enum):
+            toset.__dict__[name] = type(d)[val]
+        else:
+            toset.__dict__[name] = val
+    return set_values
+
+
+def _nested_get(obj, arg):
+    parts = arg.split('.')  # set nested objects with names like obj1.attr
+    toget = obj
+    for part in parts[:-1]:
+        if not hasattr(toget, '__dict__') or part not in toget.__dict__:
+            return None
+        toget = toget.__dict__[part]
+    return toget.__dict__[parts[-1]]
+
+
+def name_value_list(obj, prefix=''):
+    name_values = []
+    for attr, value in obj.__dict__.items():
+        # ignore members that start with '_'
+        if attr.startswith('_'):
+            continue
+        if hasattr(value, '__dict__'):
+            name_values.extend(name_value_list(value, prefix=prefix + attr + '.'))
+        else:
+            name_values.append((prefix+attr, value))
+    return name_values
+
+
+def fill_from_args(defaults):
+    """
+    Builds an argument parser, parses the arguments, updates and returns the object 'defaults'
+    :param defaults: an object with fields to be filled from command line arguments
+    :return:
+    """
+    parser = ArgumentParser()
+    # if defaults has a __required_args__ we set those to be required on the command line
+    required_args = []
+    names_values = name_value_list(defaults)
+    # print(names_values)
+    names = set([n for n, v in names_values])
+    if hasattr(defaults, '__required_args__'):
+        required_args = defaults.__required_args__
+        for reqarg in required_args:
+            if reqarg.startswith('_'):
+                raise ValueError(f'arguments should not start with an underscore ({reqarg})')
+            if reqarg not in names:
+                raise ValueError(f'argument "{reqarg}" is required, but not present in __init__')
+    for attr, value in names_values:
+        help_str = None
+        # if it is a tuple, we assume the second is the help string
+        # if type(value) is tuple and len(value) == 2 and type(value[1]) is str:
+        #     help_str = value[1]
+        #     value = value[0]
+
+        # check if it is a type we can take on the command line
+        if type(value) not in [str, int, float, bool] and not isinstance(value, Enum):
+            raise ValueError(f'Error on {attr}: cannot have {type(value)} as argument')
+        if type(value) is bool and value:
+            raise ValueError(f'Error on {attr}: boolean arguments (flags) must be false by default')
+
+        # also handle str to enum conversion
+        t = type(value)
+        if isinstance(value, Enum):
+            t = str
+
+        # don't pass defaults to argparse, just pass None we'll keep the default value if the arg value is None
+        if t is bool:
+            # support bool with store_true (required false by default)
+            parser.add_argument('--'+attr, default=None, action='store_true', help=help_str)
+        else:
+            parser.add_argument('--'+attr, type=t, default=None, help=help_str, required=(attr in required_args))
+    args = parser.parse_args()
+    # now update the passed object with the arguments
+    defaults.__passed_args__ = fill_from_dict(defaults, args.__dict__)
+    # call _post_argparse() if the method is defined
+    try:
+        defaults._post_argparse()
+    except AttributeError:
+        pass
+    return defaults
```

## primeqa/util/file_utils.py

 * *Ordering differences only*

```diff
@@ -1,254 +1,254 @@
-import logging
-import os
-import glob
-import gzip
-import bz2
-import math
-import random
-import sys
-import contextlib
-import numpy as np
-import base64
-import codecs
-
-logger = logging.getLogger(__name__)
-
-
-def block_shuffle(iter, *, block_size=20000, rand=random):
-    """
-    shuffle the possibly endless iterator by blocks
-    Good shuffling over multiple files: block_shuffle(read_lines(files, shuffled_files=rand), rand=rand, block_size=100000)
-    :param iter: the iterator we will yield shuffled items from
-    :param block_size: size of memory to use for block shuffling
-    :param rand: rand.shuffle will be used on the list block
-    :return:
-    """
-    assert block_size >= 4
-    block = []
-    for item in iter:
-        block.append(item)
-        if len(block) >= block_size:
-            rand.shuffle(block)
-            for _ in range(block_size//2):
-                yield block.pop(-1)
-    rand.shuffle(block)
-    for bi in block:
-        yield bi
-
-
-def jsonl_lines(input_files, completed_files=None, limit=0, report_every=100000, *, errors=None, shuffled=None):
-    return read_lines(jsonl_files(input_files, completed_files),
-                      limit=limit, report_every=report_every,
-                      errors=errors, shuffled_files=shuffled)
-
-
-def jsonl_files(input_files, completed_files=None):
-    return [f for f in expand_files(input_files, '*.jsonl*', completed_files) if not f.endswith('.lock')]
-
-
-def expand_files(input_files, file_pattern='*', completed_files=None):
-    """
-    expand the list of files and directories
-    :param input_files:
-    :param file_pattern: glob pattern for recursive example '*.jsonl*' for jsonl and jsonl.gz
-    :param completed_files: these will not be returned in the final list
-    :return:
-    """
-    if type(input_files) is str:
-        if ':' in input_files:
-            input_files = input_files.split(':')
-        else:
-            input_files = [input_files]
-    # expand input files recursively
-    all_input_files = []
-    if completed_files is None:
-        completed_files = []
-    for input_file in input_files:
-        if input_file in completed_files:
-            continue
-        if not os.path.exists(input_file):
-            raise ValueError(f'no such file: {input_file}')
-        if os.path.isdir(input_file):
-            sub_files = glob.glob(input_file + "/**/" + file_pattern, recursive=True)
-            sub_files = [f for f in sub_files if not os.path.isdir(f)]
-            sub_files = [f for f in sub_files if f not in input_files and f not in completed_files]
-            all_input_files.extend(sub_files)
-        else:
-            all_input_files.append(input_file)
-    all_input_files.sort()
-    return all_input_files
-
-
-def read_open(input_file, *, binary=False, errors=None):
-    """
-    Open text file for reading, assuming compression from extension
-    :param input_file:
-    :return:
-    """
-    if binary:
-        if input_file.endswith(".gz"):
-            return gzip.open(input_file, "rb")
-        elif input_file.endswith('.bz2'):
-            return bz2.open(input_file, "rb")
-        else:
-            return open(input_file, "rb")
-    else:
-        if input_file.endswith(".gz"):
-            return gzip.open(input_file, "rt", encoding='utf-8', errors=errors)
-        elif input_file.endswith('.bz2'):
-            return bz2.open(input_file, "rt", encoding='utf-8', errors=errors)
-        else:
-            return open(input_file, "r", encoding='utf-8', errors=errors)
-
-
-def write_open(output_file, *, mkdir=True, binary=False):
-    """
-    Open text file for writing, assuming compression from extension
-    :param output_file:
-    :param mkdir:
-    :return:
-    """
-    if mkdir:
-        dir = os.path.split(output_file)[0]
-        if dir:
-            os.makedirs(dir, exist_ok=True)
-    if binary:
-        if output_file.endswith('.gz'):
-            return gzip.open(output_file, 'wb')
-        elif output_file.endswith('.bz2'):
-            return bz2.open(output_file, 'wb')
-        else:
-            return open(output_file, 'wb')
-    else:
-        if output_file.endswith('.gz'):
-            return gzip.open(output_file, 'wt', encoding='utf-8')
-        elif output_file.endswith('.bz2'):
-            return bz2.open(output_file, 'wt', encoding='utf-8')
-        else:
-            return open(output_file, 'w', encoding='utf-8')
-
-
-class ShuffledWriter:
-    def __init__(self, output_dir, *, extension='.jsonl.gz', num_files=16, rand: random.Random=None):
-        self.files = [write_open(os.path.join(output_dir, f'{i}{extension}')) for i in range(num_files)]
-        self.rand = rand if rand is not None else random.Random()
-        self.current_file = 0
-        self.buffer = []
-        self.buffer_limit = 1000000
-
-    def write(self, line):
-        self.buffer.append(line)
-        if len(self.buffer) > self.buffer_limit:
-            self.rand.shuffle(self.buffer)
-            for _ in range(len(self.buffer)//2):
-                self._write(self.buffer.pop(-1))
-
-    def _write(self, line):
-        self.files[self.current_file].write(line)
-        self.current_file = (self.current_file + 1) % len(self.files)
-
-    def close(self):
-        self.rand.shuffle(self.buffer)
-        for l in self.buffer:
-            self._write(l)
-        self.buffer = []
-        for f in self.files:
-            f.close()
-
-
-@contextlib.contextmanager
-def shuffled_writer(output_dir, *, extension='.jsonl.gz', num_files=16, rand: random.Random=None):
-    sw = ShuffledWriter(output_dir, extension=extension, num_files=num_files, rand=rand)
-    try:
-        yield sw
-    finally:
-        sw.close()
-
-
-@contextlib.contextmanager
-def stdout_or_file_open(filename=None):
-    """
-    Opens the file (or stdout if filename is False or '-') for writing.
-    Used in 'with' statement.
-    :param filename:
-    :return:
-    """
-    if filename and filename != '-':
-        fh = write_open(filename)
-    else:
-        fh = sys.stdout
-
-    try:
-        yield fh
-    finally:
-        if fh is not sys.stdout:
-            fh.close()
-
-
-def np2str(nda, *, dtype=np.float16):
-    """
-    Convert numpy ndarray to compact string representation
-    :param nda: numpy array
-    :param dtype: numpy datatype to save the array as
-    :return: base64 encoded string of numpy binary
-    """
-    return base64.b64encode(nda.astype(dtype)).decode('ascii')
-
-
-def str2np(s: str, *, dtype=np.float16):
-    """
-    Convert compact string representation of numpy ndarry to numpy vector
-    :param s: base64 encoded string of numpy binary
-    :param dtype: numpy datatype of the saved array
-    :return: 1-D array (shape is not preserved)
-    """
-    return np.frombuffer(base64.decodebytes(s.encode('ascii')), dtype=dtype)
-
-
-def gzip_str(str):
-    return codecs.encode(str.encode('utf-8'), 'zlib')
-    # return gzip.compress(str.encode('utf-8'))
-
-
-def gunzip_str(bytes):
-    return codecs.decode(bytes, 'zlib').decode('utf-8')
-    # return gzip.decompress(bytes).decode('utf-8')
-
-
-def read_lines(input_files, limit=0, report_every=100000, *, errors=None, shuffled_files=None):
-    """
-    This takes a list (or single) input files and iterates over the lines in them
-    :param input_files: Directory name or list of file names
-    :param limit: maximum number of lines to read
-    :param report_every: log info after this many lines
-    :return:
-    """
-    count = 0
-    input_files = expand_files(input_files)
-    if shuffled_files:
-        if type(shuffled_files) != random.Random:
-            shuffled_files = random.Random()
-        num_open_blocks = int(math.ceil(len(input_files)/32.0))
-        for open_block_i in range(num_open_blocks):
-            open_files = [read_open(in_file, errors=errors) for in_file in input_files[open_block_i::num_open_blocks]]
-            while len(open_files) > 0:
-                fndx = shuffled_files.randint(0, len(open_files)-1)
-                next_line = open_files[fndx].readline()
-                if next_line:
-                    yield next_line
-                    count += 1
-                    if report_every > 0 and count % report_every == 0:
-                        logger.info(f'On line {count}')
-                else:
-                    open_files[fndx].close()
-                    del open_files[fndx]
-    else:
-        for input_file in input_files:
-            with read_open(input_file, errors=errors) as reader:
-                for line in reader:
-                    yield line
-                    count += 1
-                    if report_every > 0 and count % report_every == 0:
-                        logger.info(f'On line {count} in {input_file}')
-                    if 0 < limit <= count:
-                        return
+import logging
+import os
+import glob
+import gzip
+import bz2
+import math
+import random
+import sys
+import contextlib
+import numpy as np
+import base64
+import codecs
+
+logger = logging.getLogger(__name__)
+
+
+def block_shuffle(iter, *, block_size=20000, rand=random):
+    """
+    shuffle the possibly endless iterator by blocks
+    Good shuffling over multiple files: block_shuffle(read_lines(files, shuffled_files=rand), rand=rand, block_size=100000)
+    :param iter: the iterator we will yield shuffled items from
+    :param block_size: size of memory to use for block shuffling
+    :param rand: rand.shuffle will be used on the list block
+    :return:
+    """
+    assert block_size >= 4
+    block = []
+    for item in iter:
+        block.append(item)
+        if len(block) >= block_size:
+            rand.shuffle(block)
+            for _ in range(block_size//2):
+                yield block.pop(-1)
+    rand.shuffle(block)
+    for bi in block:
+        yield bi
+
+
+def jsonl_lines(input_files, completed_files=None, limit=0, report_every=100000, *, errors=None, shuffled=None):
+    return read_lines(jsonl_files(input_files, completed_files),
+                      limit=limit, report_every=report_every,
+                      errors=errors, shuffled_files=shuffled)
+
+
+def jsonl_files(input_files, completed_files=None):
+    return [f for f in expand_files(input_files, '*.jsonl*', completed_files) if not f.endswith('.lock')]
+
+
+def expand_files(input_files, file_pattern='*', completed_files=None):
+    """
+    expand the list of files and directories
+    :param input_files:
+    :param file_pattern: glob pattern for recursive example '*.jsonl*' for jsonl and jsonl.gz
+    :param completed_files: these will not be returned in the final list
+    :return:
+    """
+    if type(input_files) is str:
+        if ':' in input_files:
+            input_files = input_files.split(':')
+        else:
+            input_files = [input_files]
+    # expand input files recursively
+    all_input_files = []
+    if completed_files is None:
+        completed_files = []
+    for input_file in input_files:
+        if input_file in completed_files:
+            continue
+        if not os.path.exists(input_file):
+            raise ValueError(f'no such file: {input_file}')
+        if os.path.isdir(input_file):
+            sub_files = glob.glob(input_file + "/**/" + file_pattern, recursive=True)
+            sub_files = [f for f in sub_files if not os.path.isdir(f)]
+            sub_files = [f for f in sub_files if f not in input_files and f not in completed_files]
+            all_input_files.extend(sub_files)
+        else:
+            all_input_files.append(input_file)
+    all_input_files.sort()
+    return all_input_files
+
+
+def read_open(input_file, *, binary=False, errors=None):
+    """
+    Open text file for reading, assuming compression from extension
+    :param input_file:
+    :return:
+    """
+    if binary:
+        if input_file.endswith(".gz"):
+            return gzip.open(input_file, "rb")
+        elif input_file.endswith('.bz2'):
+            return bz2.open(input_file, "rb")
+        else:
+            return open(input_file, "rb")
+    else:
+        if input_file.endswith(".gz"):
+            return gzip.open(input_file, "rt", encoding='utf-8', errors=errors)
+        elif input_file.endswith('.bz2'):
+            return bz2.open(input_file, "rt", encoding='utf-8', errors=errors)
+        else:
+            return open(input_file, "r", encoding='utf-8', errors=errors)
+
+
+def write_open(output_file, *, mkdir=True, binary=False):
+    """
+    Open text file for writing, assuming compression from extension
+    :param output_file:
+    :param mkdir:
+    :return:
+    """
+    if mkdir:
+        dir = os.path.split(output_file)[0]
+        if dir:
+            os.makedirs(dir, exist_ok=True)
+    if binary:
+        if output_file.endswith('.gz'):
+            return gzip.open(output_file, 'wb')
+        elif output_file.endswith('.bz2'):
+            return bz2.open(output_file, 'wb')
+        else:
+            return open(output_file, 'wb')
+    else:
+        if output_file.endswith('.gz'):
+            return gzip.open(output_file, 'wt', encoding='utf-8')
+        elif output_file.endswith('.bz2'):
+            return bz2.open(output_file, 'wt', encoding='utf-8')
+        else:
+            return open(output_file, 'w', encoding='utf-8')
+
+
+class ShuffledWriter:
+    def __init__(self, output_dir, *, extension='.jsonl.gz', num_files=16, rand: random.Random=None):
+        self.files = [write_open(os.path.join(output_dir, f'{i}{extension}')) for i in range(num_files)]
+        self.rand = rand if rand is not None else random.Random()
+        self.current_file = 0
+        self.buffer = []
+        self.buffer_limit = 1000000
+
+    def write(self, line):
+        self.buffer.append(line)
+        if len(self.buffer) > self.buffer_limit:
+            self.rand.shuffle(self.buffer)
+            for _ in range(len(self.buffer)//2):
+                self._write(self.buffer.pop(-1))
+
+    def _write(self, line):
+        self.files[self.current_file].write(line)
+        self.current_file = (self.current_file + 1) % len(self.files)
+
+    def close(self):
+        self.rand.shuffle(self.buffer)
+        for l in self.buffer:
+            self._write(l)
+        self.buffer = []
+        for f in self.files:
+            f.close()
+
+
+@contextlib.contextmanager
+def shuffled_writer(output_dir, *, extension='.jsonl.gz', num_files=16, rand: random.Random=None):
+    sw = ShuffledWriter(output_dir, extension=extension, num_files=num_files, rand=rand)
+    try:
+        yield sw
+    finally:
+        sw.close()
+
+
+@contextlib.contextmanager
+def stdout_or_file_open(filename=None):
+    """
+    Opens the file (or stdout if filename is False or '-') for writing.
+    Used in 'with' statement.
+    :param filename:
+    :return:
+    """
+    if filename and filename != '-':
+        fh = write_open(filename)
+    else:
+        fh = sys.stdout
+
+    try:
+        yield fh
+    finally:
+        if fh is not sys.stdout:
+            fh.close()
+
+
+def np2str(nda, *, dtype=np.float16):
+    """
+    Convert numpy ndarray to compact string representation
+    :param nda: numpy array
+    :param dtype: numpy datatype to save the array as
+    :return: base64 encoded string of numpy binary
+    """
+    return base64.b64encode(nda.astype(dtype)).decode('ascii')
+
+
+def str2np(s: str, *, dtype=np.float16):
+    """
+    Convert compact string representation of numpy ndarry to numpy vector
+    :param s: base64 encoded string of numpy binary
+    :param dtype: numpy datatype of the saved array
+    :return: 1-D array (shape is not preserved)
+    """
+    return np.frombuffer(base64.decodebytes(s.encode('ascii')), dtype=dtype)
+
+
+def gzip_str(str):
+    return codecs.encode(str.encode('utf-8'), 'zlib')
+    # return gzip.compress(str.encode('utf-8'))
+
+
+def gunzip_str(bytes):
+    return codecs.decode(bytes, 'zlib').decode('utf-8')
+    # return gzip.decompress(bytes).decode('utf-8')
+
+
+def read_lines(input_files, limit=0, report_every=100000, *, errors=None, shuffled_files=None):
+    """
+    This takes a list (or single) input files and iterates over the lines in them
+    :param input_files: Directory name or list of file names
+    :param limit: maximum number of lines to read
+    :param report_every: log info after this many lines
+    :return:
+    """
+    count = 0
+    input_files = expand_files(input_files)
+    if shuffled_files:
+        if type(shuffled_files) != random.Random:
+            shuffled_files = random.Random()
+        num_open_blocks = int(math.ceil(len(input_files)/32.0))
+        for open_block_i in range(num_open_blocks):
+            open_files = [read_open(in_file, errors=errors) for in_file in input_files[open_block_i::num_open_blocks]]
+            while len(open_files) > 0:
+                fndx = shuffled_files.randint(0, len(open_files)-1)
+                next_line = open_files[fndx].readline()
+                if next_line:
+                    yield next_line
+                    count += 1
+                    if report_every > 0 and count % report_every == 0:
+                        logger.info(f'On line {count}')
+                else:
+                    open_files[fndx].close()
+                    del open_files[fndx]
+    else:
+        for input_file in input_files:
+            with read_open(input_file, errors=errors) as reader:
+                for line in reader:
+                    yield line
+                    count += 1
+                    if report_every > 0 and count % report_every == 0:
+                        logger.info(f'On line {count} in {input_file}')
+                    if 0 < limit <= count:
+                        return
```

## primeqa/util/metrics.py

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-import logging
-from sklearn.metrics import f1_score
-import sklearn.metrics
-
-logger = logging.getLogger(__name__)
-
-
-def simple_accuracy(preds, labels):
-    return (preds == labels).mean()
-
-
-def multiclass_score_metrics(scores, preds, labels, *, average='micro'):
-    acc = simple_accuracy(preds, labels)
-    f1 = f1_score(y_true=labels, y_pred=preds, average=average)
-    return {
-        "acc": acc,
-        "f1": f1
-    }
-
-
-def score_metrics(scores, preds, labels):
-    acc = simple_accuracy(preds, labels)
-    f1 = f1_score(y_true=labels, y_pred=preds)
-    roc = sklearn.metrics.roc_auc_score(labels == 1, scores)
-    return {
-        "acc": acc,
-        "roc": roc,
-        "f1": f1,
-        "positive_fraction": labels.mean()
-    }
+import logging
+from sklearn.metrics import f1_score
+import sklearn.metrics
+
+logger = logging.getLogger(__name__)
+
+
+def simple_accuracy(preds, labels):
+    return (preds == labels).mean()
+
+
+def multiclass_score_metrics(scores, preds, labels, *, average='micro'):
+    acc = simple_accuracy(preds, labels)
+    f1 = f1_score(y_true=labels, y_pred=preds, average=average)
+    return {
+        "acc": acc,
+        "f1": f1
+    }
+
+
+def score_metrics(scores, preds, labels):
+    acc = simple_accuracy(preds, labels)
+    f1 = f1_score(y_true=labels, y_pred=preds)
+    roc = sklearn.metrics.roc_auc_score(labels == 1, scores)
+    return {
+        "acc": acc,
+        "roc": roc,
+        "f1": f1,
+        "positive_fraction": labels.mean()
+    }
```

## primeqa/util/reporting.py

 * *Ordering differences only*

```diff
@@ -1,155 +1,155 @@
-import time
-import numpy as np
-import logging
-from typing import Iterable
-
-logger = logging.getLogger(__name__)
-
-
-def time_str(seconds: float) -> str:
-    if seconds > 60 * 60:
-        return f'{seconds/(60.0*60.0):.1f} hours'
-    if seconds > 60:
-        return f'{seconds/60.0:.1f} minutes'
-    return f'{int(seconds)} seconds'
-
-
-class Reporting:
-    def __init__(self, *, recency_weight=0.001, report_interval_secs=300, check_every=1,
-                 gather_samples:Iterable=(), num_samples=10000):
-        """
-
-        :param recency_weight: when computing the moving average, how much weight to give to the current sample
-        :param report_interval_secs: how many seconds between returning true for is_time
-        :param check_every: how often to check the time, when calling is_time
-        :param gather_samples: keep the last num_samples of the listed names (gathered from moving_averages)
-        :param num_samples: how many samples to keep
-        """
-        self.check_count = 0
-        self.check_every = check_every
-        self.start_time = time.time()
-        self.last_time = self.start_time
-        self.report_interval_secs = report_interval_secs
-        # For tracking moving averages of various values
-        self.names = None
-        self.averages = None
-        self.counts = None
-        self.recency_weight = recency_weight
-        self.per_value_recency_weight = dict()
-        self.report_count = 0
-        self._prev_check_count = 0
-        self.sample_names = list(gather_samples)
-        if len(self.sample_names) > 0:
-            self.sample_values = np.zeros((len(self.sample_names), num_samples), dtype=np.float32)
-            self.sample_ndxs = np.zeros(len(self.sample_names), dtype=np.int32)
-        else:
-            self.sample_values = None
-            self.sample_ndxs = None
-
-    def reset(self):
-        self.check_count = 0
-        self.start_time = time.time()
-        self.last_time = self.start_time
-        self.report_count = 0
-        self._prev_check_count = 0
-        if len(self.sample_names) > 0:
-            self.sample_values[:, :] = 0
-            self.sample_ndxs[:] = 0
-        if self.counts is not None:
-            self.counts[:] = 0
-            self.averages[:] = 0
-
-    def is_time(self):
-        self.check_count += 1
-        if self.check_count % self.check_every == 0:
-            elapsed = time.time() - self.last_time
-            if elapsed >= self.report_interval_secs:
-                # check the time more or less often
-                if self.check_every > 1 and self.check_count - self._prev_check_count < 5 * self.check_every:
-                    self.check_every //= 2
-                elif self.check_count - self._prev_check_count > 50 * self.check_every:
-                    self.check_every *= 2
-                self.last_time = time.time()
-                self.report_count += 1
-                self._prev_check_count = self.check_count
-                return True
-        return False
-
-    def moving_averages(self, **values):
-        # create entries in avgs and counts when needed
-        # update the avgs and counts
-        if self.names is None:
-            self.names = list(values.keys())
-            self.averages = np.zeros(len(self.names))
-            self.counts = np.zeros(len(self.names), dtype=np.int32)
-        for name in values.keys():
-            if name not in self.names:
-                self.names.append(name)
-        if self.averages.shape[0] < len(self.names):
-            old_len = self.averages.shape[0]
-            self.averages = np.resize(self.averages, len(self.names))
-            self.averages[old_len:] = 0
-            self.counts = np.resize(self.counts, len(self.names))
-            self.counts[old_len:] = 0
-        for ndx, name in enumerate(self.names):
-            if name in values:
-                self.counts[ndx] += 1
-                # support per-name recency_weight
-                if name in self.per_value_recency_weight:
-                    rweight = max(self.per_value_recency_weight[name], 1.0 / self.counts[ndx])
-                else:
-                    rweight = max(self.recency_weight, 1.0 / self.counts[ndx])
-                self.averages[ndx] = rweight * values[name] + (1.0 - rweight) * self.averages[ndx]
-        for ndx, name in enumerate(self.sample_names):
-            if name in values:
-                self.sample_values[ndx, self.sample_ndxs[ndx]] = values[name]
-                self.sample_ndxs[ndx] = (self.sample_ndxs[ndx] + 1) % self.sample_values.shape[1]
-
-    def get_samples(self, name: str):
-        for ndx, n in enumerate(self.sample_names):
-            if n == name:
-                count = self.get_count(name)
-                if count is None:
-                    count = 0
-                return self.sample_values[ndx, 0:count]  # NOTE: not in order
-        return None
-
-    def get_moving_average(self, name):
-        if self.names is None:
-            return None
-        for ndx, n in enumerate(self.names):
-            if n == name:
-                return self.averages[ndx]
-        return None
-
-    def get_count(self, name):
-        if self.names is None:
-            return None
-        for ndx, n in enumerate(self.names):
-            if n == name:
-                return self.counts[ndx]
-        return None
-
-    def elapsed_seconds(self) -> float:
-        return time.time()-self.start_time
-
-    def elapsed_time_str(self) -> str:
-        return time_str(self.elapsed_seconds())
-
-    def progress_str(self, instance_name='instance'):
-        return f'On {instance_name} {self.check_count}, ' \
-               f'{self.check_count/self.elapsed_seconds()} {instance_name}s per second.'
-
-    def display(self, *, prefix=''):
-        # display the moving averages
-        logger.info('==========================================')
-        if self.names is not None:
-            for n, v in zip(self.names, self.averages):
-                logger.info(f'{prefix}{n} = {v}')
-
-    def display_warn(self, *, prefix=''):
-        # display the moving averages
-        logger.info('==========================================')
-        if self.names is not None:
-            for n, v in zip(self.names, self.averages):
-                logger.warning(f'{prefix}{n} = {v}')
+import time
+import numpy as np
+import logging
+from typing import Iterable
+
+logger = logging.getLogger(__name__)
+
+
+def time_str(seconds: float) -> str:
+    if seconds > 60 * 60:
+        return f'{seconds/(60.0*60.0):.1f} hours'
+    if seconds > 60:
+        return f'{seconds/60.0:.1f} minutes'
+    return f'{int(seconds)} seconds'
+
+
+class Reporting:
+    def __init__(self, *, recency_weight=0.001, report_interval_secs=300, check_every=1,
+                 gather_samples:Iterable=(), num_samples=10000):
+        """
+
+        :param recency_weight: when computing the moving average, how much weight to give to the current sample
+        :param report_interval_secs: how many seconds between returning true for is_time
+        :param check_every: how often to check the time, when calling is_time
+        :param gather_samples: keep the last num_samples of the listed names (gathered from moving_averages)
+        :param num_samples: how many samples to keep
+        """
+        self.check_count = 0
+        self.check_every = check_every
+        self.start_time = time.time()
+        self.last_time = self.start_time
+        self.report_interval_secs = report_interval_secs
+        # For tracking moving averages of various values
+        self.names = None
+        self.averages = None
+        self.counts = None
+        self.recency_weight = recency_weight
+        self.per_value_recency_weight = dict()
+        self.report_count = 0
+        self._prev_check_count = 0
+        self.sample_names = list(gather_samples)
+        if len(self.sample_names) > 0:
+            self.sample_values = np.zeros((len(self.sample_names), num_samples), dtype=np.float32)
+            self.sample_ndxs = np.zeros(len(self.sample_names), dtype=np.int32)
+        else:
+            self.sample_values = None
+            self.sample_ndxs = None
+
+    def reset(self):
+        self.check_count = 0
+        self.start_time = time.time()
+        self.last_time = self.start_time
+        self.report_count = 0
+        self._prev_check_count = 0
+        if len(self.sample_names) > 0:
+            self.sample_values[:, :] = 0
+            self.sample_ndxs[:] = 0
+        if self.counts is not None:
+            self.counts[:] = 0
+            self.averages[:] = 0
+
+    def is_time(self):
+        self.check_count += 1
+        if self.check_count % self.check_every == 0:
+            elapsed = time.time() - self.last_time
+            if elapsed >= self.report_interval_secs:
+                # check the time more or less often
+                if self.check_every > 1 and self.check_count - self._prev_check_count < 5 * self.check_every:
+                    self.check_every //= 2
+                elif self.check_count - self._prev_check_count > 50 * self.check_every:
+                    self.check_every *= 2
+                self.last_time = time.time()
+                self.report_count += 1
+                self._prev_check_count = self.check_count
+                return True
+        return False
+
+    def moving_averages(self, **values):
+        # create entries in avgs and counts when needed
+        # update the avgs and counts
+        if self.names is None:
+            self.names = list(values.keys())
+            self.averages = np.zeros(len(self.names))
+            self.counts = np.zeros(len(self.names), dtype=np.int32)
+        for name in values.keys():
+            if name not in self.names:
+                self.names.append(name)
+        if self.averages.shape[0] < len(self.names):
+            old_len = self.averages.shape[0]
+            self.averages = np.resize(self.averages, len(self.names))
+            self.averages[old_len:] = 0
+            self.counts = np.resize(self.counts, len(self.names))
+            self.counts[old_len:] = 0
+        for ndx, name in enumerate(self.names):
+            if name in values:
+                self.counts[ndx] += 1
+                # support per-name recency_weight
+                if name in self.per_value_recency_weight:
+                    rweight = max(self.per_value_recency_weight[name], 1.0 / self.counts[ndx])
+                else:
+                    rweight = max(self.recency_weight, 1.0 / self.counts[ndx])
+                self.averages[ndx] = rweight * values[name] + (1.0 - rweight) * self.averages[ndx]
+        for ndx, name in enumerate(self.sample_names):
+            if name in values:
+                self.sample_values[ndx, self.sample_ndxs[ndx]] = values[name]
+                self.sample_ndxs[ndx] = (self.sample_ndxs[ndx] + 1) % self.sample_values.shape[1]
+
+    def get_samples(self, name: str):
+        for ndx, n in enumerate(self.sample_names):
+            if n == name:
+                count = self.get_count(name)
+                if count is None:
+                    count = 0
+                return self.sample_values[ndx, 0:count]  # NOTE: not in order
+        return None
+
+    def get_moving_average(self, name):
+        if self.names is None:
+            return None
+        for ndx, n in enumerate(self.names):
+            if n == name:
+                return self.averages[ndx]
+        return None
+
+    def get_count(self, name):
+        if self.names is None:
+            return None
+        for ndx, n in enumerate(self.names):
+            if n == name:
+                return self.counts[ndx]
+        return None
+
+    def elapsed_seconds(self) -> float:
+        return time.time()-self.start_time
+
+    def elapsed_time_str(self) -> str:
+        return time_str(self.elapsed_seconds())
+
+    def progress_str(self, instance_name='instance'):
+        return f'On {instance_name} {self.check_count}, ' \
+               f'{self.check_count/self.elapsed_seconds()} {instance_name}s per second.'
+
+    def display(self, *, prefix=''):
+        # display the moving averages
+        logger.info('==========================================')
+        if self.names is not None:
+            for n, v in zip(self.names, self.averages):
+                logger.info(f'{prefix}{n} = {v}')
+
+    def display_warn(self, *, prefix=''):
+        # display the moving averages
+        logger.info('==========================================')
+        if self.names is not None:
+            for n, v in zip(self.names, self.averages):
+                logger.warning(f'{prefix}{n} = {v}')
```

## primeqa/util/dataloader/distloader_base.py

 * *Ordering differences only*

```diff
@@ -1,241 +1,241 @@
-import logging
-import random
-import os
-import glob
-import torch
-from primeqa.util.transformers_utils.hypers_base import HypersBase
-from primeqa.util.file_utils import jsonl_lines, read_open
-from typing import Iterable
-import ujson as json
-from typing import List
-import itertools
-from transformers import PreTrainedTokenizer
-import numpy as np
-
-logger = logging.getLogger(__name__)
-
-
-def sentence_to_inputs(sentence: str, tokenizer: PreTrainedTokenizer, max_seq_length: int, start_end_tok_ids=None):
-    """
-    :param sentence:
-    :param tokenizer:
-    :param max_seq_length: we don't pad to this length, only truncate
-    :param start_end_tok_ids: if not None, then start_id, end_id = start_end_tok_ids
-    :return:
-    """
-    sentence = sentence.strip()
-    if not sentence:
-        return None
-    sent_tokens = tokenizer.tokenize(sentence)
-
-    if len(sent_tokens) > max_seq_length - 2:
-        sent_tokens = sent_tokens[0:max_seq_length - 2]
-
-    input_ids = tokenizer.convert_tokens_to_ids(sent_tokens)
-    if start_end_tok_ids is not None:
-        input_ids = [start_end_tok_ids[0]] + input_ids + [start_end_tok_ids[1]]
-
-    return np.array(input_ids, dtype=np.int32)
-
-
-class DistBatchesBase:
-    def __init__(self, insts, hypers: HypersBase):
-        self.insts = insts
-        self.hypers = hypers
-        self.batch_size = None
-        self.num_batches = None
-        self.displayer = None
-        self.uneven_batches = False
-
-    def post_init(self, *, batch_size, displayer=None, uneven_batches=False, random=None):
-        # CONSIDER: put batch_size in post_init, since we always pass per_gpu_batch_size to the MultiFileLoader
-        self.batch_size = batch_size
-        self.num_batches = len(self.insts) // self.batch_size
-        self.displayer = displayer
-        self.uneven_batches = uneven_batches
-        if random is not None:
-            random.shuffle(self.insts)
-        if self.uneven_batches or self.hypers.world_size == 1:
-            if len(self.insts) % self.batch_size != 0:
-                self.num_batches += 1
-        else:
-            self._distributed_min()
-
-    def _distributed_min(self):
-        if self.hypers.world_size == 1:
-            return
-        # we need to take the minimum to allow for cases where the dataloaders may have a bit different counts
-        num_batches = torch.tensor(self.num_batches, dtype=torch.long).to(self.hypers.device)
-        torch.distributed.all_reduce(num_batches, torch.distributed.ReduceOp.MIN)
-        batch_limit = num_batches.item()
-        if self.num_batches > batch_limit:
-            logger.warning(f'truncating from {self.num_batches} to {batch_limit} batches on {self.hypers.global_rank}, '
-                           f'lost {(1 - batch_limit/self.num_batches)*100} percent')
-            self.num_batches = batch_limit
-        else:
-            logger.warning(f'world rank {self.hypers.global_rank}: all workers doing '
-                           f'{self.num_batches} batches of size {self.batch_size}')
-
-    def __len__(self):
-        return self.num_batches
-
-    def make_batch(self, index, insts):
-        # NOTE: subclasses will override this
-        raise NotImplementedError
-
-    def __getitem__(self, index):
-        if index >= self.num_batches:
-            raise IndexError
-        if self.hypers.world_size == 1:
-            batch_insts = self.insts[index::self.num_batches]
-        else:
-            batch_insts = self.insts[index * self.batch_size: (index + 1) * self.batch_size]
-        batch = self.make_batch(index, batch_insts)
-        if index == 0 and self.displayer is not None:
-            self.displayer(batch)
-        return batch
-
-
-class MultiFileLoader:
-    """
-    handles the multi-file splitting across processes and the checkpointing
-    """
-    def __init__(self, hypers: HypersBase, per_gpu_batch_size: int, train_dir: str, *,
-                 checkpoint_info=None, files_per_dataloader=1, uneven_batches=False):
-        self.hypers = hypers
-        self.train_dir = train_dir
-        self.per_gpu_batch_size = per_gpu_batch_size
-        if hypers.resume_from and os.path.isfile(os.path.join(hypers.resume_from, "loader_checkpoint.json")):
-            resume_from = hypers.resume_from
-        elif os.path.isfile(os.path.join(hypers.model_name_or_path, "loader_checkpoint.json")):
-            resume_from = hypers.model_name_or_path
-        else:
-            resume_from = None
-        if checkpoint_info is None and resume_from is not None:
-            with read_open(os.path.join(resume_from, "loader_checkpoint.json")) as f:
-                checkpoint_info = json.load(f)
-            logger.info(f'loaded distloader checkpoint from {resume_from}')
-        # CONSIDER: get checkpoint as a json.load from hypers.output_dir/loader_checkpoint.json
-        if checkpoint_info and 'completed_files' in checkpoint_info:
-            self.completed_files = checkpoint_info['completed_files']
-        else:
-            self.completed_files = []
-        if checkpoint_info and 'on_epoch' in checkpoint_info:
-            self.on_epoch = checkpoint_info['on_epoch']
-        else:
-            self.on_epoch = 1
-        self.num_epochs = hypers.num_train_epochs
-        self.files_per_dataloader = files_per_dataloader
-        self.uneven_batches = uneven_batches
-        self.first_batches_loaded = False
-        self.train_files = None
-
-    def get_checkpoint_info(self):
-        # completed_files, on_epoch
-        return {'completed_files': self.completed_files, 'on_epoch': self.on_epoch}
-
-    def _get_files(self, leftover_files=None):
-        logger.info('completed files = %s, count = %i',
-                    str(self.completed_files[:5]), len(self.completed_files))
-
-        if os.path.isfile(self.train_dir):
-            self.train_files = [self.train_dir]
-        else:
-            if not self.train_dir.endswith('/'):
-                self.train_dir = self.train_dir + '/'
-            self.train_files = glob.glob(self.train_dir + '**', recursive=True)
-            self.train_files = [f for f in self.train_files if not os.path.isdir(f)]
-
-        # exclude completed files
-        self.train_files = [f for f in self.train_files if f not in self.completed_files]
-
-        self.train_files.sort()
-        random.Random(123 * self.on_epoch).shuffle(self.train_files)
-        if leftover_files is not None:
-            self.train_files = leftover_files + self.train_files
-        if self.files_per_dataloader == -1:
-            self.files_per_dataloader = max(1, len(self.train_files) // self.hypers.world_size)
-        logger.info('epoch %i, pending files = %s, count = %i',
-                    self.on_epoch, str(self.train_files[:5]), len(self.train_files))
-
-    def reset(self, *, files_per_dataloader=None, uneven_batches=None, num_epochs=None):
-        if files_per_dataloader is not None:
-            self.files_per_dataloader = files_per_dataloader
-        if uneven_batches is not None:
-            self.uneven_batches = uneven_batches
-        if num_epochs is not None:
-            self.num_epochs = num_epochs
-        self.on_epoch = 1
-        self.completed_files = []
-        self.train_files = None
-
-    def _get_input_files(self):
-        if self.train_files is None:
-            self._get_files()
-        num_files_to_use = self.files_per_dataloader * self.hypers.world_size
-        files_are_shared = False
-        # start new epoch if we have fewer train_files than world_size
-        if len(self.train_files) == 0:
-            self.completed_files = []
-            self.on_epoch += 1
-            if self.on_epoch > self.num_epochs:
-                return None, None
-            self._get_files(leftover_files=self.train_files)
-        if len(self.train_files) < self.hypers.world_size * self.files_per_dataloader:
-            num_files_to_use = len(self.train_files)
-            files_are_shared = True
-
-        assert num_files_to_use > 0
-        used_files = []
-        while len(used_files) < num_files_to_use:
-            used_files.append(self.train_files.pop())
-        self.completed_files.extend(used_files)
-
-        if files_are_shared:
-            return used_files, files_are_shared
-        else:
-            return used_files[self.hypers.global_rank::self.hypers.world_size], files_are_shared
-
-    def quick_test(self, lines: List[str]):
-        batches = self._one_load(lines)
-        batches.post_init(batch_size=self.per_gpu_batch_size * self.hypers.n_gpu, displayer=self.display_batch,
-                          uneven_batches=True, random=random.Random(123))
-        logger.info(f'batch size = {batches.batch_size}, batch count = {batches.num_batches}')
-        for b in batches:
-            logger.info(f'after first batch')
-            return b
-
-    def get_dataloader(self):
-        input_files, files_are_shared = self._get_input_files()
-        if input_files is None:
-            return None
-        lines = jsonl_lines(input_files)
-        # if input_files are supposed to be shared then get only the lines for our global_rank
-        if files_are_shared:
-            lines = itertools.islice(lines, self.hypers.global_rank, None, self.hypers.world_size)
-        logger.warning(f'on {self.hypers.global_rank} rank, using files: {input_files}, shared: {files_are_shared}')
-        batches = self._one_load(lines)
-        displayer = None
-        if not self.first_batches_loaded:
-            self.first_batches_loaded = True
-            displayer = self.display_batch
-        batches.post_init(batch_size=self.per_gpu_batch_size * self.hypers.n_gpu, displayer=displayer,
-                          uneven_batches=self.uneven_batches, random=random.Random(123 * self.on_epoch))
-        return batches
-
-    def all_batches(self):
-        while True:
-            loader = self.get_dataloader()
-            if loader is None:
-                break
-            for batch in loader:
-                yield batch
-
-    def display_batch(self, batch):
-        pass
-
-    def batch_dict(self, batch):
-        raise NotImplementedError
-
-    def _one_load(self, lines: Iterable[str]) -> DistBatchesBase:
-        raise NotImplementedError
+import logging
+import random
+import os
+import glob
+import torch
+from primeqa.util.transformers_utils.hypers_base import HypersBase
+from primeqa.util.file_utils import jsonl_lines, read_open
+from typing import Iterable
+import ujson as json
+from typing import List
+import itertools
+from transformers import PreTrainedTokenizer
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+
+def sentence_to_inputs(sentence: str, tokenizer: PreTrainedTokenizer, max_seq_length: int, start_end_tok_ids=None):
+    """
+    :param sentence:
+    :param tokenizer:
+    :param max_seq_length: we don't pad to this length, only truncate
+    :param start_end_tok_ids: if not None, then start_id, end_id = start_end_tok_ids
+    :return:
+    """
+    sentence = sentence.strip()
+    if not sentence:
+        return None
+    sent_tokens = tokenizer.tokenize(sentence)
+
+    if len(sent_tokens) > max_seq_length - 2:
+        sent_tokens = sent_tokens[0:max_seq_length - 2]
+
+    input_ids = tokenizer.convert_tokens_to_ids(sent_tokens)
+    if start_end_tok_ids is not None:
+        input_ids = [start_end_tok_ids[0]] + input_ids + [start_end_tok_ids[1]]
+
+    return np.array(input_ids, dtype=np.int32)
+
+
+class DistBatchesBase:
+    def __init__(self, insts, hypers: HypersBase):
+        self.insts = insts
+        self.hypers = hypers
+        self.batch_size = None
+        self.num_batches = None
+        self.displayer = None
+        self.uneven_batches = False
+
+    def post_init(self, *, batch_size, displayer=None, uneven_batches=False, random=None):
+        # CONSIDER: put batch_size in post_init, since we always pass per_gpu_batch_size to the MultiFileLoader
+        self.batch_size = batch_size
+        self.num_batches = len(self.insts) // self.batch_size
+        self.displayer = displayer
+        self.uneven_batches = uneven_batches
+        if random is not None:
+            random.shuffle(self.insts)
+        if self.uneven_batches or self.hypers.world_size == 1:
+            if len(self.insts) % self.batch_size != 0:
+                self.num_batches += 1
+        else:
+            self._distributed_min()
+
+    def _distributed_min(self):
+        if self.hypers.world_size == 1:
+            return
+        # we need to take the minimum to allow for cases where the dataloaders may have a bit different counts
+        num_batches = torch.tensor(self.num_batches, dtype=torch.long).to(self.hypers.device)
+        torch.distributed.all_reduce(num_batches, torch.distributed.ReduceOp.MIN)
+        batch_limit = num_batches.item()
+        if self.num_batches > batch_limit:
+            logger.warning(f'truncating from {self.num_batches} to {batch_limit} batches on {self.hypers.global_rank}, '
+                           f'lost {(1 - batch_limit/self.num_batches)*100} percent')
+            self.num_batches = batch_limit
+        else:
+            logger.warning(f'world rank {self.hypers.global_rank}: all workers doing '
+                           f'{self.num_batches} batches of size {self.batch_size}')
+
+    def __len__(self):
+        return self.num_batches
+
+    def make_batch(self, index, insts):
+        # NOTE: subclasses will override this
+        raise NotImplementedError
+
+    def __getitem__(self, index):
+        if index >= self.num_batches:
+            raise IndexError
+        if self.hypers.world_size == 1:
+            batch_insts = self.insts[index::self.num_batches]
+        else:
+            batch_insts = self.insts[index * self.batch_size: (index + 1) * self.batch_size]
+        batch = self.make_batch(index, batch_insts)
+        if index == 0 and self.displayer is not None:
+            self.displayer(batch)
+        return batch
+
+
+class MultiFileLoader:
+    """
+    handles the multi-file splitting across processes and the checkpointing
+    """
+    def __init__(self, hypers: HypersBase, per_gpu_batch_size: int, train_dir: str, *,
+                 checkpoint_info=None, files_per_dataloader=1, uneven_batches=False):
+        self.hypers = hypers
+        self.train_dir = train_dir
+        self.per_gpu_batch_size = per_gpu_batch_size
+        if hypers.resume_from and os.path.isfile(os.path.join(hypers.resume_from, "loader_checkpoint.json")):
+            resume_from = hypers.resume_from
+        elif os.path.isfile(os.path.join(hypers.model_name_or_path, "loader_checkpoint.json")):
+            resume_from = hypers.model_name_or_path
+        else:
+            resume_from = None
+        if checkpoint_info is None and resume_from is not None:
+            with read_open(os.path.join(resume_from, "loader_checkpoint.json")) as f:
+                checkpoint_info = json.load(f)
+            logger.info(f'loaded distloader checkpoint from {resume_from}')
+        # CONSIDER: get checkpoint as a json.load from hypers.output_dir/loader_checkpoint.json
+        if checkpoint_info and 'completed_files' in checkpoint_info:
+            self.completed_files = checkpoint_info['completed_files']
+        else:
+            self.completed_files = []
+        if checkpoint_info and 'on_epoch' in checkpoint_info:
+            self.on_epoch = checkpoint_info['on_epoch']
+        else:
+            self.on_epoch = 1
+        self.num_epochs = hypers.num_train_epochs
+        self.files_per_dataloader = files_per_dataloader
+        self.uneven_batches = uneven_batches
+        self.first_batches_loaded = False
+        self.train_files = None
+
+    def get_checkpoint_info(self):
+        # completed_files, on_epoch
+        return {'completed_files': self.completed_files, 'on_epoch': self.on_epoch}
+
+    def _get_files(self, leftover_files=None):
+        logger.info('completed files = %s, count = %i',
+                    str(self.completed_files[:5]), len(self.completed_files))
+
+        if os.path.isfile(self.train_dir):
+            self.train_files = [self.train_dir]
+        else:
+            if not self.train_dir.endswith('/'):
+                self.train_dir = self.train_dir + '/'
+            self.train_files = glob.glob(self.train_dir + '**', recursive=True)
+            self.train_files = [f for f in self.train_files if not os.path.isdir(f)]
+
+        # exclude completed files
+        self.train_files = [f for f in self.train_files if f not in self.completed_files]
+
+        self.train_files.sort()
+        random.Random(123 * self.on_epoch).shuffle(self.train_files)
+        if leftover_files is not None:
+            self.train_files = leftover_files + self.train_files
+        if self.files_per_dataloader == -1:
+            self.files_per_dataloader = max(1, len(self.train_files) // self.hypers.world_size)
+        logger.info('epoch %i, pending files = %s, count = %i',
+                    self.on_epoch, str(self.train_files[:5]), len(self.train_files))
+
+    def reset(self, *, files_per_dataloader=None, uneven_batches=None, num_epochs=None):
+        if files_per_dataloader is not None:
+            self.files_per_dataloader = files_per_dataloader
+        if uneven_batches is not None:
+            self.uneven_batches = uneven_batches
+        if num_epochs is not None:
+            self.num_epochs = num_epochs
+        self.on_epoch = 1
+        self.completed_files = []
+        self.train_files = None
+
+    def _get_input_files(self):
+        if self.train_files is None:
+            self._get_files()
+        num_files_to_use = self.files_per_dataloader * self.hypers.world_size
+        files_are_shared = False
+        # start new epoch if we have fewer train_files than world_size
+        if len(self.train_files) == 0:
+            self.completed_files = []
+            self.on_epoch += 1
+            if self.on_epoch > self.num_epochs:
+                return None, None
+            self._get_files(leftover_files=self.train_files)
+        if len(self.train_files) < self.hypers.world_size * self.files_per_dataloader:
+            num_files_to_use = len(self.train_files)
+            files_are_shared = True
+
+        assert num_files_to_use > 0
+        used_files = []
+        while len(used_files) < num_files_to_use:
+            used_files.append(self.train_files.pop())
+        self.completed_files.extend(used_files)
+
+        if files_are_shared:
+            return used_files, files_are_shared
+        else:
+            return used_files[self.hypers.global_rank::self.hypers.world_size], files_are_shared
+
+    def quick_test(self, lines: List[str]):
+        batches = self._one_load(lines)
+        batches.post_init(batch_size=self.per_gpu_batch_size * self.hypers.n_gpu, displayer=self.display_batch,
+                          uneven_batches=True, random=random.Random(123))
+        logger.info(f'batch size = {batches.batch_size}, batch count = {batches.num_batches}')
+        for b in batches:
+            logger.info(f'after first batch')
+            return b
+
+    def get_dataloader(self):
+        input_files, files_are_shared = self._get_input_files()
+        if input_files is None:
+            return None
+        lines = jsonl_lines(input_files)
+        # if input_files are supposed to be shared then get only the lines for our global_rank
+        if files_are_shared:
+            lines = itertools.islice(lines, self.hypers.global_rank, None, self.hypers.world_size)
+        logger.warning(f'on {self.hypers.global_rank} rank, using files: {input_files}, shared: {files_are_shared}')
+        batches = self._one_load(lines)
+        displayer = None
+        if not self.first_batches_loaded:
+            self.first_batches_loaded = True
+            displayer = self.display_batch
+        batches.post_init(batch_size=self.per_gpu_batch_size * self.hypers.n_gpu, displayer=displayer,
+                          uneven_batches=self.uneven_batches, random=random.Random(123 * self.on_epoch))
+        return batches
+
+    def all_batches(self):
+        while True:
+            loader = self.get_dataloader()
+            if loader is None:
+                break
+            for batch in loader:
+                yield batch
+
+    def display_batch(self, batch):
+        pass
+
+    def batch_dict(self, batch):
+        raise NotImplementedError
+
+    def _one_load(self, lines: Iterable[str]) -> DistBatchesBase:
+        raise NotImplementedError
```

## primeqa/util/dataloader/distloader_seq_pair.py

 * *Ordering differences only*

```diff
@@ -1,257 +1,257 @@
-import logging
-import torch
-import numpy as np
-import ujson as json
-from primeqa.util.file_utils import jsonl_lines
-from primeqa.util.dataloader.distloader_base import MultiFileLoader, DistBatchesBase, sentence_to_inputs
-from primeqa.util.transformers_utils.hypers_base import HypersBase
-from typing import List
-import traceback
-
-logger = logging.getLogger(__name__)
-
-
-def standard_json_mapper(jobj):
-    if 'text_b' in jobj:
-        return jobj['id'], jobj['text_a'], jobj['text_b'], jobj['label']
-    else:
-        return jobj['id'], jobj['text'], jobj['label']
-
-
-class SeqPairInst:
-    __slots__ = 'inst_id', 'toks_a', 'toks_b', 'label', 'teacher_labels'
-
-    def __init__(self, inst_id, toks_a, toks_b, label, teacher_labels):
-        self.inst_id = inst_id
-        self.toks_a = toks_a
-        self.label = label
-        self.toks_b = toks_b
-        self.teacher_labels = teacher_labels
-
-
-class SeqPairBatches(DistBatchesBase):
-    def __init__(self, insts: List[SeqPairInst], hypers: HypersBase, *, cls_id, sep_id, is_separate, is_single):
-        super().__init__(insts, hypers)
-        self.hypers = hypers
-        self.cls_id = cls_id
-        self.sep_id = sep_id
-        self.is_separate = is_separate
-        self.is_single = is_single
-
-    def make_batch(self, index, insts: List[SeqPairInst]):
-        if self.is_single:
-            return self.make_batch_single(index, insts)
-        elif self.is_separate:
-            return self.make_batch_separate(index, insts)
-        else:
-            return self.make_batch_joined(index, insts)
-
-    def make_batch_single(self, index, insts: List[SeqPairInst]):
-        batch_size = len(insts)
-        all_lens = np.array([len(i.toks_a) for i in insts], dtype=np.int32)
-        maxlen = np.max(all_lens)
-        # we make these as numpy first since we can't assign a np.ndarray to torch.tensor
-        all_toks = np.zeros((batch_size, maxlen+2), dtype=np.int32)
-        all_toks[:, 0] = self.cls_id
-        all_token_type = np.zeros((batch_size, maxlen+2), dtype=np.int32)
-        all_attention_mask = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
-        all_label = np.zeros(batch_size, dtype=np.long)
-        ids = [inst.inst_id for inst in insts]
-        for i, inst in enumerate(insts):
-            offset = 1
-            all_toks[i, offset:offset+len(inst.toks_a)] = inst.toks_a
-            offset += len(inst.toks_a)
-            all_toks[i, offset] = self.sep_id
-            offset += 1
-            all_attention_mask[i, :offset] = 1
-            all_label[i] = inst.label
-        tensors = ids, torch.tensor(all_toks, dtype=torch.long), torch.tensor(all_attention_mask, dtype=torch.long), \
-                  torch.tensor(all_token_type, dtype=torch.long), torch.tensor(all_label, dtype=torch.long)
-        if insts[0].teacher_labels is not None:
-            all_teacher_labels = torch.tensor([inst.teacher_labels for inst in insts], dtype=torch.float32)
-            tensors = tensors + (all_teacher_labels, )
-        return tensors
-
-    def make_batch_joined(self, index, insts: List[SeqPairInst]):
-        batch_size = len(insts)
-        all_lens = np.array([len(i.toks_a)+len(i.toks_b) for i in insts], dtype=np.int32)
-        maxlen = np.max(all_lens)
-
-        # we make these as numpy first since we can't assign a np.ndarray to torch.tensor
-        all_toks = np.zeros((batch_size, maxlen+3), dtype=np.int32)
-        all_toks[:, 0] = self.cls_id
-        all_token_type = np.zeros((batch_size, maxlen+3), dtype=np.int32)
-        all_attention_mask = np.zeros((batch_size, maxlen + 3), dtype=np.int32)
-        all_label = np.zeros(batch_size, dtype=np.int32)
-        ids = [inst.inst_id for inst in insts]
-        for i, inst in enumerate(insts):
-            # ids and pair_ids are list of ints
-            # sequence = tokenizer.build_inputs_with_special_tokens(ids, pair_ids)
-            # token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids, pair_ids)
-            offset = 1
-            all_toks[i, offset:offset+len(inst.toks_a)] = inst.toks_a
-            offset += len(inst.toks_a)
-            all_toks[i, offset] = self.sep_id
-            offset += 1
-            seq1_end = offset
-            # all_token_type[i, :seq1_end] = 0
-            all_toks[i, offset:offset+len(inst.toks_b)] = inst.toks_b
-            offset += len(inst.toks_b)
-            all_toks[i, offset] = self.sep_id
-            offset += 1
-            all_token_type[i, seq1_end:offset] = 1
-            all_attention_mask[i, :offset] = 1
-            all_label[i] = inst.label
-        tensors = ids, torch.tensor(all_toks, dtype=torch.long), torch.tensor(all_attention_mask, dtype=torch.long), \
-                  torch.tensor(all_token_type, dtype=torch.long), torch.tensor(all_label, dtype=torch.long)
-        if insts[0].teacher_labels is not None:
-            all_teacher_labels = torch.tensor([inst.teacher_labels for inst in insts], dtype=torch.float32)
-            tensors = tensors + (all_teacher_labels, )
-        return tensors
-
-    def make_batch_separate(self, index, insts: List[SeqPairInst]):
-        def make_seq_tensors(toks: List[np.ndarray]):
-            batch_size = len(toks)
-            all_lens = np.array([len(t) for t in toks], dtype=np.int32)
-            maxlen = np.max(all_lens)
-            all_toks = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
-            all_toks[:, 0] = self.cls_id
-            all_token_type = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
-            all_attention_mask = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
-            for i, tok in enumerate(toks):
-                offset = 1
-                all_toks[i, offset:offset + len(tok)] = tok
-                offset += len(tok)
-                all_toks[i, offset] = self.sep_id
-                offset += 1
-                all_attention_mask[i, :offset] = 1
-            return torch.tensor(all_toks, dtype=torch.long), \
-                   torch.tensor(all_attention_mask, dtype=torch.long), \
-                   torch.tensor(all_token_type, dtype=torch.long)
-
-        tokens_a, mask_a, token_types_a = make_seq_tensors([i.toks_a for i in insts])
-        tokens_b, mask_b, token_types_b = make_seq_tensors([i.toks_b for i in insts])
-        tensors = [i.inst_id for i in insts], \
-                  tokens_a, mask_a, token_types_a, \
-                  tokens_b, mask_b, token_types_b, \
-                  torch.tensor([i.label for i in insts], dtype=torch.long)
-        if insts[0].teacher_labels is not None:
-            all_teacher_labels = torch.tensor([inst.teacher_labels for inst in insts], dtype=torch.float32)
-            tensors = tensors + (all_teacher_labels, )
-        return tensors
-
-
-class SeqPairLoader(MultiFileLoader):
-    def __init__(self, hypers, per_gpu_batch_size: int, tokenizer, data_dir, *,
-                 files_per_dataloader=1, checkpoint_info=None, is_separate=False, is_single=False,
-                 json_mapper=standard_json_mapper, teacher_labels=None):
-        super().__init__(hypers, per_gpu_batch_size, data_dir,
-                         checkpoint_info=checkpoint_info, files_per_dataloader=files_per_dataloader)
-        self.tokenizer = tokenizer
-        # NOTE: maybe should use tokenizer.cls_token_id, tokenizer.sep_token_id
-        self.cls_id, self.sep_id = tokenizer.convert_tokens_to_ids(["[CLS]", "[SEP]"])
-        self.is_separate = is_separate
-        self.is_single = is_single
-        self.json_mapper = json_mapper
-        # just load the entire teacher predictions
-        if teacher_labels:
-            logger.info(f'loading teacher labels from {teacher_labels}')
-            self.id2teacher_labels = dict()
-            for line in jsonl_lines(teacher_labels):
-                jobj = json.loads(line)
-                id = jobj['id']
-                preds = jobj['predictions']
-                self.id2teacher_labels[id] = np.array(preds, dtype=np.float32)
-        else:
-            self.id2teacher_labels = None
-
-    def batch_dict(self, batch):
-        batch = tuple(t.to(self.hypers.device) for t in batch[1:])
-        if len(batch) < 7:
-            # single or joined (length should be 4 or 5 depending on presence of teacher_labels)
-            inputs = {"input_ids": batch[0], "attention_mask": batch[1]}
-            if self.hypers.model_type != "distilbert":
-                inputs["token_type_ids"] = (
-                    batch[2] if self.hypers.model_type in ["bert", "xlnet", "albert"] else None
-                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
-            if len(batch) > 3:
-                inputs["labels"] = batch[3]
-            if len(batch) > 4:
-                inputs['teacher_labels'] = batch[4]
-        elif len(batch) >= 7:
-            # separate (length should be 7 or 8, depending on presence of teacher labels)
-            inputs = {"input_ids_a": batch[0], "attention_mask_a": batch[1],
-                      "input_ids_b": batch[3], "attention_mask_b": batch[4],
-                      "labels": batch[6]}
-            if self.hypers.model_type != "distilbert":
-                inputs["token_type_ids_a"] = (
-                    batch[2] if self.hypers.model_type in ["bert", "xlnet", "albert"] else None
-                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
-                inputs["token_type_ids_b"] = (
-                    batch[5] if self.hypers.model_type in ["bert", "xlnet", "albert"] else None
-                )
-            if len(batch) > 7:
-                inputs['teacher_labels'] = batch[7]
-        else:
-            raise ValueError
-        return inputs
-
-    def display_batch(self, batch):
-        def to_str(tokens, mask, token_types, bi):
-            toks = [str for str in self.tokenizer.convert_ids_to_tokens(tokens[bi])]
-            for ti in range(len(toks)):
-                toks[ti] = toks[ti] + f'({token_types[bi,ti]},{mask[bi,ti]})'
-            return ' '.join(toks)
-
-        def lbl_str(labels, teacher_labels, bi):
-            if teacher_labels is None:
-                return f'{labels[bi]}'
-            else:
-                return f'{labels[bi]} ({teacher_labels[bi]})'
-
-        ids = batch[0]
-        if len(batch) in [5, 6]:
-            tokens, mask, token_types, labels = [t.cpu().numpy() for t in batch[1:5]]
-            if len(batch) == 6:
-                teacher_labels = batch[5].cpu().numpy()
-            else:
-                teacher_labels = None
-            for bi in range(min(len(ids), 3)):
-                logger.info(f'{ids[bi]} is {lbl_str(labels, teacher_labels, bi)}:\n'
-                            f'{to_str(tokens, mask, token_types, bi)}')
-        else:
-            tokens_a, mask_a, token_types_a, tokens_b, mask_b, token_types_b, labels = [t.cpu().numpy() for t in batch[1:8]]
-            if len(batch) == 9:
-                teacher_labels = batch[8].cpu().numpy()
-            else:
-                teacher_labels = None
-            for bi in range(min(len(ids), 3)):
-                logger.info(f'{ids[bi]} is {lbl_str(labels, teacher_labels, bi)}:\n'
-                            f'{to_str(tokens_a, mask_a, token_types_a, bi)} || '
-                            f'{to_str(tokens_b, mask_b, token_types_b, bi)}')
-
-    def _one_load(self, lines):
-        insts = []
-        for line in lines:
-            jobj = json.loads(line)
-            # CONSIDER: do multiprocessing?
-            try:
-                if self.is_single:
-                    inst_id, text_a, label = self.json_mapper(jobj)
-                    toks_b = None
-                else:
-                    inst_id, text_a, text_b, label = self.json_mapper(jobj)
-                    toks_b = sentence_to_inputs(text_b, tokenizer=self.tokenizer,
-                                                max_seq_length=self.hypers.max_seq_length)
-                toks_a = sentence_to_inputs(text_a, tokenizer=self.tokenizer,
-                                            max_seq_length=self.hypers.max_seq_length)
-                teacher_labels = self.id2teacher_labels[inst_id] if self.id2teacher_labels is not None else None
-                sp_inst = SeqPairInst(inst_id, toks_a, toks_b, label, teacher_labels)
-                insts.append(sp_inst)
-            except Exception as e:
-                logger.error(traceback.format_exc())
-                logger.warning(f'failed on {line}')
-
-        return SeqPairBatches(insts, self.hypers,
-                              cls_id=self.cls_id, sep_id=self.sep_id,
-                              is_separate=self.is_separate, is_single=self.is_single)
+import logging
+import torch
+import numpy as np
+import ujson as json
+from primeqa.util.file_utils import jsonl_lines
+from primeqa.util.dataloader.distloader_base import MultiFileLoader, DistBatchesBase, sentence_to_inputs
+from primeqa.util.transformers_utils.hypers_base import HypersBase
+from typing import List
+import traceback
+
+logger = logging.getLogger(__name__)
+
+
+def standard_json_mapper(jobj):
+    if 'text_b' in jobj:
+        return jobj['id'], jobj['text_a'], jobj['text_b'], jobj['label']
+    else:
+        return jobj['id'], jobj['text'], jobj['label']
+
+
+class SeqPairInst:
+    __slots__ = 'inst_id', 'toks_a', 'toks_b', 'label', 'teacher_labels'
+
+    def __init__(self, inst_id, toks_a, toks_b, label, teacher_labels):
+        self.inst_id = inst_id
+        self.toks_a = toks_a
+        self.label = label
+        self.toks_b = toks_b
+        self.teacher_labels = teacher_labels
+
+
+class SeqPairBatches(DistBatchesBase):
+    def __init__(self, insts: List[SeqPairInst], hypers: HypersBase, *, cls_id, sep_id, is_separate, is_single):
+        super().__init__(insts, hypers)
+        self.hypers = hypers
+        self.cls_id = cls_id
+        self.sep_id = sep_id
+        self.is_separate = is_separate
+        self.is_single = is_single
+
+    def make_batch(self, index, insts: List[SeqPairInst]):
+        if self.is_single:
+            return self.make_batch_single(index, insts)
+        elif self.is_separate:
+            return self.make_batch_separate(index, insts)
+        else:
+            return self.make_batch_joined(index, insts)
+
+    def make_batch_single(self, index, insts: List[SeqPairInst]):
+        batch_size = len(insts)
+        all_lens = np.array([len(i.toks_a) for i in insts], dtype=np.int32)
+        maxlen = np.max(all_lens)
+        # we make these as numpy first since we can't assign a np.ndarray to torch.tensor
+        all_toks = np.zeros((batch_size, maxlen+2), dtype=np.int32)
+        all_toks[:, 0] = self.cls_id
+        all_token_type = np.zeros((batch_size, maxlen+2), dtype=np.int32)
+        all_attention_mask = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
+        all_label = np.zeros(batch_size, dtype=np.long)
+        ids = [inst.inst_id for inst in insts]
+        for i, inst in enumerate(insts):
+            offset = 1
+            all_toks[i, offset:offset+len(inst.toks_a)] = inst.toks_a
+            offset += len(inst.toks_a)
+            all_toks[i, offset] = self.sep_id
+            offset += 1
+            all_attention_mask[i, :offset] = 1
+            all_label[i] = inst.label
+        tensors = ids, torch.tensor(all_toks, dtype=torch.long), torch.tensor(all_attention_mask, dtype=torch.long), \
+                  torch.tensor(all_token_type, dtype=torch.long), torch.tensor(all_label, dtype=torch.long)
+        if insts[0].teacher_labels is not None:
+            all_teacher_labels = torch.tensor([inst.teacher_labels for inst in insts], dtype=torch.float32)
+            tensors = tensors + (all_teacher_labels, )
+        return tensors
+
+    def make_batch_joined(self, index, insts: List[SeqPairInst]):
+        batch_size = len(insts)
+        all_lens = np.array([len(i.toks_a)+len(i.toks_b) for i in insts], dtype=np.int32)
+        maxlen = np.max(all_lens)
+
+        # we make these as numpy first since we can't assign a np.ndarray to torch.tensor
+        all_toks = np.zeros((batch_size, maxlen+3), dtype=np.int32)
+        all_toks[:, 0] = self.cls_id
+        all_token_type = np.zeros((batch_size, maxlen+3), dtype=np.int32)
+        all_attention_mask = np.zeros((batch_size, maxlen + 3), dtype=np.int32)
+        all_label = np.zeros(batch_size, dtype=np.int32)
+        ids = [inst.inst_id for inst in insts]
+        for i, inst in enumerate(insts):
+            # ids and pair_ids are list of ints
+            # sequence = tokenizer.build_inputs_with_special_tokens(ids, pair_ids)
+            # token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids, pair_ids)
+            offset = 1
+            all_toks[i, offset:offset+len(inst.toks_a)] = inst.toks_a
+            offset += len(inst.toks_a)
+            all_toks[i, offset] = self.sep_id
+            offset += 1
+            seq1_end = offset
+            # all_token_type[i, :seq1_end] = 0
+            all_toks[i, offset:offset+len(inst.toks_b)] = inst.toks_b
+            offset += len(inst.toks_b)
+            all_toks[i, offset] = self.sep_id
+            offset += 1
+            all_token_type[i, seq1_end:offset] = 1
+            all_attention_mask[i, :offset] = 1
+            all_label[i] = inst.label
+        tensors = ids, torch.tensor(all_toks, dtype=torch.long), torch.tensor(all_attention_mask, dtype=torch.long), \
+                  torch.tensor(all_token_type, dtype=torch.long), torch.tensor(all_label, dtype=torch.long)
+        if insts[0].teacher_labels is not None:
+            all_teacher_labels = torch.tensor([inst.teacher_labels for inst in insts], dtype=torch.float32)
+            tensors = tensors + (all_teacher_labels, )
+        return tensors
+
+    def make_batch_separate(self, index, insts: List[SeqPairInst]):
+        def make_seq_tensors(toks: List[np.ndarray]):
+            batch_size = len(toks)
+            all_lens = np.array([len(t) for t in toks], dtype=np.int32)
+            maxlen = np.max(all_lens)
+            all_toks = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
+            all_toks[:, 0] = self.cls_id
+            all_token_type = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
+            all_attention_mask = np.zeros((batch_size, maxlen + 2), dtype=np.int32)
+            for i, tok in enumerate(toks):
+                offset = 1
+                all_toks[i, offset:offset + len(tok)] = tok
+                offset += len(tok)
+                all_toks[i, offset] = self.sep_id
+                offset += 1
+                all_attention_mask[i, :offset] = 1
+            return torch.tensor(all_toks, dtype=torch.long), \
+                   torch.tensor(all_attention_mask, dtype=torch.long), \
+                   torch.tensor(all_token_type, dtype=torch.long)
+
+        tokens_a, mask_a, token_types_a = make_seq_tensors([i.toks_a for i in insts])
+        tokens_b, mask_b, token_types_b = make_seq_tensors([i.toks_b for i in insts])
+        tensors = [i.inst_id for i in insts], \
+                  tokens_a, mask_a, token_types_a, \
+                  tokens_b, mask_b, token_types_b, \
+                  torch.tensor([i.label for i in insts], dtype=torch.long)
+        if insts[0].teacher_labels is not None:
+            all_teacher_labels = torch.tensor([inst.teacher_labels for inst in insts], dtype=torch.float32)
+            tensors = tensors + (all_teacher_labels, )
+        return tensors
+
+
+class SeqPairLoader(MultiFileLoader):
+    def __init__(self, hypers, per_gpu_batch_size: int, tokenizer, data_dir, *,
+                 files_per_dataloader=1, checkpoint_info=None, is_separate=False, is_single=False,
+                 json_mapper=standard_json_mapper, teacher_labels=None):
+        super().__init__(hypers, per_gpu_batch_size, data_dir,
+                         checkpoint_info=checkpoint_info, files_per_dataloader=files_per_dataloader)
+        self.tokenizer = tokenizer
+        # NOTE: maybe should use tokenizer.cls_token_id, tokenizer.sep_token_id
+        self.cls_id, self.sep_id = tokenizer.convert_tokens_to_ids(["[CLS]", "[SEP]"])
+        self.is_separate = is_separate
+        self.is_single = is_single
+        self.json_mapper = json_mapper
+        # just load the entire teacher predictions
+        if teacher_labels:
+            logger.info(f'loading teacher labels from {teacher_labels}')
+            self.id2teacher_labels = dict()
+            for line in jsonl_lines(teacher_labels):
+                jobj = json.loads(line)
+                id = jobj['id']
+                preds = jobj['predictions']
+                self.id2teacher_labels[id] = np.array(preds, dtype=np.float32)
+        else:
+            self.id2teacher_labels = None
+
+    def batch_dict(self, batch):
+        batch = tuple(t.to(self.hypers.device) for t in batch[1:])
+        if len(batch) < 7:
+            # single or joined (length should be 4 or 5 depending on presence of teacher_labels)
+            inputs = {"input_ids": batch[0], "attention_mask": batch[1]}
+            if self.hypers.model_type != "distilbert":
+                inputs["token_type_ids"] = (
+                    batch[2] if self.hypers.model_type in ["bert", "xlnet", "albert"] else None
+                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
+            if len(batch) > 3:
+                inputs["labels"] = batch[3]
+            if len(batch) > 4:
+                inputs['teacher_labels'] = batch[4]
+        elif len(batch) >= 7:
+            # separate (length should be 7 or 8, depending on presence of teacher labels)
+            inputs = {"input_ids_a": batch[0], "attention_mask_a": batch[1],
+                      "input_ids_b": batch[3], "attention_mask_b": batch[4],
+                      "labels": batch[6]}
+            if self.hypers.model_type != "distilbert":
+                inputs["token_type_ids_a"] = (
+                    batch[2] if self.hypers.model_type in ["bert", "xlnet", "albert"] else None
+                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
+                inputs["token_type_ids_b"] = (
+                    batch[5] if self.hypers.model_type in ["bert", "xlnet", "albert"] else None
+                )
+            if len(batch) > 7:
+                inputs['teacher_labels'] = batch[7]
+        else:
+            raise ValueError
+        return inputs
+
+    def display_batch(self, batch):
+        def to_str(tokens, mask, token_types, bi):
+            toks = [str for str in self.tokenizer.convert_ids_to_tokens(tokens[bi])]
+            for ti in range(len(toks)):
+                toks[ti] = toks[ti] + f'({token_types[bi,ti]},{mask[bi,ti]})'
+            return ' '.join(toks)
+
+        def lbl_str(labels, teacher_labels, bi):
+            if teacher_labels is None:
+                return f'{labels[bi]}'
+            else:
+                return f'{labels[bi]} ({teacher_labels[bi]})'
+
+        ids = batch[0]
+        if len(batch) in [5, 6]:
+            tokens, mask, token_types, labels = [t.cpu().numpy() for t in batch[1:5]]
+            if len(batch) == 6:
+                teacher_labels = batch[5].cpu().numpy()
+            else:
+                teacher_labels = None
+            for bi in range(min(len(ids), 3)):
+                logger.info(f'{ids[bi]} is {lbl_str(labels, teacher_labels, bi)}:\n'
+                            f'{to_str(tokens, mask, token_types, bi)}')
+        else:
+            tokens_a, mask_a, token_types_a, tokens_b, mask_b, token_types_b, labels = [t.cpu().numpy() for t in batch[1:8]]
+            if len(batch) == 9:
+                teacher_labels = batch[8].cpu().numpy()
+            else:
+                teacher_labels = None
+            for bi in range(min(len(ids), 3)):
+                logger.info(f'{ids[bi]} is {lbl_str(labels, teacher_labels, bi)}:\n'
+                            f'{to_str(tokens_a, mask_a, token_types_a, bi)} || '
+                            f'{to_str(tokens_b, mask_b, token_types_b, bi)}')
+
+    def _one_load(self, lines):
+        insts = []
+        for line in lines:
+            jobj = json.loads(line)
+            # CONSIDER: do multiprocessing?
+            try:
+                if self.is_single:
+                    inst_id, text_a, label = self.json_mapper(jobj)
+                    toks_b = None
+                else:
+                    inst_id, text_a, text_b, label = self.json_mapper(jobj)
+                    toks_b = sentence_to_inputs(text_b, tokenizer=self.tokenizer,
+                                                max_seq_length=self.hypers.max_seq_length)
+                toks_a = sentence_to_inputs(text_a, tokenizer=self.tokenizer,
+                                            max_seq_length=self.hypers.max_seq_length)
+                teacher_labels = self.id2teacher_labels[inst_id] if self.id2teacher_labels is not None else None
+                sp_inst = SeqPairInst(inst_id, toks_a, toks_b, label, teacher_labels)
+                insts.append(sp_inst)
+            except Exception as e:
+                logger.error(traceback.format_exc())
+                logger.warning(f'failed on {line}')
+
+        return SeqPairBatches(insts, self.hypers,
+                              cls_id=self.cls_id, sep_id=self.sep_id,
+                              is_separate=self.is_separate, is_single=self.is_single)
```

## primeqa/util/dataloader/file_splitter.py

 * *Ordering differences only*

```diff
@@ -1,98 +1,98 @@
-import logging
-import os
-import random
-import numpy as np
-from primeqa.util.file_utils import jsonl_lines, write_open
-from primeqa.util.args_helper import fill_from_args
-import ujson as json
-
-logging.basicConfig(format='%(filename)s:%(lineno)d - %(message)s',
-                    datefmt='%m/%d/%Y %H:%M:%S',
-                    level=logging.INFO)
-logger = logging.getLogger(__name__)
-
-
-class Options:
-    def __init__(self):
-        self.input = ''
-        self.outdirs = ''
-        self.split_fractions = '1.0'
-        self.extension = '.jsonl.gz'
-        self.file_counts = '4'
-        self.id_field = ''  # if set the splits will be by id
-        self.exclude = ''  # the ids present here will be excluded
-        self.__required_args__ = ['input', 'outdirs']
-
-
-def main(opts: Options):
-    if opts.exclude and not opts.id_field:
-        opts.id_field = 'id'
-    # sort out output
-    outdirs = [d.strip() for d in opts.outdirs.split(',')]
-    splits = np.array([float(s) for s in opts.split_fractions.split(',')], dtype=np.float32)
-    assert len(outdirs) == len(splits)
-    splits /= splits.sum()
-    counts = np.zeros(len(outdirs), dtype=np.int32)
-    logger.info(f'Splits:')
-    for out, frac in zip(outdirs, splits):
-        logger.info(f'  {frac}: {out}')
-    file_counts = [int(fc) for fc in opts.file_counts.split(',')]
-    if len(file_counts) == 1:
-        file_counts = file_counts * len(outdirs)
-    assert len(file_counts) == len(outdirs)
-    outfiles = [[None] * fc for fc in file_counts]
-
-    # open all files
-    for ofi, outdir in zip(outfiles, outdirs):
-        for j in range(len(ofi)):
-            ofi[j] = write_open(os.path.join(outdir, f'{j}{opts.extension}'))
-
-    exclude_ids = set()
-    if opts.exclude:
-        for line in jsonl_lines(opts.exclude):
-            try:
-                inst_id = json.loads(line)[opts.id_field]
-                exclude_ids.add(inst_id)
-            except:
-                logger.warning(f'bad line: {line}')
-    excluded_count = 0
-
-    # split lines between files
-    for line in jsonl_lines(opts.input):
-        if opts.id_field:
-            inst_id = json.loads(line)[opts.id_field]
-            if inst_id in exclude_ids:
-                excluded_count += 1
-                continue
-            insplit_point = random.Random(inst_id).random()
-        else:
-            insplit_point = random.random()
-        cummulative = 0.0
-        outdir_ndx = splits.shape[0]-1
-        for i in range(splits.shape[0]-1):
-            cummulative += splits[i]
-            if insplit_point <= cummulative:
-                outdir_ndx = i
-                break
-        ofi = outfiles[outdir_ndx]
-        ofi[counts[outdir_ndx] % len(ofi)].write(line)
-        counts[outdir_ndx] += 1
-
-    if opts.exclude:
-        logger.info(f'Set to exclude {len(exclude_ids)}, excluded {excluded_count}')
-
-    # close all files
-    for i, ofi in enumerate(outfiles):
-        print(f'wrote {counts[i]} in {outdirs[i]}')
-        for j in range(len(ofi)):
-            ofi[j].close()
-
-
-"""
-python dataloader/file_splitter.py  --outdirs train,dev --split_fractions 0.8,0.2 --input 00.jsonl.gz
-"""
-
-if __name__ == "__main__":
-    opts = Options()
-    fill_from_args(opts)
-    main(opts)
+import logging
+import os
+import random
+import numpy as np
+from primeqa.util.file_utils import jsonl_lines, write_open
+from primeqa.util.args_helper import fill_from_args
+import ujson as json
+
+logging.basicConfig(format='%(filename)s:%(lineno)d - %(message)s',
+                    datefmt='%m/%d/%Y %H:%M:%S',
+                    level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+class Options:
+    def __init__(self):
+        self.input = ''
+        self.outdirs = ''
+        self.split_fractions = '1.0'
+        self.extension = '.jsonl.gz'
+        self.file_counts = '4'
+        self.id_field = ''  # if set the splits will be by id
+        self.exclude = ''  # the ids present here will be excluded
+        self.__required_args__ = ['input', 'outdirs']
+
+
+def main(opts: Options):
+    if opts.exclude and not opts.id_field:
+        opts.id_field = 'id'
+    # sort out output
+    outdirs = [d.strip() for d in opts.outdirs.split(',')]
+    splits = np.array([float(s) for s in opts.split_fractions.split(',')], dtype=np.float32)
+    assert len(outdirs) == len(splits)
+    splits /= splits.sum()
+    counts = np.zeros(len(outdirs), dtype=np.int32)
+    logger.info(f'Splits:')
+    for out, frac in zip(outdirs, splits):
+        logger.info(f'  {frac}: {out}')
+    file_counts = [int(fc) for fc in opts.file_counts.split(',')]
+    if len(file_counts) == 1:
+        file_counts = file_counts * len(outdirs)
+    assert len(file_counts) == len(outdirs)
+    outfiles = [[None] * fc for fc in file_counts]
+
+    # open all files
+    for ofi, outdir in zip(outfiles, outdirs):
+        for j in range(len(ofi)):
+            ofi[j] = write_open(os.path.join(outdir, f'{j}{opts.extension}'))
+
+    exclude_ids = set()
+    if opts.exclude:
+        for line in jsonl_lines(opts.exclude):
+            try:
+                inst_id = json.loads(line)[opts.id_field]
+                exclude_ids.add(inst_id)
+            except:
+                logger.warning(f'bad line: {line}')
+    excluded_count = 0
+
+    # split lines between files
+    for line in jsonl_lines(opts.input):
+        if opts.id_field:
+            inst_id = json.loads(line)[opts.id_field]
+            if inst_id in exclude_ids:
+                excluded_count += 1
+                continue
+            insplit_point = random.Random(inst_id).random()
+        else:
+            insplit_point = random.random()
+        cummulative = 0.0
+        outdir_ndx = splits.shape[0]-1
+        for i in range(splits.shape[0]-1):
+            cummulative += splits[i]
+            if insplit_point <= cummulative:
+                outdir_ndx = i
+                break
+        ofi = outfiles[outdir_ndx]
+        ofi[counts[outdir_ndx] % len(ofi)].write(line)
+        counts[outdir_ndx] += 1
+
+    if opts.exclude:
+        logger.info(f'Set to exclude {len(exclude_ids)}, excluded {excluded_count}')
+
+    # close all files
+    for i, ofi in enumerate(outfiles):
+        print(f'wrote {counts[i]} in {outdirs[i]}')
+        for j in range(len(ofi)):
+            ofi[j].close()
+
+
+"""
+python dataloader/file_splitter.py  --outdirs train,dev --split_fractions 0.8,0.2 --input 00.jsonl.gz
+"""
+
+if __name__ == "__main__":
+    opts = Options()
+    fill_from_args(opts)
+    main(opts)
```

## primeqa/util/transformers_utils/hypers_base.py

 * *Ordering differences only*

```diff
@@ -1,227 +1,227 @@
-import logging
-import torch
-import os
-import socket
-from primeqa.util.args_helper import fill_from_args, fill_from_dict, name_value_list
-import ujson as json
-import time
-import random
-import numpy as np
-
-logger = logging.getLogger(__name__)
-
-_dist_initialized_local_global_rank_world_size = None
-
-
-def dist_initialize():
-    """
-    initializes torch distributed
-    :return: local_rank, global_rank, world_size
-    """
-    global _dist_initialized_local_global_rank_world_size
-    if _dist_initialized_local_global_rank_world_size is not None:
-        return _dist_initialized_local_global_rank_world_size
-    if "RANK" not in os.environ:
-        local_rank = -1
-        global_rank = 0
-        world_size = 1
-    else:
-        if torch.cuda.device_count() == 0:
-            err = f'No CUDA on {socket.gethostname()}'
-            logger.error(err)
-            raise ValueError(err)
-        global_rank = int(os.environ['RANK'])
-        world_size = int(os.environ['WORLD_SIZE'])
-        env_master_addr = os.environ['MASTER_ADDR']
-        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
-        if env_master_addr.startswith('file://'):
-            torch.distributed.init_process_group(backend='nccl',
-                                                 init_method=env_master_addr,
-                                                 world_size=world_size,
-                                                 rank=global_rank)
-            logger.info("init-method file: {}".format(env_master_addr))
-            local_rank = int(os.environ['LOCAL_RANK'])
-        else:
-            torch.distributed.init_process_group(backend='nccl')
-            logger.info("init-method master_addr: {} master_port: {}".format(
-                env_master_addr, os.environ['MASTER_PORT']))
-            local_rank = int(global_rank % torch.cuda.device_count())
-    cuda_devices = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ else 'NOT SET'
-    logger.info(f"world_rank {global_rank} cuda_is_available {torch.cuda.is_available()} "
-                f"cuda_device_cnt {torch.cuda.device_count()} on {socket.gethostname()},"
-                f" CUDA_VISIBLE_DEVICES = {cuda_devices}")
-    _dist_initialized_local_global_rank_world_size = local_rank, global_rank, world_size
-    return local_rank, global_rank, world_size
-
-
-class HypersBase:
-    """
-    This should be the base hyperparameters class, others should extend this.
-    """
-    def __init__(self):
-        self.local_rank, self.global_rank, self.world_size = dist_initialize()
-        # required parameters initialized to the datatype
-        self.model_type = ''
-        self.model_name_or_path = ''
-        self.resume_from = ''  # to resume training from a checkpoint
-        self.config_name = ''
-        self.tokenizer_name = ''
-        self.cache_dir = ''
-        self.do_lower_case = False
-        self.gradient_accumulation_steps = 1
-        self.learning_rate = 5e-5
-        self.weight_decay = 0.0  # previous default was 0.01
-        self.adam_epsilon = 1e-8
-        self.max_grad_norm = 1.0
-        self.warmup_instances = 0  # previous default was 0.1 of total
-        self.warmup_fraction = 0.0  # only applies if warmup_instances <= 0
-        self.num_train_epochs = 3
-        self.no_cuda = False
-        self.n_gpu = 1
-        self.seed = 42
-        self.fp16 = False
-        self.fp16_opt_level = 'O1'  # previous default was O2
-        self.full_train_batch_size = 8  # previous default was 32
-        self.per_gpu_eval_batch_size = 8
-        self.output_dir = ''  # where to save model
-        self.log_on_all_nodes = False
-        self.server_ip = ''
-        self.server_port = ''
-        self._quiet_post_init = False
-        self.__passed_args__ = []  # will be set by fill_from_args
-        self.__required_args__ = ['model_type', 'model_name_or_path']
-
-    def set_seed(self, seed=None):
-        if seed is None:
-            seed = self.seed
-        random.seed(seed)
-        np.random.seed(seed)
-        torch.manual_seed(seed)
-        if self.n_gpu > 0:
-            torch.cuda.manual_seed_all(seed)
-
-    def set_gradient_accumulation_steps(self):
-        """
-        when searching for full_train_batch_size in hyperparameter tuning we need to update
-        the gradient accumulation steps to stay within GPU memory constraints
-        :return:
-        """
-        if self.n_gpu * self.world_size * self.per_gpu_train_batch_size > self.full_train_batch_size:
-            self.per_gpu_train_batch_size = self.full_train_batch_size // (self.n_gpu * self.world_size)
-            self.gradient_accumulation_steps = 1
-        else:
-            self.gradient_accumulation_steps = self.full_train_batch_size // \
-                                               (self.n_gpu * self.world_size * self.per_gpu_train_batch_size)
-
-    def _basic_post_init(self):
-        # Setup CUDA, GPU
-        if self.local_rank == -1 or self.no_cuda:
-            # NOTE: changed "cuda" to "cuda:0"
-            self.device = torch.device("cuda:0" if torch.cuda.is_available() and not self.no_cuda else "cpu")
-            self.n_gpu = torch.cuda.device_count()
-        else:
-            torch.cuda.set_device(self.local_rank)
-            self.device = torch.device("cuda", self.local_rank)
-            self.n_gpu = 1
-
-        if 'per_gpu_train_batch_size' not in self.__passed_args__ and self.gradient_accumulation_steps > 0:
-            self.per_gpu_train_batch_size = self.full_train_batch_size // \
-                                            ((self.n_gpu if self.n_gpu > 0 else 1) * self.world_size * self.gradient_accumulation_steps)
-        else:
-            self.gradient_accumulation_steps = self.full_train_batch_size // \
-                                            ((self.n_gpu if self.n_gpu > 0 else 1) * self.world_size * self.per_gpu_train_batch_size)
-
-        self.stop_time = None
-        if 'TIME_LIMIT_MINS' in os.environ:
-            self.stop_time = time.time() + 60 * (int(os.environ['TIME_LIMIT_MINS']) - 5)
-
-    def _post_init(self):
-        self._basic_post_init()
-
-        self._setup_logging()
-
-        # Setup distant debugging if needed
-        if self.server_ip and self.server_port:
-            # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
-            import ptvsd
-            print("Waiting for debugger attach")
-            ptvsd.enable_attach(address=(self.server_ip, self.server_port), redirect_output=True)
-            ptvsd.wait_for_attach()
-
-        if not self._quiet_post_init:
-            logger.warning(
-                "On %s, Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
-                socket.gethostname(),
-                self.local_rank,
-                self.device,
-                self.n_gpu,
-                bool(self.local_rank != -1),
-                self.fp16,
-            )
-            logger.info(f'hypers:\n{self}')
-
-    def _setup_logging(self):
-        # force our logging style
-        for handler in logging.root.handlers[:]:
-            logging.root.removeHandler(handler)
-        if self.log_on_all_nodes:
-            grank = self.global_rank
-            class HostnameFilter(logging.Filter):
-                hostname = socket.gethostname()
-                if '.' in hostname:
-                    hostname = hostname[0:hostname.find('.')]  # the first part of the hostname
-
-                def filter(self, record):
-                    record.hostname = HostnameFilter.hostname
-                    record.global_rank = grank
-                    return True
-
-            handler = logging.StreamHandler()
-            handler.setLevel(logging.INFO)
-            handler.addFilter(HostnameFilter())
-            format = logging.Formatter('%(hostname)s[%(global_rank)d] %(filename)s:%(lineno)d - %(message)s',
-                                       datefmt='%m/%d/%Y %H:%M:%S')
-            handler.setFormatter(format)
-            logging.getLogger('').addHandler(handler)
-        else:
-            logging.basicConfig(format='%(filename)s:%(lineno)d - %(message)s',
-                                datefmt='%m/%d/%Y %H:%M:%S',
-                                level=logging.INFO)
-        if self.global_rank != 0 and not self.log_on_all_nodes:
-            try:
-                logging.getLogger().setLevel(logging.WARNING)
-            except:
-                pass
-
-    def kofn(self, kofn: str):
-        """
-        ''     -> 0, 1
-        '1of2' -> 0, 2
-        '2of2' -> 1, 2
-        :param kofn:
-        :return:
-        """
-        if not kofn:
-            return 0, 1
-        k, n = [int(i) for i in kofn.lower().split('of')]
-        assert 1 <= k <= n
-        return k-1, n
-
-    def to_dict(self):
-        d = self.__dict__.copy()
-        del d['device']
-        return d
-
-    def from_dict(self, a_dict):
-        fill_from_dict(self, a_dict)
-        self._basic_post_init()  # setup device and per_gpu_batch_size
-        return self
-
-    def __str__(self):
-        nvl = {n: (v if type(v) in [int, float, str, bool] else str(v)) for n, v in name_value_list(self)}
-        return json.dumps(nvl, indent=2)
-
-    def fill_from_args(self):
-        fill_from_args(self)
-        self._post_init()
-        return self
+import logging
+import torch
+import os
+import socket
+from primeqa.util.args_helper import fill_from_args, fill_from_dict, name_value_list
+import ujson as json
+import time
+import random
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+_dist_initialized_local_global_rank_world_size = None
+
+
+def dist_initialize():
+    """
+    initializes torch distributed
+    :return: local_rank, global_rank, world_size
+    """
+    global _dist_initialized_local_global_rank_world_size
+    if _dist_initialized_local_global_rank_world_size is not None:
+        return _dist_initialized_local_global_rank_world_size
+    if "RANK" not in os.environ:
+        local_rank = -1
+        global_rank = 0
+        world_size = 1
+    else:
+        if torch.cuda.device_count() == 0:
+            err = f'No CUDA on {socket.gethostname()}'
+            logger.error(err)
+            raise ValueError(err)
+        global_rank = int(os.environ['RANK'])
+        world_size = int(os.environ['WORLD_SIZE'])
+        env_master_addr = os.environ['MASTER_ADDR']
+        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
+        if env_master_addr.startswith('file://'):
+            torch.distributed.init_process_group(backend='nccl',
+                                                 init_method=env_master_addr,
+                                                 world_size=world_size,
+                                                 rank=global_rank)
+            logger.info("init-method file: {}".format(env_master_addr))
+            local_rank = int(os.environ['LOCAL_RANK'])
+        else:
+            torch.distributed.init_process_group(backend='nccl')
+            logger.info("init-method master_addr: {} master_port: {}".format(
+                env_master_addr, os.environ['MASTER_PORT']))
+            local_rank = int(global_rank % torch.cuda.device_count())
+    cuda_devices = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ else 'NOT SET'
+    logger.info(f"world_rank {global_rank} cuda_is_available {torch.cuda.is_available()} "
+                f"cuda_device_cnt {torch.cuda.device_count()} on {socket.gethostname()},"
+                f" CUDA_VISIBLE_DEVICES = {cuda_devices}")
+    _dist_initialized_local_global_rank_world_size = local_rank, global_rank, world_size
+    return local_rank, global_rank, world_size
+
+
+class HypersBase:
+    """
+    This should be the base hyperparameters class, others should extend this.
+    """
+    def __init__(self):
+        self.local_rank, self.global_rank, self.world_size = dist_initialize()
+        # required parameters initialized to the datatype
+        self.model_type = ''
+        self.model_name_or_path = ''
+        self.resume_from = ''  # to resume training from a checkpoint
+        self.config_name = ''
+        self.tokenizer_name = ''
+        self.cache_dir = ''
+        self.do_lower_case = False
+        self.gradient_accumulation_steps = 1
+        self.learning_rate = 5e-5
+        self.weight_decay = 0.0  # previous default was 0.01
+        self.adam_epsilon = 1e-8
+        self.max_grad_norm = 1.0
+        self.warmup_instances = 0  # previous default was 0.1 of total
+        self.warmup_fraction = 0.0  # only applies if warmup_instances <= 0
+        self.num_train_epochs = 3
+        self.no_cuda = False
+        self.n_gpu = 1
+        self.seed = 42
+        self.fp16 = False
+        self.fp16_opt_level = 'O1'  # previous default was O2
+        self.full_train_batch_size = 8  # previous default was 32
+        self.per_gpu_eval_batch_size = 8
+        self.output_dir = ''  # where to save model
+        self.log_on_all_nodes = False
+        self.server_ip = ''
+        self.server_port = ''
+        self._quiet_post_init = False
+        self.__passed_args__ = []  # will be set by fill_from_args
+        self.__required_args__ = ['model_type', 'model_name_or_path']
+
+    def set_seed(self, seed=None):
+        if seed is None:
+            seed = self.seed
+        random.seed(seed)
+        np.random.seed(seed)
+        torch.manual_seed(seed)
+        if self.n_gpu > 0:
+            torch.cuda.manual_seed_all(seed)
+
+    def set_gradient_accumulation_steps(self):
+        """
+        when searching for full_train_batch_size in hyperparameter tuning we need to update
+        the gradient accumulation steps to stay within GPU memory constraints
+        :return:
+        """
+        if self.n_gpu * self.world_size * self.per_gpu_train_batch_size > self.full_train_batch_size:
+            self.per_gpu_train_batch_size = self.full_train_batch_size // (self.n_gpu * self.world_size)
+            self.gradient_accumulation_steps = 1
+        else:
+            self.gradient_accumulation_steps = self.full_train_batch_size // \
+                                               (self.n_gpu * self.world_size * self.per_gpu_train_batch_size)
+
+    def _basic_post_init(self):
+        # Setup CUDA, GPU
+        if self.local_rank == -1 or self.no_cuda:
+            # NOTE: changed "cuda" to "cuda:0"
+            self.device = torch.device("cuda:0" if torch.cuda.is_available() and not self.no_cuda else "cpu")
+            self.n_gpu = torch.cuda.device_count()
+        else:
+            torch.cuda.set_device(self.local_rank)
+            self.device = torch.device("cuda", self.local_rank)
+            self.n_gpu = 1
+
+        if 'per_gpu_train_batch_size' not in self.__passed_args__ and self.gradient_accumulation_steps > 0:
+            self.per_gpu_train_batch_size = self.full_train_batch_size // \
+                                            ((self.n_gpu if self.n_gpu > 0 else 1) * self.world_size * self.gradient_accumulation_steps)
+        else:
+            self.gradient_accumulation_steps = self.full_train_batch_size // \
+                                            ((self.n_gpu if self.n_gpu > 0 else 1) * self.world_size * self.per_gpu_train_batch_size)
+
+        self.stop_time = None
+        if 'TIME_LIMIT_MINS' in os.environ:
+            self.stop_time = time.time() + 60 * (int(os.environ['TIME_LIMIT_MINS']) - 5)
+
+    def _post_init(self):
+        self._basic_post_init()
+
+        self._setup_logging()
+
+        # Setup distant debugging if needed
+        if self.server_ip and self.server_port:
+            # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
+            import ptvsd
+            print("Waiting for debugger attach")
+            ptvsd.enable_attach(address=(self.server_ip, self.server_port), redirect_output=True)
+            ptvsd.wait_for_attach()
+
+        if not self._quiet_post_init:
+            logger.warning(
+                "On %s, Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
+                socket.gethostname(),
+                self.local_rank,
+                self.device,
+                self.n_gpu,
+                bool(self.local_rank != -1),
+                self.fp16,
+            )
+            logger.info(f'hypers:\n{self}')
+
+    def _setup_logging(self):
+        # force our logging style
+        for handler in logging.root.handlers[:]:
+            logging.root.removeHandler(handler)
+        if self.log_on_all_nodes:
+            grank = self.global_rank
+            class HostnameFilter(logging.Filter):
+                hostname = socket.gethostname()
+                if '.' in hostname:
+                    hostname = hostname[0:hostname.find('.')]  # the first part of the hostname
+
+                def filter(self, record):
+                    record.hostname = HostnameFilter.hostname
+                    record.global_rank = grank
+                    return True
+
+            handler = logging.StreamHandler()
+            handler.setLevel(logging.INFO)
+            handler.addFilter(HostnameFilter())
+            format = logging.Formatter('%(hostname)s[%(global_rank)d] %(filename)s:%(lineno)d - %(message)s',
+                                       datefmt='%m/%d/%Y %H:%M:%S')
+            handler.setFormatter(format)
+            logging.getLogger('').addHandler(handler)
+        else:
+            logging.basicConfig(format='%(filename)s:%(lineno)d - %(message)s',
+                                datefmt='%m/%d/%Y %H:%M:%S',
+                                level=logging.INFO)
+        if self.global_rank != 0 and not self.log_on_all_nodes:
+            try:
+                logging.getLogger().setLevel(logging.WARNING)
+            except:
+                pass
+
+    def kofn(self, kofn: str):
+        """
+        ''     -> 0, 1
+        '1of2' -> 0, 2
+        '2of2' -> 1, 2
+        :param kofn:
+        :return:
+        """
+        if not kofn:
+            return 0, 1
+        k, n = [int(i) for i in kofn.lower().split('of')]
+        assert 1 <= k <= n
+        return k-1, n
+
+    def to_dict(self):
+        d = self.__dict__.copy()
+        del d['device']
+        return d
+
+    def from_dict(self, a_dict):
+        fill_from_dict(self, a_dict)
+        self._basic_post_init()  # setup device and per_gpu_batch_size
+        return self
+
+    def __str__(self):
+        nvl = {n: (v if type(v) in [int, float, str, bool] else str(v)) for n, v in name_value_list(self)}
+        return json.dumps(nvl, indent=2)
+
+    def fill_from_args(self):
+        fill_from_args(self)
+        self._post_init()
+        return self
```

## primeqa/util/transformers_utils/model_utils.py

 * *Ordering differences only*

```diff
@@ -1,117 +1,117 @@
-import logging
-import torch
-import os
-from transformers import PreTrainedTokenizer
-from primeqa.util.transformers_utils.hypers_base import HypersBase
-
-logger = logging.getLogger(__name__)
-
-
-def save_transformer(hypers: HypersBase, model, tokenizer, *, save_dir=None):
-    if hypers.global_rank == 0:
-        if save_dir is None:
-            save_dir = hypers.output_dir
-        # Create output directory if needed
-        os.makedirs(save_dir, exist_ok=True)
-        logger.info("Saving model checkpoint to %s", save_dir)
-        # Save a trained model, configuration and tokenizer using `save_pretrained()`.
-        # They can then be reloaded using `from_pretrained()`
-        model_to_save = (
-            model.module if hasattr(model, "module") else model
-        )  # Take care of distributed/parallel training
-        torch.save(hypers, os.path.join(save_dir, "training_args.bin"))
-        model_to_save.save_pretrained(save_dir)
-        if tokenizer is not None:
-            tokenizer.save_pretrained(save_dir)
-
-
-def load_tokenizer(hypers: HypersBase, tokenizer_class, additional_special_tokens=()):
-    if len(additional_special_tokens) == 0 or len(additional_special_tokens[0]) == 0:
-        additional_special_tokens = None
-    if additional_special_tokens is not None:
-        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
-            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
-            do_lower_case=hypers.do_lower_case,
-            cache_dir=hypers.cache_dir if hypers.cache_dir else None,
-            additional_special_tokens=additional_special_tokens
-        )
-    else:
-        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
-            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
-            do_lower_case=hypers.do_lower_case,
-            cache_dir=hypers.cache_dir if hypers.cache_dir else None
-        )
-    return tokenizer
-
-
-def load_pretrained(hypers: HypersBase, config_class, model_class, tokenizer_class,
-                    additional_special_tokens=(), **extra_model_args):
-    # Load pretrained model and tokenizer
-    if hypers.local_rank not in [-1, 0]:
-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
-    config = config_class.from_pretrained(
-        hypers.config_name if hypers.config_name else hypers.model_name_or_path,
-        cache_dir=hypers.cache_dir if hypers.cache_dir else None,
-        **extra_model_args
-    )
-    if len(additional_special_tokens) == 0 or len(additional_special_tokens[0]) == 0:
-        additional_special_tokens = None
-    if additional_special_tokens is not None:
-        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
-            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
-            do_lower_case=hypers.do_lower_case,
-            cache_dir=hypers.cache_dir if hypers.cache_dir else None,
-            additional_special_tokens=additional_special_tokens
-        )
-    else:
-        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
-            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
-            do_lower_case=hypers.do_lower_case,
-            cache_dir=hypers.cache_dir if hypers.cache_dir else None
-        )
-    model = model_class.from_pretrained(
-        hypers.model_name_or_path,
-        from_tf=bool(".ckpt" in hypers.model_name_or_path),
-        config=config,
-        cache_dir=hypers.cache_dir if hypers.cache_dir else None,
-    )
-    if hypers.local_rank == 0:
-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
-    if additional_special_tokens is not None:
-        # do it when we load and again here
-        tokenizer.add_special_tokens({'additional_special_tokens': additional_special_tokens})
-        model.resize_token_embeddings(len(tokenizer))
-    model.to(hypers.device)
-    return model, tokenizer
-
-
-def save_extended_model(hypers: HypersBase, model, tokenizer: PreTrainedTokenizer, *, save_dir=None):
-    if hypers.global_rank == 0:
-        if save_dir is None:
-            save_dir = hypers.output_dir
-        # Create output directory if needed
-        os.makedirs(save_dir, exist_ok=True)
-        logger.info("Saving model to %s", save_dir)
-        # Save a trained model, configuration and tokenizer using `save_pretrained()`.
-        # They can then be reloaded using `from_pretrained()`
-        model_to_save = (
-            model.module if hasattr(model, "module") else model
-        )  # Take care of distributed/parallel training
-        torch.save(hypers, os.path.join(save_dir, "training_args.bin"))
-        torch.save(model_to_save.state_dict(), os.path.join(save_dir, "model.bin"))
-        if tokenizer is not None:
-            tokenizer.save_pretrained(save_dir)
-
-
-def load_only_extended_model(args: HypersBase, extended_model_class, saved_dir: str, *, strict=True):
-    logger.info(f'loading model from {saved_dir}')
-    if strict:
-        hypers = torch.load(os.path.join(saved_dir, "training_args.bin"), map_location='cpu')
-        hypers.device = args.device
-    else:
-        hypers = args
-    model = extended_model_class(hypers)
-    model_state_dict = torch.load(os.path.join(saved_dir, "model.bin"), map_location='cpu')
-    model.load_state_dict(model_state_dict, strict=strict)
-    model.to(args.device)
-    return model, hypers
+import logging
+import torch
+import os
+from transformers import PreTrainedTokenizer
+from primeqa.util.transformers_utils.hypers_base import HypersBase
+
+logger = logging.getLogger(__name__)
+
+
+def save_transformer(hypers: HypersBase, model, tokenizer, *, save_dir=None):
+    if hypers.global_rank == 0:
+        if save_dir is None:
+            save_dir = hypers.output_dir
+        # Create output directory if needed
+        os.makedirs(save_dir, exist_ok=True)
+        logger.info("Saving model checkpoint to %s", save_dir)
+        # Save a trained model, configuration and tokenizer using `save_pretrained()`.
+        # They can then be reloaded using `from_pretrained()`
+        model_to_save = (
+            model.module if hasattr(model, "module") else model
+        )  # Take care of distributed/parallel training
+        torch.save(hypers, os.path.join(save_dir, "training_args.bin"))
+        model_to_save.save_pretrained(save_dir)
+        if tokenizer is not None:
+            tokenizer.save_pretrained(save_dir)
+
+
+def load_tokenizer(hypers: HypersBase, tokenizer_class, additional_special_tokens=()):
+    if len(additional_special_tokens) == 0 or len(additional_special_tokens[0]) == 0:
+        additional_special_tokens = None
+    if additional_special_tokens is not None:
+        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
+            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
+            do_lower_case=hypers.do_lower_case,
+            cache_dir=hypers.cache_dir if hypers.cache_dir else None,
+            additional_special_tokens=additional_special_tokens
+        )
+    else:
+        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
+            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
+            do_lower_case=hypers.do_lower_case,
+            cache_dir=hypers.cache_dir if hypers.cache_dir else None
+        )
+    return tokenizer
+
+
+def load_pretrained(hypers: HypersBase, config_class, model_class, tokenizer_class,
+                    additional_special_tokens=(), **extra_model_args):
+    # Load pretrained model and tokenizer
+    if hypers.local_rank not in [-1, 0]:
+        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
+    config = config_class.from_pretrained(
+        hypers.config_name if hypers.config_name else hypers.model_name_or_path,
+        cache_dir=hypers.cache_dir if hypers.cache_dir else None,
+        **extra_model_args
+    )
+    if len(additional_special_tokens) == 0 or len(additional_special_tokens[0]) == 0:
+        additional_special_tokens = None
+    if additional_special_tokens is not None:
+        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
+            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
+            do_lower_case=hypers.do_lower_case,
+            cache_dir=hypers.cache_dir if hypers.cache_dir else None,
+            additional_special_tokens=additional_special_tokens
+        )
+    else:
+        tokenizer: PreTrainedTokenizer = tokenizer_class.from_pretrained(
+            hypers.tokenizer_name if hypers.tokenizer_name else hypers.model_name_or_path,
+            do_lower_case=hypers.do_lower_case,
+            cache_dir=hypers.cache_dir if hypers.cache_dir else None
+        )
+    model = model_class.from_pretrained(
+        hypers.model_name_or_path,
+        from_tf=bool(".ckpt" in hypers.model_name_or_path),
+        config=config,
+        cache_dir=hypers.cache_dir if hypers.cache_dir else None,
+    )
+    if hypers.local_rank == 0:
+        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
+    if additional_special_tokens is not None:
+        # do it when we load and again here
+        tokenizer.add_special_tokens({'additional_special_tokens': additional_special_tokens})
+        model.resize_token_embeddings(len(tokenizer))
+    model.to(hypers.device)
+    return model, tokenizer
+
+
+def save_extended_model(hypers: HypersBase, model, tokenizer: PreTrainedTokenizer, *, save_dir=None):
+    if hypers.global_rank == 0:
+        if save_dir is None:
+            save_dir = hypers.output_dir
+        # Create output directory if needed
+        os.makedirs(save_dir, exist_ok=True)
+        logger.info("Saving model to %s", save_dir)
+        # Save a trained model, configuration and tokenizer using `save_pretrained()`.
+        # They can then be reloaded using `from_pretrained()`
+        model_to_save = (
+            model.module if hasattr(model, "module") else model
+        )  # Take care of distributed/parallel training
+        torch.save(hypers, os.path.join(save_dir, "training_args.bin"))
+        torch.save(model_to_save.state_dict(), os.path.join(save_dir, "model.bin"))
+        if tokenizer is not None:
+            tokenizer.save_pretrained(save_dir)
+
+
+def load_only_extended_model(args: HypersBase, extended_model_class, saved_dir: str, *, strict=True):
+    logger.info(f'loading model from {saved_dir}')
+    if strict:
+        hypers = torch.load(os.path.join(saved_dir, "training_args.bin"), map_location='cpu')
+        hypers.device = args.device
+    else:
+        hypers = args
+    model = extended_model_class(hypers)
+    model_state_dict = torch.load(os.path.join(saved_dir, "model.bin"), map_location='cpu')
+    model.load_state_dict(model_state_dict, strict=strict)
+    model.to(args.device)
+    return model, hypers
```

## primeqa/util/transformers_utils/optimizer_utils.py

 * *Ordering differences only*

```diff
@@ -1,163 +1,163 @@
-import torch
-import os
-import logging
-try:
-    from apex import amp
-except ModuleNotFoundError:
-    pass
-from primeqa.util.transformers_utils.hypers_base import HypersBase
-from primeqa.util.reporting import Reporting
-from transformers import (
-    AdamW,
-    get_linear_schedule_with_warmup,
-)
-from primeqa.util.transformers_utils.torch_utils import reduce
-
-logger = logging.getLogger(__name__)
-
-
-class LossHistory:
-    def __init__(self, one_epoch_batch_count, *, loss_points_per_epoch=10, recency_weight=0.001):
-        self.avg_loss = 0
-        self.batch_count = 0
-        self.recency_weight = recency_weight
-        self.loss_history = []
-        self.record_loss_every = max(1, one_epoch_batch_count // loss_points_per_epoch)
-
-    def note_loss(self, loss_val, *, hypers: HypersBase = None):
-        self.batch_count += 1
-        rweight = max(self.recency_weight, 1.0 / self.batch_count)
-        self.avg_loss = (1.0 - rweight) * self.avg_loss + rweight * loss_val
-        if self.batch_count % self.record_loss_every == 0:
-            if hypers is not None and hypers.world_size > 1:
-                self.avg_loss = reduce(hypers, self.avg_loss).item() / hypers.world_size
-            self.loss_history.append(self.avg_loss)
-            logger.info(f'loss point {self.batch_count//self.record_loss_every} = {self.avg_loss}')
-            return True
-        return False
-
-
-class TransformerOptimize:
-    """
-    Collects standard steps to train transformer
-    call step_loss after computing each loss
-    """
-    def __init__(self, hypers: HypersBase, num_instances_to_train_over: int, model):
-        self.step = 0
-        self.global_step = 0
-        self.hypers = hypers
-        self.model = model
-        instances_per_step = hypers.full_train_batch_size // hypers.gradient_accumulation_steps
-        self.reporting = Reporting(recency_weight=0.0001 * instances_per_step)
-        args = self.hypers
-
-        self.t_total = num_instances_to_train_over // args.full_train_batch_size
-
-        # Prepare optimizer and schedule (linear warmup and decay)
-        no_decay = ["bias", "LayerNorm.weight"]
-        optimizer_grouped_parameters = [
-            {
-                "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
-                "weight_decay": args.weight_decay,
-            },
-            {"params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
-             "weight_decay": 0.0},
-        ]
-
-        warmup_instances = args.warmup_instances
-        if hasattr(args, 'warmup_fraction') and args.warmup_fraction > 0 and args.warmup_instances <= 0:
-            warmup_instances = args.warmup_fraction * num_instances_to_train_over
-            logger.info(f'From {args.warmup_fraction} warm up fraction, computed warm up instances = {warmup_instances}')
-        if warmup_instances < 0:
-            warmup_instances = 0
-
-        self.optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
-        self.scheduler = get_linear_schedule_with_warmup(
-            self.optimizer, num_warmup_steps=warmup_instances // args.full_train_batch_size,
-            num_training_steps=self.t_total
-        )
-
-        # Check if saved optimizer or scheduler states exist
-        if args.resume_from and os.path.isfile(os.path.join(args.resume_from, "optimizer.pt")) and \
-                os.path.isfile(os.path.join(args.resume_from, "scheduler.pt")):
-            resume_from = args.resume_from
-        elif os.path.isfile(os.path.join(args.model_name_or_path, "optimizer.pt")) and \
-                os.path.isfile(os.path.join(args.model_name_or_path, "scheduler.pt")):
-            resume_from = args.model_name_or_path
-        else:
-            resume_from = None
-        if resume_from is not None:
-            # Load in optimizer and scheduler states
-            self.optimizer.load_state_dict(torch.load(os.path.join(resume_from, "optimizer.pt"), map_location='cpu'))
-            self.scheduler.load_state_dict(torch.load(os.path.join(resume_from, "scheduler.pt"), map_location='cpu'))
-            logger.info(f'loaded optimizer and scheduler from {resume_from}')
-
-        if args.fp16:
-            self.model, optimizer = amp.initialize(self.model, self.optimizer, opt_level=args.fp16_opt_level)
-
-        # multi-gpu training (should be after apex fp16 initialization)
-        if args.n_gpu > 1:
-            # NOTE: won't work at O2, only O1
-            self.model = torch.nn.DataParallel(self.model, device_ids=list(range(args.n_gpu)))
-
-        # Distributed training (should be after apex fp16 initialization)
-        if args.local_rank != -1:
-            self.model = torch.nn.parallel.DistributedDataParallel(
-                self.model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True,
-            )
-        # set_seed(args)
-        assert args.per_gpu_train_batch_size * (args.n_gpu if args.n_gpu > 0 else 1) * \
-               args.world_size * args.gradient_accumulation_steps == args.full_train_batch_size
-        logger.info("***** Running training *****")
-        logger.info("  Instantaneous batch size per GPU = %d", args.per_gpu_train_batch_size)
-        logger.info("  Total train batch size (w. parallel, distributed & accumulation) = %d", args.full_train_batch_size)
-        logger.info("  Gradient Accumulation steps = %d", args.gradient_accumulation_steps)
-        logger.info("  Total optimization steps = %d", self.t_total)
-
-    def should_continue(self):
-        return self.global_step < self.t_total
-
-    def backward_on_loss(self, loss, **moving_averages):
-        if self.hypers.n_gpu > 1:
-            loss = loss.mean()  # mean() to average on multi-gpu parallel training
-        loss_val = loss.item()
-        if self.hypers.gradient_accumulation_steps > 1:
-            loss = loss / self.hypers.gradient_accumulation_steps
-        if self.hypers.fp16:
-            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
-                scaled_loss.backward()
-        else:
-            loss.backward()
-        self.reporting.moving_averages(loss=loss_val, **moving_averages)
-        return loss_val
-
-    def optimizer_step(self):
-        if self.global_step >= self.t_total:
-            logger.warning(f'Warning, exceeded total steps! {self.global_step} step of {self.t_total}')
-            return False
-        if (self.step + 1) % self.hypers.gradient_accumulation_steps == 0:
-            if self.hypers.max_grad_norm > 0:
-                if self.hypers.fp16:
-                    torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), self.hypers.max_grad_norm)
-                else:
-                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.hypers.max_grad_norm)
-
-            self.optimizer.step()
-            self.scheduler.step()  # Update learning rate schedule
-            self.model.zero_grad()
-            self.global_step += 1
-        self.step += 1
-
-        if self.reporting.is_time():
-            self.reporting.display()
-            inst_count = self.hypers.world_size * self.hypers.n_gpu * self.hypers.per_gpu_train_batch_size * self.reporting.check_count
-            learning_rate_scalar = self.scheduler.get_lr()[0]
-            logger.info(f'{inst_count/self.reporting.elapsed_seconds()} instances per second; {inst_count} total ({learning_rate_scalar} learn rate)')
-        return True
-
-    def step_loss(self, loss, **moving_averages):
-        loss_val = self.backward_on_loss(loss, **moving_averages)
-        if self.optimizer_step():
-            return loss_val
-        else:
-            return None
+import torch
+import os
+import logging
+try:
+    from apex import amp
+except ModuleNotFoundError:
+    pass
+from primeqa.util.transformers_utils.hypers_base import HypersBase
+from primeqa.util.reporting import Reporting
+from transformers import (
+    AdamW,
+    get_linear_schedule_with_warmup,
+)
+from primeqa.util.transformers_utils.torch_utils import reduce
+
+logger = logging.getLogger(__name__)
+
+
+class LossHistory:
+    def __init__(self, one_epoch_batch_count, *, loss_points_per_epoch=10, recency_weight=0.001):
+        self.avg_loss = 0
+        self.batch_count = 0
+        self.recency_weight = recency_weight
+        self.loss_history = []
+        self.record_loss_every = max(1, one_epoch_batch_count // loss_points_per_epoch)
+
+    def note_loss(self, loss_val, *, hypers: HypersBase = None):
+        self.batch_count += 1
+        rweight = max(self.recency_weight, 1.0 / self.batch_count)
+        self.avg_loss = (1.0 - rweight) * self.avg_loss + rweight * loss_val
+        if self.batch_count % self.record_loss_every == 0:
+            if hypers is not None and hypers.world_size > 1:
+                self.avg_loss = reduce(hypers, self.avg_loss).item() / hypers.world_size
+            self.loss_history.append(self.avg_loss)
+            logger.info(f'loss point {self.batch_count//self.record_loss_every} = {self.avg_loss}')
+            return True
+        return False
+
+
+class TransformerOptimize:
+    """
+    Collects standard steps to train transformer
+    call step_loss after computing each loss
+    """
+    def __init__(self, hypers: HypersBase, num_instances_to_train_over: int, model):
+        self.step = 0
+        self.global_step = 0
+        self.hypers = hypers
+        self.model = model
+        instances_per_step = hypers.full_train_batch_size // hypers.gradient_accumulation_steps
+        self.reporting = Reporting(recency_weight=0.0001 * instances_per_step)
+        args = self.hypers
+
+        self.t_total = num_instances_to_train_over // args.full_train_batch_size
+
+        # Prepare optimizer and schedule (linear warmup and decay)
+        no_decay = ["bias", "LayerNorm.weight"]
+        optimizer_grouped_parameters = [
+            {
+                "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
+                "weight_decay": args.weight_decay,
+            },
+            {"params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
+             "weight_decay": 0.0},
+        ]
+
+        warmup_instances = args.warmup_instances
+        if hasattr(args, 'warmup_fraction') and args.warmup_fraction > 0 and args.warmup_instances <= 0:
+            warmup_instances = args.warmup_fraction * num_instances_to_train_over
+            logger.info(f'From {args.warmup_fraction} warm up fraction, computed warm up instances = {warmup_instances}')
+        if warmup_instances < 0:
+            warmup_instances = 0
+
+        self.optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
+        self.scheduler = get_linear_schedule_with_warmup(
+            self.optimizer, num_warmup_steps=warmup_instances // args.full_train_batch_size,
+            num_training_steps=self.t_total
+        )
+
+        # Check if saved optimizer or scheduler states exist
+        if args.resume_from and os.path.isfile(os.path.join(args.resume_from, "optimizer.pt")) and \
+                os.path.isfile(os.path.join(args.resume_from, "scheduler.pt")):
+            resume_from = args.resume_from
+        elif os.path.isfile(os.path.join(args.model_name_or_path, "optimizer.pt")) and \
+                os.path.isfile(os.path.join(args.model_name_or_path, "scheduler.pt")):
+            resume_from = args.model_name_or_path
+        else:
+            resume_from = None
+        if resume_from is not None:
+            # Load in optimizer and scheduler states
+            self.optimizer.load_state_dict(torch.load(os.path.join(resume_from, "optimizer.pt"), map_location='cpu'))
+            self.scheduler.load_state_dict(torch.load(os.path.join(resume_from, "scheduler.pt"), map_location='cpu'))
+            logger.info(f'loaded optimizer and scheduler from {resume_from}')
+
+        if args.fp16:
+            self.model, optimizer = amp.initialize(self.model, self.optimizer, opt_level=args.fp16_opt_level)
+
+        # multi-gpu training (should be after apex fp16 initialization)
+        if args.n_gpu > 1:
+            # NOTE: won't work at O2, only O1
+            self.model = torch.nn.DataParallel(self.model, device_ids=list(range(args.n_gpu)))
+
+        # Distributed training (should be after apex fp16 initialization)
+        if args.local_rank != -1:
+            self.model = torch.nn.parallel.DistributedDataParallel(
+                self.model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True,
+            )
+        # set_seed(args)
+        assert args.per_gpu_train_batch_size * (args.n_gpu if args.n_gpu > 0 else 1) * \
+               args.world_size * args.gradient_accumulation_steps == args.full_train_batch_size
+        logger.info("***** Running training *****")
+        logger.info("  Instantaneous batch size per GPU = %d", args.per_gpu_train_batch_size)
+        logger.info("  Total train batch size (w. parallel, distributed & accumulation) = %d", args.full_train_batch_size)
+        logger.info("  Gradient Accumulation steps = %d", args.gradient_accumulation_steps)
+        logger.info("  Total optimization steps = %d", self.t_total)
+
+    def should_continue(self):
+        return self.global_step < self.t_total
+
+    def backward_on_loss(self, loss, **moving_averages):
+        if self.hypers.n_gpu > 1:
+            loss = loss.mean()  # mean() to average on multi-gpu parallel training
+        loss_val = loss.item()
+        if self.hypers.gradient_accumulation_steps > 1:
+            loss = loss / self.hypers.gradient_accumulation_steps
+        if self.hypers.fp16:
+            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
+                scaled_loss.backward()
+        else:
+            loss.backward()
+        self.reporting.moving_averages(loss=loss_val, **moving_averages)
+        return loss_val
+
+    def optimizer_step(self):
+        if self.global_step >= self.t_total:
+            logger.warning(f'Warning, exceeded total steps! {self.global_step} step of {self.t_total}')
+            return False
+        if (self.step + 1) % self.hypers.gradient_accumulation_steps == 0:
+            if self.hypers.max_grad_norm > 0:
+                if self.hypers.fp16:
+                    torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), self.hypers.max_grad_norm)
+                else:
+                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.hypers.max_grad_norm)
+
+            self.optimizer.step()
+            self.scheduler.step()  # Update learning rate schedule
+            self.model.zero_grad()
+            self.global_step += 1
+        self.step += 1
+
+        if self.reporting.is_time():
+            self.reporting.display()
+            inst_count = self.hypers.world_size * self.hypers.n_gpu * self.hypers.per_gpu_train_batch_size * self.reporting.check_count
+            learning_rate_scalar = self.scheduler.get_lr()[0]
+            logger.info(f'{inst_count/self.reporting.elapsed_seconds()} instances per second; {inst_count} total ({learning_rate_scalar} learn rate)')
+        return True
+
+    def step_loss(self, loss, **moving_averages):
+        loss_val = self.backward_on_loss(loss, **moving_averages)
+        if self.optimizer_step():
+            return loss_val
+        else:
+            return None
```

## primeqa/util/transformers_utils/torch_utils.py

 * *Ordering differences only*

```diff
@@ -1,41 +1,41 @@
-import logging
-import torch
-from primeqa.util.transformers_utils.hypers_base import HypersBase
-
-logger = logging.getLogger(__name__)
-
-
-def to_tensor(hypers: HypersBase, tensor):
-    if not isinstance(tensor, torch.Tensor):
-        tensor = torch.tensor(tensor, device=hypers.device)
-    else:
-        tensor = tensor.to(hypers.device).detach()
-    return tensor
-
-
-def all_gather(tensor):
-    """
-    all gather the tensor with dimensions [d0 x d1 x...], returning a tensor with dimensions [d0*world_size x d1 x...]
-    :param tensor: this process's tensor
-    :return:
-    """
-    tensor = tensor.detach()
-    gather_list = [torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())]
-    torch.distributed.all_gather(gather_list, tensor)
-    return torch.cat(gather_list, 0).detach()
-
-
-def reduce(hypers: HypersBase, tensor, *, op=torch.distributed.ReduceOp.SUM):
-    """
-    all reduce the tensor, modifying the tensor
-    :param tensor: the tensor that will be all-reduced
-    :param op: operation to reduce (example: torch.distributed.ReduceOp.SUM)
-    :param check_id: identifier for this call to all_reduce (to check that there is no cross talk)
-    :return:
-    """
-    tensor = to_tensor(hypers, tensor)
-    if hypers.world_size == 1:
-        return tensor
-    torch.distributed.all_reduce(tensor, op)
-    return tensor
-
+import logging
+import torch
+from primeqa.util.transformers_utils.hypers_base import HypersBase
+
+logger = logging.getLogger(__name__)
+
+
+def to_tensor(hypers: HypersBase, tensor):
+    if not isinstance(tensor, torch.Tensor):
+        tensor = torch.tensor(tensor, device=hypers.device)
+    else:
+        tensor = tensor.to(hypers.device).detach()
+    return tensor
+
+
+def all_gather(tensor):
+    """
+    all gather the tensor with dimensions [d0 x d1 x...], returning a tensor with dimensions [d0*world_size x d1 x...]
+    :param tensor: this process's tensor
+    :return:
+    """
+    tensor = tensor.detach()
+    gather_list = [torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())]
+    torch.distributed.all_gather(gather_list, tensor)
+    return torch.cat(gather_list, 0).detach()
+
+
+def reduce(hypers: HypersBase, tensor, *, op=torch.distributed.ReduceOp.SUM):
+    """
+    all reduce the tensor, modifying the tensor
+    :param tensor: the tensor that will be all-reduced
+    :param op: operation to reduce (example: torch.distributed.ReduceOp.SUM)
+    :param check_id: identifier for this call to all_reduce (to check that there is no cross talk)
+    :return:
+    """
+    tensor = to_tensor(hypers, tensor)
+    if hypers.world_size == 1:
+        return tensor
+    torch.distributed.all_reduce(tensor, op)
+    return tensor
+
```

## Comparing `primeqa-0.8.1.dist-info/LICENSE` & `primeqa-0.9.7.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,201 +1,201 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "{}"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright {yyyy} {name of copyright owner}
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "{}"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright {yyyy} {name of copyright owner}
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
```

## Comparing `primeqa-0.8.1.dist-info/METADATA` & `primeqa-0.9.7.dist-info/METADATA`

 * *Files 20% similar despite different names*

```diff
@@ -1,130 +1,177 @@
 Metadata-Version: 2.1
 Name: primeqa
-Version: 0.8.1
+Version: 0.9.7
 Summary: State-of-the-art Question Answering
 Home-page: https://github.com/primeqa/primeqa
-Author: Bhavani Iyer <bsiyer@us.ibm.com>, Avirup Sil <avi@us.ibm.com>, Martin Franz <franzm@us.ibm.com>, Mihaela Bornea <mabornea@us.ibm.com>, Sara Rosenthal <sjrosenthal@us.ibm.com>, Avirup Sil <avi@us.ibm.com>, Scott McCarley <jsmc@us.ibm.com>, Rong Zhang <zhangr@us.ibm.com>, Jaydeep Sen <jaydesen@in.ibm.com>, Yulong Li <yulongli@us.ibm.com>, Md. Arafat Sultan <Arafat.Sultan@ibm.com>, Vishwajeet Kumar024 <vishk024@in.ibm.com>, Saneem A Chemmengath <saneem.cg@in.ibm.com>, Anthony Ferritto
+Author: PrimeQA Team
 Author-email: primeqa@us.ibm.com
 License: Apache
-Keywords: NLP transformers QA question answering mrc rc machine reading comprehension IR information retrieval deep learning pytorch BERT RoBERTa T5 generation table
-Platform: UNKNOWN
+Keywords: Question Answering (QA),Machine Reading Comprehension (MRC),Information Retrieval (IR)
+Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: Programming Language :: Python :: 3
 Requires-Python: >=3.7.0, <3.10.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: bitarray (~=2.3.7)
 Requires-Dist: click (~=8.0.4)
-Requires-Dist: datasets[apache-beam] (~=2.0.0)
+Requires-Dist: datasets[apache-beam] (~=2.3.2)
 Requires-Dist: faiss-cpu (~=1.7.2)
-Requires-Dist: faiss-gpu (~=1.7.2)
 Requires-Dist: gitpython (~=3.1.27)
 Requires-Dist: jsonlines (~=3.0.0)
 Requires-Dist: ninja (~=1.10.2.3)
 Requires-Dist: nltk (~=3.7)
-Requires-Dist: numpy (~=1.21.5)
 Requires-Dist: packaging (~=21.3)
 Requires-Dist: pandas (~=1.3.5)
 Requires-Dist: psutil (~=5.9.0)
 Requires-Dist: pyserini (~=0.16.0)
 Requires-Dist: scikit-learn (~=1.0.2)
 Requires-Dist: signals (~=0.0.2)
 Requires-Dist: spacy (~=3.2.2)
 Requires-Dist: stanza (~=1.4.0)
 Requires-Dist: torch (~=1.11.0)
 Requires-Dist: transformers (~=4.17.0)
 Requires-Dist: sentencepiece (~=0.1.96)
-Requires-Dist: protobuf (~=3.20.0)
-Requires-Dist: tqdm (~=4.64.0)
 Requires-Dist: ujson (~=5.1.0)
+Requires-Dist: tqdm (~=4.64.0)
 Requires-Dist: frozendict
 Requires-Dist: nlp
-Requires-Dist: nltk (~=3.6)
+Requires-Dist: protobuf (~=3.20.0)
 Requires-Dist: tabulate (~=0.8.9)
 Requires-Dist: rouge-score
+Requires-Dist: grpcio (~=1.48.1)
+Requires-Dist: grpcio-tools (~=1.48.1)
+Requires-Dist: fastapi (~=0.85.0)
+Requires-Dist: uvicorn (~=0.18.0)
+Requires-Dist: cachetools (~=5.2.0)
+Requires-Dist: sqlitedict (~=2.0.0)
 Provides-Extra: all
 Requires-Dist: docutils (<0.18,>=0.14) ; extra == 'all'
 Requires-Dist: bitarray (~=2.3.7) ; extra == 'all'
 Requires-Dist: bump2version (~=1.0.1) ; extra == 'all'
 Requires-Dist: click (~=8.0.4) ; extra == 'all'
-Requires-Dist: datasets[apache-beam] (~=2.0.0) ; extra == 'all'
-Requires-Dist: myst-parser (~=0.17.2) ; extra == 'all'
+Requires-Dist: datasets[apache-beam] (~=2.3.2) ; extra == 'all'
 Requires-Dist: faiss-cpu (~=1.7.2) ; extra == 'all'
 Requires-Dist: faiss-gpu (~=1.7.2) ; extra == 'all'
 Requires-Dist: gitpython (~=3.1.27) ; extra == 'all'
 Requires-Dist: ipykernel (~=6.13.0) ; extra == 'all'
 Requires-Dist: ipywidgets (~=7.7.0) ; extra == 'all'
 Requires-Dist: jsonlines (~=3.0.0) ; extra == 'all'
 Requires-Dist: ninja (~=1.10.2.3) ; extra == 'all'
 Requires-Dist: nltk (~=3.7) ; extra == 'all'
-Requires-Dist: numpy (~=1.21.5) ; extra == 'all'
 Requires-Dist: packaging (~=21.3) ; extra == 'all'
 Requires-Dist: pandas (~=1.3.5) ; extra == 'all'
 Requires-Dist: psutil (~=5.9.0) ; extra == 'all'
-Requires-Dist: pydata-sphinx-theme (~=0.8.0) ; extra == 'all'
 Requires-Dist: pyserini (~=0.16.0) ; extra == 'all'
 Requires-Dist: pytest (~=7.1.1) ; extra == 'all'
 Requires-Dist: pytest-cov (~=3.0.0) ; extra == 'all'
 Requires-Dist: pytest-mock (~=3.7.0) ; extra == 'all'
 Requires-Dist: pytest-rerunfailures (~=10.2) ; extra == 'all'
 Requires-Dist: scikit-learn (~=1.0.2) ; extra == 'all'
 Requires-Dist: signals (~=0.0.2) ; extra == 'all'
 Requires-Dist: spacy (~=3.2.2) ; extra == 'all'
 Requires-Dist: stanza (~=1.4.0) ; extra == 'all'
-Requires-Dist: sphinx (~=4.4.0) ; extra == 'all'
 Requires-Dist: torch (~=1.11.0) ; extra == 'all'
 Requires-Dist: tox (~=3.24.5) ; extra == 'all'
 Requires-Dist: transformers (~=4.17.0) ; extra == 'all'
 Requires-Dist: sentencepiece (~=0.1.96) ; extra == 'all'
-Requires-Dist: protobuf (~=3.20.0) ; extra == 'all'
-Requires-Dist: tqdm (~=4.64.0) ; extra == 'all'
 Requires-Dist: ujson (~=5.1.0) ; extra == 'all'
+Requires-Dist: tqdm (~=4.64.0) ; extra == 'all'
 Requires-Dist: frozendict ; extra == 'all'
 Requires-Dist: nlp ; extra == 'all'
-Requires-Dist: nltk (~=3.6) ; extra == 'all'
+Requires-Dist: protobuf (~=3.20.0) ; extra == 'all'
 Requires-Dist: tabulate (~=0.8.9) ; extra == 'all'
 Requires-Dist: rouge-score ; extra == 'all'
+Requires-Dist: myst-parser (~=0.17.2) ; extra == 'all'
+Requires-Dist: pydata-sphinx-theme (~=0.9.0) ; extra == 'all'
+Requires-Dist: sphinx (~=4.4.0) ; extra == 'all'
+Requires-Dist: sphinx-design (~=0.2.0) ; extra == 'all'
+Requires-Dist: recommonmark (~=0.7.1) ; extra == 'all'
+Requires-Dist: grpcio (~=1.48.1) ; extra == 'all'
+Requires-Dist: grpcio-tools (~=1.48.1) ; extra == 'all'
+Requires-Dist: fastapi (~=0.85.0) ; extra == 'all'
+Requires-Dist: uvicorn (~=0.18.0) ; extra == 'all'
+Requires-Dist: cachetools (~=5.2.0) ; extra == 'all'
+Requires-Dist: sqlitedict (~=2.0.0) ; extra == 'all'
 Provides-Extra: dev
 Requires-Dist: bump2version (~=1.0.1) ; extra == 'dev'
 Provides-Extra: docs
 Requires-Dist: myst-parser (~=0.17.2) ; extra == 'docs'
-Requires-Dist: pydata-sphinx-theme (~=0.8.0) ; extra == 'docs'
+Requires-Dist: pydata-sphinx-theme (~=0.9.0) ; extra == 'docs'
 Requires-Dist: sphinx (~=4.4.0) ; extra == 'docs'
+Requires-Dist: sphinx-design (~=0.2.0) ; extra == 'docs'
+Requires-Dist: recommonmark (~=0.7.1) ; extra == 'docs'
+Provides-Extra: gpu
+Requires-Dist: bitarray (~=2.3.7) ; extra == 'gpu'
+Requires-Dist: click (~=8.0.4) ; extra == 'gpu'
+Requires-Dist: datasets[apache-beam] (~=2.3.2) ; extra == 'gpu'
+Requires-Dist: faiss-cpu (~=1.7.2) ; extra == 'gpu'
+Requires-Dist: faiss-gpu (~=1.7.2) ; extra == 'gpu'
+Requires-Dist: gitpython (~=3.1.27) ; extra == 'gpu'
+Requires-Dist: jsonlines (~=3.0.0) ; extra == 'gpu'
+Requires-Dist: ninja (~=1.10.2.3) ; extra == 'gpu'
+Requires-Dist: nltk (~=3.7) ; extra == 'gpu'
+Requires-Dist: packaging (~=21.3) ; extra == 'gpu'
+Requires-Dist: pandas (~=1.3.5) ; extra == 'gpu'
+Requires-Dist: psutil (~=5.9.0) ; extra == 'gpu'
+Requires-Dist: pyserini (~=0.16.0) ; extra == 'gpu'
+Requires-Dist: scikit-learn (~=1.0.2) ; extra == 'gpu'
+Requires-Dist: signals (~=0.0.2) ; extra == 'gpu'
+Requires-Dist: spacy (~=3.2.2) ; extra == 'gpu'
+Requires-Dist: stanza (~=1.4.0) ; extra == 'gpu'
+Requires-Dist: torch (~=1.11.0) ; extra == 'gpu'
+Requires-Dist: transformers (~=4.17.0) ; extra == 'gpu'
+Requires-Dist: sentencepiece (~=0.1.96) ; extra == 'gpu'
+Requires-Dist: tqdm (~=4.64.0) ; extra == 'gpu'
+Requires-Dist: frozendict ; extra == 'gpu'
+Requires-Dist: nlp ; extra == 'gpu'
+Requires-Dist: protobuf (~=3.20.0) ; extra == 'gpu'
+Requires-Dist: tabulate (~=0.8.9) ; extra == 'gpu'
+Requires-Dist: rouge-score ; extra == 'gpu'
+Requires-Dist: grpcio (~=1.48.1) ; extra == 'gpu'
+Requires-Dist: grpcio-tools (~=1.48.1) ; extra == 'gpu'
+Requires-Dist: fastapi (~=0.85.0) ; extra == 'gpu'
+Requires-Dist: uvicorn (~=0.18.0) ; extra == 'gpu'
+Requires-Dist: cachetools (~=5.2.0) ; extra == 'gpu'
+Requires-Dist: sqlitedict (~=2.0.0) ; extra == 'gpu'
 Provides-Extra: install
 Requires-Dist: bitarray (~=2.3.7) ; extra == 'install'
 Requires-Dist: click (~=8.0.4) ; extra == 'install'
-Requires-Dist: datasets[apache-beam] (~=2.0.0) ; extra == 'install'
+Requires-Dist: datasets[apache-beam] (~=2.3.2) ; extra == 'install'
 Requires-Dist: faiss-cpu (~=1.7.2) ; extra == 'install'
-Requires-Dist: faiss-gpu (~=1.7.2) ; extra == 'install'
 Requires-Dist: gitpython (~=3.1.27) ; extra == 'install'
 Requires-Dist: jsonlines (~=3.0.0) ; extra == 'install'
 Requires-Dist: ninja (~=1.10.2.3) ; extra == 'install'
 Requires-Dist: nltk (~=3.7) ; extra == 'install'
-Requires-Dist: numpy (~=1.21.5) ; extra == 'install'
 Requires-Dist: packaging (~=21.3) ; extra == 'install'
 Requires-Dist: pandas (~=1.3.5) ; extra == 'install'
 Requires-Dist: psutil (~=5.9.0) ; extra == 'install'
 Requires-Dist: pyserini (~=0.16.0) ; extra == 'install'
 Requires-Dist: scikit-learn (~=1.0.2) ; extra == 'install'
 Requires-Dist: signals (~=0.0.2) ; extra == 'install'
 Requires-Dist: spacy (~=3.2.2) ; extra == 'install'
 Requires-Dist: stanza (~=1.4.0) ; extra == 'install'
 Requires-Dist: torch (~=1.11.0) ; extra == 'install'
 Requires-Dist: transformers (~=4.17.0) ; extra == 'install'
 Requires-Dist: sentencepiece (~=0.1.96) ; extra == 'install'
-Requires-Dist: protobuf (~=3.20.0) ; extra == 'install'
-Requires-Dist: tqdm (~=4.64.0) ; extra == 'install'
 Requires-Dist: ujson (~=5.1.0) ; extra == 'install'
+Requires-Dist: tqdm (~=4.64.0) ; extra == 'install'
 Requires-Dist: frozendict ; extra == 'install'
 Requires-Dist: nlp ; extra == 'install'
-Requires-Dist: nltk (~=3.6) ; extra == 'install'
+Requires-Dist: protobuf (~=3.20.0) ; extra == 'install'
 Requires-Dist: tabulate (~=0.8.9) ; extra == 'install'
 Requires-Dist: rouge-score ; extra == 'install'
+Requires-Dist: grpcio (~=1.48.1) ; extra == 'install'
+Requires-Dist: grpcio-tools (~=1.48.1) ; extra == 'install'
+Requires-Dist: fastapi (~=0.85.0) ; extra == 'install'
+Requires-Dist: uvicorn (~=0.18.0) ; extra == 'install'
+Requires-Dist: cachetools (~=5.2.0) ; extra == 'install'
+Requires-Dist: sqlitedict (~=2.0.0) ; extra == 'install'
 Provides-Extra: notebooks
 Requires-Dist: ipykernel (~=6.13.0) ; extra == 'notebooks'
 Requires-Dist: ipywidgets (~=7.7.0) ; extra == 'notebooks'
 Provides-Extra: tests
 Requires-Dist: docutils (<0.18,>=0.14) ; extra == 'tests'
 Requires-Dist: pytest (~=7.1.1) ; extra == 'tests'
 Requires-Dist: pytest-cov (~=3.0.0) ; extra == 'tests'
@@ -144,41 +191,44 @@
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 -->
 
-# PrimeQA
 <h3 align="center">
+    <img width="350" alt="primeqa" src="docs/_static/img/PrimeQA.png">
     <p>The prime repository for state-of-the-art Multilingual and Multimedia Question Answering research and development.</p>
 </h3>
 
 ![Build Status](https://github.com/primeqa/primeqa/actions/workflows/primeqa-ci.yml/badge.svg)
 [![LICENSE|Apache2.0](https://img.shields.io/github/license/saltstack/salt?color=blue)](https://www.apache.org/licenses/LICENSE-2.0.txt)
+[![sphinx-doc-build](https://github.com/primeqa/primeqa/actions/workflows/sphinx-doc-build.yml/badge.svg)](https://github.com/primeqa/primeqa/actions/workflows/sphinx-doc-build.yml)   
 
 PrimeQA is a public open source repository that enables researchers and developers to train state-of-the-art models for question answering (QA). By using PrimeQA, a researcher can replicate the experiments outlined in a paper published in the latest NLP conference while also enjoying the capability to download pre-trained models (from an online repository) and run them on their own custom data. PrimeQA is built on top of the [Transformers](https://github.com/huggingface/transformers) toolkit and uses [datasets](https://huggingface.co/datasets/viewer/) and [models](https://huggingface.co/PrimeQA) that are directly downloadable.
 
 
 The models within PrimeQA supports End-to-end Question Answering. PrimeQA answers questions via 
 - [Information Retrieval](https://github.com/primeqa/primeqa/tree/main/primeqa/ir): Retrieving documents and passages using both traditional (e.g. BM25) and neural (e.g. ColBERT) models
 - [Multilingual Machine Reading Comprehension](https://huggingface.co/ibm/tydiqa-primary-task-xlm-roberta-large): Extract and/ or generate answers given the source document or passage.
 - [Multilingual Question Generation](https://huggingface.co/PrimeQA/mt5-base-tydi-question-generator): Supports generation of questions for effective domain adaptation over [tables](https://huggingface.co/PrimeQA/t5-base-table-question-generator) and [multilingual text](https://huggingface.co/PrimeQA/mt5-base-tydi-question-generator).
 
 Some examples of models (applicable on benchmark datasets) supported are :
 - [Traditional IR with BM25](https://github.com/primeqa/primeqa/tree/main/primeqa/ir/) Pyserini
-- [Neural IR with ColBERT](https://github.com/primeqa/primeqa/tree/main/primeqa/ir), DPR (coming soon): to replicate the experiments that [Dr. Decr](https://huggingface.co/ibm/DrDecr_XOR-TyDi_whitebox) (Li et. al, 2022) performed to reach the top of the XOR TyDI leaderboard. Collaboration with [Stanford NLP](https://nlp.stanford.edu/) IR led by [Chris Potts](https://web.stanford.edu/~cgpotts/) & [Matei Zaharia](https://cs.stanford.edu/~matei/).
+- [Neural IR with ColBERT, DPR](https://github.com/primeqa/primeqa/tree/main/primeqa/ir) (collaboration with [Stanford NLP](https://nlp.stanford.edu/) IR led by [Chris Potts](https://web.stanford.edu/~cgpotts/) & [Matei Zaharia](https://cs.stanford.edu/~matei/)).
+Replicating the experiments that [Dr. Decr](https://huggingface.co/ibm/DrDecr_XOR-TyDi_whitebox) (Li et. al, 2022) performed to reach the top of the XOR TyDI leaderboard.
 - [Machine Reading Comprehension with XLM-R](https://github.com/primeqa/primeqa/tree/main/primeqa/mrc): to replicate the experiments to get to the top of the TyDI leaderboard similar to the performance of the IBM GAAMA system. Coming soon: code to replicate GAAMA's performance on Natural Questions. 
 - [Multimedia QA over news & movies](https://arxiv.org/abs/2112.10728): coming soon! to replicate the experiments run over multi-hop QA over images, text over variety of domains. Collaboration with [UIUC Blender lab](https://blender.cs.illinois.edu/).
 
 
 
-## Getting Started
+## ✔️ Getting Started
 
-## Installation
+### Installation
+[Installation doc](https://primeqa.github.io/primeqa/installation.html)       
 
 ```shell
 # cd to project root
 
 # If you want to run on GPU make sure to install torch appropriately
 
 # E.g. for torch 1.11 + CUDA 11.3:
@@ -186,51 +236,93 @@
 
 # Install as editable (-e) or non-editable using pip, with extras (e.g. tests) as desired
 # Example installation commands:
 
 # Minimal install (non-editable)
 pip install .
 
+# GPU support
+pip install .[gpu]
+
 # Full install (editable)
 pip install -e .[all]
 ```
 
 Please note that dependencies (specified in [setup.py](./setup.py)) are pinned to provide a stable experience.
 When installing from source these can be modified, however this is not officially supported.
 
-## JAVA requirements
+**Note:** in many environments, conda-forge based faiss libraries perform substantially better than the default ones installed with pip. To install faiss libraries from conda-forge, use the following steps:
+
+- Create and activate a conda environment
+- Install faiss libraries, using a command
+
+```conda install -c conda-forge faiss=1.7.0 faiss-gpu=1.7.0```
+
+- In `setup.py`, remove the faiss-related lines:
+
+```commandline
+"faiss-cpu~=1.7.2": ["install", "gpu"],
+"faiss-gpu~=1.7.2": ["gpu"],
+```
+
+- Continue with the `pip install` commands as desctibed above.
+
+
+### JAVA requirements
 Java 11 is required for BM25 retrieval. 
 
 Download Java 11 package from https://jdk.java.net/archive/ and uncompress
 
 Set JAVA_HOME:
 ```shell
 export JAVA_HOME=<jdk-dir>
 export PATH=$JAVA_HOME/bin:$PATH
 ```
 
-## Learn more
-
-| Section | Description |
-|-|-|
-| [Documentation](https://github.com/primeqa/primeqa/) | TODO: Full API documentation and tutorials |
-| [Quick tour: Entry Points for PrimeQA](https://github.com/primeqa/primeqa/tree/main/primeqa) | Different entry points for PrimeQA: Information Retrieval, Reading Comprehension, TableQA and Question Generation |
-| [Tutorials: Jupyter Notebooks](https://github.com/primeqa/primeqa/tree/main/notebooks) | Notebooks to get started on QA tasks |
-| [Examples: Applying PrimeQA on various QA tasks](https://github.com/primeqa/primeqa/tree/main/examples) | Example scripts for fine-tuning PrimeQA models on a range of QA tasks |
-| [Model sharing and uploading](https://huggingface.co/docs/transformers/model_sharing) | Upload and share your fine-tuned models with the community |
-
-
-## Unit Tests
+## 🧪 Unit Tests
+[Testing doc](https://primeqa.github.io/primeqa/testing.html)       
 
 To run the unit tests you first need to [install PrimeQA](#Installation).
 Make sure to install with the `[tests]` or `[all]` extras from pip.
 
 From there you can run the tests via pytest, for example:
 ```shell
 pytest --cov PrimeQA --cov-config .coveragerc tests/
 ```
 
 For more information, see:
 - Our [tox.ini](./tox.ini)
-- The [pytest](https://docs.pytest.org) and [tox](https://tox.wiki/en/latest/) documentation
+- The [pytest](https://docs.pytest.org) and [tox](https://tox.wiki/en/latest/) documentation    
+
 
+## 🔭 Learn more
 
+| Section | Description |
+|-|-|
+| 📒 [Documentation](https://primeqa.github.io/primeqa) | Full API documentation and tutorials |
+| 🏁 [Quick tour: Entry Points for PrimeQA](https://github.com/primeqa/primeqa/tree/main/primeqa) | Different entry points for PrimeQA: Information Retrieval, Reading Comprehension, TableQA and Question Generation |
+| 📓 [Tutorials: Jupyter Notebooks](https://github.com/primeqa/primeqa/tree/main/notebooks) | Notebooks to get started on QA tasks |
+| 💻 [Examples: Applying PrimeQA on various QA tasks](https://github.com/primeqa/primeqa/tree/main/examples) | Example scripts for fine-tuning PrimeQA models on a range of QA tasks |
+| 🤗 [Model sharing and uploading](https://huggingface.co/docs/transformers/model_sharing) | Upload and share your fine-tuned models with the community |
+| ✅ [Pull Request](https://primeqa.github.io/primeqa/pull_request_template.html) | PrimeQA Pull Request |
+| 📄 [Generate Documentation](https://primeqa.github.io/primeqa/README.html) | How Documentation works |        
+| 🛠 [Orchestrator Service REST Microservice](https://primeqa.github.io/primeqa/orchestrator.html) | Proof-of-concept code for PrimeQA Orchestrator microservice |        
+| 📖 [Tooling UI](https://primeqa.github.io/primeqa/tooling_ui.html) | Demo UI |        
+
+## ❤️ PrimeQA collaborators include       
+
+| | | | |
+|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|
+|<img width="75" alt="stanford" src="docs/_static/img/collab-stanford-circle.png">| Stanford NLP |<img width="75" alt="i" src="docs/_static/img/collab-i-circle.png">| University of Illinois |
+|<img width="75" alt="stuttgart" src="docs/_static/img/collab-stuttgart-circle.png">| University of Stuttgart | <img width="75" alt="notredame" src="docs/_static/img/collab-notredame-circle.png">| University of Notre Dame |
+|<img width="75" alt="ohio" src="docs/_static/img/collab-ohio-circle.png">| Ohio State University |<img width="75" alt="carnegie" src="docs/_static/img/collab-carnegie-circle.png">| Carnegie Mellon University |
+|<img width="75" alt="massachusetts" src="docs/_static/img/collab-massachusetts-circle.png">| University of Massachusetts |
+| | | | |
+
+
+<br>
+<br>
+<br>
+<br>
+<div align="center">
+    <img width="30" alt="primeqa" src="docs/_static/primeqa_logo.png">
+</div>
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `primeqa-0.8.1.dist-info/RECORD` & `primeqa-0.9.7.dist-info/RECORD`

 * *Files 21% similar despite different names*

```diff
@@ -1,236 +1,288 @@
-primeqa/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/__init__.py,sha256=FOACVZh96Gi8lV0ZG065-OVYvkfy1XpqqkJ9Tx2e33o,349
 primeqa/boolqa/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/boolqa/boolqa.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/boolqa/run_boolqa_classifier.py,sha256=jqV-VR3uvLpynLd-Lxw8jTyWjUHaWQuZYU-z3BOBkKc,14204
-primeqa/boolqa/run_score_normalizer.py,sha256=P64QnUsexuWOms-aFT_Gq2EVyvFa2YgUSAQAJT5xTdI,1651
+primeqa/boolqa/run_boolqa_classifier.py,sha256=tMTv1i0F4JnKgkrrtbMdUjetBa377uJwuaGxxTf8hVI,13846
+primeqa/boolqa/run_score_normalizer.py,sha256=bsZJXBWjGqkf0rktC3fJ35owBCwKE9Nyz7ckPvST7_A,1614
 primeqa/boolqa/processors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/boolqa/processors/dataset/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/boolqa/processors/dataset/mrc2dataset.py,sha256=JUUrSPrcl95sP3CdWelWOwhR2AKnqDccwrZgU3d7UMo,2725
+primeqa/boolqa/processors/dataset/mrc2dataset.py,sha256=XLIiT0YwUOOTglsDPfiPiHa0FMwFuWFlxaFC42LPdMU,2654
 primeqa/boolqa/processors/postprocessors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/boolqa/processors/postprocessors/boolqa_classifier.py,sha256=SjB0xOF32HVZhh-0KolYWsW2J61Hc5LUHmkkjmSWxcU,4150
-primeqa/boolqa/processors/postprocessors/extractive.py,sha256=xPqJ1HQiLnEo_1x9hw8jBqZg63tVRZipYD4-vIGoIVc,2287
+primeqa/boolqa/processors/postprocessors/boolqa_classifier.py,sha256=PhGKvbHOzkpMW4XK3LQasVcJCiZevEFG2CLfIXCYgIA,4045
+primeqa/boolqa/processors/postprocessors/extractive.py,sha256=JfvU1VN7LhIHcbM6iC1JIv05HNQgKqH63equ8iWalcU,2231
 primeqa/boolqa/processors/preprocessors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/boolqa/processors/preprocessors/boolqa_classifier.py,sha256=ZRnxI_3jVATgZ8BICcoprPKBXJHCIzh1dwH3_5NpEAM,4559
+primeqa/boolqa/processors/preprocessors/boolqa_classifier.py,sha256=AKZeadwgquNQ__uSEMkU1ndz2yZbJKsnjFGBLvbAS8w,4445
 primeqa/boolqa/score_normalizer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/boolqa/score_normalizer/score_normalizer.py,sha256=xUWUrfNG8i0fh84WIY74sJIHxzz5lH71QTem02ylbs4,3586
+primeqa/boolqa/score_normalizer/score_normalizer.py,sha256=F-V1rb5g-z_-TFEuPRU6c267NJvOBJWuHKsMPKVwx8Y,3497
 primeqa/calibration/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/calibration/calibration.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/calibration/confidence_scorer.py,sha256=pdNeybU5V947UpwkDXHXgFJZzxx94QPxEcxjgu8x9ZA,11179
-primeqa/calibration/train_confidence_calibrator.py,sha256=s3PR3PpxN86U7Baa5DUQ_qtR3o50505ZeguntlemwS0,24873
+primeqa/calibration/confidence_scorer.py,sha256=dWtxpNxLfVVsa2GmKv_5GYAbqN7ABhqewC08pfYk3Lk,10918
+primeqa/calibration/train_confidence_calibrator.py,sha256=jhcweir1Lsmgl-8BC9nnbEh7yPil5aewwzCTTNRrbMs,24341
 primeqa/distillation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/distillation/distillation.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/ir/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/ir.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/run_bm25_retrieval.py,sha256=SC77qXaKUA_LEjcKp0H6MRJKJ5Et-NfyYVJCWQrJv5I,4340
-primeqa/ir/run_ir.py,sha256=wiNEZmwzW8pX7V5lxTsmmrrqrd-9wn4TZu3FjlQcO1o,6844
-primeqa/ir/dense/__init__.py,sha256=frcCV1k9oG9oKj3dpUqdJg1PxRT2RSN_XKdLCPjaYaY,2
+primeqa/ir/run_bm25_retrieval.py,sha256=Dt3LFDqc6rt7C8Y2HTzJQhr6BsXUlgtXHKFdHUkCwsc,4234
+primeqa/ir/run_ir.py,sha256=AyF31cwX4ETzdYwY4jvijN6pV3YegqAIzqRyUXTKHwA,7763
+primeqa/ir/dense/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 primeqa/ir/dense/colbert_top/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/setup.py,sha256=aicF0gU2gTn1oWAku_F7tSgsXgGuDyhAuTxXiX9sX38,555
-primeqa/ir/dense/colbert_top/colbert/__init__.py,sha256=Lv_89xL56U7v0Va2PrD9ALkcuge9zDfr-naCyb8f3Qw,137
-primeqa/ir/dense/colbert_top/colbert/index.py,sha256=X12O9yTGhHTUMda8SNMXbg9RcKMbwHn_qGu8jTiQ7a8,303
-primeqa/ir/dense/colbert_top/colbert/indexer.py,sha256=iU5K1xgAkkKvkji6hfSxaZKctj0K-SFh8SN15nAW2I0,3457
-primeqa/ir/dense/colbert_top/colbert/parameters.py,sha256=68JCGrVBTD8MrxqQOrDO4JvOvUhmMQCj1IJjQgI9A4A,571
-primeqa/ir/dense/colbert_top/colbert/run_indexer.py,sha256=4sWLsNsIvS6-mATfbZfZEXwQ2Z09hpaWk2DzJN8QBX0,1407
-primeqa/ir/dense/colbert_top/colbert/run_searcher.py,sha256=u_GDZ1sOxiRsdlx63xEqMiLfyKES1CTHWmGDi1RhyGI,1783
-primeqa/ir/dense/colbert_top/colbert/run_trainer.py,sha256=gzHGX5mcJFA_brT9lHwVuxb1XP9yu9st2-KnWOzXuJo,1899
-primeqa/ir/dense/colbert_top/colbert/searcher.py,sha256=M4Ritb3ZOW53-_KQU_wC0zrdgJ-lIdR_KEGLRso54k8,4285
-primeqa/ir/dense/colbert_top/colbert/trainer.py,sha256=9IuwWz64qVUulf4mAkd-v0E9xxQelcujr6t2xieIX1c,1409
-primeqa/ir/dense/colbert_top/colbert/data/__init__.py,sha256=NW6aA-nZI6tOkuS4CGOXzrsTOfKgj2NOj-snLtuhbwM,102
-primeqa/ir/dense/colbert_top/colbert/data/collection.py,sha256=fgrGu4taJqEbVME9TYg5nlSQSiJT3zQYB1EkCBsmb_4,3386
-primeqa/ir/dense/colbert_top/colbert/data/dataset.py,sha256=opj6-MCzHICwY3S9GT96_h2MtZ6eBW0tcVBnP5ER4fk,435
-primeqa/ir/dense/colbert_top/colbert/data/examples.py,sha256=UokVyZM89pt_5MEcb69AOZA8HovxHWWwN5SnHrgqU7Q,2914
-primeqa/ir/dense/colbert_top/colbert/data/queries.py,sha256=lDbVx0vxnd0_o1KWn908A4Z9_PZqIERDeQ3cANXS5dE,4614
-primeqa/ir/dense/colbert_top/colbert/data/ranking.py,sha256=nCJ4Uc5tPexNQDGVQQ64EKjKwMda52ZzEM8oAol2n0E,2984
+primeqa/ir/dense/colbert_top/setup.py,sha256=sOPoOtwMWOS95Dvl7e2YwQaF_EyklBKQFp8ZSnnPYSE,538
+primeqa/ir/dense/colbert_top/colbert/__init__.py,sha256=BlClkow70F-e1esTS99QV4JqDvLzl__x48UPDDghcow,133
+primeqa/ir/dense/colbert_top/colbert/index.py,sha256=DxPcp-iU2bn5aJV5fRxCmUU3rTekgpr81kExFNhGCKI,286
+primeqa/ir/dense/colbert_top/colbert/indexer.py,sha256=xFXmFhzYCdb0Du6zbj45OAKljZWPq_uP4PW-OC5-n9Y,3356
+primeqa/ir/dense/colbert_top/colbert/parameters.py,sha256=bq_7S8gOghqHVE1HbXaP1_rQWyyM1bNkdYfHv6K6E3E,554
+primeqa/ir/dense/colbert_top/colbert/run_indexer.py,sha256=9_oTpJaYwzWuNWiYFeDpJ8D5txtaRLXUAKy99Kft8J4,1374
+primeqa/ir/dense/colbert_top/colbert/run_searcher.py,sha256=K3ufyeKiSE61NDF0y5-x2vLspbroRMgZNThnSMT4vpw,1518
+primeqa/ir/dense/colbert_top/colbert/run_trainer.py,sha256=KxIduYA-SuxyFn6w7DiR5Jm3INYcqcaQTd9nc1nusZQ,1860
+primeqa/ir/dense/colbert_top/colbert/searcher.py,sha256=KOjDGPgcrlQC763ElWukUM-RYWXbLWP1ph_wyg8uQ6M,4504
+primeqa/ir/dense/colbert_top/colbert/trainer.py,sha256=Zuedr6W4-0HNUQrXOtl9YgCl4k2xS32YO1P0AMt5EcU,1373
+primeqa/ir/dense/colbert_top/colbert/data/__init__.py,sha256=2-792uEzX8CA_6t1BDCAVGjIjBSyHFweIItbUfs4VKw,97
+primeqa/ir/dense/colbert_top/colbert/data/collection.py,sha256=87FQtWixbM2BV5RcJS4A3WBsucyE2hr8StOoGikimaw,3286
+primeqa/ir/dense/colbert_top/colbert/data/dataset.py,sha256=zxeN8dmjM2l5xhJq6sF-SY-H8r3wTlswbDH92W5RMp0,421
+primeqa/ir/dense/colbert_top/colbert/data/examples.py,sha256=_oUsQ4gfFrN1MgP8x6yUKQ2qNeTH21nd4qs2RnPxToc,2832
+primeqa/ir/dense/colbert_top/colbert/data/queries.py,sha256=6YyHXvR9zA7Gc9P8n78VogC9_3aOpVLP6cHwofCoA5U,4451
+primeqa/ir/dense/colbert_top/colbert/data/ranking.py,sha256=PklaoCOWrZErF4z4ROo0Bhl4gK1heh9uHlc9LtZLMck,2960
 primeqa/ir/dense/colbert_top/colbert/distillation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/distillation/ranking_scorer.py,sha256=YNqLJeL0utnT51jR2hmje8YWinIHj5Eeq2S6XjuUVZs,1843
-primeqa/ir/dense/colbert_top/colbert/distillation/scorer.py,sha256=pazkki214ATVX_wpLLmkYARj1ixwq0x3JGIRMW4khVg,2562
+primeqa/ir/dense/colbert_top/colbert/distillation/ranking_scorer.py,sha256=EUl_AkoDlezpq_oOqhzQRNaic9NJF0dlfF-8_RYHw1I,1791
+primeqa/ir/dense/colbert_top/colbert/distillation/scorer.py,sha256=STrc9HYcSSDasH4wX0B3nVu-Y5GPBqSuMpSsoFiwiD4,2494
 primeqa/ir/dense/colbert_top/colbert/evaluation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/evaluation/load_model.py,sha256=tZncEJC5ke2XmghIuP18x3gg2srgvp5OkZPuIYc5vCM,1034
-primeqa/ir/dense/colbert_top/colbert/evaluation/loaders.py,sha256=sLlSdu4yWSQjecxtrenaMaVaAPOh8IyCSQ-OdTwPoFE,7260
-primeqa/ir/dense/colbert_top/colbert/evaluation/metrics.py,sha256=I4zJTuGxCX7I9_SNdgdd3YbGXJe5TimhtrfwIlrcQaA,4447
+primeqa/ir/dense/colbert_top/colbert/evaluation/load_model.py,sha256=B6SWUVVI7ugnMKBoOjk2u-5f0B8Fn5kidBgo-1GvvyE,1006
+primeqa/ir/dense/colbert_top/colbert/evaluation/loaders.py,sha256=iGDJMS0YWOQUXCQ1reajfVIVPaJdlHjkKt-fndhrfrA,7051
+primeqa/ir/dense/colbert_top/colbert/evaluation/metrics.py,sha256=sN7t6yh4bSCA6Hx9h4mNHPDFUpO1npBuI-eheG896w4,4333
 primeqa/ir/dense/colbert_top/colbert/indexing/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/indexing/collection_encoder.py,sha256=h4yr3cHfzOlb4LJZC8ATHkW1Xtna-BscKYV1M3MeyLc,1931
-primeqa/ir/dense/colbert_top/colbert/indexing/collection_indexer.py,sha256=Y-q9MHjf8GKd9q-u-btHWl-t3wx2qoIDOtW5_l3jkQE,18291
-primeqa/ir/dense/colbert_top/colbert/indexing/index_manager.py,sha256=FGR3DmsvUYp2Vnb2F8HMDZ5oMFf1WmdpkYlm3JMZ0eY,916
-primeqa/ir/dense/colbert_top/colbert/indexing/index_saver.py,sha256=CPqjduq1JRGYemEWncF8PrpER9HJzzP5a5Gzri1nhig,2015
-primeqa/ir/dense/colbert_top/colbert/indexing/loaders.py,sha256=IAG06EVKluhUQy1tzU3UPknZEMPFUnpXEZxl9AhsDSc,2061
-primeqa/ir/dense/colbert_top/colbert/indexing/utils.py,sha256=gKYPecWYZ45ejemgJft3FrTJ1DRFZR1Ro3ygi4i8rfs,1875
+primeqa/ir/dense/colbert_top/colbert/indexing/collection_encoder.py,sha256=kwjYHT9CASrk7R-JTVuRFpqrsKcPaoI8hWx_LDP6hYU,1886
+primeqa/ir/dense/colbert_top/colbert/indexing/collection_indexer.py,sha256=EjWDAnKURMylUqSV81dLWcuTeWl7EBcfE5ibbZ7dmGk,17860
+primeqa/ir/dense/colbert_top/colbert/indexing/index_manager.py,sha256=3XehsNZHtnOFYXERpj5Wbu1vtbj_xuQD_Vu70Y8KPww,878
+primeqa/ir/dense/colbert_top/colbert/indexing/index_saver.py,sha256=Ggy45megTc4xPKKF98qCe4s9d8sVRENqTPoG-sC4Hz0,1954
+primeqa/ir/dense/colbert_top/colbert/indexing/loaders.py,sha256=KSCuMOG3_tqsqaZT9mKSAhJrnawpaiBCcuTq-SaPnk8,1995
+primeqa/ir/dense/colbert_top/colbert/indexing/utils.py,sha256=oFW84NBZBi6FIGjQ1Zk-g2ydmCcZfLAHFrK3mc455LM,1822
 primeqa/ir/dense/colbert_top/colbert/indexing/codecs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual.py,sha256=ueksA8K36-fD4HwlTXZdCrDzggnz5wgI08ZEoZA_NeE,11631
-primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings.py,sha256=BIrzTeJUQVizpkLTm0jlcq25_YpVawgKggxhS4NEUn8,3200
-primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings_strided.py,sha256=uZXbJR3EmwwsntE1B5koOvr-_iOyVNN7gLJhzhcbcO0,1407
-primeqa/ir/dense/colbert_top/colbert/infra/__init__.py,sha256=PMkO47hxi5yT3T26_7VHc44DwllN7kpPDAo7toN0KGs,41
-primeqa/ir/dense/colbert_top/colbert/infra/launcher.py,sha256=R5SpsbCObp_zYkhh8ILR2HGmDcjdbJ4-s-VoZAjj3_M,4914
-primeqa/ir/dense/colbert_top/colbert/infra/provenance.py,sha256=eVpzeCecQOj3gDYstVY-P3xVwyoeSFDvp9cKDiEyvbE,974
-primeqa/ir/dense/colbert_top/colbert/infra/run.py,sha256=2aRRXnSQje1SyYCrajUPOzMywZGFKQLbfZZd_5QlbCA,2691
-primeqa/ir/dense/colbert_top/colbert/infra/config/__init__.py,sha256=2nIbXQJaN4ilmlG_goj7zCOJSwRoc6bo3GiqOHPple0,46
-primeqa/ir/dense/colbert_top/colbert/infra/config/base_config.py,sha256=ZnVOQtPR3rBbs5aM9NOf_01nu9O0NQz8oER6Y9U4Xxc,4456
-primeqa/ir/dense/colbert_top/colbert/infra/config/config.py,sha256=xgifpvKqS94gvjNo0DmkyYb99Z7vz0VpWw2sud4cqVw,360
-primeqa/ir/dense/colbert_top/colbert/infra/config/core_config.py,sha256=tY4E77yQAIg4jWAZvxasVkvCOE5YrnazFjG8_eijP3o,2425
-primeqa/ir/dense/colbert_top/colbert/infra/config/settings.py,sha256=EzOFYryr3AdW1wfMwVh5Y9eFKqQEj3hWhjfDPRRfjo4,5453
+primeqa/ir/dense/colbert_top/colbert/indexing/codecs/decompress_residuals.cpp,sha256=6vtkbee92iXQ4GqNCB46oPrteznawVsBjTnD_nrkb8A,982
+primeqa/ir/dense/colbert_top/colbert/indexing/codecs/decompress_residuals.cu,sha256=OkKGHnOlNfknvkIHy1cL4g-mp8PxH1rTR_zBFuvmN3s,2982
+primeqa/ir/dense/colbert_top/colbert/indexing/codecs/packbits.cpp,sha256=EbMJWkrYYIC0EuxbXWYUI6Rhr12CD3AlynPM-ZZDp8s,284
+primeqa/ir/dense/colbert_top/colbert/indexing/codecs/packbits.cu,sha256=Bo_FWeRDTVZG_txIEPF_1AAT39MCZfgTJB4wp-t5Vbo,1558
+primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual.py,sha256=GZCJ3moDLbpNh8Zw1iA2IK7WZ9oQUT9M3v7rqnmrfHc,11332
+primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings.py,sha256=og1-5psIF6ghzF-0cLBk0AjaMpOHxrUja0GdHyh1seM,3109
+primeqa/ir/dense/colbert_top/colbert/indexing/codecs/residual_embeddings_strided.py,sha256=QON2MvR_bPWUWiuXEDR54fgKfbnygRV4pOudV-TaPck,1377
+primeqa/ir/dense/colbert_top/colbert/infra/__init__.py,sha256=03wROCw7AvXsDmSE8gVYmCm1TJfpyNu4YuBaa2lL9Zo,40
+primeqa/ir/dense/colbert_top/colbert/infra/launcher.py,sha256=cAgjezd4ev0yVjWxuVwLMqzohKAo-MgyW8Zdv8tUD4w,4767
+primeqa/ir/dense/colbert_top/colbert/infra/provenance.py,sha256=FRviCzXX7hszsPC6BRTrPXGg2l_Id1N8ojnJ6gbDlz8,932
+primeqa/ir/dense/colbert_top/colbert/infra/run.py,sha256=cP9cFgEE1kku8zpTk2G2k9TwApo1GD2Vy3h2CdfPLPg,2600
+primeqa/ir/dense/colbert_top/colbert/infra/config/__init__.py,sha256=hOWxRQM3sY7kgVwtL5dKRPnBvePtfakIklXOdq1NP7s,45
+primeqa/ir/dense/colbert_top/colbert/infra/config/base_config.py,sha256=sxNh1D0GX65cmNqZ94zwhoY0PckFYR_x5-qGQnHAeX4,4334
+primeqa/ir/dense/colbert_top/colbert/infra/config/config.py,sha256=8R8ubMzyeiAsTSD8TD82JDm_ouoStcT9s61Wu4fdQS4,345
+primeqa/ir/dense/colbert_top/colbert/infra/config/core_config.py,sha256=DX-CfRBTnE_TR_DpwzdcSW8A8Xr-CpGoq23KbkHRZXQ,2339
+primeqa/ir/dense/colbert_top/colbert/infra/config/settings.py,sha256=efJpGUKIV9p-SjThByp0jp7YxMbYeNuF9FWBocu_VcU,5430
 primeqa/ir/dense/colbert_top/colbert/infra/utilities/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/infra/utilities/create_triples.py,sha256=4nER_EHU-BbUjU_FTKy_Z7RN5TRsgOVw85guIDHv0tg,2050
-primeqa/ir/dense/colbert_top/colbert/infra/utilities/minicorpus.py,sha256=_oD2xVc0M0Rwqtu5DKXKhiwKP3DKRZFcGPsFmplwiis,2525
+primeqa/ir/dense/colbert_top/colbert/infra/utilities/create_triples.py,sha256=_vftx8LNg7Ta-I0AyuHoCp0a6dHZm_wnKvooQDJ504A,1998
+primeqa/ir/dense/colbert_top/colbert/infra/utilities/minicorpus.py,sha256=VM_VZr6FiemKhReAGVJbJqtdtg50PIoY37h_rd0rEeo,2461
 primeqa/ir/dense/colbert_top/colbert/modeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/modeling/base_colbert.py,sha256=uxgL7ygz4SYrDjVviDHeTw26ZIq7FgP7nAALwGZ_yNc,2276
-primeqa/ir/dense/colbert_top/colbert/modeling/checkpoint.py,sha256=OJ4awA4UJpPkm-5gkiK-Wcoo2GU7lpvFis6PZzXixXU,6495
-primeqa/ir/dense/colbert_top/colbert/modeling/colbert.py,sha256=0u0KolOt__SSL1TpxypUK3K1rEtTCHl1Q3XX2s5Mnws,10107
-primeqa/ir/dense/colbert_top/colbert/modeling/factory.py,sha256=LXUfhWasdOY0FKoFL-mWq5Lrj0S8MbgmWPvr37ERyp4,4865
-primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert.py,sha256=_js7XQiSCi2tXTH0mhWL8JoTMYUdSFVyJxOngMEUfTM,3195
-primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert_xlmr.py,sha256=PFIwYUJ3LRiXQENHCZH1jyqFL4iGFRwmUiwSQIoo0Xs,3529
+primeqa/ir/dense/colbert_top/colbert/modeling/base_colbert.py,sha256=_EQX47N0q6-LCYpobD7vAKd-4CbTnTvVN4mv2JisvpE,2211
+primeqa/ir/dense/colbert_top/colbert/modeling/checkpoint.py,sha256=hDHmAOZYyn2csgq2368Fh5xPwNGp7A8PcxoBZ1eYXII,6330
+primeqa/ir/dense/colbert_top/colbert/modeling/colbert.py,sha256=t-p1QyjBDNfLYezJaXGoE17juR341mjj4S7QXDueqpQ,9898
+primeqa/ir/dense/colbert_top/colbert/modeling/factory.py,sha256=QIdwCiv_kZh8f9AIT_bbg9aB4fnoR_ywKbGstF-dwyE,4758
+primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert.py,sha256=TIZ61pDmnR8Vvl4hQWd-6FRk3iV3r_12pEY1TcMYl-U,3260
+primeqa/ir/dense/colbert_top/colbert/modeling/hf_colbert_xlmr.py,sha256=dY1qlCSW5fgEZg2QDdhi4kYKcf-2rxw6mXUuWyD1gDI,3648
+primeqa/ir/dense/colbert_top/colbert/modeling/segmented_maxsim.cpp,sha256=knHlzOOWsXEBrzW18WB1CUfM5L59edUkEf_ilLyrQ_w,2782
 primeqa/ir/dense/colbert_top/colbert/modeling/reranker/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/modeling/reranker/electra.py,sha256=CTsVUWOcV8Sl1TmSrIF0hcL0tu3Gv_AtwMEhJgm0LLc,1306
-primeqa/ir/dense/colbert_top/colbert/modeling/reranker/tokenizer.py,sha256=9XYjtzGu8y2_awNh2mv4__2j3k_E0L585xx02unlxvw,623
-primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/__init__.py,sha256=Z6Q9o6b77tK3l3xmK4n-6kcIhzqNRums--syRE5a4to,280
-primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization.py,sha256=rW8IfEhs-KxIdQu21YQXm6aiuywBHMF_3osbDVmHnJw,3207
-primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization_xlmr.py,sha256=TY2uj-go3rwt8wJDcfaKgMO9ROJd4800Pk5hzZpzdck,3203
-primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization.py,sha256=gMBAvkkhAZz_xHrHmnv3IOHBK6gXiHdZy-m6y3SOjO8,4466
-primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization_xlmr.py,sha256=OGpPovl4nSOSpP6Bn8wc6jpklRQ0lMR_4tS_6fcvetg,5064
-primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/utils.py,sha256=IrbaB36vW5emHhWRY03EN-1eWLmDkq3PGndVv5xIj6s,2117
+primeqa/ir/dense/colbert_top/colbert/modeling/reranker/electra.py,sha256=TnF37gE6nnbf3gQZtYckUzV6wiHnOVHyUKeITCt4BQM,1272
+primeqa/ir/dense/colbert_top/colbert/modeling/reranker/tokenizer.py,sha256=Ybg2x0sAtO-TQCN4XMMnFUCaBwuzzPftVMJrlv0atBk,608
+primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/__init__.py,sha256=KO5KW3PTJmvjkCF_f61T4Xnln5ye9Ctlw1xVUJZLuLk,277
+primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization.py,sha256=EFiA7rrYQONUJH-F5eUxeeAaWw7vK6mjWI4mLcrrqLA,3126
+primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/doc_tokenization_xlmr.py,sha256=sv3XJ2mpMzMFWT-oe1V1pbHtySFZpNIbPHJ3sqxIOqM,3124
+primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization.py,sha256=_iojOh5lagpr5qZyUma8Xxu7Taklt5eGK-z60m5K0wE,4365
+primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/query_tokenization_xlmr.py,sha256=_zvXL7VYuLWG_cx_DRqOnxHZEB12y8W1g6OJ-fR8MHs,4952
+primeqa/ir/dense/colbert_top/colbert/modeling/tokenization/utils.py,sha256=X69NzeHyzfEAAAI4h7__yHx2hjvyck93v6JMvS8Tkxk,2054
 primeqa/ir/dense/colbert_top/colbert/ranking/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/ir/dense/colbert_top/colbert/search/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/search/candidate_generation.py,sha256=p1JtarQxH1h2yuo0GZJvqrd2hgjVD8ZLSgXkbq300IM,2075
-primeqa/ir/dense/colbert_top/colbert/search/index_loader.py,sha256=yxZKvFNQAi7E06YID-_9rzK8VBs6_Wqc7m_m9Ep0_LQ,2869
-primeqa/ir/dense/colbert_top/colbert/search/index_storage.py,sha256=AL8EMvBqrCn3ND3PltOB74bSIvwh612R89K5z4AqwaM,8520
-primeqa/ir/dense/colbert_top/colbert/search/strided_tensor.py,sha256=kPvQxXQ0b0-206YYw-xFoPozOylrDcY1e2IdudzJ7Kk,7419
-primeqa/ir/dense/colbert_top/colbert/search/strided_tensor_core.py,sha256=RsyRz-InsRAu3Thgu2Fd6IrhGfLXVbF65DCrzWYirNI,4109
+primeqa/ir/dense/colbert_top/colbert/search/candidate_generation.py,sha256=8dUj5VRGfr0Pnv3xG44o6D7djoLtBeG-qeGE5ja-tYc,2012
+primeqa/ir/dense/colbert_top/colbert/search/decompress_residuals.cpp,sha256=tHkbsc8kRYujDgDPBmwajIYXVd5DDI9ZJpVb3ypJ1IU,5844
+primeqa/ir/dense/colbert_top/colbert/search/filter_pids.cpp,sha256=w4e1x5wn64AcY7xIrvAdZvPLAgFL6_fTycPSil84TcI,5678
+primeqa/ir/dense/colbert_top/colbert/search/index_loader.py,sha256=O92Anh7Kx8HcEzR3VvO6p4kUu0BvhCCBIf99XNo8BTs,2791
+primeqa/ir/dense/colbert_top/colbert/search/index_storage.py,sha256=E9Rmhr7kO2-gzyCy_olngW2Fr1Qecy-9oa_Z8Kx22Ks,8332
+primeqa/ir/dense/colbert_top/colbert/search/segmented_lookup.cpp,sha256=5hG22fF71s4vubjImXBiZkThS3dV1iUiNC-QvBW5awY,4455
+primeqa/ir/dense/colbert_top/colbert/search/strided_tensor.py,sha256=FHaEYh8AJCtoa_enngnDf1ePzrwlgUk7uYQns9fcTj8,7197
+primeqa/ir/dense/colbert_top/colbert/search/strided_tensor_core.py,sha256=ASfBoe3Pf5DvyoWC14I5iAI3q8Idhcaojf2Ya6zqLgU,3992
 primeqa/ir/dense/colbert_top/colbert/training/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/training/eager_batcher_v2.py,sha256=dMFbf-Gyv5HITtUFOXwLICYcSVCHNXuPrUhjB6kxBPg,4511
-primeqa/ir/dense/colbert_top/colbert/training/lazy_batcher.py,sha256=rXSozxjmA8FpY53jiUrL4KqkJFFrj0Kky8HYoYWqbPU,3386
-primeqa/ir/dense/colbert_top/colbert/training/rerank_batcher.py,sha256=_NaN2GQcmz1QTUdXK_3caiGMtzjxmwnE3bRogwTXUpo,2859
-primeqa/ir/dense/colbert_top/colbert/training/training.py,sha256=3aW3q5_h5N4_PXP8zcp0Kr-DwBeB5838tfl1DW64J44,25533
-primeqa/ir/dense/colbert_top/colbert/training/utils.py,sha256=rnJHJ2EWFpA3MOhaSxS5urg9uZ8QHYa3t0DiYmFZcWk,5941
+primeqa/ir/dense/colbert_top/colbert/training/eager_batcher_v2.py,sha256=LAa21HTDdc4dimoi5H24C_yTNEAq4d1WYtZrI06OI3I,4411
+primeqa/ir/dense/colbert_top/colbert/training/lazy_batcher.py,sha256=iLuZYOCYE6sgTAApqFlLKW2A1DGCMC9awGBmwy7FTGY,3297
+primeqa/ir/dense/colbert_top/colbert/training/rerank_batcher.py,sha256=JEEA5S3vvNUk9hNSP3N5kvwJTOMPeGsEHJ5a3rBywZA,2784
+primeqa/ir/dense/colbert_top/colbert/training/training.py,sha256=qw48axO85DYQjHa_15Or1TvUmQXs1RDGdWNnALgxXSw,25040
+primeqa/ir/dense/colbert_top/colbert/training/utils.py,sha256=SD_CPtuftFgCkXzfSyh_nSZsXXdCRUBZBT1qEnruCIg,5811
 primeqa/ir/dense/colbert_top/colbert/utilities/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/utilities/minicorpus.py,sha256=gFvlipKDqMhF-f3q7tMZ-X9-zNiXUpWAPD645LV6V0U,2660
+primeqa/ir/dense/colbert_top/colbert/utilities/minicorpus.py,sha256=p0NQOTnuF-yehrKfQSKDF26J9WRJxPgXsL_KBLd19Qo,2594
 primeqa/ir/dense/colbert_top/colbert/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/colbert/utils/amp.py,sha256=ytuczRATcoR23cGLB0zotNysuFKilsYCfD6xQTdieUw,1124
-primeqa/ir/dense/colbert_top/colbert/utils/distributed.py,sha256=Hw3UFkFvRTt_3aqQuB1-Hxk9Pbok4vI0c4ZQ-ndN8Ik,1087
-primeqa/ir/dense/colbert_top/colbert/utils/logging.py,sha256=uI_Ot-Kh9MxYqAwQwJXIfYDxdoikB6Q1Wllce8eYo_w,3444
-primeqa/ir/dense/colbert_top/colbert/utils/parser.py,sha256=EMq1vuntCtUHQQEYIsOhqoPH32jG44lhDNR2iWQLUWA,9405
-primeqa/ir/dense/colbert_top/colbert/utils/runs.py,sha256=LMRwCdTsZySxJr0Kt2AhEql7Y924-bMP82g4gczbJqw,3694
-primeqa/ir/dense/colbert_top/colbert/utils/signals.py,sha256=c_ZrMBXM1fO5dJKWZijHE-cTOJoKCK6AAIyxVw0kyUg,929
-primeqa/ir/dense/colbert_top/colbert/utils/utils.py,sha256=PJQkRbFWFUlJrfgT8XStNdzMe6w_uLBd4XW6G-mu2Ys,9481
+primeqa/ir/dense/colbert_top/colbert/utils/amp.py,sha256=Os3-DB319-Y3gUndrtjmdWPgD8x0QdhNnQItdCKnWlc,1087
+primeqa/ir/dense/colbert_top/colbert/utils/distributed.py,sha256=L0BJRvKP3v67VhrC0oG_vUfueW6eBwWVvVsVLuLfpB0,1050
+primeqa/ir/dense/colbert_top/colbert/utils/logging.py,sha256=flV4ZJQvEJd3w1RG6prHs37o4i-MUJq6IwrvF0V36qc,3344
+primeqa/ir/dense/colbert_top/colbert/utils/parser.py,sha256=raeJvoalgI1pl-YtpeYeRuOMSiZEDgNfr8oOo26UFIk,9514
+primeqa/ir/dense/colbert_top/colbert/utils/runs.py,sha256=aK83i_3sxoeHLCl5o1P9RANiL3Z7LKSIodtvoOjRACU,3589
+primeqa/ir/dense/colbert_top/colbert/utils/signals.py,sha256=qa6p56vED_gfFZuMAPKDDYzQWGwYHxzWQZjZWD_uo40,886
+primeqa/ir/dense/colbert_top/colbert/utils/utils.py,sha256=6tKvrTFpBaWwTZwLXL9-F9Jzm1N47_riAuCIXL5e0QA,9138
 primeqa/ir/dense/colbert_top/utility/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/ir/dense/colbert_top/utility/evaluate/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/ir/dense/colbert_top/utility/evaluate/annotate_EM.py,sha256=9s6O9Cl5Oq3XGVMJSk4cdyctw9eswT57yXEg_OQ9NCo,3122
+primeqa/ir/dense/colbert_top/utility/evaluate/annotate_EM_helpers.py,sha256=b_f0QZJYkVDg2BqD9wF6RP7ttoz31RWXqlHOSRp8cSw,2635
 primeqa/ir/dense/colbert_top/utility/preprocess/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/utility/preprocess/docs2passages.py,sha256=8fo7_RsaySi0sNjjHDjpIKjdgYt5RVxSsdK6bkwsAxk,5171
+primeqa/ir/dense/colbert_top/utility/preprocess/docs2passages.py,sha256=6sWnr0tPOFwzgfPXtyHAarYIx9JmNyH2S6lumrfLUPg,5017
 primeqa/ir/dense/colbert_top/utility/rankings/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/utility/rankings/dev_subsample.py,sha256=9I7XPxZo4uoQOrFbDySqrEIN3WpcYq3q3XFEbKxPuvY,1455
-primeqa/ir/dense/colbert_top/utility/rankings/merge.py,sha256=zUFhZZ4wj0KuHRqmzY9wXSgAMugsbzVweYUI99RLPRk,1726
-primeqa/ir/dense/colbert_top/utility/rankings/split_by_offset.py,sha256=WhSAOzlbKXoW8OYDE17hC-nja0P58k3fzYoiutimhw0,1378
-primeqa/ir/dense/colbert_top/utility/rankings/split_by_queries.py,sha256=TL5WizicHQD--Zs9x-OGbBZzFses7rBrbAt0lqfLe-c,1832
-primeqa/ir/dense/colbert_top/utility/rankings/tune.py,sha256=7jRLNYrJrLq478L1leCGD9vhMKtG5nw5nGLAwsJkhIM,1865
+primeqa/ir/dense/colbert_top/utility/rankings/dev_subsample.py,sha256=IzXU2KD72S2psdPJApCB62i1kP95Ni2R8es-S7fBU0Q,1408
+primeqa/ir/dense/colbert_top/utility/rankings/merge.py,sha256=2UFSm_7GfBA1L13OoNnyb1B5avOmmgxdcXyCcHStfJE,1669
+primeqa/ir/dense/colbert_top/utility/rankings/split_by_offset.py,sha256=a7XI59DfIOVaijB4P8i1VYaB8-eeLRwBD1jkfLJVrJk,1334
+primeqa/ir/dense/colbert_top/utility/rankings/split_by_queries.py,sha256=V_WrW6J-46sflivvMCb6XHyhqEgul5PlSWOQQ1fWtO4,1765
+primeqa/ir/dense/colbert_top/utility/rankings/tune.py,sha256=18MXgE7_zb0Qer0cSiPvFLOSHKILaJDPwedS5L_eamo,1799
 primeqa/ir/dense/colbert_top/utility/supervision/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/ir/dense/colbert_top/utility/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/dense/colbert_top/utility/utils/qa_loaders.py,sha256=NGJOUqW7C0XW_jWSRaXuHwaGLdy662vBfCjWPHKu4LU,788
-primeqa/ir/dense/colbert_top/utility/utils/save_metadata.py,sha256=rpaE1LV3tMQiLk_as8V4Yzba85cDsYAVlI05QrqLjus,2196
+primeqa/ir/dense/colbert_top/utility/utils/qa_loaders.py,sha256=gsmBOWkdsqJ0IGMMSeOl6n7Hbj5WTV8blzm44EsNeDw,755
+primeqa/ir/dense/colbert_top/utility/utils/save_metadata.py,sha256=VIGdhKf3KWkItnJ2naHx2EqCGMFCZxgi_qMmLth8C4w,2129
 primeqa/ir/sparse/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/ir/sparse/bm25_engine.py,sha256=FFjv_mb8NoXr_KQrXtbVvEXv1gD0Wz3B-vlRnlujwXI,1957
-primeqa/ir/sparse/config.py,sha256=FCW3Ki5cj1zU_V61JvTxhC39M_c3LbwMowxTqIg4JF8,1649
-primeqa/ir/sparse/indexer.py,sha256=s9HeCsVD0VWHPcF-QADjnHtTdSPq28QXbnpsnxbBcvE,4052
-primeqa/ir/sparse/retriever.py,sha256=SsaqpWOQjxppnF00E2_TB4q2P3PVbXea-e712gUFZfU,4659
-primeqa/ir/sparse/utils.py,sha256=yCRQeEl2Nskui_FovWTNlCiY2qKjeIHLhbfynuWnobk,2504
+primeqa/ir/sparse/bm25_engine.py,sha256=bymar7XtJ9p1nXUoTqXvzDuc2ooqiG5HAV6xZnl89v0,1946
+primeqa/ir/sparse/config.py,sha256=hRYRF-EbZ6fR-hkOxTZyCZz6-e9vTMkzG9XYZ4_eVok,1609
+primeqa/ir/sparse/indexer.py,sha256=ekDaa0SSClkOVIsUqTn-CJRqqoF26CEjaLqXHRQG4Gk,3958
+primeqa/ir/sparse/retriever.py,sha256=zcH--DC6g3pPkJ7QwOgnUVUPXHCi_fP1TwNwILvSPYM,4522
+primeqa/ir/sparse/utils.py,sha256=YPURy9i7w_9_qAG_REMyh3lSWitaFsdAQ99bWWbPGmc,2444
 primeqa/ir/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/ir/util/corpus_reader.py,sha256=h5tl8mAuWwUQR06FRhgRVkGux1LRCcRV43rgHUrT9cc,2472
+primeqa/logger/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/logger/logging_config.ini,sha256=QfcsZWZcLW7xn24NPPFOral9gvyJm1Rg2F77sHo45b0,487
+primeqa/logger/verbose_logging_config.ini,sha256=RhnB9uezzMIsxWz53niVW-FhZVepusx9xKzDuH9niMg,489
 primeqa/mrc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/run_mrc.py,sha256=ZzgrOuAbtQislmpR27Vs5jakD5EvaFuqdzxpSjFMx24,24418
+primeqa/mrc/run_mrc.py,sha256=Rf7luM6YnXS2pCHdyRqZ8mEUnNfz2wYL6GBTBzH9C4c,25752
 primeqa/mrc/data_models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/data_models/eval_prediction_with_processing.py,sha256=ws0yhTQfodehJiFbxUh6Q3U0AOiCOFzik2TvJJSsX00,578
-primeqa/mrc/data_models/subsample_type.py,sha256=kxE1lEqTow7GzBtV74-CEz7Y5_TpYGHYw36vY_Z1zcY,302
-primeqa/mrc/data_models/target_type.py,sha256=8G3VfScCNCYRg3VQ0YhCcihQkaW7NaGy2tQ1_3oSb7M,677
+primeqa/mrc/data_models/eval_prediction_with_processing.py,sha256=cRGPGl_LpOo0-GqDMHgwVJA4B2mrEqvlTm694QI-3yE,561
+primeqa/mrc/data_models/subsample_type.py,sha256=Li_Y7DOFbVOoBKfdb5SDFO6r31RmzRBpNrVqohQ3980,290
+primeqa/mrc/data_models/target_type.py,sha256=FYN3lmkmQgUS6mpnXr0m4n-UqeEgKibnupBnOvSDdVI,648
 primeqa/mrc/data_models/model_outputs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/data_models/model_outputs/extractive.py,sha256=uUJ-SAzrmGrTE4sc28KQvkd95qK-ysbHvBmMnb4grOQ,1412
+primeqa/mrc/data_models/model_outputs/extractive.py,sha256=BFU7dimhhNPy51KEqf8479mOfDqI3vOEaEYBP7pGxas,1375
 primeqa/mrc/metrics/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/mrc/metrics/mlqa/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/mrc/metrics/mlqa/mlqa.py,sha256=vpfMiYlx_2k-QiaGgE9MKg3BSpLNu0qeltkESQa3-K4,4164
+primeqa/mrc/metrics/mlqa/mlqa_evaluation_v1.py,sha256=Vc21IIrIRWv4vNYtRfaAGCATA82_ZZ_v2NtyAkHFn0w,5558
 primeqa/mrc/metrics/nq_f1/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/metrics/nq_f1/eval_utils.py,sha256=U5g0w2tu8E5G6g3a1z16nk1UvXk44ymWnMFAf2zt27s,16436
-primeqa/mrc/metrics/nq_f1/nq_eval.py,sha256=Is-1bBXrNoIaJf7D2LbRWZeqIsm52PMLbjFHxNEp_AA,18258
-primeqa/mrc/metrics/nq_f1/nq_f1.py,sha256=VPkCrtU7cWiqmv7ieYlVCoIK8zXk-5xctDMXFAgUqJA,6470
+primeqa/mrc/metrics/nq_f1/eval_utils.py,sha256=B1wGPdfsLvkvEAOjQeaH8pQ-g76Mf8pCcIvUiGre7oM,16022
+primeqa/mrc/metrics/nq_f1/nq_eval.py,sha256=E2CqdpVjd4e1QxfdLDm192uQDiJV22j9Hr8s4RcYjks,17826
+primeqa/mrc/metrics/nq_f1/nq_f1.py,sha256=YALpXSnWfVx1kxLSzUemCN5VlN3vWykQK5DoNXcjJiE,6304
 primeqa/mrc/metrics/squad/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/metrics/squad/evaluate.py,sha256=vS4IPM_Z9O1LJ4KE-DO4jOqRL0S8cXYP-nmbfc-M3Ac,3399
-primeqa/mrc/metrics/squad/squad.py,sha256=oGJrb24u6xBvzwF4SOSz3kSLruKm9mLm8aHreduAQ_g,4814
+primeqa/mrc/metrics/squad/evaluate.py,sha256=IvlOgjdF0WDktBMOu6f2TkzlLXIm4xHWhLmyccM6KNc,3307
+primeqa/mrc/metrics/squad/squad.py,sha256=5nmj4DbaqJpuu-aeKJyuU0Wl30dV80o8JnJ09mxPI8M,4699
 primeqa/mrc/metrics/tydi_f1/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/metrics/tydi_f1/eval_utils.py,sha256=_QuPG-3kZjhXfAce1SLfF6ZQXaq7H6PkSu4Gl0Pzgfk,8405
-primeqa/mrc/metrics/tydi_f1/tydi_eval.py,sha256=Bh9askEMidyLw4Vr7kmlxyvALPeb40mZHvXYiUmjiSc,23329
-primeqa/mrc/metrics/tydi_f1/tydi_f1.py,sha256=0pGbUFeCtGbeMLyC85a2A2nqZMDBQLjPHB8gGTEXnSA,6501
+primeqa/mrc/metrics/tydi_f1/eval_utils.py,sha256=sV4H0VM2I--TXMoGj9Ts5w7TmTVhPR_iPiJc34wK9L0,8192
+primeqa/mrc/metrics/tydi_f1/tydi_eval.py,sha256=3CyayeyCViR-22UDHLH3Pub-38_QnfoUlGF56tQRucM,22775
+primeqa/mrc/metrics/tydi_f1/tydi_f1.py,sha256=3zLmR14aWWggpu2ktCLlTAST55ra2M1hQqpHd0gYkjg,6341
 primeqa/mrc/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/models/task_model.py,sha256=CMmdMyqB5M7-XLw451GxfH4OI7HuHCHcq6vytt74A-U,5459
+primeqa/mrc/models/task_model.py,sha256=3xMekXQ65HSaGKJpFVnbL1UdO5JB1Ntv68Ot1Apfb1I,5328
 primeqa/mrc/models/heads/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/models/heads/abstract.py,sha256=Z9431nIKQijbaFuW5CyfibNZOAb86hhFWfYCtK0mS9o,1102
-primeqa/mrc/models/heads/extractive.py,sha256=UMkqKDtDk701OoZ8ZNiv5k-uXzrBvyX8EghbTmkoQq8,14847
+primeqa/mrc/models/heads/abstract.py,sha256=OfvyLLdlRV3vwIT99YY0SW_7XtGFh8TwI0ibEK21tls,1066
+primeqa/mrc/models/heads/extractive.py,sha256=GczMngUe_XWNRPOV3V67iEQGl93mB7rsRQi40GqXi-8,14555
 primeqa/mrc/processors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 primeqa/mrc/processors/postprocessors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/processors/postprocessors/abstract.py,sha256=5INzuRi00X4I8FLIOaN0wKa7MOx2eFmJ9xiayB0tub8,1524
-primeqa/mrc/processors/postprocessors/extractive.py,sha256=mmCviT4dUGdnQ0KxFRkurvPmQv-XYbE2pojjPRcRnLk,15348
-primeqa/mrc/processors/postprocessors/natural_questions.py,sha256=RZlDmEhUdTZibkRT6EghcIBv6AZtAy1jk7AzrxhIVH8,4092
-primeqa/mrc/processors/postprocessors/scorers.py,sha256=zM1Von8FFqwz-2c5d9KsZqy5BARyoAhAsXcJaNuB5-U,3891
-primeqa/mrc/processors/postprocessors/squad.py,sha256=SPdj-FcxW3LaHzpSqaBkM1qylAHBrMXcAkcjqycOYro,2321
+primeqa/mrc/processors/postprocessors/abstract.py,sha256=RHlwIHk8ldQPb4CwUIOmVnf-lNdDyaF-O-AqyPEpaTY,1477
+primeqa/mrc/processors/postprocessors/extractive.py,sha256=Lamd18wX9HXcywqFaQpWvAajFrAnKyKEXKN2FHZuuEc,15065
+primeqa/mrc/processors/postprocessors/natural_questions.py,sha256=yj_JplRMhIKq6sUU-te2enwm-jro5SfPsTP9OPT8qzs,4010
+primeqa/mrc/processors/postprocessors/scorers.py,sha256=72Bq8ow9cTwBO-BEJ174c2e3Z21FWWHK1gnnuqqL4W8,3798
+primeqa/mrc/processors/postprocessors/squad.py,sha256=ivnr8CjPkJ-gOEAfsdGgDDKuChgTTO2z-xrS0DZ4eWQ,2266
 primeqa/mrc/processors/preprocessors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/processors/preprocessors/abstract.py,sha256=XIdnOYc1UN5TGqzYKgki5uwaq1rWJ_J4742T2uXfqB0,6760
-primeqa/mrc/processors/preprocessors/base.py,sha256=DctTnPlnSSiy5o2_yRosQbX1zA2bRve9IwY4EGQS46k,20677
-primeqa/mrc/processors/preprocessors/natural_questions.py,sha256=IiPEIYsaI0KUkOJdIU7zAwKQbjCcbANF2M-CREy0pzQ,11476
-primeqa/mrc/processors/preprocessors/squad.py,sha256=hTLI1xsvopuNgF2Bz6D5FAPn2hZGwLaceenuqmbyYr8,2287
-primeqa/mrc/processors/preprocessors/tydiqa.py,sha256=TmYQARASUZ9QJo8QY82rjFspD-LxvNUp5abIEX6dn3k,5041
-primeqa/mrc/processors/preprocessors/tydiqa_google.py,sha256=79k9JZ-PrrI9sGaG2dsoQ4SsJtqB7ZMTjrWY9NeR4XU,5849
+primeqa/mrc/processors/preprocessors/abstract.py,sha256=mKM66nOkZOFnrHZ81VbGo5_00l9bI2K2o8BKaSAwb5w,6596
+primeqa/mrc/processors/preprocessors/base.py,sha256=iF5o2Zrz1wQOmmYa9HSm7xbJJDYvF5pgBdSH9IKt83U,20245
+primeqa/mrc/processors/preprocessors/mrqa.py,sha256=R9hODgUQTqCvJlXfejDrQoAuqZ4LDpwtj-hIq6qRo6Q,2406
+primeqa/mrc/processors/preprocessors/natural_questions.py,sha256=SP0dWrQlMdtTvi0OIe9Ka1jAwoxqOY3uUB99WUiAUFo,11243
+primeqa/mrc/processors/preprocessors/squad.py,sha256=P0PT6u3gwvmEZ4xMHETB9xCegIgVI9A3hhcpdJA4980,2235
+primeqa/mrc/processors/preprocessors/tydiqa.py,sha256=MtlD7h3n37H5fw7aadZIiMlCpKcUD2YfSgKVX1DOZ8Y,4943
+primeqa/mrc/processors/preprocessors/tydiqa_google.py,sha256=Dpo066NDDwk5cBOZbeA7k2KV_FXYCWIW0PZ_yXrpTys,5737
 primeqa/mrc/trainers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/mrc/trainers/mrc.py,sha256=aG6PALk7BqcDWNpvpV9tYcaTh_HNneulOPFN70qpJ58,12694
+primeqa/mrc/trainers/mrc.py,sha256=FhJRI_ST3ew7HUYGseLNLLztTWJjJ1w2eU33K7hOrKg,12415
 primeqa/pipelines/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/pipelines/extractive_mrc_pipeline.py,sha256=govSWI92YNG49g888wMsvSowHarktnnAoLio9tiHDSo,2774
+primeqa/pipelines/base.py,sha256=waOwihQ7GvFyvvpFm7RbOYfZy-QYCfUiyf2-QX-ofo0,622
+primeqa/pipelines/qa_pipeline.py,sha256=1zeX_liuMLB3BZ8o7mTPBtTPs6yq70pBiZqJbXTX-rg,510
+primeqa/pipelines/components/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/pipelines/components/base.py,sha256=qe9MbJbRgWqGmV5w6wmuyyuklvDraLSdR-ZfTcfILj4,1371
+primeqa/pipelines/components/indexer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/pipelines/components/indexer/dense.py,sha256=m9vmBKrigRPm-vJ8e9ORT6dntWr3DLTQx9J685KaFAQ,3910
+primeqa/pipelines/components/indexer/sparse.py,sha256=iOZWkvaVW8hIz04lDOA2xcse6aNzvrfyHaHI_U1osLQ,317
+primeqa/pipelines/components/reader/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/pipelines/components/reader/extractive.py,sha256=Hcsd6spxkdEZ_3dA5bDsYCv3ww5XB_ITGZZpbv3Yd7o,7856
+primeqa/pipelines/components/reader/generative.py,sha256=SbtQ7e3wZg47oCMUXbjQYBmTsSK6v4MVRAhUmpCK_ps,533
+primeqa/pipelines/components/retriever/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/pipelines/components/retriever/dense.py,sha256=DXn_TXvivIp06qMSIohLhGMWCj_0t-ngFXGlI49-x40,3111
+primeqa/pipelines/components/retriever/sparse.py,sha256=a4NpDA5n8QZHshzQzbDLCDNwRn1_3BKXJpObV9MMLrc,1040
 primeqa/qg/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/run_qg.py,sha256=cpgp0RbH1pi9Qv80T0A1mw6F2ZK8SMl77h2mxTF4CLQ,8722
+primeqa/qg/run_qg.py,sha256=wdafs0SFSsqgLS_udehgYMk7RTBasoS6vrZsN3FfOCs,9047
 primeqa/qg/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/models/qg_model.py,sha256=1M0B0zaC7zLY-wb1q_TCprXhYGwzlsrJ0Q8RYqu4AcU,4053
+primeqa/qg/models/qg_model.py,sha256=pQdPzfYyCyhVWA-Gmhyji-SQzmGMQFI1B2M10UJV1x4,4237
 primeqa/qg/models/passage_qg/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/models/passage_qg/answer_sampler.py,sha256=kwUgRMIvv94-iJnoVtRCD4a5DoG5W_PW4l4D7vimDJ0,3970
+primeqa/qg/models/passage_qg/answer_sampler.py,sha256=91Bu17yhEQmlVu1xAdyxy9rwe6xzdgELq1VmUfdBtys,3896
 primeqa/qg/models/table_qg/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/models/table_qg/sql_sampler.py,sha256=3WEMKGiu8vqIVdEvHbzAzl3eV3f10gtFK02C5Gt3q-s,19227
+primeqa/qg/models/table_qg/sql_sampler.py,sha256=g8tyxrMcvKlgtFDtzjmXjXB5Y6eCX4OwkfWFV2EAnUY,18761
 primeqa/qg/processors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/processors/data_loader.py,sha256=8i1GKssXiEYcjHIJSYugK096Vw_IrGATI6UfN-6iD_Q,1865
+primeqa/qg/processors/data_loader.py,sha256=4MHdtf-H8ytgR4PzgjqBX5TxENdysRmMh56QnslK4-I,2111
 primeqa/qg/processors/passage_qg/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/processors/passage_qg/squad_processor.py,sha256=VwHkO5_fDbXL-wjRTQHr6d4OKNuBK_PJEQunAO5HTMk,638
-primeqa/qg/processors/passage_qg/tydiqa_processor.py,sha256=PM1pWwinoKECjFtVaXsGT4rBU6BA6O4Kq7j1qP6pr2M,598
+primeqa/qg/processors/passage_qg/qg_processor.py,sha256=jY9JxH8Jbnpbw0ht2TtbZMNCWgUXr1hYAS1OGJA0TX0,1825
 primeqa/qg/processors/table_qg/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/processors/table_qg/wikisql_processor.py,sha256=KnRSB3AwlAVaeJRQFJIMsbCPAwLbz_PtriRSUeGpjOg,4516
+primeqa/qg/processors/table_qg/sql_processor.py,sha256=hr9z3XwPzF4KD-CtC86dA4xgvoFV7xZsDNNYLBsR7yg,5472
 primeqa/qg/trainers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/trainers/qg_trainer.py,sha256=t6J-KmwhD07m0tPKlRHLPJdhMpq5cHj3t39h_VHAWNM,706
+primeqa/qg/trainers/qg_trainer.py,sha256=ZF_Xnq8IqbRaCsAhwc8ESywKH3LzcB4i9vcZXRVB5UA,323
 primeqa/qg/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/qg/utils/constants.py,sha256=VZG2cV9pupEg0wfhRBoRf-HWy1Rfrt4L41um93lJRrg,406
-primeqa/qg/utils/data_collator.py,sha256=Ph3tK2xobpXm2K_RYyFY1rOeUUl7VmR6q6EVBx1n4ZQ,977
-primeqa/re2g/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/re2g/re2g.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/qg/utils/constants.py,sha256=N42916iBoOuWk4y6AOeBzkwdzz4Ys1lEYQACa0TBTHU,390
+primeqa/qg/utils/data_collator.py,sha256=yEwTxlTAIWRvKlmFBLkpGxerZtp5K7xtebkd_7cKX14,952
+primeqa/services/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/services/application.py,sha256=KfMKGpnf7BMLTf_FwuOvLXbAxXEw3emlcnMYKWex5Nw,433
+primeqa/services/configurations.py,sha256=4KCrQKfJQNB2eFAeR0UzZIWWpIEbAoWabt82xh9RkS0,10041
+primeqa/services/constants.py,sha256=VnBNbkACIHwR-JOI5IPQIlSERRrKchSoLFegnnhdKUU,215
+primeqa/services/cred_helpers.py,sha256=wQxA4f1S4i06kRoWHYnqbV1mxOpCYwsM01Wmcw__7SU,2307
+primeqa/services/exceptions.py,sha256=XI8dAc3hh5_ZSzxpGg2jQbplQaKdHv_AihjWuINJaws,1193
+primeqa/services/factories.py,sha256=yIYskYef9DkY-2V5RaaMNX8rl4Q52fv9RWRWOoosH0I,9376
+primeqa/services/parameters.py,sha256=poQ7r5NMlH4tagj5KCc0PcjW2b3q-7Jj2I7pj_7poto,566
+primeqa/services/store.py,sha256=QYY3AP0dExa1bmSNhUU-314KTdXa6rdKOljOVm1nBAA,9412
+primeqa/services/utils.py,sha256=D6B3-Cmvm-3imaSitXae_negGKxVIx6QbGbY-JxSZiI,1167
+primeqa/services/config/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/services/config/config.ini,sha256=Xp5jiAhyeyFoEjik_E6kuy4K_E5QjN1AbsE46Xq3Q4U,605
+primeqa/services/grpc_server/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/services/grpc_server/indexer_service.py,sha256=JNt6Ho6j5VUJey1yHEDRBM-9VgDcaQVWYGjX06T7lkY,9577
+primeqa/services/grpc_server/reader_service.py,sha256=Cdrl1A98QW8nwoZ8MeP9BtpiEXzNzgmA7EXbLZt82As,7044
+primeqa/services/grpc_server/retriever_service.py,sha256=xHJmV_SwLFh4jqyzH5nXvgQlKajzYMgVWPCv51694Ik,7967
+primeqa/services/grpc_server/server.py,sha256=u5NCYVW_rUzRyg3KmM8nJUPrfBy_m8QvvdDmZ3elqP0,3334
+primeqa/services/grpc_server/utils.py,sha256=LvAk9V72T7zbBiOy3uaR49GJ1uVImB8nhB0MpLiWiAk,3824
+primeqa/services/grpc_server/grpc_generated/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/services/grpc_server/grpc_generated/indexer_pb2.py,sha256=EOgSXuvkxofRjtaYKZZmjQ0hTcr3tawS3Uvt-AaaDaA,7845
+primeqa/services/grpc_server/grpc_generated/indexer_pb2_grpc.py,sha256=mr6t43dCvaI-w47OZS-BEpnECkvDHqBKi2kRqJTKc1c,6996
+primeqa/services/grpc_server/grpc_generated/parameter_pb2.py,sha256=c3EDNE1bJuoaHzRQJUNM90_WpcuW2jotI0mSnL943v8,1580
+primeqa/services/grpc_server/grpc_generated/parameter_pb2_grpc.py,sha256=1oboBPFxaTEXt9Aw7EAj8gXHDCNMhZD2VXqocC9l_gk,159
+primeqa/services/grpc_server/grpc_generated/reader_pb2.py,sha256=d3B1izI0DEyyV9wSdSYcqhlSGYSk9hiC-d00KdQL2A8,5972
+primeqa/services/grpc_server/grpc_generated/reader_pb2_grpc.py,sha256=v0NUXm8tixUijhppJv06jzJWFoaGxjCEUdMOC3Homgc,3992
+primeqa/services/grpc_server/grpc_generated/retriever_pb2.py,sha256=ZHbpNOofSY-hjxSRIrmP7VuGsG_Ro90ZxZ-Vi6USjYE,4943
+primeqa/services/grpc_server/grpc_generated/retriever_pb2_grpc.py,sha256=fdoIUPzlMzwmdNhT_tMpJWkbr6Rsy81LzwNEBG_n130,3988
+primeqa/services/rest_server/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/services/rest_server/data_models.py,sha256=lksKotj9FS6zuoOCYYwsoptXAk5ADsxW0woB1ka_15E,4121
+primeqa/services/rest_server/server.py,sha256=AHvtcljCVluISiW2w4zH9c15ndIl8Vw_V9rVAlg3dMo,23908
+primeqa/services/rest_server/utils.py,sha256=R6vDSppZgpSSTyK5YdrZU4O2FGEUTamqF_ifGJyLXcA,1913
 primeqa/tableqa/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqa/run_tableqa.py,sha256=VpZoqcTcd-5L1yOZK5qBUxhOXjhTz_V6MpngKWIQhMU,4536
+primeqa/tableqa/run_tableqa.py,sha256=mYv6jgwPJkxKw54CsxOc2gemKJ6Ll3DClmk17zpxirQ,4436
 primeqa/tableqa/metrics/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqa/metrics/answer_accuracy.py,sha256=nvOV-xhv59u5Zryms0TiuQJJR1QRyLX89R4fnqe8QrI,675
+primeqa/tableqa/metrics/answer_accuracy.py,sha256=B0elaCCaerIcEeWHWgZcr7X7GdGcbBjpgIUqkyf3gp4,653
 primeqa/tableqa/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqa/models/tableqa_model.py,sha256=_uTXpltPCCAiOMvZ2J7f8t_agWtco1ywjasZaYY1yCA,2704
+primeqa/tableqa/models/tableqa_model.py,sha256=MbRXzNKiiFpDS8PmF4wvEV7n1_yAC5Z1U00Ahia0mB0,2618
 primeqa/tableqa/postprocessor/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqa/postprocessor/wikisql.py,sha256=GQS-DZW08ocwLv22s9O_afSA0_4w-7jbzPkop-Ke2v0,2405
+primeqa/tableqa/postprocessor/wikisql.py,sha256=SCIJ3wa_QXjcZQ2_1zyhlUUxfJFrNs9_NIsbFDOYY1c,2340
 primeqa/tableqa/preprocessors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqa/preprocessors/convert_to_sqa_format.py,sha256=vG--U2Pbxhg0VuD9KJFPrlDuHhZPFHsYFf1WVFEOZWY,15289
-primeqa/tableqa/preprocessors/dataset.py,sha256=aWP87aINqFIM30mddaeRKa266OmitQAwL4HpKNBHP9g,3511
-primeqa/tableqa/preprocessors/wikisql_preprocessor.py,sha256=GTdXUw-CCRWIip4lOqSbAWAZxixGn49ouiIEMW1-qpc,5003
+primeqa/tableqa/preprocessors/convert_to_sqa_format.py,sha256=EYFhFDVZpfKtQJhAPofBhsvOkjGAUusCiSLPFacf-jQ,14844
+primeqa/tableqa/preprocessors/dataset.py,sha256=q9m-1Pno2tvA4HnK5CTDA_EA-d2UqXB-tSXPQhj-ykw,3426
+primeqa/tableqa/preprocessors/wikisql_preprocessor.py,sha256=FMy6dQJdCqp3VNi8hAF0bDh-Pj00njn9W-w9uFvBZXc,4877
 primeqa/tableqa/trainers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqa/trainers/tableqa_trainer.py,sha256=JlbUz2uXg7pCbrjSkrkx8NpspnHMwL7an8YDUnxbxyA,2645
+primeqa/tableqa/trainers/tableqa_trainer.py,sha256=T5OfKxRFz32bWWvON7yndc3DkcjhobjyzODCynjlCps,2587
 primeqa/tableqa/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqa/utils/data_collator.py,sha256=J4HyarsFFd8VlUknX9Q2fWzYh4eo5A4GHjRvuTJEeIU,1172
-primeqa/tableqg/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/tableqg/tableqg.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+primeqa/tableqa/utils/data_collator.py,sha256=DGHybO4fHglg4O1jt5rQ9QOUHihIPe93D1raDANuVEc,1144
 primeqa/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/util/args_helper.py,sha256=dzBxn0cYeaSgHLiT30geQmUdDerj6ZlUWxaYeMUQiNs,3934
-primeqa/util/file_utils.py,sha256=Ee9mCzWcXj1dICWWDZHFbdfGdr2PyKrZkfh9Y-gEdak,8906
-primeqa/util/metrics.py,sha256=lO5W5un3yha_gqd_govuNXflyfWSDXQVBaJwUTJWKvM,766
-primeqa/util/reporting.py,sha256=xHhdZf6i0DRe_dfHmNAAqQp9S3gitynxREv-WY7zElY,6389
+primeqa/util/args_helper.py,sha256=pAuOj3VH8Xw16QKl7Kw2iRQjtk6NrbMXaztWJiMcx9E,3833
+primeqa/util/file_utils.py,sha256=lFyca0CRnS6MpNTtTiBzwWkrvdMCrYaVQy7UyKzQLhM,8652
+primeqa/util/metrics.py,sha256=qbujMOZ5Cm-e2VxYaJBmH08XrP2cASqv4aKygaZxYJo,736
+primeqa/util/reporting.py,sha256=Sls8KF5DzhcCfws66YgvwYUW6iJMFFMQ_pfXxw8Dz5I,6234
 primeqa/util/dataloader/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/util/dataloader/distloader_base.py,sha256=qehRxo7FIbrHIATG9ZILLpxHSWV2AXeKHpbboNX12Og,10324
-primeqa/util/dataloader/distloader_seq_pair.py,sha256=lTU2vmA6hKqU5llS6MDKEheU5oALSySXZFTFW9T857U,12778
-primeqa/util/dataloader/file_splitter.py,sha256=S12mx44gj3zgzfP5rh30o_xDzVo4A4_QRe-1SPpNx58,3310
+primeqa/util/dataloader/distloader_base.py,sha256=HTgYzZEowDIDY_jicurGNASN74-0fvHaopD5sG7u-R8,10083
+primeqa/util/dataloader/distloader_seq_pair.py,sha256=sopS1mn2TD_TwCVrbM72qNe_q5xzAUgQpZoaJLEHl6Q,12521
+primeqa/util/dataloader/file_splitter.py,sha256=YXJae-i5rxKsRbH4iQ9GdGpBSlbbdI3BVo7zL2_hNVE,3212
 primeqa/util/transformers_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-primeqa/util/transformers_utils/hypers_base.py,sha256=qXDGDbUsYtgBIYxV7SOlWC7RExH-h56oVySwEishbZs,9503
-primeqa/util/transformers_utils/model_utils.py,sha256=HFUEXV5YXtUKttwKCo_2ApAtWql01t7JFOmj5eq1m2o,5633
-primeqa/util/transformers_utils/optimizer_utils.py,sha256=WZVDUEfIZhfSd_bXCwpzSYY5gMB1rl2tm3JGnHQ9Jgw,7817
-primeqa/util/transformers_utils/torch_utils.py,sha256=BA6WtvEgo8uXtLq4p1zkmX0uRYJBmHg6drFvSqiRO-Y,1387
-primeqa-0.8.1.dist-info/LICENSE,sha256=4DukHX-rIHAHaf5BGLq1DYAMt0-ZA1OgXS9f_xwig2M,11558
-primeqa-0.8.1.dist-info/METADATA,sha256=TqOlsyuyRBI1Bl3K2DtV6ZMt3x938DRIbwMMYE3Mc98,12285
-primeqa-0.8.1.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
-primeqa-0.8.1.dist-info/top_level.txt,sha256=QKLKtik9yunUd33vH91OHiW2Xzr1BJn2W219IlzN-DY,8
-primeqa-0.8.1.dist-info/RECORD,,
+primeqa/util/transformers_utils/hypers_base.py,sha256=VD5owpkPJsXBsnyU55R5EPQjZlYC6ht4OPyJb0OK6nU,9276
+primeqa/util/transformers_utils/model_utils.py,sha256=a09Hkpll3aSXmSIv525q_e4CgDbSitsJynNLwgQSMCE,5516
+primeqa/util/transformers_utils/optimizer_utils.py,sha256=8uarkQ41pbx4tS94WH7AQ_mbxKfUgX1G8-0q4bTBLj8,7654
+primeqa/util/transformers_utils/torch_utils.py,sha256=nCaBHEP491mooXli9xa4X7yM3ra-AL3gVLOnlkFzOWA,1346
+primeqa-0.9.7.dist-info/LICENSE,sha256=tAkwu8-AdEyGxGoSvJ2gVmQdcicWw3j1ZZueVV74M-E,11357
+primeqa-0.9.7.dist-info/METADATA,sha256=5jqPt1hoTK8bnW1MlZ0ozGyNbgaPXrWnJY5vbIRNDTA,16706
+primeqa-0.9.7.dist-info/WHEEL,sha256=z9j0xAa_JmUKMpmz72K0ZGALSM_n-wQVmGbleXx2VHg,110
+primeqa-0.9.7.dist-info/top_level.txt,sha256=QKLKtik9yunUd33vH91OHiW2Xzr1BJn2W219IlzN-DY,8
+primeqa-0.9.7.dist-info/RECORD,,
```

